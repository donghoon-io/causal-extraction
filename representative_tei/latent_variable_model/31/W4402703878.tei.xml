<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Unveiling the Statistical Foundations of Chain-of-Thought Prompting Methods</title>
				<funder ref="#_M3GhC9T">
					<orgName type="full">National Science Foundation</orgName>
					<orgName type="abbreviated">NSF</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2024-08-28">28 Aug 2024</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Xinyang</forename><surname>Hu</surname></persName>
							<email>xinyang.hu@yale.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Yale University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Fengzhuo</forename><surname>Zhang</surname></persName>
							<email>fzzhang@u.nus.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Siyu</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Yale University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhuoran</forename><surname>Yang</surname></persName>
							<email>zhuoran.yang@yale.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Yale University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Unveiling the Statistical Foundations of Chain-of-Thought Prompting Methods</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2024-08-28">28 Aug 2024</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2408.14511v2[cs.AI]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-28T23:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Chain-of-Thought (CoT) prompting and its variants have gained popularity as effective methods for solving multi-step reasoning problems using pretrained large language models (LLMs). In this work, we analyze CoT prompting from a statistical estimation perspective, providing a comprehensive characterization of its sample complexity. To this end, we introduce a multi-step latent variable model that encapsulates the reasoning process, where the latent variable encodes the task information. Under this framework, we demonstrate that when the pretraining dataset is sufficiently large, the estimator formed by CoT prompting is equivalent to a Bayesian estimator. This estimator effectively solves the multi-step reasoning problem by aggregating a posterior distribution inferred from the demonstration examples in the prompt. Moreover, we prove that the statistical error of the CoT estimator can be decomposed into two main components: (i) a prompting error, which arises from inferring the true task using CoT prompts, and (ii) the statistical error of the pretrained LLM. We establish that, under appropriate assumptions, the prompting error decays exponentially to zero as the number of demonstrations increases. Additionally, we explicitly characterize the approximation and generalization errors of the pretrained LLM. Notably, we construct a transformer model that approximates the target distribution of the multi-step reasoning problem with an error that decreases exponentially in the number of transformer blocks. Our analysis extends to other variants of CoT, including Self-Consistent CoT, Tree-of-Thought, and Selection-Inference, offering a broad perspective on the efficacy of these methods. We also provide numerical experiments to validate the theoretical findings.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Autoregressive Large Language Models (LLMs) have tremendously revolutionized the field of Natural Language Processing (NLP) and related areas due to their striking ability to understand languages and follow instructions. These models, based on the transformer architecture <ref type="bibr" target="#b56">(Vaswani et al., 2017)</ref>, are probabilistic models that predict the next token based on preceding tokens, also known as a prompt. The training of LLMs typically involves two phases: pretraining and post-training. During the pretraining phase, the LLMs are trained on vast text corpora via unsupervised learning <ref type="bibr" target="#b1">(Ahmad et al., 2021;</ref><ref type="bibr" target="#b78">Zoph et al., 2020;</ref><ref type="bibr">Erhan et al., 2010;</ref><ref type="bibr" target="#b18">Hendrycks et al., 2019)</ref>. This process allows them to acquire a broad understanding of language and general knowledge. Subsequently, additional posttraining approaches, including supervised fine-tuning <ref type="bibr" target="#b63">(Wei et al., 2021)</ref> and reinforcement learning with human feedback (RLHF) <ref type="bibr" target="#b39">(Ouyang et al., 2022)</ref>, are adopted to enhance the chat capabilities of LLMs. Finally, the trained LLMs are deployed to interact with human users, with their neural network parameters remaining fixed.</p><p>Human users interact with LLMs through prompting, which refers to text generation conditioned on the prompts provided by the users. Designing effective prompts to induce specific desired behaviors in LLMs is known as prompt engineering <ref type="bibr" target="#b46">(Sahoo et al., 2024)</ref>, which is largely a heuristic enterprise. Prompt engineering represents a paradigm shift from standard statistical learning. Specifically, when using LLMs to solve a task via prompting, the LLMs essentially "learn" from the prompts by passing them through the neural network with fixed parameters, which have been trained without data from the desired task.</p><p>One of the most widely used prompting heuristics is In-Context Learning (ICL) <ref type="bibr">(Brown et al., 2020;</ref><ref type="bibr">Dong et al., 2022)</ref>, a technique that enables LLMs to comprehend concepts by including a few examples in the prompt. This involves feeding the LLM with a few inputoutput examples and then asking for the output corresponding to a new input. On many tasks, the LLM can successfully extract the relationship between inputs and outputs and generalize it to the new input to get the desired output. This simple and intuitive prompting method has recently drawn considerable research interest and has become the foundation for many sophisticated prompting methods designed for more complicated tasks <ref type="bibr" target="#b64">(Wei et al., 2022;</ref><ref type="bibr" target="#b77">Zhou et al., 2022;</ref><ref type="bibr" target="#b24">Kim et al., 2022;</ref><ref type="bibr" target="#b75">Zhang et al., 2022;</ref><ref type="bibr" target="#b45">Rubin et al., 2021;</ref><ref type="bibr" target="#b49">Sorensen et al., 2022;</ref><ref type="bibr">Creswell et al., 2022;</ref><ref type="bibr" target="#b70">Yao et al., 2023;</ref><ref type="bibr" target="#b60">Wang et al., 2022)</ref>.</p><p>A prominent example of ICL is Chain-of-Thought (CoT) prompting <ref type="bibr" target="#b64">(Wei et al., 2022)</ref>, which generalizes ICL for multi-step reasoning tasks. Specifically, the vanilla version of fewshot CoT proposes including intermediate reasoning steps in addition to input and output in the demonstration examples, helping LLMs understand the reasoning path from input to output. Building upon vanilla CoT, other sophisticated variants of CoT have been proposed to efficiently select reasoning paths via majority votes or tree search <ref type="bibr">(Creswell et al., 2022;</ref><ref type="bibr" target="#b70">Yao et al., 2023;</ref><ref type="bibr" target="#b60">Wang et al., 2022)</ref>.</p><p>While CoT prompting methods have found great empirical success in multi-step reasoning problems such as arithmetic, commonsense, and symbolic reasoning, there is still a lack of theoretical understanding of why CoT works and how it compares with vanilla ICL. In this work, we aim to rigorously understand why the practice of "pretrained LLM + CoT prompting" is capable of solving multi-step reasoning problems. Additionally, we aim to demystify how the transformer architecture of the LLMs and the intermediate reasoning steps in the prompts contribute to this success. Specifically, we aim to answer the following four questions: To answer Question (a), we introduce a multi-step latent variable model that captures the data-generating process involved in multi-step reasoning. Under this model, a sequence of H + 1 variables {Z 0 , . . . , Z H } is generated according to a distribution conditioned on a latent variable θ * , where H stands for the number of reasoning steps. Here, Z 0 and Z H are the input and output, respectively, and {Z h } H-1 h=1 are the intermediate steps. The parameter θ * captures the underlying statistical task and is a random variable taking values in a set Θ. Under this model, in CoT prompting, the LLM is given n examples of such sequences sampled conditioned on θ * and asked to generate the output Z H corresponding to a queried input Z 0 = z test 0 . We further assume the LLM is pretrained by predicting the next reasoning step, with the training data generated from this model with θ * sampled from the prior distribution. Under this setup, we answer Question (a) by proving that CoT prompting methods based on the pretrained LLM produce Bayesian Model Averaging (BMA) estimators <ref type="bibr" target="#b19">(Hoeting et al., 1999)</ref>. Specifically, based on the examples in the prompt, the Large Language Models (LLM) implicitly learns a posterior distribution over the latent variable θ * and then generates the output Z H by aggregating over this posterior distribution.</p><p>Furthermore, the main effort of this paper is devoted to answering Question (b). To analyze the statistical error of the CoT estimator, we first decompose the statistical error err CoT into the sum of (i) pretraining error and (ii) prompting error. In particular, the pretraining error characterizes the statistical error arising from training the LLM to predict the next reasoning step on finite data. This error is further upper-bounded by the sum of approximation error and generalization error. To control the approximation error, we construct a class of transformers that directly approximate the population distribution while capturing the multi-step problem structure. This result is combined with a Pac-Bayes generalization error bound <ref type="bibr" target="#b34">(McAllester, 1998;</ref><ref type="bibr" target="#b4">Alquier, 2021)</ref> to establish the pretraining error bound. Moreover, the prompting error reflects the statistical error of the BMA estimator based on finite examples. We upper-bound this statistical error for variants of CoT methods in terms of both the last-iterate and average-iterate errors. By combining the analyses of pretraining and prompting errors, we provide a complete answer to Question (b).</p><p>Furthermore, to answer Question (c), we show that the attention mechanism enables the LLM to approximately encode BMA within the transformer architecture. In other words, prompting a pretrained LLM induces an output distribution that closely approximates the BMA estimator. Additionally, the transformer architecture also plays a role in the analysis of the approximation error, which is a byproduct of the analysis of pretraining error.</p><p>Finally, to answer Question (d), we specialize the theoretical result to the case where H = 1, which reduces to vanilla ICL. Our theory shows that CoT is always no worse than vanilla ICL in expectation among all tasks, but this dominance does not hold point-wisely for arbitrary tasks or prompts. Intuitively, with only a finite number of examples, it is possible that CoT jeopardizes performance when the intermediate reasoning steps included in the prompt are not sufficiently informative. We empirically validate this argument on a synthetic task, showing that whether CoT prompting is beneficial is nuanced.</p><p>In conclusion, as an initial attempt to bridge the gap between theory and practice, this paper establishes a comprehensive understanding of CoT and its variant prompting methods with theoretical guarantees. We hope this work will pave the way for further exploration into the theoretical underpinnings of prompt engineering.</p><p>Notation. Let [T ] denote {1, • • • , T }. We use V i to denote the i-th coordinate of the vector V . We adopt {P θ | θ ∈ Θ} to denote a parametric family of distributions parameterized by θ ∈ Θ. The <ref type="bibr">Kullback-Leibler (KL)</ref> divergence between two distributions P and Q is denoted by KL(P, Q) = E P(x) log p(x) q(x) ,</p><p>where p and q are the densities of P and Q with respect to a reference distribution. Furthermore, we define the conditional KL divergence as</p><formula xml:id="formula_0">KL P(Y = • | X = x), Q(Y = • | X = x) = E y∼P(• | x) log p(y | x) q(y | x) ,</formula><p>which is a function of x.</p><p>Let P and Q denote two probability measures defined over a measurable space (Ω, F). The total variation (TV) distance between P and Q is</p><formula xml:id="formula_1">TV(P, Q) = sup A∈F |P(A) -Q(A)|,</formula><p>which is also half of the ℓ 1 distance between density function p(x) and q(x).</p><p>The Hellinger distance between two distributions P and Q is</p><formula xml:id="formula_2">H(P, Q) = 1 √ 2 p(x) -q(x) 2 dx 1/2 = 1 - p(x)q(x)dx 1/2</formula><p>.</p><p>In addition, we use softmax(•) to denote the softmax function, which maps a vector to a probability distribution. In particular, for any vector x ∈ R n and any i ∈ [n], the i-th entry of softmax(x) is given by [softmax(x)] i = exp(x i )/ n j=1 exp(x j ). For a matrix X ∈ R d 1 ×d 2 , we denote the i-th row and column using X i: and X :,i , respectively. The ℓ p,q norm of X is defined as ∥X∥ p,q = ( d 2 i=1 ∥X :,i ∥ q p ) 1/q . Furthermore, we use ∥X∥ F = ∥X∥ 2,2 to denote Frobenius norm. For a set L, let P(L) denote the family of probability distributions over L. We use z 1:h to denote the vector [z 1 , • • • , z h ]. We adopt L * to denote the set of all the sequences where each component is in the set L.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works</head><p>Our work adds to the literature on theoretically understanding prompting methods. In particular, our work is closely related to CoT prompting and its variants. In addition, our work is related to the body of works that aim to understand the ability of ICL and CoT from both empirical and theoretical perspectives.</p><p>CoT Prompting and its Variants. The vanilla CoT prompting method is proposed in <ref type="bibr" target="#b64">Wei et al. (2022)</ref> for solving multi-step reasoning problems using LLMs. Based on this work, many variants of CoT have been proposed to enhance the efficiency and reliability of LLMs in solving multi-step reasoning problems. See, e.g., <ref type="bibr" target="#b70">Yao et al. (2023)</ref>; <ref type="bibr" target="#b60">Wang et al. (2022)</ref>; <ref type="bibr">Creswell et al. (2022)</ref>; <ref type="bibr" target="#b77">Zhou et al. (2022)</ref>; <ref type="bibr" target="#b37">Chen et al. (2022)</ref>; <ref type="bibr" target="#b76">Zhang et al. (2023c)</ref>; <ref type="bibr" target="#b10">Besta et al. (2024)</ref> and also see <ref type="bibr">Chu et al. (2023)</ref>; <ref type="bibr" target="#b74">Zhang et al. (2023b)</ref> for recent surveys of CoT methods. In particular, our work offers a theoretical understanding for vanilla CoT and variants including Self-Consistency (SC) CoT <ref type="bibr" target="#b60">(Wang et al., 2022)</ref>, Selection-Inference (SI) <ref type="bibr">Creswell et al. (2022)</ref>, and Tree-of-thoughts (ToT) <ref type="bibr" target="#b70">(Yao et al., 2023)</ref>.</p><p>Existing Research on Understanding ICL. Our work is closely related to the body of works that aim to understand the ability of ICL from both empirical and theoretical perspectives. From an empirical point of view, <ref type="bibr" target="#b14">Garg et al. (2022)</ref>; <ref type="bibr" target="#b36">Min et al. (2022)</ref>; <ref type="bibr" target="#b27">Krishnamurthy et al. (2024)</ref>; <ref type="bibr" target="#b75">Zhang et al. (2022)</ref>; <ref type="bibr">Dziri et al. (2024)</ref>; <ref type="bibr" target="#b37">Olsson et al. (2022)</ref> explore the understanding of the behavior and capability of ICL. In particular, <ref type="bibr" target="#b14">Garg et al. (2022)</ref> show that transformers can learn unseen linear functions via ICL. <ref type="bibr" target="#b36">Min et al. (2022)</ref> demonstrate that shuffled input-output pairs in few-shot ICL induce little degradation in the performance on a range of classification and multi-choice tasks. Dziri et al. ( <ref type="formula">2024</ref>) study how transformer-based LLMs solve compositional tasks and their limitations in reasoning.</p><p>From a theoretical perspective, <ref type="bibr" target="#b3">Akyürek et al. (2022)</ref>; Von <ref type="bibr" target="#b57">Oswald et al. (2023)</ref>; <ref type="bibr" target="#b8">Bai et al. (2023)</ref>; <ref type="bibr" target="#b52">Dai et al. (2023)</ref>; <ref type="bibr" target="#b59">Wang et al. (2023a)</ref> establish theoretical understandings of ICL. The theories proposed in these works mainly offer two explanations of ICL: (i) LLMs perform ICL by running iteration optimization algorithms such as gradient descent, and (ii) LLMs perform ICL by implementing Bayesian inference through the architecture. The works <ref type="bibr" target="#b3">Akyürek et al. (2022)</ref>; Von <ref type="bibr" target="#b57">Oswald et al. (2023)</ref>; <ref type="bibr" target="#b8">Bai et al. (2023)</ref>; <ref type="bibr" target="#b52">Dai et al. (2023)</ref> indicate that ICL implicitly implements the gradient descent or least-square algorithms from the function approximation perspective. <ref type="bibr" target="#b20">Hou et al. (2023)</ref> hypothesize that LLMs implicitly perform multi-step reasoning within their architecture by going through a reasoning tree. <ref type="bibr" target="#b29">Li et al. (2023a)</ref> derive the generalization bound for ICL from the view of multi-task learning. <ref type="bibr" target="#b15">Hahn and Goyal (2023)</ref> adopt a linguistic point of view and bounds the ICL error using description length. The works <ref type="bibr" target="#b2">Ahn et al. (2023)</ref>; <ref type="bibr" target="#b22">Huang et al. (2023b);</ref><ref type="bibr">Fu et al. (2023)</ref>; <ref type="bibr" target="#b33">Mahankali et al. (2023)</ref>; <ref type="bibr" target="#b66">Wu et al. (2023a)</ref> consider linear attention models to study the performance of ICL, which restricts the function class that can be represented by transformers to linear functions.</p><p>Another line of work lies in the Bayesian interpretation of the ICL paradigm <ref type="bibr" target="#b23">(Jiang, 2023;</ref><ref type="bibr" target="#b61">Wang et al., 2023b;</ref><ref type="bibr" target="#b68">Xie et al., 2021;</ref><ref type="bibr" target="#b65">Wies et al., 2023;</ref><ref type="bibr" target="#b73">Zhang et al., 2023a;</ref><ref type="bibr" target="#b17">He et al., 2024)</ref>. Under the Bayesian framework, <ref type="bibr" target="#b68">Xie et al. (2021)</ref> use Hidden Markov Model (HMM) <ref type="bibr" target="#b43">(Rabiner and Juang, 1986)</ref> to model the token generation process and assume access to the true language distribution. However, the HMM assumption is restrictive, and the perfect pretraining assumption does not incorporate the pretraining phase into the story. To this end, <ref type="bibr" target="#b65">Wies et al. (2023)</ref> relax these two assumptions by adopting a general i.i.d. data model and analyzing a pretrained model that well approximates the true distribution given any token sequence, which is also unrealistic. These works do not mention the relationship between transformer architecture, pretraining process, and the Bayesian interpretation of ICL.</p><p>Among these works, our work is most related to <ref type="bibr" target="#b73">Zhang et al. (2023a)</ref> and <ref type="bibr" target="#b17">He et al. (2024)</ref>. In particular, <ref type="bibr" target="#b73">Zhang et al. (2023a)</ref> adopt a latent variable model that generalizes the HMM model in <ref type="bibr" target="#b68">Xie et al. (2021)</ref>, and show that ICL can be explained as a BMA estimator under this model. They also establish the statistical error of the BMA estimator and connect it to the attention mechanism. <ref type="bibr" target="#b17">He et al. (2024)</ref> further extend this BMA framework for studying LLM-based decision-making problems, where an LLM is used as a policy. They bring about the equivalence between the LLM-based policy trained by predicting the next action given the history and a Bayesian version of imitation learning. This ability of LLM allows the decision maker to take optimal actions in each timestep when the pretraining data contains the optimal actions provided by the oracle. Our work builds on the ideas of <ref type="bibr" target="#b73">Zhang et al. (2023a)</ref> and extends the Bayesian framework to CoT and its variants method. Compared to these works, the output of CoT is obtained by multi-step generating using the LLM, but ICL and imitation learning only involve one-step generations. To this end, we propose a multistep latent variable model, and establish new analyses for the errors in both pretraining and prompting stages. For example, to bound the pretraining error, we construct a family of transformer models that explicitly take the multi-step structure into account.</p><p>Existing Research on Understanding CoT. Our work aims to understand the capability and behavior of CoT. The following works provide an interpretation of CoT from both experimental and theoretical perspectives. <ref type="bibr" target="#b47">Saparov and He (2022)</ref>; <ref type="bibr" target="#b48">Shi et al. (2022)</ref>; <ref type="bibr" target="#b40">Paul et al. (2023)</ref>; <ref type="bibr">Wang et al.;</ref><ref type="bibr" target="#b51">Tang et al. (2023)</ref>; <ref type="bibr" target="#b32">Madaan and Yazdanbakhsh (2022)</ref> offer practical insight by exploring the performance and capability of CoT reasoning empirically. On the theoretical side, <ref type="bibr" target="#b35">Merrill and Sabharwal (2023)</ref>; <ref type="bibr" target="#b21">Feng et al. (2023)</ref>; <ref type="bibr" target="#b30">Li et al. (2023b)</ref>; <ref type="bibr" target="#b42">Prystawski et al. (2024)</ref> explore the reason behind the improvement in reasoning induced by CoT. <ref type="bibr" target="#b67">Wu et al. (2023b)</ref>; <ref type="bibr" target="#b55">Tutunov et al. (2023)</ref>; <ref type="bibr" target="#b20">Hou et al. (2023)</ref>; <ref type="bibr" target="#b59">Wang et al. (2023a)</ref> investigate the ability demonstrated by CoT through examining the internal mechanism of the transformer architecture.</p><p>Currently, the understanding of CoT is still limited and requires further investigation. In this work, we adopt a statistical point of view to establish a refined characterization of the statistical properties of CoT in both the pretraining and prompting stages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Background</head><p>In this section, we introduce the background knowledge about transformer-based large language models and CoT prompting.</p><p>Autoregressive LLMs. Most commercial LLMs such as <ref type="bibr">GPT-4 (OpenAI, 2023)</ref>, Claude <ref type="bibr" target="#b5">(Anthropic, 2023)</ref>, Llama <ref type="bibr" target="#b54">(Touvron et al., 2023)</ref>, and Gemini <ref type="bibr" target="#b52">(Team et al., 2023)</ref>, are autoregressive in the sense that they generate in a token-by-token fashion. An autoregressive LLM, denoted by P LLM , is a conditional probability model that continuously predicts future tokens based on a sequence of past tokens, known as the prompt. Here we denote the space of all the tokens as X . Given an input prompt S t = (x 1 , ..., x t ) ∈ X t , to generate the response to it, the LLM first generate the next token as x t+1 ∈ X ∼ P LLM (• | S t ). Then it appends the generated token x t+1 to the end of S t to form S t+1 = (S t , x t+1 ). The LLM will generate x t+2 based on S t+1 , and it repeats this generation process till the generation of the end of the sentense.</p><p>Transformers and Attention Mechanism. The transformer model is based on the Multi-Head Attention (MHA) mechanism <ref type="bibr" target="#b7">(Bahdanau et al., 2014;</ref><ref type="bibr" target="#b41">Phuong and Hutter, 2022)</ref>, together with other modules such as the tokenizer and the positional embeddings <ref type="bibr" target="#b62">(Wang and Chen, 2020;</ref><ref type="bibr" target="#b50">Su et al., 2023)</ref>, residual connections, feed-forward networks, and layer normalization <ref type="bibr" target="#b6">(Ba et al., 2016)</ref>. The tokenizer maps the input sequence to a sequence of vectors in Euclidean space, and the positional embeddings add the position information of tokens to these vectors.</p><p>The attention mechanism captures the relationship between different tokens, which is the backbone of transformer-based LLM <ref type="bibr">(Devlin et al., 2018)</ref>. The attention mechanism takes in queries, keys, and values as inputs, and outputs the response of each query as a weighted sum of values, where the weights are the similarity scores between the query and the keys. Specifically, let K ∈ R L×d k and V ∈ R L×dv denote the L key and value vectors, respectively. The attention output of a single query q ∈ R d k is computed as:</p><formula xml:id="formula_3">attn(q, K, V ) = V T softmax(Kq),<label>(3.1)</label></formula><p>where softmax(Kq) is a probability distribution over <ref type="bibr">[L]</ref>. Here softmax(Kq) quantifies the similarity between the query q and each row of K, which is used to aggregate the value vectors of V . The attention attn(Q, K, V ) that takes in multiple queries outputs the responses as V T softmax(KQ T ), where Q ∈ R d k contains L query vectors. The predefined attention mechanism captures the relationship between the keys and queries via a single softmax module, and thus is called single-head attention. MHA refers to passing the inputs through multiple attention functions in parallel, and outputs the aggregation of these sub-modules.</p><p>Taking X ∈ R L×r as the input, a MHA layer with η heads outputs mha(X,</p><formula xml:id="formula_4">W mha ) = η i=1 attn(XW Q i , XW K i , XW V i ),<label>(3.2)</label></formula><p>The parameter set</p><formula xml:id="formula_5">W mha = {W Q i , W K i , W V i } η i=1</formula><p>are the weight matrices for queries, keys, and values, where</p><formula xml:id="formula_6">W Q i ∈ R r×d k , W K i ∈ R r×d k , and W V i ∈ R r×dv .</formula><p>Intuitively, different heads can attend to different parts of the data, and thus MHA offers a more expressive model class. Compared to the MHA defined in <ref type="bibr" target="#b56">Vaswani et al. (2017)</ref>, we absorb the matrix W O i into W V i for each head. Each MHA layer is followed by a Feed-Forward (FF) layer. Given an input X ∈ R L×r , a FF layer with d F neurons maps the input X to ff(X, W ff ) = ReLU(XW ff,1 )W ff,2 ,</p><p>where</p><formula xml:id="formula_7">W ff = {W ff,1 ∈ R r×d F , W ff,2 ∈ R d F ×r } (3.3)</formula><p>are weight matrices. There are also normalization layers between the MHA and FF layers. We defer their details to Appendix G.1 for brevity.</p><p>LLM Training. The training of an LLM involves two stages: (i) pretraining <ref type="bibr" target="#b78">(Zoph et al., 2020)</ref> and (ii) post-training <ref type="bibr" target="#b39">(Ouyang et al., 2022;</ref><ref type="bibr" target="#b63">Wei et al., 2021)</ref>. In the pre-training stage, the LLM is trained to predict the next token based on a large corpus of text data by maximizing likelihood. The log likelihood function of a token sequence S T = (x 1 , • • • , x T ) for LLMs is given by T t=1 log P LLM (x t |S t-1 ), where P LLM denotes the conditional distribution induced by the LLM. The pretraining dataset consists of a large number of token sequences from diverse datasets. For the popular LLMs such as <ref type="bibr">GPT-4 (OpenAI, 2023)</ref>, pretraining datasets are internet-scale and contain billions or trillions of tokens from a variety of sources, such as Wikipedia, news articles, and books <ref type="bibr" target="#b38">(OpenAI, 2023)</ref>. The goal of pretraining is to learn a general-purpose LLM that can generate coherent text and capture the statistical structure of natural language. Then in the second stage, the pretrained LLM is finetuned on a much smaller labeled dataset consisting of question-answer pairs or human feedbacks <ref type="bibr" target="#b63">(Wei et al., 2021;</ref><ref type="bibr" target="#b39">Ouyang et al., 2022)</ref>. Fine-tuning can be either based on supervised learning, reinforcement learning, or both. The goal is to adapt the LLM to a chatbot-style model that can interact with humans and generate conversations that align with human values.</p><p>Prompting a Pretrained LLM. Users interact with LLMs by providing a piece of text, known as the "prompt", and let the LLMs generate a token sequence based on the given prompt. Here the network parameters of the LLM is fixed and the LLM is not trained on the prompt. Due to the autoregressive nature, in the sequel, we slightly abuse the notation by regarding P LLM as a mapping from a prompt to a probability distribution over the output token sequence. Then prompting an LLM is equivalent to sampling a the output token sequence, output ∼ P LLM (• | prompt), where prompt is the input token sequence.</p><p>In-Context Learning. ICL refers to the learning process of the LLMs where they learn from prompts without tuning the parameters <ref type="bibr">(Dong et al., 2022)</ref>. In the vanilla version of ICL, we prompt an LLM with a collection of input-output pairs, known as "examples" or "demonstrations", and a new input query. We expect the LLMs to learn the underlying pattern of the input-output pairs and generate a desired output associated with the input query following the same pattern. More concretely, let {(x i , y i )} T i=1 be a collection of T examples satisfying y i = f * (x i ), where f * is the underlying input-output relationship, and x i , y i ∈ X * . Let x q denote a new input query, and we concatenate the examples and the query to form the prompt prompt = (x 1 , y 1 , • • • , x T , y T , x q ). The LLM is able to learn in an in-context fashion if y q ∼ P LLM (• | prompt) satisfies y q = f * (x q ). For example, we can prompt the LLMs with "grass is green, apple is red, sky is" to let the LLMs output the color of the sky. Here ("grass is", "green") and ("apple is", "red") are examples, "sky is" is the query, and the desired output y q is blue. Generating y q based on prompt is called "in-context learning" because the LLM learns the desired relationship f * purely from the prompt without updating the network parameters of the LLM.</p><p>Chain of Thought and its Variants. When the input-output relationship is complex, it is challenging for LLMs to learn this relationship directly from input-output pairs in the prompt. The complex relationship usually appears in the multi-step reasoning problem. For example, calculating a long math equation 5 × 20 + (5 + 3) ÷ 2 = 100 + 8 ÷ 2 = 100 + 4 = 104 involves a series of operations. It is difficult to learn a function that can directly output the result. CoT is a prompting technique that aims to solve multi-step reasoning tasks by providing multiple input-output examples together with intermediate reasoning steps in the prompts <ref type="bibr" target="#b64">(Wei et al., 2022)</ref>. By guiding LLMs through a sequence of intermediate reasoning steps before arriving at a final answer, we expect to decompose a complicated reasoning problem into a sequence of simple subtasks that can be learned via vanilla ICL. We formulate a CoT prompt with H steps as prompt CoT (n) = (z 1 0:H , • • • , z n 0:H , z test 0 ), and we will denote this as ({z i 0:H } n i=1 , z test 0 ) in the following for ease of notation. Here for each i ≤ n, (z i 0 , z i H ) corresponds to the input-output pair of the example (x i , y i ) in vanilla ICL, and (z i 1 , • • • , z i H-1 ) denotes the intermediate reasoning steps of the example. For example, in the math equation calculation problem, z 0 ="5 × 20 + (5 + 3) ÷ 2", z 1 ="100 + 8 ÷ 2", and z 3 ="104". Here H is fixed throughout this paper. We recover the vanilla ICL prompts by omitting the intermediate steps, i.e., setting H = 1. We will show more concrete examples in Section 4.1. For simplicity, we assume each reasoning step z takes value in a finite set L ⊆ X * , with each element uniquely identified with an embedding vector in the Euclidean space R d Z for some integer d Z . We let L * denote the set of sequences consisting of reasoning steps, e.g., prompt CoT (n) ∈ L * .</p><p>Furthermore, as conditional probability models, LLMs are intrinsically stochastic. For problems such as solving mathematical questions, however, there is often a unique answer. To further boost the probability of finding the correct answer, variants of CoT leverage multi-step reasoning with various selection techniques to solve more complicated reasoning and decision-making problems. For instance, SC-CoT <ref type="bibr" target="#b60">(Wang et al., 2022)</ref> uses majority vote, ToT <ref type="bibr" target="#b70">(Yao et al., 2023)</ref> adopts tree search methods, and SI <ref type="bibr">(Creswell et al., 2022)</ref> further introduces a selection module in each reasoning step.</p><p>In the next section, we will introduce a multi-step latent variable model to interpret the CoT prompting method as a Bayesian model averaging estimator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">A Latent Variable View of Multi-Step Reasoning</head><p>In this section, we show that CoT prompting can be understood as a Bayesian estimator on a multi-step latent variable dynamical model. In particular, we propose a multi-step latent variable model in Section 4.1 to capture the multi-step reasoning process, which is further generalized in Appendix A to the non-i.i.d. setting. Then in Section 4.2, we study the practice of CoT prompting of pretrained LLMs from a statistical perspective. In Section 4.3, we show that such a practice is equivalent to a BMA estimator for the multi-step latent variable model, which answers Question (a) raised in Section 1. Moreover, we show that the softmax attention mechanism in the transformer architecture parameterizes the BMA algorithm, which partially answers Question (c).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">A Multi-Step Latent Variable Model</head><p>We introduce a multi-step latent variable model to capture the multi-step reasoning process of CoT, which serves as the data-generating model for studying CoT.</p><p>CoT Prompting Paradigm. Recall that we define the CoT prompt prompt</p><formula xml:id="formula_8">CoT (n) = ({z i 0:H } n i=1 , z test 0 ) = (Υ n , z test 0 ) in Section 3, which contains n demonstration examples Υ n = {s i } n i=1 = {z i 0:H } n i=1</formula><p>and a testing query z test 0 . To generate such a prompt, we first specify a latent concept vector, which is denoted as θ * ∈ Θ. Here Θ denotes the set of all the latent concepts. Semantically, θ * determines the task we would like to achieve via CoT, e.g., the color description of objects, the calculation of math equations. Thus, we will use the terms task and latent concept interchangeably in the following. Statistically, the latent concept θ * specifies the task-specific joint distribution P(• | θ * ) of demonstration examples and testing query in the prompt, which will be specified later in (4.2). Given the generated prompt prompt CoT (n), we feed it to the LLM, and the LLM recursively generates the intermediate steps (z test 1 , . . . , z test H-1 ) and the final answer z test H via</p><formula xml:id="formula_9">z test h+1 ∼ P LLM (• | prompt CoT (n), z test 1 , . . . , z test h ), ∀h ∈ [H -1]. (4.1)</formula><p>To evaluate the performance of CoT, we compare the distribution of z test H in (4.1) with the ground truth distribution P(z test H | prompt CoT (n), θ * ), which is the target task-specific distribution of the final answer given the prompt. We illustrate the CoT paradigm with a concrete example as follows.</p><p>As a concrete example, consider the task θ * = "calculate twice the area code of the given country." The prompt in Figure <ref type="figure" target="#fig_1">1</ref> is a CoT prompt with n = 2 and H = 2, where the input of the first example is z<ref type="foot" target="#foot_0">foot_0</ref> 0 ="The US = ?", the first step of the solution is z 1 1 ="The US has area code 1", and the second step of the solution is z 1 2 ="so the answer is 2". The query is z test 0 ="Japan = ?", and the desired taskspecific answer is "126". When tested on ChatGPT <ref type="bibr" target="#b0">(Achiam et al., 2023)</ref>, it indeed outputs the correct answer with an intermediate reasoning step: "Japan has area code 81, so the answer is 162." In comparison, the vanilla ICL prompt has x 1 ="The US = ?", y 1 ="The answer is 2", x 2 ="France = ?", y 2 ="The answer is 66", and x test ="Japan = ?". In this case, however, ChatGPT is unable to provide the correct answer because it fails to find the relationship between the area code and the country. 1 See Figure <ref type="figure" target="#fig_1">1</ref> for a visual illustration of CoT and vanilla ICL prompts. Thus, seen from this example, by providing additional reasoning steps, CoT prompts can significantly boost the accuracy of the LLM compared with vanilla ICL prompts.  The Multi-Step Latent Variable Model. To analyze CoT from a statistical perspective, we need to specify the pre-mentioned task-specific distribution P(• | θ * ), which serves as the data-generating distribution for the CoT prompt. We assume that the concept θ * is a random variable sampled from a prior π ∈ P(Θ) and the examples {s i } n i=1 are i.i.d. sequences conditioning on θ * ∈ Θ. For any θ ∈ Θ, when θ * = θ, within the reasoning chain i ∈ [n], we sample z i 0:H according to the following stochastic dynamical system with joint distribution P(s i | θ * = θ) given by</p><formula xml:id="formula_10">P(s i | θ * = θ) : z i 0 = f θ ζ i , z i h = F θ z i 0 , • • • , z i h-1 , ϵ i h , ∀1 ≤ h ≤ H. (4.2) Here {ζ i , {ϵ i h } h∈[H] } i∈[n] are i.i.d.</formula><p>noise variables, and f θ and F θ are two functions parameterized by θ ∈ Θ. The same is true for the test sample z test 0:H and this distribution will serve as the target distribution for LLM to learn in context during the prompting stage. Specifically, f θ generates the first query z i 0 based on the task θ * = θ, and F θ models the evolution of the "reasoning process" {z i h } h∈[H] . Specifically, each z i h depends on all of the previous reasoning steps as well as the latent variable θ * . The rationale behind this model is that the generation of these reasoning steps is autoregressive and the distribution of the whole sequence is specific to the task θ * . The random variables {ϵ i h } h∈[H] allow the reasoning process to be stochastic. See Figure <ref type="figure" target="#fig_3">2</ref> for an illustration of this model. Intuitively, θ * represents the latent concept that specifies the task, e.g., "calculate twice the area code of the given country, including the identification of the area code and the multiplication calculation", "solving an arithmetic problem with each intermediate step" or "writing a science fiction novel, detailing the thought process at each step". As a concrete example, consider θ * as the task of "solving an arithmetic problem with each intermediate step". The input z i 0 is an arithmetic problem described using natural language. To get the final answer, a few intermediate arithmetic operations need to be performed. The intermediate reasoning steps just corresponds to these operations described in natural language, and thus the transition depends on the task θ * . See Figure <ref type="figure" target="#fig_4">3</ref> for an illustration.</p><p>Finally, note that setting H = 1, we obtain a latent variable model for vanilla ICL, which is studied in <ref type="bibr" target="#b61">Wang et al. (2023b)</ref>. Furthermore, our general model can be made more concrete by defining θ * as a sequence of latent variables {θ * h } h∈[H] characterizing the distribution of z i 0:H , where the latent variables also have an autoregressive structure. Such a model is studied in <ref type="bibr" target="#b23">Jiang (2023)</ref>. An limitation of our model in (4.2) is that the n demonstration examples {s i } n i=1 in prompt CoT (n) are assumed to be i.i.d. In practice, the demonstration examples might be composed in a dependent manner which is beyond the i.i.d. assumption. We will introduce a more general model Appendix A which (i) includes latent variables for each reasoning process that are governed by a latent dynamical system, and (ii) allows the generation of the demonstration examples to be dependent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Pretrained LLM + CoT Prompting</head><p>The previous section proposes a latent variable model that captures the multi-step reasoning process of CoT. Based on this model, we will formulate the estimator constructed by CoT prompting on a pretrained autoregressive LLM from a statistical perspective.</p><p>Pretraining LLM. We assume that the LLM is pretrained with data generated according We let {P ρ | ρ ∈ P LLM } denote the conditional distributions induced by the LLM with parameters ρ ∈ P LLM , where P LLM is the parameter space of LLM and will be specified in Section 6. Then pretraining the autoregressive LLM corresponds to finding the maximum likelihood estimator ρ, i.e.,</p><formula xml:id="formula_11">ρ = argmin ρ∈P LLM - 1 N T (H + 1) ℓ∈[N ],t∈[T ] H h=0 log P ρ z t,ℓ h | Υ t-1,ℓ , {z t,ℓ j } h-1 j=0 , (4.3) where Υ t,ℓ = {s k,ℓ } k∈[t]</formula><p>is the first t examples in the ℓ-th document. Given ρ, we denote P LLM = P ρ as the distribution induced by the pretrained LLM and write them interchangeably in the sequel.</p><p>CoT Prompting as an Estimator. After pretraining, we fix the parameter of the LLM as ρ and prompt the LLM with a CoT prompt prompt CoT (n) = (Υ n , z test 0 ). To connect the pretraining and prompting stages, we note that prompting a pretrained LLM with prompt induces a conditional distribution P LLM • | prompt . When using a CoT prompt, we aim to induce the LLM to eventually generate a desired final answer defined by (4.2). </p><formula xml:id="formula_12">err CoT = KL P y test = • | prompt CoT (n), θ * , P LLM y test = • | prompt CoT (n) .</formula><p>(4.4)</p><p>The error metric in (4.4) is of particular interest to us, as our primary concern is the accuracy of the final result. In the sequel, we consider θ * ∈ Θ to be fixed but unknown for our result. Note that err CoT is a random variable where the randomness stems from prompt CoT (n) during the prompting stage and the learned model parameters ρ during the pretraining.</p><p>Error Decomposition. In the following, we briefly outline the error decomposition of the statistical error err CoT in (4.4). Intuitively, the statistical error has two sources: a pretraining error and a prompting error. The pretraining error arises due to the finiteness of training data points and it decays to zero as N increases. This pretraining error essentially is the statistical error of pretraining problem in (4.3) and is irrelevant to the prompting stage. The prompting error reflects the error incurred by using n examples to elicit the desired answer from the LLM. Such an error appears even when the LLM is perfectly pretrained. Intuitively, with more examples, the LLM has more information to infer the task θ * and learn to generate the desired reasoning steps. Thus, the prompting error should decrease as n increases. Moreover, the success of CoT prompting also depends on how well the examples in the prompt align with the testing query z test 0 . If the examples in the prompt are not informative enough for answering the testing query, the prompting error will be large. Such an intuition is formalized by Lemma 5.2 in Section 5, which shows that the prompting error can be further decomposed into two parts: a query error and an in-context error. The query error quantifies the distributional shift between the testing query and the examples in the prompt, and the in-context error quantifies the error due to the LLM not knowing the true task θ * and having to make an inference based on the n examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">BMA Interpretation of CoT</head><p>In the following, we show that the CoT estimator P LLM • | prompt CoT (n) can be understood as a Bayesian model averaging (BMA) estimator for the latent variable model in (4.2).</p><p>Pretrained LLM + CoT ≈ BMA. Recall that the pretraining process of LLM is given in (4.3), where the data is generated from the latent variable model in (4.2). When N and T are sufficiently large, we expect the pretrained LLM to approximate the true distribution of the pretraining dataset well. Note that the tasks in the pretraining dataset are sampled from the prior π. When we replace P ρ in (4.3) by the true data distribution, for any random document with T examples, by Bayes' rule, we have that</p><formula xml:id="formula_13">P z t h = • | Υ t-1 , {z t j } h-1 j=0 = Θ P z t h = • | {z t j } h-1 j=0 , θ • π θ | Υ t-1 , {z t j } h-1 j=0 dθ, (4.5) where Υ t-1 = {s k } k∈[t-1] contains the first t-1 examples in the document, and π(• | Υ t-1 , {z t j } h-1 j=0</formula><p>) is the posterior distribution of the task. Here we use the fact that the examples are i.i.d. conditioning on the task.</p><p>Note that we expect that the pretrained LLM is approximately the same as the left-hand side of (4.5). Based on (4.5), we can further marginalize the intermediate steps and obtain a similar factorization for P LLM (z t H | Υ t-1 , z t,ℓ 0 ). Since the examples in the CoT prompt are generated from the same distribution as the pretraining data, we can set t = n + 1 and get the following lemma.</p><p>Lemma 4.1. Let the pretraining data be generated according to the latent variable model specified in (4.2). Consider the population counterpart of the MLE in (4.3), i.e., we let the number of documents N goes to infinity. Suppose that the LLMs have enough capacity, i.e., P ∈ {P ρ | ρ ∈ P LLM }, and the CoT prompt prompt CoT (n) has nonzero density under the pretraining distribution, we have</p><formula xml:id="formula_14">P LLM y test = • | prompt CoT (n) = Θ P(y test = • | z test 0 , θ)π θ | prompt CoT (n) dθ.</formula><p>A detailed proof of this lemma is deferred to Appendix B.1. This lemma implies that CoT prompting based on a perfectly pretrained LLM performs BMA. That is, the CoT estimator is constructed in three steps: (i) the LLM first constructs a posterior of the task θ, then (ii)for each task θ, the LLM predicts the final answer y test based on the prompt, and (iii) finally, the LLM aggregates the predictions over the posterior of the task θ. Such a BMA interpretation is also established for vanilla ICL in <ref type="bibr" target="#b73">Zhang et al. (2023a)</ref>, which is recovered by our result when setting H = 1. We provide a detailed proof of this result and extend it to the more complicated model in Appendix B.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Attention Approximately Parameterizes BMA</head><p>We now show that the attention mechanism in the transformer architecture is able to encode the BMA algorithm for a special case of the latent variable model in (4.2).</p><p>A Simplified Model. In this special case, we Let f θ * in (4.2) be a function independent of θ * , i.e., the inputs do not depend on θ * . Moreover, we assume that F θ * in (4.2) encodes a linear model in the latent space. Specifically, for any h ∈ [H], let d k and d v be two integers and let k : L → R d k and be v : L → R dv be two feature mappings that maps each reasoning step z i h to vectors. Moreover, assume v is invertible. Then, we assume each reasoning step z i h is generated from a Gaussian linear model with another feature mapping ϕ:</p><formula xml:id="formula_15">z i h = v -1 θ * ϕ(k i h ) + ϵ i h = F θ * z i 0 , z i 1 , • • • , z i h-1 , ϵ i h , ∀h ∈ [H]. (4.6)</formula><p>Here we define ∼ N (0, σ 2 ) are i.i.d. and independent of everything else. We note that our theoretical result in this section only relies on the invertibility of v and that there exist feature maps v, k, and ϕ such that the model in (4.2) admits a simpler form as in (4.6). This model specifies a linear dynamical system in the feature space. Thanks to the flexibility of these feature maps, this model captures a rich class of distributions.</p><formula xml:id="formula_16">k i h = k(z i 0 ), k(z i 1 ), • • • , k(z i h-1 ), 0, • • • , 0 ∈ R</formula><p>The BMA Estimator. To study the BMA estimator under this model, we further impose a Gaussian prior over θ * . Specifically, we assume that the entries of θ * are i.i.d. with prior distribution N (0, λ) for some fixed λ &gt; 0. Based on the n examples in the CoT prompt prompt CoT (n), we define</p><formula xml:id="formula_17">V n = (v(z i h )) H,n h=1,i=1 ∈ R dv×Hn and K n = (k i h ) H,n h=1,i=1 ∈ R d k ×Hn and let ϕ(K n ) denote the R d ϕ ×Hn feature matrix induced by K n . Under the simplified model, the inputs {z i 0 } i∈[n]</formula><p>and z test 0 do not contain information about θ * . Thus, conditioning on prompt CoT (n), the posterior distribution of θ * is a Gaussian distribution, centered at the ridge estimator</p><formula xml:id="formula_18">θn = V n ϕ(K n ) ⊤ ϕ(K n )ϕ(K n ) ⊤ + σ 2 /λ • I -1 ,</formula><p>where I is the identity matrix of size R </p><formula xml:id="formula_19">k test h = k(z test 0 ), k(z test 1 ) • • • , k(z test h-1 ), 0 • • • , 0 . (4.7)</formula><p>Therefore, to get the BMA estimator, we aggregate the distribution of v test h according to the posterior distribution of θ, and return the mean value as the predictor, which is given by vtest</p><formula xml:id="formula_20">h = V n ϕ(K n ) ⊤ ϕ(K n )ϕ(K n ) ⊤ + σ 2 /λ • I -1 ϕ(k test h ). (4.8)</formula><p>The final BMA estimator is given by {v</p><formula xml:id="formula_21">-1 (v test h )} h∈[H] .</formula><p>Estimator Produced by Transformer. In the following, we introduce another autoregressive estimator based on a transformer with softmax attention. Transformer is a mapping that maps a sequence of vectors to another sequence of vectors and the mapping involves three components. In particular, we pack the n examples in the prompt as a sequence of Hn vectors, followed by the test instance.</p><p>Our transformer is a composition of a copy head, a softmax attention layer, and a positionwise transformation. The copy head takes the original sequence as input, and copies the previous reasoning steps within the same example at each position. Specifically, for any example i and any h, the output of the copy head is (z i 0 , . . . , z i h , 0, . . . , 0), where H -h + 1 zeros so that the output vectors have the same dimension. The same operation is done for the test instance. Such a copy head can be explicitly constructed in theory using standard transformer architectures <ref type="bibr" target="#b21">(Feng et al., 2023)</ref> and is also shown to emerge in various empirical works <ref type="bibr" target="#b37">(Olsson et al., 2022;</ref><ref type="bibr" target="#b57">Von Oswald et al., 2023)</ref>.</p><p>The output of the copy head is then passed to a standard softmax attention layer, which involves the construction of keys, queries, values, and their calculations. Moreover, these three quantities are defined for each position. Specifically, for each (i, h), we define both the key and query as k i h in (4.6), and the value as</p><formula xml:id="formula_22">v i h = v(z i h )</formula><p>, where v is the feature map appearing in (4.6). Moreover, for the test example, for each h ≥ 0, we define the query as q test h = ϕ(k test h ), which is used to attend to the keys of the n examples, aggregate the corresponding values, and get the output. More concretely, we define the attention output given prompt CoT (n) and h -1 intermediate outputs z test 1 , . . . , z test h-1 , the output of the softmax attention is given by</p><formula xml:id="formula_23">attn(q test h , keys, values) = i∈[n],ℓ∈[H] exp(⟨q test h , k i ℓ ⟩) • v i ℓ i ′ ∈[n],ℓ ′ ∈[H] exp(⟨q test h , k i ′ ℓ ′ ⟩) . (4.9)</formula><p>Finally, the output is passed through a transformation function v -1 , which yields</p><formula xml:id="formula_24">z test h = v -1 (attn(q test h , keys, values)), ∀h ∈ [H].</formula><p>This newly generated z test h is then used to compute the query q test h+1 , which is then used for generating z test h+1 , and so on. See Figure <ref type="figure">4</ref> for an illustration of this transformer. In the following proposition, we prove that under certain conditions, the BMA estimator coincides with the transformer output up to a scaling factor when n goes to infinity. Proposition 4.2. We assume the feature mappings k and v take bounded values and ∥v(z)∥ 2 = 1 for all input z ∈ L. Besides, let ϕ in (4.6) be a feature map with finite dimension. Then, there exists an absolute constant C, and parameter λ = n -2/3 such that for any fixed k test h , the BMA estimator in (4.8) and the attention output in (4.9) coincide as n goes to infinity up to a scaling factor. That is, we have</p><formula xml:id="formula_25">lim n→∞ max h∈[H] ∥v test h -C • attn(q test h , keys, values)∥ 2 = 0.</formula><p>This proposition shows that there exists a special model satisfying (4.2) (the model in (4.6)) such that the BMA estimator of this model can be approximately implemented by a transformer. Furthermore, to see why such a proposition can be expected, we note that the attention output in (4.9) corresponds to the Nadaraya-Watson Kernel regressor Figure <ref type="figure">4</ref>: An illustration of how CoT is represented by a two-layer transformer. Layer 1 serves as a copy head, which copies the previous steps {z i j } h-1 j=1 to the current position z i h . Next, the feature mappings v and k map the outputs of Layer 1 to values and keys, respectively. During the generation of z test h+1 , the attention mechanism takes in key and value matrices keys and values from the demonstrations to predict the result for query q test h , where keys and values are formed by stacking {k i h } n,H i=1,h=1 and {v i h } n,H i=1,h=1 , respectively. Note that keys and values do not contain keys and values computed from the generated reasoning steps {z test j } h j=1 . We can achieve this by masking out the corresponding positions. <ref type="bibr" target="#b16">(Hastie et al., 2009)</ref> with an exponential kernel, where we regress the value on the key, and predict on the query. Moreover, the BMA estimator in (4.8) corresponds to ridge regression. These two estimators are both consistent and thus converge to the same thing as n goes to infinity. Furthermore, we will provide a detailed proof in Appendix B.2. This proof generalizes a similar result in <ref type="bibr" target="#b73">Zhang et al. (2023a)</ref> for vanilla ICL by handling the multi-step autoregressive structure of CoT prompting. Note that we assume that the feature mappings k and v take reasoning steps as input, making the keys and values specific to reasoning steps rather than tokens. However, this can be easily generalized to tokens.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Statistical Errors of CoT Prompting</head><p>In this section, we study the error incurred during the prompting stage. We first state an error decomposition result and then study the vanilla CoT prompting in Section 5.1. Then we extend the theory to three variants of CoT in Section 5.2 and compare CoT with vanilla ICL in Section 5.3. Regarding the four questions raised in the introduction, this section answers Question (b) partly and Question (d).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Statistical Errors of Vanilla CoT</head><p>Recall that we define the statistical error induced by the CoT prompting err CoT in equation (4.4) and the error comes from both pertaining and prompting stages, as listed in table <ref type="table" target="#tab_0">1</ref>. We explicitly decompose these two error sources as follows. To this end, we first state a regularity condition for the pretrained LLM. Before we proceed, let us define the following partial prompt prompt h CoT (i). For any integers</p><formula xml:id="formula_26">i ∈ [0, n -1] and h ∈ [H], we let prompt h CoT (i) = {s j } j≤i ∪ {z i+1 0 , . . . , z i+1 h-1 }.</formula><p>That is, prompt h CoT (i) contains the first i demonstration examples and the first h steps of the (i + 1)-th example. Let θ * denote the target task. In Section 5, we assume that the prompt is generated from the ground truth distribution, meaning that prompt CoT (n) ∼ P(• | θ * ).</p><p>Assumption 5.1. We assume there exists a positive number b * such that for any 0 ≤ h ≤ H, 0 ≤ i ≤ n-1, and prompt h CoT (i) ∈ L * , we have for the data distribution P and the pretrained model P LLM that sup</p><formula xml:id="formula_27">z∈L log P(z i+1 h = z | prompt h CoT (i)) -log P LLM (z i+1 h = z | prompt h CoT (i)) ≤ b * .</formula><p>This assumption postulates that the true distribution P of the model in (4.2) and that learned by the LLM are close. The proximity is measured in terms of the log likelihood. We will justify the existence of b * in Section 6 under explicit assumptions on pretraining.</p><p>Lemma 5.2 (CoT Error Decomposition). Under Assumption 5.1, the statistical error err CoT in (4.4) can be upper bounded by the sum of a pretraining error and a prompting error, i.e., err CoT ≤ err pre (P, P ρ ; prompt CoT (n)) + err prompt (P, θ * , prompt CoT (n)).</p><p>where we define the prompting error as</p><formula xml:id="formula_28">err prompt (P, θ * , prompt CoT (n)) = KL P(y test = • | z test 0 , θ * ), P(y test = • | prompt CoT (n)) + 2 √ 2Hb * • KL 1/2 P(y test = • | z test 0 , θ * ), P(y test = • | prompt CoT (n)) , (5.1)</formula><p>and the pretraining error as</p><formula xml:id="formula_29">err pre (P, P ρ ; prompt CoT (n)) = KL P(y test = • | prompt CoT (n)), P ρ (y test = • | prompt CoT (n)) . (5.2)</formula><p>We provide a detailed proof of this lemma in Appendix C.1. Note that the prompting error is defined on the distribution P, which corresponds to the perfectly pretrained LLM, and thus this error is independent of the pretraining of LLMs. In the following, we focus solely on the prompting error. Moreover, we assume that z test 0 has the same distribution as {z i 0 } i∈[n] for simplicity, i.e., the query error is zero. We will allow a distributional shift in the next section.</p><p>With no distributional shift in z test 0 , we can essentially regard the test instance as the (n + 1)-th example, since all the examples are conditionally i.i.d. when they are conditioned on θ * . Thus, in this section, we will only study how the n prompt examples help a perfectly pretrained LLM infer θ * , namely, the in-context error.</p><p>Equivalence Class Induced by Multi-Step Reasoning. In the following, to simplify the notation, we use X = Z 0 , Z 1 , . . . , Z H = Y to denote a random trajectory sampled from the model in (4.2). Note that the prompting error in (5.1) only concerns the distribution of the output Y and neglects the intermediate reasoning steps Z 1 , . . . , Z H-1 . As a result, it is possible that there exists another θ ∈ Θ with the same distribution of Y . Such a relationship induces a set of equivalence classes over Θ.</p><formula xml:id="formula_30">Definition 5.3 (Equivalence Classes over Θ). Let P(Z 0 , Z 1 , • • • , Z H-1 , Y | θ)</formula><p>denote the joint distribution of Z 0:H conditioning on the latent variable θ * = θ. We define an equivalence relation ∼ based on conditional density of Y given Z 0 as follows.</p><formula xml:id="formula_31">θ ∼ θ ′ if and only if P(Y = y | Z 0 = z 0 , θ) = P(Y = y | Z 0 = z 0 , θ ′ ), ∀(z 0 , y).</formula><p>This relation ∼ induces a set of equivalence classes over Θ. In particular, for any θ, define Θ eq (θ) = {θ ′ ∈ Θ : P(y | z 0 , θ) = P(y | z 0 , θ ′ ), ∀(z 0 , y)} as the set of parameters equivalent to θ, i.e., the equivalence class represented by θ. Let Θ denote the complete set of representatives of all disjoint equivalent classes. Then Θ eq (θ) ∩ Θ eq (θ ′ ) = ∅ for all θ, θ ′ ∈ Θ and we can further write Θ as ∪ θ∈ Θ Θ eq (θ).</p><p>The intuition of the equivalence relation ∼ is that there might be multiple reasoning paths that all lead to the correct answer. For example, Newtonian, Lagrangian, and Hamiltonian mechanics are three different approaches to classical mechanics. Their intermediate steps are different but will lead to the same answer. Based on this intuition, any parameter in Θ eq (θ * ) is equally good for predicting Y , and we only need to infer Θ eq (θ * ) from CoT prompts.</p><p>We state a regularity condition for CoT prompting in terms of Θ eq (θ * ).</p><p>Assumption 5.4. Given a task θ * during CoT prompting, let Θ ∁ = Θ\Θ eq (θ * ) denote the complement of the equivalence class of θ * . We assume that there exists a strict separation between the ground truth task θ * and any other tasks θ ∈ Θ ∁ . Specifically, there exists λ &gt; 0 that lower bounds the Hellinger distance:</p><formula xml:id="formula_32">inf θ∈Θ ∁ H 2 P(Z 0:H = • | θ * ), P(Z 0:H = • | θ) ≥ λ,</formula><p>where H 2 (•, •) denotes the squared Hellinger distance. Moreover, we assume tasks in Θ eq (θ * ) are well covered by the pretraining distribution in the sense that π Θ eq (θ * ) &gt; 0.</p><p>This assumption requires the true task θ * is λ-separated from any other θ outside of the equivalence class Θ eq (θ * ). Parameter λ serves as a margin of separation. Moreover, we assume that the prior π put considerable density on Θ eq (θ * ). This means that the task tested during prompting stage has been covered in the pretraining dataset. Based on this assumption, we establish the statistical error of the CoT estimator as follows.</p><p>Theorem 5.5. Let err pre denote the pretraining error defined in (5.2) and assume Θ to be a finite and discrete set. Under Assumptions 5.1 and 5.4, with probability 1 -δ, the statistical error err CoT defined in (4.4) satisfy</p><formula xml:id="formula_33">err CoT ≤ O Hb * δ -1 • π(θ * ) -1/2 • Θ ∁ • e -λn + err pre .</formula><p>Here O(•) hides absolute constants and the probability is with respect to the randomness of the CoT prompt.</p><p>This theorem shows that when the tasks are well separated, the prompting error converges to zero exponentially fast when n increases. Note that the convergence rate depends on the separation λ. A larger λ means that Θ eq (θ * ) is more distinguishable from the rest of the tasks, leading to a faster convergence rate. Besides, the error also depends on H and the size of Θ ∁ . Intuitively, these two terms characterize how the error increases as the problem size grows. Moreover, the dependence on b * comes from Lemma 5.2, which is due to replacing the pretrained LLM by the population distribution. The statistical error also depends on π(θ * ) -1/2 , which means the prompting error is smaller for tasks better covered by the pretraining distribution.</p><p>Here we assume that Θ is finite. In Appendix C.2 we will further extend Theorem 5.5 to the more challenging case where Θ can be a continuous set and present a detailed proof.</p><p>Ideally, we would like to have an upper bound that scales with | Θ| because it is the actual number of all possible hypotheses when it comes to predicting Y based on X. Whereas |Θ ∁ | in Theorem 5.5 can be much larger than | Θ| because it is comparable to |Θ|. To have a better bound, we impose an additional assumption postulating that the distributions within each equivalence class are close.</p><p>Assumption 5.6. Let Θ be a representative set of the equivalence classes introduced in Definition 5.3. We assume that there exist positive numbers α and α 0 such that for all θ ∈ Θ and θ ′ ∈ Θ eq (θ), we have sup</p><formula xml:id="formula_34">z 0:H log P(Z 0:H = z 0:H | θ) P(Z 0:H = z 0:H | θ ′ ) ≤ α, sup z 0 log P(Z 0 = z 0 | θ) P(Z 0 = z 0 | θ ′ ) ≤ α 0 .</formula><p>Moreover, we assume that α ∈ (0, λ), where λ appears in Assumption 5.4.</p><p>Assumptions 5.4 and 5.6 imply that distributions are similar within each equivalence class but disparate between equivalence classes. We establish a new upper bound as follows.</p><p>Theorem 5.7. Under Assumptions 5.1, 5.4, and 5.6, with probability 1 -δ over the randomness of the CoT prompt, we have</p><formula xml:id="formula_35">err CoT ≤ O Hb * π(θ * ) -1/2 δ -1 Θ e -(λ-α)n+α 0 + err pre .</formula><p>Compared with the previous Theorem 5.5 that solely requires Assumption 5.4, we have a better dependency on the size of parameter space, from |Θ ∁ | to | Θ|, at a cost of a slower rate of exponential decay. The proof of this theorem is deferred to Appendix C.2, where we also include an extension to the case where Θ is continuous.</p><p>In summary, we have shown that in the prompting stage, as the number of examples grows, the statistical error of CoT prompting decays exponentially to an intrinsic error due to pretraining. In the following, we will extend the above results to a few variants of CoT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Statistical Errors of Variants of CoT</head><p>The predictions of LLMs are inherently stochastic, which is a main source of LLM hallucination <ref type="bibr">(Huang et al., 2023a;</ref><ref type="bibr" target="#b53">Tonmoy et al., 2024)</ref>. To increase the prediction accuracy, various selection techniques such as majority vote <ref type="bibr" target="#b60">(Wang et al., 2022)</ref> and tree search <ref type="bibr" target="#b70">(Yao et al., 2023)</ref> are combined with CoT. In the following, we modify Theorem 5.5 for a few variants of CoT, including Self-Consistency CoT <ref type="bibr" target="#b60">(Wang et al., 2022)</ref>, Tree-of-Thought <ref type="bibr" target="#b70">(Yao et al., 2023), and</ref><ref type="bibr">Selection-Inference (Creswell et al., 2022)</ref>. For simplicity, we also assume zero pretraining error and input query does not have a distributional shift, i.e., P LLM = P and z test 0</p><formula xml:id="formula_36">∼ P(• | θ * ).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Self-Consistency CoT (SC-COT)</head><p>Given the same prompt as in vanilla CoT, i.e., prompt CoT (n), SC-CoT first generate K i.i.d. reasoning paths and then output the final answer by a majority vote. That is, we first sample K i.i.d. reasoning paths {z test,i 1:H } K i=1 ∼ P • | prompt CoT (n) and then report the mode of the empirical distribution of {y test,i } K i=1 , denoted by y * K . The empirical distribution of {y test,i } K i=1 is denoted by p K (y) = K -1 K i=1 1{y test,i = y}, ∀y ∈ L. The sample mode y * K is defined as y * K = argmax y∈L p K (y), where we pick any element if there are multiple maximizers. See Figure <ref type="figure" target="#fig_5">5</ref> for an illustration.</p><p>Recall that if the underlying task θ * is already known, the answers should be generated according to P(y test = • | z test 0 , θ * ). We assume the desired answer is the mode of this distribution and it is unique.</p><p>Assumption 5.8. We define y * as the mode of the distribution P(y test = • | z test 0 , θ * ), i.e., y * = argmax y∈L P(y test = y | z test 0 , θ * ) Moreover, we define the gap between the mode and the second-largest probability mass as</p><formula xml:id="formula_37">ϵ = min y∈L,y̸ =y * {P y test = y * | z test 0 , θ * -P y test = y | z test 0 , θ * },</formula><p>which is assumed to be strictly positive.</p><p>This assumption ensures that the population mode y * is uniquely defined with a margin ϵ. This condition is satisfied by reasoning problems where the answer is unique, e.g., factual commonsense and mathematical reasoning. Intuitively, when the number of examples in CoT prompt is large, with probability at least 1 -e -λn/2 , the probability that the SC-CoT produces the wrong output decreases exponentially in K, i.e.,</p><formula xml:id="formula_38">P(y test = • | prompt CoT (n)) is close to P(y test = • | z test 0 , θ * ), as guaranteed</formula><formula xml:id="formula_39">P y * K ̸ = y * | prompt CoT (n) ≤ 2|L| • exp - 3Kϵ 2 24 + 8ϵ .</formula><p>This corollary shows that sampling K independent reasoning paths boosts the output accuracy. In particular, when ϵ ∈ (0, 1), for any δ ∈ (0, 1), as long as K = Ω(log(|L|/δ)/ϵ), y * K = y * holds with probability at least 1 -e -λn/2 -δ. The proof of this corollary can be found in Appendix D.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Tree-of-Thought (ToT)</head><p>Recall that SC-CoT samples multiple parallel reasoning paths and performs a selection in the last step. Tree-of-Thought <ref type="bibr" target="#b70">(Yao et al., 2023)</ref> instead proposes to include selection in each step. In this setup, the goal is to generate a reasoning path z test 1:H that maximizes a task-specific value function V * θ . We define this population problem as follows. Population Problem. The goal of ToT is to select the optimal reasoning path that solves a desired task. Mathematically, for each step h, let t h = (z 0 , . . . , z h ) denote the partial history up to step h. Let V θ * be a function that maps each partial history to a value in [0, 1]. Intuitively, V θ * can be viewed as the success probability of the partial history for solving task θ * . Starting from z test 0 , the optimal reasoning path is obtained by solving</p><formula xml:id="formula_40">t test, * h = (t test, * h-1 , z test, * h ), where z test, * h = argmax z test h V θ * (t test, * h-1 , z test h ), t test, * 0 = z test 0 . (5.3)</formula><p>Moreover, let P(z test 0:H = • | θ * ) be the task-specific distribution of the multi-step latent variable model defined in (4.2). At the population level, the goal is to draw samples from such a distribution, and select the optimal reasoning path according to the value function V θ * . In the following, we condition on prompt CoT (n), and thus the optimal reasoning path z test, * 0:H can be regarded fixed.</p><p>Tree-of-Thought with Breadth-First-Search. As we do not have access to the distribution P(z test 0:H = • | θ * ), ToT proposes to sample from the LLM and then approximately solve (5.3) via selection. To simplify the notation, for each h ∈ [H], we denote t test h-1 = (z test 0 , . . . , z test h-1 ), which is the partial history of the test example up to step h -1. In step h, instead of passing the complete prompt prompt CoT (n), we truncate each demonstration in prompt CoT (n) up to step h and denote the truncated prompt by prompt h</p><formula xml:id="formula_41">(n) = {z i 0:h | z i 0:H ∈ prompt CoT (n)}. Then the LLM samples z test h ∼ P(• | prompt h (n), t test h-1</formula><p>) and obtain t test h , and so on.</p><p>In the sequel, we only discuss a version of ToT that maintains a candidate set of partial histories T h for each step h, constructed using Breadth-First-Search (BFS). Specifically, the algorithm involves two integer parameters, K and B, which specify the number of samples drawn in each step and the size of each T h , respectively. Let T 0 = {z test 0 }. Suppose T h is already constructed and |T h | = B. Let its elements be denoted by {t 1 h , . . . , t B h }. For any b ∈ [B], the algorithm will include both t b h and prompt h+1 (n) as the prompt sequence and do K i.i.d. one-step reasoning with the perfectly trained LLM, i.e., {z b,i h+1 } K i=1 i.i.d.</p><p>∼ P(• | prompt h+1 (n), t b h ). Thus, we obtain KB partial histories for the (h + 1)-th step:</p><formula xml:id="formula_42">{(t b h , z b,i h+1 ) : i ∈ [K], b ∈ [B]}.</formula><p>Then we sort these partial histories according to the value function V θ * , and define T h+1 as the top B elements. That is,</p><formula xml:id="formula_43">T h+1 = {t b h+1 } b∈[B] = {top B (t b h , z b,i h+1 )'s in terms of V θ * (t b h , z b,i h+1 ), i ∈ [K], b ∈ [B]}. (5.4)</formula><p>Finally, when T H is constructed, we define t H = argmax t H ∈T H V θ * (t H ) and use t H as the final prediction. See Figure <ref type="figure" target="#fig_6">6</ref> for an illustration.</p><p>Compared to SC-CoT, ToT uses a more sophisticated selection method based on the value function. For this to be effective, V θ * need to sign well with the task-specific distribution P(• | θ * ). Recall that for each h we do one-step reasoning for K times and only keep a candidate set of the histories. To ensure that the desired reasoning path is contained in the candidate set, we require that the optimal one-step reasoning z test, * h can be sampled out with high probability for each h ∈ [H], which in turn requires sufficient coverage of t test, * h under P(• | θ * ) which is approximated by the LLM. For example, t test, * h should have sufficient probability under P(• | θ * ), and different tasks should have sufficient separation. We impose the following assumption for theoretical analysis.</p><p>Assumption 5.10. For the given task θ * , we assume the optimal reasoning path t * H is uniquely defined by (5.3). Moreover, we assume that the task θ * is well covered by the pretraining distribution, i.e., π(θ * ) &gt; 0. Furthermore, we assume that the tasks in Θ are well separated such that the following two conditions are satisfied: (i) Task θ * is uniquely identified by the optimal reasoning path, i.e., θ</p><formula xml:id="formula_44">* = argmax θ ′ ∈Θ P(t test, * h | θ = θ ′ ) for each h ∈ [H]; (ii) For any h ∈ [H], there exists λ h &gt; 0 such that H 2 P(Z 0:h = • | θ), P(Z 0:h = • | θ * ) ≥ λ h .</formula><p>for all θ ̸ = θ * , where H(•, •) denotes the Hellinger distance.</p><p>We note that here we require a separation in Hellinger distance for every step h, which is slightly stronger than Assumption 5.4. Moreover, condition (i) above shows that the equivalence classes specified in Definition 5.3 are in fact singletons.</p><p>Proposition 5.11 (Statistical Error of ToT). Consider ToT prompting based on n CoT examples and BFS with B = 1. Let t H be the final output and define λ * = min h∈[H] λ h . Let ϵ ∈ (0, 1) be any sufficiently small number. Under Assumption 5.10, when n is sufficiently large such that</p><formula xml:id="formula_45">n ≥ 1 λ * 2 log H|Θ| + log (1 -π(θ * ))/π(θ * ) + log(1/ϵ) ,</formula><p>then with probability at least 1-e -nλ * /2 , the probability of outputting a suboptimal reasoning path t H decreases exponentially with K. That is, we have</p><formula xml:id="formula_46">P t H ̸ = t test, * H | prompt CoT (n) ≤ H h=1 1 -p * h + ϵp * h K , where p * h = P(z test, * h | t test, * h-1 , θ * ) for each h ∈ [H].</formula><p>This proposition shows that ToT significantly reduces the probability of introducing a suboptimal optimal reasoning path, which decreases exponentially in K. Without the BFSbased selection step, even when n goes to infinity, the probability of generating t test, * H is only</p><formula xml:id="formula_47">h∈[H] p * h .</formula><p>Here we only focus on the simplest case where B = 1, but our analysis can be generalized to B &gt; 1 with some additional effort. We defer the proof to Appendix D.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Selection-Inference (SI)</head><p>Selection-Inference (SI) <ref type="bibr">(Creswell et al., 2022)</ref> is a structured LLM reasoning method that decomposes each step of reasoning into two components -a selection module that retrieves relevant facts from the context and an inference module that predicts the next step solely based on the selected facts. To this end, SI uses an LLM as both a selection module and an inference module through prompting. The selection module extracts information from the reasoning path and the inference module predicts the next reasoning step based on the information extracted from the selection module.</p><p>A Hierarchical Latent Variable Model. In the context of SI, we assume a special case of the model in (4.2) with a hierarchical structure. Specifically, we assume the latent variable θ * has two component θ * = (θ * se , θ * in ) and the examples of reasoning paths are i.i.d. given θ * , which has a prior distribution π. Let {z 0 , . . . , z H } be a reasoning path. We let t h = {z 0 , . . . , z h } be the partial history up to step h. We assume that z h+1 depends on t h only through a subset of t h , denoted by τ h+1 , and τ h+1 ⊆ t h is selected from t h . Specifically, the joint distribution of P(z 0:H | θ * ) is given by</p><formula xml:id="formula_48">z 0 ∼ P(z 0 = • | θ * ), τ h+1 ∼ P(τ h+1 = • | t h , θ * se ), z h+1 ∼ P(z h+1 = • | τ h+1 , θ * in ), (5.5)</formula><p>where t 0 = {z 0 } and t h = t h-1 ∪ {z h }. Intuitively, this model captures the fact that reasoning often involves summarizing existing information and making predictions. The selection module outputs a summary of the existing information that is sufficient for reasoning, and the inference module conducts reasoning based on summarized information. In the example shown in Figure <ref type="figure" target="#fig_7">7</ref>, z 0 contains the background information and a question, τ 1 summarizes part of the information contained in z 0 and generates the first intermediate reasoning step z 1 . Then τ 2 summarizes {z 0 , z 1 } and z 2 is generated from τ 2 , which answers the question. The first inference step z 1 (top green box) uses the information from τ 1 to infer that Alice and Bob are on the same team. Together, τ 1 and z 1 form a single reasoning step. In the second selection step, τ 2 (bottom yellow box) selects information from t 1 = {z 0 , z 1 }, with the first sentence of τ 2 from z 1 and the second from z 0 . The second inference step z 2 (bottom green box) answers the question using only the information provided in τ 2 .</p><p>SI Prompting. The SI prompting method solves a multi-step reasoning problem following the hierarchical structure specified in (5.5), with the unknown task θ * inferred implicitly via in-context learning. Specifically, given a desired task θ * and a query input z test 0 , we sample n i.i.d. samples from the distribution in (5.5), denoted by {z i 0:H , τ i 1:h , } h∈[H],i∈ <ref type="bibr">[n]</ref> . We define S se (n) and S in (n) as</p><formula xml:id="formula_49">S se (n) = {t i h-1 , τ i h } h∈[H],i∈[n] , S in (n) = {τ i h , z i h } h∈[H],i∈[n] ,</formula><p>(5.6)</p><p>where t i h is the partial history of the i-th example. That is, S se (n) and S in (n) contain the demonstration examples for selection and inference, respectively. </p><formula xml:id="formula_50">τ test h ∼ P LLM (• | S se (n), t test h-1 ), z test h ∼ P LLM (• | S in (n), τ test h ), t test h = {t test h , z test h },<label>(5.7)</label></formula><p>for all h ∈ [H]. The final output is y test = z test H . See Figure <ref type="figure" target="#fig_8">8</ref> for an illustration of the prompting process. Notice that when z test 0 , S se (n) and S in (n) are fixed, (5.7) specifies a Markov chain such that the marginal distribution of y test is fully determined by the LLM. We let P SI (y test = • | S se (n), S in (n), z test 0 ) denote such a distribution, which is essentially the estimator constructed by SI prompting.</p><p>We note that SI can be viewed as a generalization of CoT in the sense that there are still H reasoning steps. However, SI has an additional hierarchical structure that selects subsets of the partial histories. We can interpret SI as a version of CoT where each step of CoT is decomposed into two substeps, aiming to conduct Bayesian inference of θ * se and θ * in separately.</p><p>In the following, we will establish the statistical error of the estimator constructed by SI prompting, under the assumption that the underlying data distribution is specified by the model in (5.5) and the LLM is perfectly pretrained. We introduce an assumption in the same vein as Assumption 5.4. Assumption 5.12. Given a target task θ * , let Θ ∁ = Θ\Θ eq (θ * ) denote the complement of the equivalence class of θ * . We assume that there exists a strict separation between the ground truth task θ * and any other task in Θ ∁ . Specifically, there exist positive numbers</p><formula xml:id="formula_51">λ q , λ I , λ S &gt; 0 such that inf θ∈Θ ∁ H 2 P(z 0 = • | θ * ), P(z 0 = • | θ) ≥ λ q , inf θ∈Θ ∁ H h=1 E θ * H 2 P(τ h = • | θ * se , t h-1 ), P(τ h = • | θ se , t h-1 ) ≥ λ S , inf θ∈Θ ∁ H h=1 E θ * H 2 P(z h = • | θ * in , τ h ), P(z h = • | θ in , τ h ) ≥ λ I .</formula><p>This assumption specifies the separation requirements for θ * se and θ * in individually. Based on this assumption, we establish the statistical error of the SI estimator as follows.</p><p>Corollary 5.13 (Sample Complexity of Selection-Inference). Consider SI prompting with n examples whose distribution is given by (5.5) with a given task θ * (θ * se , θ * in ). We assume that Θ is a finite set and the LLM is perfectly pretrained with data according to the model in (5.5) with a prior distribution π. Under Assumption 5.12, we have</p><formula xml:id="formula_52">KL P(y test = • | z test 0 , θ * ), P SI y test = • | S se (n), S in (n), z test 0 ≤ O π(θ * ) -1 δ -2 Θ ∁ 2 e -2λ SI n ,</formula><p>with probability 1 -δ, where λ SI = λ S + λ I + λ q . Here O(•) hides absolute constants and</p><formula xml:id="formula_53">P SI (y test | S se (n), S in (n), z test 0 )</formula><p>is the marginal distribution of y test according to (5.7).</p><p>This corollary shows that the prompting error of the SI decays to zero exponentially fast as n goes to infinity. Moreover, here the exponential factor depends on λ SI , which contains the separation of both the selection and inference parts. We defer the proof to Appendix D.3.</p><p>In summary, in this part, we extend the statistical analysis of the vanilla CoT estimator to three variants of CoT -SC-CoT, ToT, and SI. We interpret these prompting methods as statistical estimators, and establish their statistical errors of them under an ideal case where the pretraining error of the LLM is zero. The analysis can be easily extended to the realistic case with a nonzero pretraining error, which is separately discussed in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Vanilla CoT versus Vanilla ICL and Truncated CoT</head><p>Recall that vanilla ICL is a special case of CoT without intermediate reasoning steps, i.e., H = 1. In the following, we aim to address Question (d) raised in the introduction by directly comparing vanilla ICL and CoT under the same model. We focus on the latent variable model in (4.2). Let θ * denote the task during the prompting stage, and let prompt</p><formula xml:id="formula_54">ICL (n) = {z i 0 , z i H } n i=1 ∪ {z test 0 } and prompt CoT (n) = {z i 0:H } n i=1 ∪ {z test 0 } denote a ICL prompt</formula><p>and a CoT prompt respectively. Thus, vanilla ICL and CoT yield estimators</p><formula xml:id="formula_55">P LLM (y test = • | prompt ICL (n)) and P LLM (y test = • | prompt CoT (n)) respectively.</formula><p>Recall that we show in Section 4.3 that the CoT estimator based on a perfectly pretrained LLM corresponds to a Bayesian model averaging estimator. Such a claim also holds for vanilla ICL. Therefore, we have <ref type="bibr">(5.8)</ref> where P(y test = • | θ, z test 0 ) is the marginal distribution of y test given z test 0 under the model in (4.2) with parameter θ, and π(θ | prompt ICL (n)) is the posterior distribution. We will justify (5.8) in Appendix E.2. The following proposition shows that CoT always outperforms vanilla ICL in an average sense.</p><formula xml:id="formula_56">P LLM (y test = • | prompt ICL (n)) ≈ Θ P y test = • | θ, z test 0 • π(θ | prompt ICL (n))dθ,</formula><p>Proposition 5.14 (CoT Outperforms Vanilla ICL). Let π denote the prior distribution over Θ. Consider the ideal case where the LLM is perfectly pretrained, for any number of demonstration examples n ≥ 0, we have</p><formula xml:id="formula_57">E θ * ∼π E prompt CoT (n)∼P(• | θ * ) KL P LLM (y test = • | z test 0 , θ * ), P y test = • | prompt CoT (n) ≤ E θ * ∼π E prompt ICL (n)∼P(• | θ * ) KL P(y test = • | z test 0 , θ * ), P y test = • | prompt ICL (n) .</formula><p>This proposition shows that averaged over the randomness of the task θ ∼ π and prompts, CoT is at least as good as vanilla ICL. Intuitively, this makes sense because conditioning more information yields a better posterior estimator. Since these estimators can both be interpreted as BMA estimators, having a better posterior leads to a smaller statistical error.</p><p>We can also extend this property to truncated CoT methods, which refers to prompting with demonstrations that omit some intermediate steps. More precisely, let J ⊂ [H -1] contain the indices of intermediate steps that are included in the reasoning path. We define a truncated CoT prompt with n examples as prompt J (n) = {z i 0 , y i } n i=1 ∪ {z i j } j∈J ∪ {z test 0 }. Then vanilla ICL is a special case where all intermediate steps are omitted, i.e., J = ∅, and CoT corresponds to the case where J = [H -1]. We extend Proposition 5.14 to such a general case in Appendix E.2, which shows that including more reasoning steps in the prompt is always beneficial in an average sense.</p><p>However, we would like to emphasize that the dominance of CoT over vanilla ICL does not hold pointwisely for an arbitrary task θ * ∈ Θ. In other words, it is possible that there exists a task θ * and n prompt examples such that CoT is worse than vanilla ICL. Intuitively, this happens when the intermediate reasoning paths are not sufficiently informative. This phenomenon is empirically observed in <ref type="bibr" target="#b28">Lanham et al. (2023)</ref> on the HellaSwag benchmark <ref type="bibr" target="#b72">(Zellers et al., 2019)</ref>. In the following, we also provide numerical experiments based on a specially designed toy task to illustrate this fact.</p><p>Vanilla ICL vs. COT on the CityEquation Task. We handcraft an arithmetic reasoning task named "CityEquation", which involves solving arithmetic calculations based on city names. Each equation involves addition (+) or the minus (-) operations between city names, where the output is obtained by evaluating the formula with city names substituted by their longitudes. For instance, "Paris + Beijing = 118" because the longitudes of Paris and Beijing are 2 and 116 respectively.</p><p>Data Construction. We choose 20 major cities around the world, and generate random city equations by randomly selecting two cities and an operation in {+, -}. We construct the test data set using 200 distinct equations and use another 10 different equation as the examples in the prompting stage.</p><p>Prompting Methods and Results. We test five prompting methods: vanilla ICL and four CoT variants. We consider an informative version of CoT that includes the full reasoning path and four partially informative versions that either contain some irrelevant facts or omit some relevant intermediate steps. In particular, in partially informative CoT-(b), we include some demographic information of the cities in the equations, which, although truthful facts, are not related to longitudes, which is the key to getting the final answer. Then the last two versions additionally include some useful reasoning steps. See Table <ref type="table">2</ref> for an example in the prompts and Appendix I for more details. When evaluating these methods, we include 10 examples in the prompt, followed by a new testing instance. The prompt is passed to GPT-4 <ref type="bibr" target="#b0">(Achiam et al., 2023)</ref> with the temperature set to zero, and the reported answer is compared with the desired answer to evaluate the accuracy. We report the average accuracy over 200 random testing instances in Table <ref type="table" target="#tab_3">3</ref>.</p><p>As shown in this table, informative CoT achieves the highest accuracy at 81.5%, the four versions of partially informative (PI) CoT have accuracy levels of 70.5%, 2.5%, 66%, and 80%, respectively In conclusion, CoT prompting can perform worse than vanilla ICL. While CoT offers more information, its effectiveness depends on the relevance of the additional information provided in intermediate steps. Truncated informative CoT slightly enhances vanilla ICL's accuracy by hinting at relevant details like longitude. Informative CoT performs even better by outlining the entire reasoning process. In contrast, uninformative CoT, despite providing more information, introduces irrelevant details that disrupt performance. Partially informative CoT combines useful and irrelevant information, resulting in accuracy between that of truncated informative CoT and uninformative CoT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Statistical Errors of CoT with Pretraining Errors</head><p>Recall that in Lemma 5.2 we decompose the CoT error err CoT into (i) a pretraining error (5.2) and (ii) a prompting error (5.1). We have analyzed the prompting error in Section 5. In this section, we establish a statistical analysis of the pretraining error and then obtain a Type An Example Vanilla ICL Q: "London -Lagos" A: "-3." Informative CoT Q: "London -Lagos" A: 'Using the longitudes of cities, the equation "London -Lagos" translates as "London" = 0, "Lagos" = 3. Here the longitudes of the western hemisphere are negative numbers. And we round the coordinates to the nearest integer. This gives the result.</p><p>The answer is -3." Partially informative CoT-(a)</p><p>Q: "London -Lagos"A: "London has longitude: 0. The answer is -3." Partially informative CoT-(b)</p><p>Q: "London -Lagos" A: "London is home to approximately 9 million residents, with 59.8 percent being White, 18.5 percent Asian, 13.3 percent Black, 5 percent Mixed, and 3.4 percent identifying as Other. The answer is -3." Partially informative CoT-(c) Q: "London -Lagos" A: "London is home to approximately 9 million residents, with 59.8 percent being White, 18.5 percent Asian, 13.3 percent Black, 5 percent Mixed, and 3.4 percent identifying as Other. London has longitude: 0. The answer is -3." Partially informative <ref type="bibr">CoT-(d)</ref> Q: "London -Lagos" A: "London is home to approximately 9 million residents, with 59.8 percent being White, 18.5 percent Asian, 13.3 percent Black, 5 percent Mixed, and 3.4 percent identifying as Other. London has longitude: 0. Lagos has longitude: 3. The answer is -3." Table <ref type="table">2</ref>: An example of the five prompting methods evaluated on the CityEquation task.</p><p>complete characterization of the error of the estimator constructed by "pretrained LLM + CoT prompting". Thus, in this section, we answer Question (b) raised in the introduction. We rigorously describe the pretraining process in Section 6.1. Next, in Section 6.3, we construct a class of transformer networks that directly approximates the underlying distribution P. We characterize the pretraining error in Section 6.2 and establish the statistical errors of CoT under realistic assumptions in Section 6.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Setup of LLM Pretraining</head><p>Recall that we introduce the pretraining of LLM in Section 4.2. We consider the pretraining of an autoregressive LLM with data sampled from the model in (4.2). The LLM is a transformer that maps a sequence of reasoning steps to a probability distribution. Here queries and keys share the same dimension to calculate the inner product. We assume that d v = r to guarantee that the output dimension is the same as the input dimension, which avoids defining the dimensions for modules in all the layers. The results for the general case can be easily generalized. For the FF layer in (3.3), the dimensions of the hidden feature and the output are d F and r, respectively. Both components have residual connections, followed by layer normalization. See Figure <ref type="figure">9</ref> for an illustration of the transformer architecture, where the transformer block consisting of a MHA and a FF layer is illustrated on the right.</p><p>In terms of the network parameters, for any d ∈ [D], we let</p><formula xml:id="formula_58">W d mha = (W Q,d i , W K,d i , W V,d i ) η i=1</formula><p>denote the weight matrices of the η heads, and let W d ff,1 and W d ff,2 denote the weight matrices of the FF layer. The mathematical expressions of MHA and FF layers are given in (3.2) and (3.3). We adopt γ d 1 , γ d 2 ∈ R r×r to denote the parameters of the residual links in the d-th module. Moreover, for the output softmax layer, we fix the temperature as τ and let W softmax denote the weight matrix. For ease of presentation, we defer the mathematical details of the transformer to Appendix G.1. We let ρ denote all the network parameters of the transformer. Furthermore, we consider a bounded transformer class with parameters bounded in</p><formula xml:id="formula_59">P LLM = ρ : ∥γ d 1 ∥ ∞ , ∥γ d 2 ∥ ∞ ≤ 1, ∥W Q,d i ∥ F , ∥W K,d i ∥ F , ∥W V,d i ∥ F ≤ B M , ∥W d ff,1 ∥ F , ∥W d ff,2 ∥ F ≤ B F , ∥W softmax ∥ 1,2 ≤ B S , ∀d ∈ [D], i ∈ [η] ⊆ T F(D, η, r, d F , d k , d v ), (6.1)</formula><p>where B M , B F , B S are the upper bounds are upper bounds of the norm of weight matrices.</p><p>We assume these parameters are fixed and larger than one.</p><p>Pretraining Data and MLE Estimation. </p><formula xml:id="formula_60">k,ℓ = (z k,ℓ 0 , • • • , z k,ℓ</formula><p>H ) denotes the k-th example of the task θ * ℓ , and we view each z k,ℓ j as a reasoning step. Thus a sequence contains T (H + 1) elements in total. For any h ≥ 0, the reasoning steps before z t,ℓ h is denoted by S t,ℓ h = (Υ t-1,ℓ , {z t,ℓ j } h-1 j=0 ), where Υ t-1,ℓ contains the first t -1 examples of the ℓ-th trajectory. Then we can write the dataset D N,T as {(S t,ℓ h , z t,ℓ h )} H,T,N h=0,t=1,ℓ=1 . The pre-trained LLM, denoted by ρ, as defined in (4.3), is obtained by solving the maximum likelihood estimation (MLE) based on the dataset D N,T . We set P LLM = P ρ , where P ρ denotes the conditional distribution specified by the transformer with parameter ρ. We neglect the optimization issue and assume that the MLE in (4.3) can be obtained. We note that when the transformer class is sufficiently expressive, we expect that P LLM learns the conditional distribution of z t,ℓ h given S t,ℓ h , which is given in (4.5). We note that in the pretraining process described above, we train a transformer that takes all sequences of the reasoning steps S ∈ L * as input and predicts the next reasoning step z ∈ L. This setup can be easily generalized to the autoregressive prediction of the next token instead of the next reasoning step based on the prompt.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Pretraining Error Analysis</head><p>We will show that the pretraining error can be written as a sum of an approximation error and a generalization error. The analysis is based on the PAC-Bayes framework <ref type="bibr" target="#b34">(McAllester, 1998;</ref><ref type="bibr" target="#b4">Alquier, 2021)</ref>. Before presenting this result, we introduce two regularity assumptions as follows.</p><p>Assumption 6.1. Note that we assume that each reasoning step in L is identified with a unique Euclidean vector. We assume that L is a bounded set. That is, there exists R &gt; 0 We assume that there exists R &gt; 0 such that ∥z∥ 2 ≤ R for all z ∈ L.</p><p>This assumption ensures that the input space of the transformer network is bounded, which is commonly imposed by the literature on nonparametric statistics <ref type="bibr" target="#b73">(Zhang et al., 2023a)</ref>. Assumption 6.2. For the model in (4.2), we assume that for any z ∈ L, θ ∈ Θ, and any sequence of reasoning steps S ∈ L * , P(z | S, θ) &gt; c 0 for some constant c 0 &gt; 0.</p><p>This assumption requires the conditional probability of the next reasoning step z to be lower-bounded at any element of L. This means that the generation of the reasoning path is stochastic. Similar assumptions have also been imposed in existing works <ref type="bibr" target="#b68">(Xie et al., 2021;</ref><ref type="bibr" target="#b23">Jiang, 2023)</ref>. As we will see in Appendix F.1, this assumption implies Assumption 5.1 with b</p><formula xml:id="formula_61">* = log(max{c -1 0 , 1 + |L| exp(B S /τ )})</formula><p>, where B S appears in (6.1). Besides, to simplify the notation, we let E S∼D denote the empirical distribution with respect to the pretraining data set D N,T . Specifically, for any function f : L * → R we define</p><formula xml:id="formula_62">E S∼D [f (S)] = N -1 (H + 1) -1 • T -1 N ℓ=1 T t=1 H h=0 E[f (S t,ℓ h )], (6.2)</formula><p>where the expectation is taken with respect to the joint distribution of D N,T . We establish the pretraining error in the following proposition.</p><p>Proposition 6.3 (Pretraining Error Bound). Under Assumptions 6.1 and 6.2, with probability at least 1 -δ, the pretrained LLM P ρ in (4.3) satisfies that</p><formula xml:id="formula_63">E S∼D TV P(• | S), P ρ (• | S) = O inf ρ * ∈P LLM E S∼D KL P(•|S), P ρ * (•|S) + √ b * log(T H/δ) N 1/4 approximation error + 1 √ N D log(1 + N T H B) + log T H δ generalization error , where B = τ -1 RηB S B 2 F B 3</formula><p>M and D = D 2 r(d F +d k +r)+r•d y are parameters determined by the transformer architecture in (6.1). Besides, we have b * = log max{c -1 0 , 1 + |L| exp(B S /τ )} . We use ∆ pre (N, T, δ) to denote the right-hand side of this equation. This Proposition is proved using the PAC-Bayes framework. The proof is adapted from <ref type="bibr" target="#b73">Zhang et al. (2023a)</ref> and deferred to Appendix F.1. Proposition 6.3 shows that the pretraining error can be decomposed into an approximation error and a generalization error. The approximation error is a sum of a KL divergence term, and an additional N -1/4 terms that arise from concentration. The approximation error is small if the transformer class is sufficiently expressive. Moreover, the generalization error decays to zero as N increases, and D captures the complexity of the transformer model. This error increases with the sequence length T • H mildly through a logarithmic factor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Transformers as Conditional Distribution Approximators</head><p>In the following, we present the approximation result. We will construct a transformer with parameters in P LLM that captures the multi-step reasoning structure of CoT. More importantly, we will prove that the approximation error decays to zero exponentially as the network depth D increases. We first present an informal version of the theory as follows.</p><p>Proposition 6.4 (Approximation Error, Informal). Let S t h = (Υ t-1 , {z t j } h-1 j=0 ) be the sequence of reasoning steps that includes t -1 examples of reasoning paths Υ t-1 and the first h -1 steps of the t-th example {z t-1 j } h j=0 . Then if the target distribution P of the model in (4.2) has a sufficiently smooth density and the transformer model P LLM in (6.1) is sufficiently expressive, then there exists a transformer with at most O(D) number of blocks and parameter ρ * ∈ P LLM such that max</p><formula xml:id="formula_64">S t h ∈L * KL P(z t h = • | S t h ), P ρ * (z t h = • | S t h ) = O exp - D -C log(2H))/H 1/4 5B ,</formula><p>for any t ∈ [T ] and 0 ≤ h ≤ H when D goes to infinity. In particular, C &gt; 0 is an absolute constant, and B appears in Assumption F.5.</p><p>This proposition shows that the approximation error decays exponentially in D. This exponential accuracy is based on the construction of a neural network approximator in Elbrächter et al. ( <ref type="formula">2021</ref>) for smooth functions. Moreover, note that S t h has (t -1) • (H + 1) + h reasoning steps in total. An appealing feature of this proposition is that the approximation error is independent of t, thanks to leveraging the permutation invariance structure of the target distribution. Specifically, when viewing P(z t h = • | S t h ) as a function of S t h , it is invariant to the permutation of the t -1 examples. Our transformer approximator directly leverages such invariance in the attention mechanism, thus obtaining an approximation error independent of t. However, P(z</p><formula xml:id="formula_65">t h = • | S t h ) can be drastically different across h ∈ [H].</formula><p>Concretely, each reasoning step represents a different procedure described by different distributions. To handle this fact, our transformer treats each step h ∈ [H] differently and uses a separate transformer subnetwork to predict each z h . These subnetworks, each containing multiple attention blocks, are stacked vertically. And we leverage the position embedding to let the transformer identify the step index h of S t h , and then pass the input to the h-th subnetwork. See Figure <ref type="figure" target="#fig_11">10</ref> for an illustration of the construction. The formal statement of Proposition 6.4 and its detailed proof are deferred to Appendix F.6. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Statistical Error of CoT with Out-of-Distribution Queries</head><p>In this section, we combine the analysis of pretraining error and prompting error to derive a comprehensive characterization of the statistical error of CoT. Moreover, we will tackle the query error, which arises due to the distributional shift of the test instance z test 0 . Specifically, the query error arises if z test 0 is not sampled from the same task θ * as the n prompt examples. For instance, if the examples in the prompts are about the "solving arithmetic problems", but we query a new philosophical question. Then the knowledge incorporated in the examples is not useful for answering the query and thus we expect a large error. More rigorously, let</p><formula xml:id="formula_66">Υ n = {s j } n j=1 denote the n CoT examples sampled the model in (4.2) with task θ * . Thus, prompt CoT (n) = (z test 0 , Υ n ). Under this model, the query has a distribution P(z test 0 = • | θ * ). We let µ(z test 0 = • | Υ n ) denote</formula><p>the distribution of an out-of-distribution (OOD) query, whose distribution might depend on Υ n . The difference between these two distributions reflects the query error.</p><p>Besides, we let P CoT denote the joint distribution of Υ n and an OOD query, i.e.,</p><formula xml:id="formula_67">P CoT (prompt CoT (n) | θ * ) = P(Υ n | θ * ) • µ(z test 0 | Υ n ).</formula><p>We make the following assumption about the distributional shift due to the OOD query. Assumption 6.5. We assume the distributional shift is mild in the sense that P CoT is covered by the pretraining distribution. That is, for any fixed θ * ∈ Θ, there exists a constant κ &gt; 0 such that</p><formula xml:id="formula_68">P CoT (prompt CoT (n) | θ * ) ≤ κP(prompt CoT (n) | θ * ) for any prompt CoT (n) ∈ L * with n ≤ T .</formula><p>Here κ captures the magnitude of the distributional shift. This assumption requires that the test query z test 0 cannot be too arbitrary -its distribution should have sufficient density under the pretraining distribution. Intuitively, we cannot expect the LLMs to answer questions beyond the knowledge contained in the pretraining dataset. Note that when there is no distributional shift in the query, we have κ = 1. Then the analysis of err prompt is reduced to Theorem 5.5. Recall that the pretraining data distribution mixes the task distribution θ ∼ π. Under the model in (4.2), this assumption is satisfied if we set</p><formula xml:id="formula_69">κ = sup z∈L,S∈L * µ(z test 0 = z | Υ n = S)/P(z test 0 = z | θ * ) ,</formula><p>which is no more than 1/c 0 under Assumption 6.2. Thus, the distributional shift is small if task θ * is well covered in the pretraining distribution, and the distribution µ is similar to the true query distribution P(z</p><formula xml:id="formula_70">test 0 = • | θ * ).</formula><p>Combining Pretraining and Prompting Errors. With this assumption, we combine Theorem 5.5, Proposition 6.4 and Proposition 6.3 to obtain a complete characterization of the statistical error of CoT --the statistical estimator obtained by first pretraining an LLM using dataset D N T and then prompting the pretrained LLM using a CoT prompt with n examples. The result is given in the following corollary. Corollary 6.6 (Complete Characterization of err cot ). Recall that err CoT is defined in (4.4). Under Assumptions 5.4, 6.1, 6.2, and 6.5, with probability at least 1 -δ, we have</p><formula xml:id="formula_71">E P CoT [err CoT ] ≤ O Hb * π(Θ ∁ ) π(θ * ) C(θ * )κ 1/4 • Θ ∁ 1/2 • exp(-nλ/2) + κT Hπ(θ * ) -1 • (1 + b * ) • ∆ pre (N, T, δ)</formula><p>when Θ is finite, where C(θ * ) is defined as</p><formula xml:id="formula_72">C(θ * ) = sup θ∈Θ ∁ χ 2 P(z test 0 = • | θ), P(z test 0 = • | θ * ) + 1.</formula><p>In this corollary, we combine the previous result for the perfectly pretrained model in Theorem 5.5 with the pretraining error analysis in Proposition 6.3, and the query error that quantifies the distributional shift of the prompt. The proof can be found in Appendix F.4. For conciseness, we stated the result for the case where Θ is discrete and finite. This can be generalized the result to the continuous case by applying the second part of Theorem C.1. This corollary shows that, when the distributional shift is mild (κ = O(1)) and |Θ| is finite, to achieve any desired accuracy level ε ∈ (0, 1), it suffices to let:</p><formula xml:id="formula_73">• D = H 5B • log κT Hπ(θ * ) -1 • 3(1 + b * )/ϵ 4 + C log(2H)</formula><p>for the transformer depth, where the absolute constant C &gt; 0 is from Proposition 6.4.</p><formula xml:id="formula_74">• n ≥ 2/λ • log 3Hb * π(Θ ∁ ) • (π(θ * )) -1 • C(θ * ) • κ 1/4 • Θ ∁ 1/2 /ϵ in prompt CoT (n). • T ≥ n + 1, and N ≥ 3 √ b * • log(T H/δ) • κT Hπ(θ * ) -1 • (1 + b * )/ϵ 4 in the pretraining dataset D N,T .</formula><p>Then Corollary 6.6 shows that E P CoT [err CoT ] ≤ ε with probability at least 1 -δ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Experiments</head><p>In this section, we provide empirical evidence to support the theory. We validate the statistical errors of the CoT estimator and compare it with vanilla ICL. To this end, we train transformer models from scratch, where the training data is sampled from random regression tasks satisfying (4.2). The pretrained transformer is then tested on a new task via prompting. We present the experiment results as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Experiment Settings</head><p>Regression Tasks. We consider the in-context regression problem <ref type="bibr" target="#b14">(Garg et al., 2022)</ref> where the goal is to learn a class of functions F = {f (•; θ), θ ∈ Θ)} via prompting a pretrained transformer. Here, f (•; θ) is a function with parameter θ. We consider two types of Ftwo-layer neural networks (NNs) and decision trees. For both cases, for any θ, we generate input-output examples of form (x, f (x, θ)), together with a single intermediate step, i.e., H = 2. Here we assume x ∼ N (0, I d in ) where d in = 10 for two-layer NNs and d in = 20 for decision trees. In other words, each CoT example is of the form {x, z, f (x; θ)} and each ICL example is of the form {x, f (x; θ)}, where z is the intermediate step.</p><p>Two-Layer Neural Networks. For a a two-layer NN, we write θ = {W, v} where v ∈ R 4 and W ∈ R 10×4 . Under the prior distribution, W and v are independent, with v ∼ N (0, I 4 ) and</p><formula xml:id="formula_75">W ij i.i.d ∼ N (0, 1/2) for all i ∈ [10] and j ∈ [4]. The neural network output is f (x; θ) = v T ReLU(W x), where ReLU(x) = max(0, x) is the ReLu activation function. We consider two kinds of CoT examples, CoT 1 : {x, ReLU(W x), f (x; θ)} and CoT 2 : {x, v, f (x; θ)}.</formula><p>Decision Tree. We let θ = {c j , o j } j∈ <ref type="bibr">[15]</ref> denote the parameters of binary decision tree of depth four, where c j ∼ Unif <ref type="bibr">([20]</ref>) and o j i.i.d ∼ N (0, 1). The output f (x; θ) with input x ∈ R 20 is defined as follows. Let c 1 correspond to the root node, c 2 , c 3 be nodes of the second layer, c 4 , . . . , c 7 be nodes of the third layer, and c 8 , . . . , c 15 be nodes of the last layer. Each c j indexes a coordinate of x and o j is the corresponding target value. Note that d in = 20, and thus we sample each c j uniformly over <ref type="bibr">[20]</ref>. To evaluate the decision tree, starting from the root node c 1 , if x[c 1 ] &lt; 0, we go to the left child c 2 . Otherwise, we go to the right child c 3 . Here we let x[i] denote the i-th coordinate of x. Then we continue to look at the sign of c 2 or c 3 and go to a child node in the third layer. We continue this process until a leaf node, i.e., a node in the last layer, is reached, and we output the corresponding o j . In other words, at each level, we look at the sign of the corresponding coordinate of the input x move to a child. The output f (x, θ) corresponds to the number in {o 8 , . . . , o 15 } corresponding to the leaf node that is reached. The intermediate reasoning steps correspond to the four entries of x used to make decisions. Thus, a CoT example is of the form {x, x[node 1 ], . . . , x[node 4 ], f (x, θ)}, where node 1 , . . . , node 4 are the four nodes of the selected path, including the root and leaf nodes.</p><p>Transformer Model. We train a decoder-only transformer from the GPT-2 family <ref type="bibr" target="#b44">(Radford et al., 2019)</ref> separately for CoT and ICL. We construct pretraining datasets following the setting in Section 6.1. That is, we sample N i.i.d. tasks {θ * ℓ } ℓ∈ <ref type="bibr">[N ]</ref> and T examples from each task, where T = 101 and N = 2.56 × 10 7 . Then we build a loss function similar to that in (4.3), but with the negative loglikelihood replaced by the mean-squared error. The loss function is optimized using the Adam algorithm <ref type="bibr" target="#b25">(Kingma and Ba, 2014)</ref> for 4 × 10 5 steps, with the batch size set to 64. We let TF CoT and TF ICL denote the learned transformer model. Evaluation. To evaluate the performances, we sample a random task θ * and n i.i.d. exam-</p><formula xml:id="formula_76">ples {x i , f (x i ; θ * )} i∈[n]</formula><p>, and ask the pretrained transformers to predict f (x test ; θ * ) on a new i.i.d. input x test . We include intermediate steps to these n examples to obtain CoT prompt. We evaluate the performance of CoT and ICL via the mean-squared error (MSE):</p><formula xml:id="formula_77">MSE CoT = TF CoT prompt CoT (n) ∪ { z} -f (x test ; θ * ) 2 , MSE ICL = TF ICL prompt ICL (n) -f (x test ; θ * ) 2 ,</formula><p>where z = TF CoT prompt CoT (n)) is the intermediate step predicted by the transformer on the test example. We compute the MSE by averaging over 100 independent experiments with a fixed θ * for each n, and we test for all n in {1, . . . , 100}. We plot the final MSE by further averaging over 10 independent θ * 's.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Experiment Results</head><p>Two-Layer Neural Network. We plot the errors of the CoT and ICL estimators in Figure <ref type="figure" target="#fig_12">11</ref> against the number of demonstration examples n ∈ [100]. The curves with labels "CoT 1 " and "CoT 2 " are the transformer trained using the two kinds of CoT prompts, respectively, and the label " vanilla ICL" stands for the error of the vanilla ICL. In Figure <ref type="figure" target="#fig_12">11-(a)</ref> we plot the MSE of these methods. In all these three cases, MSE decays rapidly to zero as n increases. Moreover, we observe that CoT 1 method with z = ReLU(W x) exhibits significant improvement over vanilla ICL. Moreover, CoT 2 only slightly improves upon vanilla ICL, which shows that the quality of intermediate steps is crucial to the success of CoT. This finding coincides with our theory and experiment in Section 5.3. A plausible explanation for the superiority of CoT 1 over CoT 2 is that the major challenge of learning a two-layer In this case, the pretraining error is not negligible and thus the linear trend stops. Thus, Figure <ref type="bibr">(b)</ref> shows that the statistical error of these methods decays exponentially in n up to a pretraining error. This observation corroborates Theorem 5.5.</p><p>Decision Tree. We plot the errors of CoT and vanilla ICL in Figure <ref type="figure" target="#fig_3">12</ref>, with (a) and (b) showing the MSE and its logarithm respectively. In both cases, MSE decays rapidly as n increases and there is a strong linear trend when n ≤ 65. This aligns with our theory in Theorem 5.5, and the MSE after n ≥ 65 is close to the pretraining error. Moreover, we observe that the transformer trained via the CoT method learns faster than that trained by vanilla ICL, but they achieve similar accuracy when n becomes large. When given n = 100 in-context demonstrations, both models give a testing loss of around 0.12. In summary, our experiments on the two-layer neural network and decision tree tasks validate the statistical error of the "pretraining LLM + CoT prompting" approach. We empirically validate the exponential error decay with respect to the number of prompt examples, and we show whether CoT significantly outperforms vanilla ICL depending on the choice of intermediate reasoning steps. We leave further details of the experiment setup in </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Additional Details of Numerical Experiments</head><p>In the following, we present the details of the numerical experiments in Section 7.</p><p>Training Data and Algorithm. The transformer models are pretrained using the Adam algorithm <ref type="bibr" target="#b25">Kingma and Ba (2014)</ref>, which is a minibatch and stochastic-gradient-based algorithm. We set the batch size to 64, and in each step, we sample new training data from the model in (4.2). That is, we sample 64 random tasks from the prior distribution, and T = 101 examples from each task. In each task, we pack the T examples into a single trajectory, and build the MSE loss function by predicting the next step autoregressive. For vanilla ICL, there are 2T steps in total, and for CoT, there are 3T steps in total. We run Adam for 4 × 10 5 steps in total, and thus the total tasks sampled is equal to N = 2.56 × 10 7 . Moreover, when implementing Adam, we set the learning rate (stepsize) to be 10 -5 and the momentum parameter to be (0.9, 0.999). GPT-2 Transformer Model. We adopt the GPT-2 transformer architecture. Here the transformer model reads in in a sequence of input vectors in R r and produces an output vector in the same space, where r is the embedding dimension. Additionally, to handle the vector-valued inputs with different sizes, we adopt a universal read-in function that maps the prompts into the latent embedding space of the transformer through a (learnable) linear transformation and we use separate read-out functions for the predictions of inputs x ∈ R d in , Figure <ref type="figure" target="#fig_4">13</ref>: An illustration of the GPT-2 based transformer model. We begin by embedding the input prompt using a universal read-in function. The embedding is then processed by our backbone model, which is based on the GPT-2 architecture. We extract the last hidden state and apply separate read-out functions for inputs x, intermediate steps z, and outputs y. During testing, we concatenate the inferred z test with prompt CoT (n) to infer y test . intermediate steps z ∈ R 4 , and outputs y ∈ R. Here d in = 10 for two-layer NNs and d in = 20 for decision trees. These read-in and read-out functions are all linear layers. Between these read-in and read-out functions are multiple transformer blocks stacked vertically. The details of these transformer blocks are introduced in Section 6.1 and Appendix G.1. See Figure <ref type="figure" target="#fig_4">13</ref> for an illustration of the transformer architecture.</p><p>For the two-layer NN task, we adopt GPT-2 transformer models with D = 12 layers, η = 4 heads, and an embedding dimension of r = 128. The embedding dimension of the queries, key, and values are 32. For the decision tree task, we adopt GPT-2 transformer models consisting of D = 12 layers, η = 8 heads, and an embedding dimension of r = 512. The embedding dimension of the queries, key, and values are blue 32.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>In this paper, we explore the theoretical underpinnings of CoT prompting and its variants through a statistical lens. In particular, under a latent variable model that depicts multistep reasoning, we showed the estimators induced by CoT prompting on a pretrained LLM is approximately equivalent to a Bayesian estimator. More importantly, we prove that the statistical error of CoT can be upper bounded by a sum of pretraining error and promoting error, and we explicitly analyze them separately. In particular, we prove that prompting error decreases exponentially with the increasing number of demonstrations included in the prompt, and the statistical error of the pretrained LLM is analyzed under the PAC-Bayes framework. We also extend our analysis to various CoT variants and establish exponential rates of convergence. Moreover, we establish both theoretical and empirical comparisons between CoT and vanilla ICL, which shed new light on the role played by intermediate reasoning steps. In future work, we hope to extend our theoretical framework to better understand other prompting methods beyond CoT, shedding light on their effectiveness and potential for improvement. Chen, W., Ma, X., <ref type="bibr">Wang, X. and Cohen, W. W. (2022)</ref> </p><formula xml:id="formula_78">θ * ∼ π, z i 0 = f θ * {s j } i-1 j=1 , ζ i , u i h = g θ * {u j k } i-1,H j=1,k=1 , u i 1 , • • • , u i h-1 , ξ i h , z i h = F (z i 0 , • • • , z i h , u i h , ϵ i h ), ∀h ∈ [H]. (A.1)</formula><p>Here f θ * and g θ * are functions depending on θ * , ζ i , ξ i h , and ϵ i h are independent noise terms, and F is a function that does not depend on θ * . The key assumption of (A.1) is that each reasoning step z i h depends on the task parameter θ * only through a latent variable u i h , and these latent variables {u i h } H,n h=1,i=1 form a dynamical system in the latent space. The evolution of u i h depends on θ * and all the latent variables of the previous examples {s j } j&lt;i . Thus, each z i h is allowed to implicitly depend on the previous examples as well. Such a latent dynamical system captures the fact that the demonstration examples are created with a dependent structure. In comparison, in the simpler model in (4.2), the examples are i.i.d. given θ * . We recover the simpler model in (4.2) by setting u i h = θ * and z i 0 = f θ * ζ i in (A.1). Intuitively, θ * represents the latent concept specifying the task, such as "calculate twice the area code of the given country," "solve an arithmetic problem," or "write a science fiction novel." In (A.1), we model LLM-based reasoning as a hierarchical process: first, we generate a sequence of task-specific latent "goals" {u i h } h∈[H] , then we translate these goals into natural language {z i h } h∈ <ref type="bibr">[H]</ref> . Each goal corresponds to a specific reasoning step. The sequence of latent variables completely determines the reasoning process, and thus we assume F in (A.1) does not involve θ * . See Figure <ref type="figure" target="#fig_14">14</ref> for an illustration of this model.</p><p>As a concrete example, consider θ * as the task of "solving an arithmetic problem". The problem might be described in a specific context, such as using the number of apples. However, the underlying reasoning process is independent of this context and relies solely on a sequence of arithmetic operations. These operations can be viewed as the latent variables u i h in our model, while describing them in the context of apples can be seen as generating the natural language z i h from these latent variables. Additionally, the random variables ζ i , ϵ i h (for h ∈ [H]) determine how the arithmetic formula is contextualized. The random variables ξ i h introduce stochasticity into the reasoning process. See Figure <ref type="figure" target="#fig_5">15</ref> for an illustration.</p><p>We remark that our model is a general formulation that recovers many existing models proposed in the existing works. Specifically, we recover the models in Jiang (2023) by setting Finally, some of our results extend to the generalized model A.1. Specifically, Lemma B.1 demonstrates that LLMs perform BMA during CoT under this generalized model. In Section 6, we describe the pretraining process and analyze its performance within the framework of the generalized model (A.1). Figure <ref type="figure" target="#fig_5">15</ref>: An example of 2 demonstrations generated from the generalized model A.1, where we omit some edges in the graph to simplify the representation. In this example, we want to highlight two features of our model. The first and second rows correspond to two demonstrations. Note that the reasoning steps {z i h } depend on latent concept θ * implicitly via latent variables {u i h }, which give specific instructions for each step in the corresponding example. Furthermore, the latent variables u 1 1:3 are different from u 2 1:3 , providing more flexibility and diversity among the demonstrations. In addition, the second query z 2 0 is not independent of the previous demonstration z 1 0:3 since it is a more complicated version of an arithmetic problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Proofs of the Results in Section 4</head><p>In this section, we prove the results in Section 4. We first prove Lemma 4.1 and its extension to the generalized multi-step latent variable model and then prove Proposition 4.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Proof of Lemma 4.1 and Its Extension</head><p>We prove Lemma 4.1 and introduce its extension to the generalized multi-step latent variable in (A.1) with a proof.</p><p>Proof of Lemma 4.1. When N goes to infinity, we only need to consider the population counterpart of the likelihood loss in (4.3), which can be written as</p><formula xml:id="formula_79">L(ρ) = - 1 T (H + 1) t∈[T ] H h=0 E log P ρ z t h | Υ t-1 , {z t j } h-1 j=0 (B.1) = 1 T (H + 1) t∈[T ] H h=0 E KL P • | Υ t-1 , {z t j } h-1 j=0 ∥ P ρ • | Υ t-1 , {z t j } h-1 j=0 + Const,</formula><p>where Const denotes a constant that does not depend on ρ. Here the second equality follows from the definition of KL divergence. When the LLM class is sufficiently expressive, i.e.,</p><formula xml:id="formula_80">P ∈ {P ρ | ρ ∈ P LLM }, for any (Υ t-1 , {z t j } h-1 j=0</formula><p>) that has nonzero density in the pretraining distribution, the minimizer of L(ρ) in (B.1) must satisfy</p><formula xml:id="formula_81">P LLM z t h = • | Υ t-1 , {z t j } h-1 j=0 = P z t h = • | Υ t-1 , {z t j } h-1 j=0 (B.2)</formula><p>for all h ∈ [H] and t ∈ [T ], where we let P LLM denote the distribution induced by the minimizer of L(•). Here (B.2) holds for all prompts Υ t-1 ∪{z t j } h-1 j=0 ∈ L * with a nonzero density under P. Thus, (B.2) shows that the perfectly pretrained LLM matches the pretraining distribution of predicting the next reasoning step.</p><p>In the following, we will prove the desired result by generalizing (B.2) to the next multistep reasoning. Now, when we generate the answer y test given the CoT prompt using P LLM , when prompt CoT (n) has a positive density under P, by (B.2) we have</p><formula xml:id="formula_82">P LLM z test 1 = • | prompt CoT (n) = P z test 1 = • | prompt CoT (n) . (B.3)</formula><p>Let L 1 ⊆ L denote the subset of reasoning steps such that the conditional distribution in (B.3) is positive when z test 1 ∈ L 1 , and let L ∁ 1 denote its complement. Since P prompt CoT (n)) &gt; 0, we have</p><formula xml:id="formula_83">P(prompt CoT (n) ∪ {z test 1 = z 1 } &gt; 0, ∀z 1 ∈ L 1 .</formula><p>Then using (B.2), since prompt CoT (n) ∪ {z test 1 = z 1 } has positive density under P, we have</p><formula xml:id="formula_84">P LLM z test 2 = • | prompt CoT (n), z test 1 = z 1 = P z test 2 = • | prompt CoT (n), z test 1 = z 1 , ∀z 1 ∈ L 1 .</formula><p>Moreover, we have</p><formula xml:id="formula_85">P LLM (z test 1 = z 1 | prompt CoT (n)) = 0 for all z 1 ∈ L ∁ 1 .</formula><p>Therefore, by direct computation, we have</p><formula xml:id="formula_86">P LLM z test 2 = • | prompt CoT (n) = L 1 P LLM z test 2 = • | prompt CoT (n), z test 1 = z 1 • P LLM z test 1 = z 1 | prompt CoT (n) dz 1 = L 1 P z test 2 = • | prompt CoT (n), z test 1 = z 1 • P z test 1 = z 1 | prompt CoT (n) dz 1 = P z test 2 = • | prompt CoT (n) .</formula><p>We can generalize this argument to z test H = y test . Specifically, let L * denote the reasoning steps {z test 1 , . . . , z test H-1 } with positive density under P(• | prompt CoT (n)). Let each element in L * be denoted by z = {z 1 , . . . , z H-1 }. Then we have</p><formula xml:id="formula_87">P y test = • | prompt CoT (n) = z∈ L * P y test = •, {z test 1 , . . . , z test H-1 } = z | prompt CoT (n) dz = z∈ L * H h=1 P z test h = • | prompt CoT (n), z test 1 = z 1 , . . . , z test h-1 = z h-1 dz 1 • • • dz H-1 . (B.4)</formula><p>Here the second inequality follows from the factorization of joint probability into a product of conditional probabilities. Since P(prompt</p><formula xml:id="formula_88">CoT (n), z test 1 = z 1 , . . . , z test h-1 = z h-1</formula><p>) &gt; 0 by the definition of L * , by (B.2) we have</p><formula xml:id="formula_89">P z test h = • | prompt CoT (n), z test 1 = z 1 , . . . , z test h-1 = z h-1 = P LLM z test h = • | prompt CoT (n), z test 1 = z 1 , . . . , z test h-1 = z h-1 . (B.5)</formula><p>Therefore, this equality implies that P LLM cannot assign any positive density on the complement of L * , otherwise (B.5) is violated. Thus, combining (B.4) and (B.5), we conclude that</p><formula xml:id="formula_90">P y test = • | prompt CoT (n) = z∈ L * H h=1 P z test h = • | prompt CoT (n), z test 1 = z 1 , . . . , z test h-1 = z h-1 dz 1 • • • dz H-1 = z∈ L * H h=1 P LLM z test h = • | prompt CoT (n), z test 1 = z 1 , . . . , z test h-1 = z h-1 dz 1 • • • dz H-1 = z∈ L * P LLM y test = •, {z test 1 , . . . , z test H-1 } = z | prompt CoT (n) dz = P LLM y test = • | prompt CoT (n) .</formula><p>Here the second equality follows from (B.5) and the last equality follows from the fact that P LLM is supported on L * . Finally, by the Bayes' rule and the fact that y test is independent of the n examples in prompt CoT (n) conditioning on θ, we have</p><formula xml:id="formula_91">P LLM y test = • | prompt CoT (n) = P y test = • | prompt CoT (n) = Θ π θ | prompt CoT (n) P y test = • | z test 0 , θ dθ.</formula><p>Therefore, we conclude the proof.</p><p>In the following, we extend Lemma to the generalized multi-step latent variable in (A.1) and present its proof.</p><p>Lemma B.1. With pretraining data sampled from the generalized model in (A.1), we consider the population counterpart of the MLE in (4.3) with N going to infinity. Suppose that the CoT prompt prompt CoT (n) has nonzero density under the pretraining distribution. Then the perfectly pretrained LLMs perform BMA during CoT prompting. Namely, we have that</p><formula xml:id="formula_92">P LLM y test = • | prompt CoT (n) = Θ P y test = • | prompt CoT (n), θ π θ | prompt CoT (n) dθ.</formula><p>Proof. When the pretraining dataset is sampled according to the model in (A.1), by Bayes's rule, we have Since the MLE loss does not concern the underlying distribution, our analysis in the proof of Lemma 4.1 still holds. In particular, (B.2) holds on all prompts with a positive density under the pretraining distribution. As a result, starting from a CoT prompt prompt CoT (n) with a positive density, we can show that</p><formula xml:id="formula_93">P y test = • | prompt CoT (n) = Θ π θ | prompt CoT (n) P y test = • | prompt CoT (n),</formula><formula xml:id="formula_94">P z test 1 = •, . . . , z test H = • | prompt CoT (n) = P LLM z test 1 = •, . . . , z test H = • | prompt CoT (n)</formula><p>by writing the joint probability as a product of H conditional probabilities. Therefore, we similarly have</p><formula xml:id="formula_95">P LLM y test = • | prompt CoT (n) = P y test = • | prompt CoT (n)</formula><p>when N does to infinity. We conclude the proof by combining this fact with (B.6).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Proof of Proposition 4.2</head><p>Proof. We aim to show that the attention mechanism approximates BMA, meaning that vtest and attn(q test h , keys, values) converge to the same limit as n → ∞. To this end, we show that both quantities converge to a population-level estimator based on population matrices. The proof is divided into three steps. First, we define the population-level estimator that bridges the BMA estimator and the attention estimator. Then, in the second and the last step, we prove that these estimators converge the population-level estimator as n goes to infinity. Our proof generalizes the proof in <ref type="bibr" target="#b73">Zhang et al. (2023a)</ref> for vanilla ICL.</p><p>Step 1: Define the population-level estimator. Recall that we focus on a simplified model defined in (4.6), which involves a feature mappings ϕ, k, and v. Also note that we define </p><formula xml:id="formula_96">k i h = k(z i 0 ), k(z i 1 ), • • • , k(z i h-1 ), 0, • • • , 0 ∈ R H•d k as the features of the first h -1 steps of the i-th example and define v i h = C v • v(z i h ) for some constant C v &gt; 0.</formula><formula xml:id="formula_97">V | K] = θ * ϕ(K)</formula><p>. Thus, θ * can be viewed as the parameter of a linear regression problem, with K being the covariate and V being the response. To define a population-level estimator of θ * , we define two population matrices</p><formula xml:id="formula_98">C KK = 1/H H h=1 E ϕ(k i h )ϕ(k i h ) ⊤ ∈ R d ϕ ×d ϕ , C VK = 1/H H h=1 E v(z i h )ϕ(k i h ) ⊤ ∈ R dv×d ϕ . (B.7)</formula><p>Then we have θ * = C VK C -1 KK . When using this population-level estimator of θ * to make predictions on the test instance, for any h ≥ 0, we use k test h in (4.7) as the new covariate and predict the corresponding response, which is given by C VK C -1 KK ϕ(k test h ). In the rest of the proof, we relate vtest h defined in (4.8) and the attn(q test h , keys, values) defined in (4.9) to this estimator, where</p><formula xml:id="formula_99">q test h = ϕ(k test h ). Step 2: Relate vtest h to C VK C -1 KK ϕ(k test h ).</formula><p>In the following, for ease of notation, we let ∥ • ∥ op and ∥ • ∥ 2 denote the operator norm of matrices and the L 2 norm of vectors. Note that according to the model in (4.6), θ * is shared in all the H steps. Thus, we can pool all the reasoning steps together to estimate θ * . Notice that vtest h defined in (4.8) is exactly the ridge regression estimator for the linear model V = θ * ϕ(K) + noise, where the noise term is an independent N (0, σ 2 ) random variable. To simplify the notation, let L = nH. Recall that we define ϕ(K n ) and V n in Section 4.4, where ϕ(K n ) ∈ R d ϕ ×L and V n ∈ R dv×L . We define the sample-based counterparts of C KK and C VK in (B.7) as</p><formula xml:id="formula_100">C KK = L -1 • ϕ(K n )ϕ(K n ) ⊤ ∈ R d ϕ ×d ϕ , C VK = L -1 • (V n )ϕ(K n ) ⊤ ∈ R dv×d ϕ . (B.8)</formula><p>In addition, we define the empirical correlation between value vectors as</p><formula xml:id="formula_101">C VV = L -1 • (V n )(V n ) ⊤ ∈ R dv×dv .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>By definition, for any h ∈ [H], can write vtest</head><p>h in (4.8) as</p><formula xml:id="formula_102">vtest h = C VK ( C KK + L -1 • σ 2 /λ • I) -1 ϕ(k test h ). By triangle inequality, we have vtest h -C VK C -1 KK ϕ(k test h ) 2 ≤ C VK ( C KK + L -1 • σ 2 /λ • I) -1 ϕ(k test h ) -C VK (C KK + L -1 σ 2 /λ • I) -1 ϕ(k test h ) 2 (i) + C VK (C KK + L -1 • σ 2 /λ • I) -1 ϕ(k test h ) -C VK C -1 KK q test h 2 (ii) . (B.9)</formula><p>Here (i) is the variance term of ridge regression which decays with the sample size L, and (ii) is the bias term that decays with the regularization parameter. Our analysis of these terms is similar to that in <ref type="bibr" target="#b73">Zhang et al. (2023a)</ref>. Note that in contrast to <ref type="bibr" target="#b73">Zhang et al. (2023a)</ref>,</p><formula xml:id="formula_103">C KK and C VK defined in (B.8) involve {(k i j , v(z i j ))} H,n</formula><p>j=1,i=1 that are dependent in j. We handle such dependency by decomposing the double sum according to different step j ∈ [H] and apply concentration to each step. We first control the norm of (i) in (B.9) as</p><formula xml:id="formula_104">(i) ≤ C VK ( C KK + L -1 • σ 2 /λ • I) -1 ϕ(k test h ) -C VK (C KK + L -1 • σ 2 /λ • I) -1 ϕ(k test h ) 2 + ( C VK -C VK )(C KK + L -1 • σ 2 /λ • I) -1 ϕ(k test h ) 2 = C VK ( C KK + L -1 • σ 2 /λ • I) -1 ( C KK -C KK )(C KK + L -1 • σ 2 /λ • I) -1 ϕ(k test h ) 2 + ( C VK -C VK )(C KK + L -1 • σ 2 /λ • I) -1 ϕ(k test h ) 2 , (B.10)</formula><p>where the first inequality follows from the triangle inequality and the second follows from the fact that</p><formula xml:id="formula_105">C VK ( C KK + L -1 • σ 2 /λ • I) -1 -C VK (C KK + L -1 • σ 2 /λ • I) -1 = C VK ( C KK + L -1 • σ 2 /λ • I) -1 (C KK + L -1 • σ 2 /λ • I)(C KK + L -1 • σ 2 /λ • I) -1 -C VK ( C KK + L -1 • σ 2 /λ • I) -1 ( C KK + L -1 • σ 2 /λ • I)(C KK + L -1 • σ 2 /λ • I) -1 = C VK ( C KK + L -1 • σ 2 /λ • I) -1 (C KK -C KK )(C KK + L -1 • σ 2 /λ • I) -1 ,</formula><p>where the equality follows from rearranging the terms. We next separately bound the two terms in (B.10). For the first term in the right-hand side of (B.10), we have that</p><formula xml:id="formula_106">C VK ( C KK + L -1 • σ 2 /λ • I) -1 ( C KK -C KK )(C KK + L -1 • σ 2 /λ • I) -1 ϕ(k test h ) 2 ≤ ∥ C VV ∥ 1/2 op • C 1/2 KK ( C KK + L -1 • σ 2 /λ • I) -1/2 op • ( C KK + L -1 • σ 2 /λ • I) -1/2 op • ( C KK -C KK )(C KK + L -1 • σ 2 /λ • I) -1 ϕ(k test h ) 2 ≤ (L -1 • σ 2 /λ) -1/2 • ( C KK -C KK )(C KK + L -1 • σ 2 /λ • I) -1 ϕ(k test h ) 2 , (B.11)</formula><p>where the first inequality follows from the cross-covariance operator decomposition (Theorem 1 in <ref type="bibr" target="#b9">Baker (1973)</ref>)</p><formula xml:id="formula_107">C VK = C 1/2 VV W C 1/2</formula><p>KK for W such that ∥W ∥ op ≤ 1, and the second inequality follows from the facts that</p><formula xml:id="formula_108">∥ C VV ∥ 2 op ≤ L -1 n i=1 H h=1 ∥v(z i h )∥ 2 2 ≤ 1, C 1/2 KK ( C KK + L -1 • σ 2 /λ • I) -1/2</formula><p>op ≤ 1, and</p><formula xml:id="formula_109">( C KK + L -1 • σ 2 /λ • I) -1/2 op ≤ (L -1 • σ 2 /λ) -1/2 .</formula><p>We then upper bound the second term of the right-hand side of (B.10) using concentration inequality. To this end, based on the random variables K and V, we consider a random variable</p><formula xml:id="formula_110">Vϕ(K) ⊤ (C KK + L -1 σ 2 /λI) -1 ϕ(k test h ) ∈ R dv . Since ∥v(z)∥ 2 = 1 for all input z ∈ L, therefore ∥V∥ 2 ≤ 1.</formula><p>Since the mapping ϕ has a bounded image set, we have</p><formula xml:id="formula_111">Vϕ(K) ⊤ (C KK + L -1 σ 2 /λI) -1 ϕ(k test h ) 2 ≤ ∥V∥ 2 • ∥ϕ(K) ⊤ ∥ 2 • ∥(C KK + L -1 σ 2 /λI) -1 ∥ op • ∥ϕ(k test h )∥ 2 ≤ C • H • (L -1 σ 2 /λ) -1 , (B.12)</formula><p>where C is an absolute constant such that ∥ϕ(•)∥ 2 ≤ C • H. We further bound the expected squared norm as</p><formula xml:id="formula_112">E Vϕ(K) ⊤ (C KK + L -1 σ 2 /λI) -1 ϕ(k test h ) 2 2 ≤ E ∥V∥ 2 2 • ϕ(K) ⊤ (C KK + L -1 σ 2 /λI) -1 2 2 • ϕ(k test h ) 2 2 ≤ C • H • E ϕ(K) ⊤ (C KK + L -1 σ 2 /λI) -1 2 2 ≤ C • H • (L -1 σ 2 /λ) -1 • E (C KK + L -1 σ 2 /λI) -1 ϕ(K), ϕ(K) . (B.13)</formula><p>For the expectation in the right-hand side of (B.13), we further have that</p><formula xml:id="formula_113">E (C KK + L -1 σ 2 /λI) -1 ϕ(K), ϕ(K) = E Trace (C KK + L -1 σ 2 /λI) -1 ϕ(K)ϕ(K) ⊤ = Trace (C KK + L -1 σ 2 /λI) -1 C KK = d ϕ -L -1 σ 2 /λ • Trace (C KK + L -1 σ 2 /λI) -1 ≤ d ϕ , (B.14)</formula><p>where we assume ϕ(•) to be finite dimensional. The last line follows from the direct calculation and the fact that (C KK + L -1 σ<ref type="foot" target="#foot_1">foot_1</ref> /λI) -1 is positive definite. Thus, we have</p><formula xml:id="formula_114">E Vϕ(K) ⊤ (C KK + L -1 σ 2 /λI) -1 ϕ(k test h )</formula><p>for some constant C &gt; 0. Recall that V and K are defined as uniform mixtures of v(z </p><formula xml:id="formula_115">= 2C • H • (L -1 σ 2 /λ) -1 and variance as C • H • (L -1 σ 2 /λ) -1 d ϕ .</formula><p>Therefore, with probability at least 1 -δ we have</p><formula xml:id="formula_116">C VK (C KK + L -1 σ 2 /λI) -1 ϕ(k test h ) -C VK (C KK + L -1 σ 2 /λI) -1 ϕ(k test h ) 2 (B.16) = H j=1 1 H 1 n n i=1 v(z i j )ϕ(k i j ) ⊤ (C KK + L -1 σ 2 /λI) -1 ϕ(k test h ) -E v(z i j )ϕ(k i j ) ⊤ (C KK + L -1 σ 2 /λI) -1 ϕ(k test h ) 2 ≤ H j=1 1 H 1 n n i=1 v(z i j )ϕ(k i j ) ⊤ (C KK + L -1 σ 2 /λI) -1 ϕ(k test h ) -E v(z i j )ϕ(k i j ) ⊤ (C KK + L -1 σ 2 /λI) -1 ϕ(k test h ) 2 ≤ C • H(L -1 σ 2 /λ) -1 n -1 + H(L -1 σ 2 /λ) -1 d ϕ n • log 2H δ = C • H • Hλ/σ 2 + λ • d ϕ /σ 2 • log 2H δ ,</formula><p>where C &gt; 0 is an absolute constant. Here the second line results from triangle inequality and the third line follows from Lemma H.1. Similarly, we perform the same analysis for C KK and we obtain with probability at least 1 -δ that</p><formula xml:id="formula_117">C KK (C KK + L -1 σ 2 /λI) -1 -C KK (C KK + L -1 σ 2 /λI) -1 2 (B.17) ≤ C ′ • H • Hλ/σ 2 + λ • d ϕ /σ 2 • log 2H δ .</formula><p>Here C ′ &gt; 0 is an absolute constant. Therefore, we bound the first term in the upper bound in (B.9) using (B.10), (B.11), (B.16), and (B.17) as</p><formula xml:id="formula_118">C VK ( C KK + L -1 σ 2 /λI) -1 -C VK (C KK + L -1 σ 2 /λI) -1 2 (B.18) ≤ C ′′ • L σ 2 /λ • H • Hλ/σ 2 + λ • d ϕ /σ 2 log 2H δ .</formula><p>which holds with probability at least 1 -δ.</p><p>To bound the second term in (B.9), we can apply the bound in Equation (E.13) from the proof of <ref type="bibr" target="#b73">Zhang et al. (2023a)</ref> directly. Namely, we have that for any q ∈ R H</p><formula xml:id="formula_119">•d k , C VK (C KK + L -1 • σ 2 /λI) -1 ϕ(q) -C VK C -1 KK ϕ(q) 2 ≤ C • σ 2 /λL -1 , (B.19)</formula><p>where C &gt; 0 is an absolute constant. Combing (B.18) and (B.19), we obtain</p><formula xml:id="formula_120">vtest h -CME(k test h , P K,V ) 2 = O L • λ σ 2 • H • Hλ/σ 2 + d ϕ • λ/σ 2 log 2H δ + σ 2 /λL -1 . (B.20)</formula><p>Thus the error bound in (B.20) goes to 0 by selecting σ 2 /λ = (nH) 2/3 .</p><p>Step 3: Relate attn(q test h , keys, values) to</p><formula xml:id="formula_121">C VK C -1 KK ϕ(k test h ).</formula><p>In this step, we want to prove that attn(q, keys, values) defined in (4.9) converges to</p><formula xml:id="formula_122">C VK C -1 KK ϕ(k test h ) as n → ∞. Recall that we define v i h = C v • v(z i h )</formula><p>as the value of softmax attention. To achieve this goal, we aim to show that C • attn(q, keys, values)</p><formula xml:id="formula_123">= S dv -1 v P V | K (v | q)dv, where P V | K (v | q</formula><p>) is an estimator of the conditional distribution of V given K, and C is an absolute constant. Here we let S d denote a d-dimensional unit sphere. We define the empirical kernel conditional density as follows,</p><formula xml:id="formula_124">P V | K (v | q) = ι n i=1 H h=1 exp ⟨k i h , q⟩ exp ⟨v i h , v⟩ n i=1 H h=1 exp ⟨k i h , q⟩</formula><p>,</p><formula xml:id="formula_125">where ι = 1/ S d k -1 exp ⟨v i h , v⟩ dv = 1/ S d k -1 exp ⟨v(z i h )</formula><p>, v⟩ dv is a normalizing constant to make sure that P V | K is a probability measure. Furthermore, note that ι does not depend on v i h due to the symmetry of the unit sphere. We compute the integration over v as</p><formula xml:id="formula_126">S dv -1 v P V | K (v | q)dv = ι • n i=1 H h=1 exp ⟨k i h , q⟩ S dv -1 v exp ⟨v(z i h ), v⟩ dv n i=1 H h=1 exp ⟨k i h , q⟩ = ι • n i=1 H h=1 exp ⟨k i h , q⟩ • C 1 • v i h n i=1 H h=1 exp ⟨k i h , q⟩ = C 1 ι • attn(q, keys, values).</formula><p>Since v, v(z i h ) ∈ S dv-1 , we can apply Lemma H.4 with γ = 1 to the integration S dv -1 v exp(⟨C v • v(z i h ), v⟩)dv, and obtain that the second line holds for some constant C 1 &gt; 0. The last line follows directly from the definition of softmax attention 3.1.</p><p>Due to the condition where</p><formula xml:id="formula_127">P V | K (v | q) → P(v | q) uniformly for any q ∈ S dq as n → ∞, the integral S dv -1 v P V | K (v | q)dv → E[V | K = q] as n → ∞.</formula><p>Combining with the previous argument, we have shown that,</p><formula xml:id="formula_128">C 1 ι • attn(q, keys, values) → E[V | K = q] as n → ∞, (B.21)</formula><p>C Proofs and Additional Results of Section 5.1</p><p>This section provides proofs and additional results about the statistical properties of the vanilla CoT estimator. In Appendix C.1 we prove Lemma 5.2, which establishes the error decomposition of vanilla CoT error err CoT . In Appendix C.2, we extend Theorems 5.5 and 5.7 to the scenario where Θ is continuous, and provide their corresponding proofs. We conclude this section with an additional result that characterizes err CoT under the general model in (A.1).</p><p>C.1 Proof of Lemma 5.2</p><p>Proof. Recall that the error of the CoT estimator is defined as</p><formula xml:id="formula_129">err CoT = KL P(y test = • | z test 0 , θ * ), P ρ (y test = • | prompt CoT (n)) .</formula><p>We decompose the KL divergence into three terms by direct computation: </p><formula xml:id="formula_130">err CoT = KL P(y test = • | z test 0 , θ * ), P(y test = • | prompt CoT (n)) + KL P(y test = • | prompt CoT (n)), P ρ (y test = • | prompt CoT (n)) + P(y test = y | z test 0 , θ * ) -P(y test = y | prompt CoT (n)) • log P(y test = y | prompt CoT (n)) P ρ (y test = y | prompt CoT (n))</formula><formula xml:id="formula_131">P ρ (y test = y | prompt CoT (n)) (C.2) = log z 1:(H-1) ∈L * P(z test 1:(H-1) = z 1:(H-1) , y test = y | prompt CoT (n)) z ′ 1:(H-1) ∈L * P ρ (z test 1:(H-1) = z ′ 1:(H-1) , y test = y | prompt CoT (n))</formula><p>≤ log max</p><formula xml:id="formula_132">z 1:(H-1) ∈L * P(z test 1:(H-1) = z 1:(H-1) , y test = y | prompt CoT (n)) P ρ (z test 1:(H-1) = z 1:(H-1) , y test = y | prompt CoT (n))</formula><p>, where the first line follows from marginalizing over the intermediate steps z test 1:(H-1) , and the second line follows from generalized mediant inequality.</p><p>Next, we use the chain rule to decompose the joint distribution as</p><formula xml:id="formula_133">P(z test 1:(H-1) = z 1:(H-1) , y test = y | prompt CoT (n)) P ρ (z test 1:(H-1) = z ′ 1:(H-1) , y test = y | prompt CoT (n)) (C.3) = H h=1 P(z test h = z h | prompt h CoT (n)) P ρ (z test h = z ′ h | prompt h CoT (n))</formula><p>≤ By Pinsker's inequality, we further have that</p><formula xml:id="formula_134">2 TV P(y test = • | z test 0 , θ * ), P(y test = • | prompt CoT (n)) • Hb * ≤ 2 √ 2Hb * • KL P(y test = • | z test 0 , θ * ), P(y test = • | prompt CoT (n)) 1/2 . (C.4)</formula><p>Combining (C.1) and (C.4), we conclude that</p><formula xml:id="formula_135">err CoT ≤ KL P(y test | prompt CoT (n)), P ρ (y test | prompt CoT (n)) err pre + KL P(y test | z test 0 , θ * ), P(y test | prompt CoT (n)) err prompt -(i) + 2 √ 2Hb * • KL 1/2 P(y test | z test 0 , θ * ), P(y test | prompt CoT (n)) err prompt -(ii)</formula><p>.</p><p>Therefore, we conclude the proof. Here the upper bound consists of three parts. The first term characterizes the pretraining error by comparing P and P ρ . The second and third terms involve the KL divergence between the true distribution P(• | θ * ) and the distribution induced by the CoT prompt and P.</p><p>C.2 Proofs of Theorems 5.5 and 5.7 and Extension</p><p>In the sequel, we generalize the results in Theorems 5.5 and 5.7 to the scenario where Θ is continuous and provide the corresponding proofs. The proofs of the supporting lemmas used in this subsection are deferred to Appendix G.2. We begin this section by specifying the distance measurement on the general Θ, which can be continuous. For any two hidden concept θ 1 , θ 2 ∈ Θ, we define a loglikelihood metric between them as</p><formula xml:id="formula_136">∥θ 0 -θ 1 ∥ Θ = sup (Z 0 ,Z 1 ,••• ,Z H-1 ,Y ) log P(Z 0 , Z 1 , • • • , Z H-1 , Y | θ 0 ) P(Z 0 , Z 1 , • • • , Z H-1 , Y | θ 1 )</formula><p>.</p><p>We note that the loglikelihood metric is indeed a semi-metric. For any θ ∈ Θ, any other concepts in the neighborhood of it share a similar conditional distribution on one example.</p><p>Based on this semi-metric, we then define α-cover of S and the corresponding N (α, S) for any number α &gt; 0 and any set S ⊆ Θ. That is, C(α, S)</p><formula xml:id="formula_137">= {θ i } N i=1 ⊆ Θ is a α-cover of S of size N if S ⊆ N i=1 B(θ i , α), where B(θ i , α) is a neighborhood of θ i with radius α, i.e., B(θ, α) = θ ∈ Θ ∁ : ∥ θ -θ∥ Θ ≤ α .</formula><p>(C.5)</p><p>The minimal N such that there exists a α-cover of S of size N is called the covering number of S, which is denoted as N (α, S). In the following, we consider the complement of the equivalence class of the target concept Θ ∁ = Θ\Θ eq (θ * ). Without misunderstanding, we adopt N (α) to denote N (α, Θ ∁ ) in the following. Then we restate Theorem 5.5 with Θ allowed to be a continuous set.</p><p>Theorem C.1. Let err pre denote the pretraining error defined in (5.2) and denote Θ ∁ = Θ\Θ eq (θ * ). Under Assumptions 5.1 and 5.4, the statistical error err CoT defined in (4.4) is bounded under the following two cases:</p><p>• When Θ is a discrete and finite set, with probability 1 -δ, we have</p><formula xml:id="formula_138">err CoT ≤ O Hb * • π(θ * ) -1/2 • δ -1 • Θ ∁ • e -λn + err pre .</formula><p>• When Θ is continuous, let N (α) denote the covering number of Θ ∁ with precision α with respect to the log-likelihood metric ∥•∥ Θ . Under Assumption 6.2, with probability 1 -δ, we have</p><formula xml:id="formula_139">err CoT ≤ O Hb * • π Θ eq (θ * ) -1 • c -nH 0 • δ -2 • N (α) 2 • e -2nλ + nα + err pre .</formula><p>Here the probability is with respect to the randomness of the CoT prompt. Moreover, we remark that when the LLM is perfectly pretrained, with probability 1 -δ with respect to the randomness of CoT prompt, we have</p><formula xml:id="formula_140">err CoT ≤ O π(θ * ) -1 δ -2 |Θ ∁ | 2 e -2λn when Θ is finite, O π Θ eq (θ * ) -1 c -nH 0 δ -2 N (α) 2 e -2nλ + nα when Θ is continuous.</formula><p>When the set Θ is continuous, the statistical rate becomes slower due to the complicated structure of the task parameter space Θ. In the theorem statement, we do not specify the value of the covering number α. In fact, α should be chosen depending on n such that α • n converges to 0 as n increases. One can easily obtain a more concrete statistical rate by specifying a concrete α depending on specific assumptions of the covering number. For instance, suppose N (α) = O(α -V ) for some constant V , by selecting</p><formula xml:id="formula_141">α = O n -1/(2V +1) exp{-n(2λ + H ln c 0 )/(2V + 1)} ,</formula><p>prompting error err prompt is O n V /(2V +1) exp{-n(λ + H ln c 0 /2)/(2V + 1)} with probability at least 1 -δ.</p><p>Proof of Theorem 5.5. We divide the proof into three parts. In Step 1, we derive an upper bound for on KL divergence to prepare for later analysis. In Step 2, we analyze the case where Θ ∁ is discrete and finite. In Step 3, we extend to continuous Θ ∁ .</p><p>Step 1: Derive an upper bound on KL divergence. We first invoke the following proposition to establish an upper-bound of KL(P(</p><formula xml:id="formula_142">y test = • | z test 0 , θ * ), P(y test = • | prompt CoT (n))).</formula><p>Proposition C.2. For some fixed task θ * ∈ Θ, we provide upper bounds for the KLdivergence of the ground truth distribution P(y test = • | z test 0 , θ * ) from the conditional pretrained distributions P(y test = • | prompt CoT (n)) as follows</p><formula xml:id="formula_143">KL P(y test = • | z test 0 , θ * ), P y test = • | prompt CoT (n) ≤ log 1 + Θ ∁ P prompt CoT (n) θ π(θ)dθ Θeq(θ * ) P prompt CoT (n) θ π(θ)dθ .</formula><p>Proof. See Appendix G.2.1 for a detailed proof. This proposition applies evidence lower bound <ref type="bibr" target="#b26">(Kingma and Welling, 2013)</ref> to upper bound the KL divergence using likelihood ratios. The proof involves using a variational distribution that is only supported on Θ eq and proportional to the posterior distribution. This proposition reduces the problem of bounding the KL divergence to comparing the likelihood functions on Θ eq and Θ ∁ . We consider the cases where Θ is finite and continuous separately.</p><p>Step 2: Statistical rate for the case with a discrete and finite Θ. In this step, we assume the parameter space Θ is discrete and finite. Then by Proposition C.2, we have</p><formula xml:id="formula_144">KL P(y test = • | z test 0 , θ * ), P y test = • | prompt CoT (n) ≤ log 1 + θ∈Θ ∁ P prompt CoT (n) θ π(θ) θ∈Θeq(θ * ) P prompt CoT (n) θ π(θ) ≤ log 1 + θ∈Θ ∁ P prompt CoT (n) θ π(θ) P prompt CoT (n) θ * π(θ * ) . (C.6)</formula><p>Here the first inequality follows from Proposition C.2 by changing the integration signs into summations. In the second inequality, we drop some terms in the denominator to get an upper bound. Note that we have converted the upper bound into the logarithm of a weighted sum of likelihood ratios, with the weights being the prior distribution. We invoke the following lemma to establish an upper bound for each likelihood ratio.</p><formula xml:id="formula_145">Lemma C.3. Let S n = {S i } i∈[n] , where S i = z i 0:H i.i.d.</formula><p>∼ P(• | θ * ) denotes a set of n reasoning paths of length H sampled independently from the model in (4.2) with task θ * . Let J i ⊆ [H -1] for each i ∈ [n], and we use S i J i to denote a truncated version of the i-th trajectory S i corresponding to the indices specified by J i . Namely, S i J i = {z i 0 } {z i j } j∈J i {z i H }. Then for any n ≥ 1, θ ∈ Θ, and δ &gt; 0, we have</p><formula xml:id="formula_146">P({S J i } n i=1 | θ) P({S J i } n i=1 | θ * ) ≤ exp -2 n i=1 H 2 P(S J i | θ * ), P(S J i | θ) + 2 log(δ -1 ) ,</formula><p>with probability at least 1 -δ. Here the probability is with respect to the randomness of S n , and H 2 (•, •) denotes the squared Hellinger distance. Furthermore, let z n+1 0 be the input query of the n + 1-th reasoning path, with probability at least 1 -δ, we further have</p><formula xml:id="formula_147">P({S J i } n i=1 , z n+1 0 | θ) P({S J i } n i=1 , z n+1 0 | θ * ) ≤ exp -2 n i=1 H 2 P(S J i , | θ * ), P(S J i | θ) -2H P(z n+1 0 | θ * ), P(z n+1 0 | θ) + 2 log(δ -1 ) ,</formula><p>Proof. See Appendix G.2.2 for a detailed proof.</p><p>This lemma provides an upper bound on the likelihood ratio of generating trajectories Recall that prompt CoT (n) contains n complete trajectories and a testing query z test 0 . Applying Lemma C.3 to any θ ∈ Θ ∁ and θ * and taking a union bound, we conclude that, sup</p><formula xml:id="formula_148">{S J i } n i=1 from</formula><formula xml:id="formula_149">θ∈Θ ∁ P prompt CoT (n) | θ P prompt CoT (n) | θ * ≤ sup θ∈Θ ∁ exp -2nH 2 P(Z 0:H | θ * ), P(Z 0:H | θ) -2H 2 P(z 0 | θ * ), P(z 0 | θ) + 2 log δ -1 |Θ ∁ | ≤ exp -2nλ + 2 log δ -1 |Θ ∁ |</formula><p>holds with probability at least 1 -δ. The first inequality follows from Lemma C.3 and the second inequality follows from Assumption 5.4 and the fact that H 2 P(z 0 | θ * ), P(z 0 | θ) ≥ 0. Thus, plugging this inequality into the upper bound in (C.6), with probability at least 1 -δ, we have</p><formula xml:id="formula_150">KL P(y test = • | z test 0 , θ * ), P y test = • | prompt CoT (n) ≤ log 1 + 1 π(θ * ) • exp -2nλ + 2 log δ -1 |Θ ∁ | • θ∈Θ ∁ π(θ) = log 1 + 1 -π Θ eq (θ * ) π(θ * ) • exp -2nλ + 2 log δ -1 |Θ ∁ | ,</formula><p>Therefore, when Θ is discrete and finite, with probability at least 1 -δ, we have</p><formula xml:id="formula_151">KL P(y test = • | z test 0 , θ * ), P y test = • | prompt CoT (n) = O π(θ * ) -1 δ -2 |Θ ∁ | 2 e -2λn , (C.7)</formula><p>where O(•) only omits an absolute constant. Here we use the fact that 1 -π Θ eq (θ * ) &lt; 1.</p><p>Recall that the prompting error is defined as</p><formula xml:id="formula_152">err prompt = KL P(y test = • | z test 0 , θ * ), P(y test = • | prompt CoT (n)) + 2 √ 2Hb * KL P(y test = • | z test 0 , θ * ), P(y test = • | prompt CoT (n)) .</formula><p>Therefore, we conclude that for any task θ * with separation from Θ ∁ , the prompting error goes to zero exponentially fast at a rate of order O Hb * π(θ * ) -1/2 δ -1 |Θ ∁ |e -λn . This proves Theorem 5.5.</p><p>Step 3: Statistical rate for the case with a continuous Θ. Our analysis in Step 2 requires Θ to be discrete and finite. To handle the continuous case, we use the cover of Θ ∁ to discretize it, at the cost of introducing an additional error involving the covering granularity. Specifically, let C(α) be an α-cover of Θ according to the semi-metric ∥ • ∥ Θ . For any θ, let the neighborhood B(θ, α) be defined in (C.5). Let N (α) denote the covering number N (α, Θ ∁ ). Using the minimal α-cover of Θ ∁ , we can construct a partition of Θ ∁ into at most N (α) disjoint sets, with each set contained in a neighborhood of radius α. To see this, let</p><formula xml:id="formula_153">C(α) = {θ i } N (α) i=1 be the α-cover of Θ ∁ . We can construct set C(θ i ) ⊂ B(θ i , α) such that {C(θ i )} N (α)</formula><p>i=1 form a partition of Θ ∁ by shrinking each B(θ i , α) to remove overlapping parts. Then, we have C(θ i ) ∩ C(θ j ) = ∅ for all i ̸ = j, and N (α)  i=1 C(θ i ) = Θ ∁ . We characterize the discretization error due to the α-cover via</p><formula xml:id="formula_154">log θ∈Θ ∁ P prompt CoT (n) | θ π(θ)dθ θ∈C(α) P prompt CoT (n) | θ π C(θ) = log θ∈C(α) π C(θ) θ ′ ∈C(θ) P prompt CoT (n) | θ ′ • π(θ ′ )/π(C(θ) dθ ′ θ∈C(α) P prompt CoT (n) | θ π C(θ) ≤ log max θ∈C(α) θ ′ ∈C(θ) P prompt CoT (n) | θ ′ π(θ ′ )/π C(θ) dθ ′ P prompt CoT (n) | θ ≤ log sup θ∈C(α),θ ′ ∈C(θ) P prompt CoT (n) | θ ′ P prompt CoT (n) | θ ≤ nα. (C.8)</formula><p>The first equality follows from decomposing the integral taken over Θ ∁ into a double integral taken over the covering C(α) and then within the induced partition C(θ). The second and third inequality follows from generalized mediant inequality. The last inequality follows from the definition of B(θ, α) in (C.5) and the fact that C(θ) ⊆ B(θ, α) for all θ ∈ C(α). Now we have controlled the error introduced by approximating Θ ∁ using C(α). Next, we apply Assumption 6.2 to lower bound the likelihood integrated over Θ eq (θ * ):</p><formula xml:id="formula_155">Θeq(θ * ) P prompt CoT (n) | θ ′ π(θ ′ )dθ ′ ≥ c nH 0 • P prompt CoT (n) | θ * • π(Θ eq (θ * )). (C.9)</formula><p>The inequality follows from the fact that prompt CoT (n) is of length nH and Assumption 6.2, which provides a lower bound for the conditional probability of the next reasoning step. Combining (C.9) and (C.8) and using the same technique as in Step 2, we obtain</p><formula xml:id="formula_156">KL P(y test = • | z test 0 , θ * ), P y test = • | prompt CoT (n) ≤ log 1 + θ∈Θ ∁ P prompt CoT (n) | θ π(θ) dθ c nH 0 • P prompt CoT (n) | θ * • π(Θ eq (θ * )) ≤ log 1 + C(α) P prompt CoT (n) | θ π C n (θ) c nH 0 • P prompt CoT (n) | θ * • π(Θ eq (θ * )) + nα ≤ log 1 + 1 -π Θ eq (θ * ) c nH 0 • π(Θ eq (θ * )) exp -2nλ + 2 log δ -1 N (α) + nα,</formula><p>with probability at least 1 -δ. Here the first inequality is due to Proposition C.2 and (C.9).</p><p>The second inequality follows from (C.8), accounting for the discretization error induced by the α-cover. The last inequality follows from the same strategy as in Step 2, where apply Lemma C.3. Finally, we conclude that , with probability at least 1 -δ, we have</p><formula xml:id="formula_157">KL P(y test = • | z test 0 , θ * ), P y test = • | prompt CoT (n) = O π Θ eq (θ * ) -1 • c -nH 0 • δ -2 • N (α) 2 • e -2nλ + nα ,</formula><p>where O(•) only hides absolute constants and we use the fact that the numerator 1π Θ eq (θ * ) &lt; 1. Therefore, we conclude the proof.</p><p>The rest of this section generalizes Theorem 5.7 to the case where Θ is continuous and provides the proof.</p><p>Theorem C.4. Let err pre denote the pretraining error defined in (5.2), and let Θ ∁ = Θ\Θ eq (θ * ) with Θ ∁ as its representative set. Under Assumptions 5.1, 5.4, and 5.6, the statistical error err CoT defined in (4.4) is bounded under the following two cases:</p><p>• When Θ ∁ is a discrete and finite set, with probability 1 -δ, we have</p><formula xml:id="formula_158">err CoT ≤ O Hb * • π(θ * ) -1/2 • δ -1 • Θ ∁ • e -(λ-α)n+α 0 + err pre .</formula><p>• When Θ is continuous, let N (α) denote the covering number of Θ ∁ with precision α.</p><p>With probability 1 -δ, we have:</p><formula xml:id="formula_159">err CoT ≤ O Hb * • π Θ eq (θ * ) -1 • δ -2 • N (α) 2 • e -2n(λ-α)+2α 0 + nα + err pre .</formula><p>Here the probability is with respect to the randomness of CoT prompts prompt CoT (n). Parameters α and α 0 are introduced in Assumption 5.6. Moreover, we remark that when the LLM is perfectly pretrained, with probability 1 -δ with respect to the randomness of CoT prompt, we have</p><formula xml:id="formula_160">err CoT ≤ O π Θ eq (θ * ) -1 • δ -2 • | Θ ∁ | 2 • e -2(λ-α)n+2α 0 when Θ is finite, O δ -2 • π Θ eq (θ * ) -1 • N (α) 2 • e -2n(λ-α)+2α 0 + nα when Θ is continuous.</formula><p>These error bound follow from the fact that, when the LLM is perfectly pretrained, err CoT = KL P(</p><formula xml:id="formula_161">y test = • | z test 0 , θ * ), P y test = • | prompt CoT (n) .</formula><p>Theorem C.4 builds on Theorem C.1 by incorporating Assumption 5.6, which postulates that distributions within each equivalence class are close. This leads to improved dependency on the parameter space size, shifting from Θ ∁ to Θ ∁ , despite a slower rate of exponential decay. Note that the statistical rate of err CoT with a continuous Θ ∁ does not require Assumption 6.2, which is required by Theorem C.1. The reason is technical: Assumption 5.6 plays a similar role as Assumption 6.2 and is sufficient to establish the result.</p><p>Proof of Theorem 5.7. We split the proof into three parts. First, we apply Proposition C.2 and Assumption 5.6 to derive an upper bound of KL divergence involving Θ ∁ . Then in the second and last part, we consider discrete and continuous cases separately.</p><p>Step 1: Derive upper bound for KL divergence at an equivalence class level. We fix some concept θ * as the true latent task parameter. Let Θ ∁ = Θ\Θ eq (θ * ) and Θ ∁ denote a representative set of Θ ∁ . In light of Proposition C.2, we first write</p><formula xml:id="formula_162">Θ ∁ P prompt CoT (n) θ π(θ)dθ = θ∈ Θ ∁ π Θ eq (θ) θ ′ ∈Θeq(θ) P prompt CoT (n) | θ ′ π(θ ′ )/π Θ eq (θ) dθ ′ dθ. (C.10)</formula><p>That is, we decompose the integral over Θ ∁ into a double integral: the inner integral averages the likelihood P prompt CoT (n) | θ ′ within each equivalence class, and the outer integral averages across all equivalence classes. By Assumption 5.6, there exists a representative set Θ such that θ * is in Θ, and distributions within the same equivalence class are close to each other. Thus, we can derive both upper and lower bounds for the averaged likelihood within each equivalence class. For any θ ∈ Θ, we have </p><formula xml:id="formula_163">exp(-nα -α 0 ) ≤ θ ′ ∈Θeq(θ) P prompt CoT (n) | θ ′ P prompt CoT (n) | θ • π(θ ′ ) π Θ eq (θ) dθ ′ ≤ exp(nα + α 0 ). (C.</formula><formula xml:id="formula_164">= • | prompt CoT (n) ≤ log 1 + θ∈ Θ ∁ π Θ eq (θ) θ ′ ∈Θeq(θ) P prompt CoT (n) | θ ′ π(θ ′ )/π Θ eq (θ) dθ ′ dθ θ ′′ ∈Θeq(θ * ) P prompt CoT (n) θ ′′ π(θ ′′ )dθ ′′ ≤ log 1 + e nα+α 0 • θ∈ Θ ∁ π Θ eq (θ) P prompt CoT (n) | θ dθ e -nα-α 0 • P prompt CoT (n) | θ * π Θ eq (θ * )</formula><p>.</p><p>The first inequality follows from Proposition C.2 and (C.10) and the second inequality is due to (C.11). Note that the representative set Θ ∁ may not be unique, but this does not affect our analysis because we use θ ∈ Θ ∁ only as a reference and the value of P(prompt CoT (n) | θ) stays the same within each equivalence class. The definition is consistent under any selection of the representative set. Therefore, we rewrite the upper bound for the KL divergence at the equivalence class level:</p><formula xml:id="formula_165">KL P(y test = • | z test 0 , θ * ), P y test = • | prompt CoT (n) ≤ log 1 + e 2nα+2α 0 θ∈ Θ ∁ π Θ eq (θ) P prompt CoT (n) | θ dθ P prompt CoT (n) | θ * π Θ eq (θ * ) . (C.12)</formula><p>Here, Assumption 5.6 reduces the integration region from Θ ∁ to Θ ∁ , at the cost of introducing the terms e 2αn+2α 0 .</p><p>Step 2: Statistical rate for the discrete case. In this step, we assume the parameter space Θ ∁ is discrete and finite. For any θ ∈ Θ ∁ , with probability at least 1 -δ, we obtain an upper bound for the averaged likelihood ratio as follows:</p><formula xml:id="formula_166">P prompt CoT (n) | θ P prompt CoT (n) | θ * ≤ exp -2nH 2 P(Z 0 , Z 1 , • • • , Z H-1 , Y | θ * ), P(Z 0 , Z 1 , • • • , Z H-1 , Y | θ) -2H 2 P(Z 0 | θ * ), P(Z 0 | θ) + 2 log δ -1 ≤ exp -2nλ + 2 log δ -1 . (C.13)</formula><p>Here the first inequality follows from the Lemma C.3, where we leverage the conditional independence between the n CoT demonstrations and the query given task parameter θ.</p><p>The second inequality is due to the Assumption 5.4, which specifies that θ and θ * are strictly separated with a margin λ, and the non-negativity of the Hellinger distance. Combing (C.13) with (C.12), with probability at least 1 -δ, we have</p><formula xml:id="formula_167">KL P(y test = • | z test 0 , θ * ), P y test = • | prompt CoT (n) ≤ log 1 + e 2nα+2α 0 • θ∈ Θ ∁ π Θ eq (θ) P prompt CoT (n) | θ P prompt CoT (n) | θ * π Θ eq (θ * ) ≤ log 1 + e 2nα+2α 0 π Θ eq (θ * ) • θ∈ Θ ∁ exp -2nλ + 2 log δ -1 | Θ ∁ | π Θ eq (θ) = log 1 + 1 -π Θ eq (θ * ) π Θ eq (θ * ) • exp -2n(λ -α) + 2α 0 + 2 log δ -1 | Θ ∁ | .</formula><p>Here the first inequality follows from (C.12) and the fact that Θ ∁ is discrete and finite. The second inequality is due to (C.13), and the final line results from rearranging terms.</p><p>In sum, we have that when Θ ∁ is discrete and finite, with probability at least 1 -δ we have</p><formula xml:id="formula_168">KL P(y test = • | z test 0 , θ * ), P y test = • | prompt CoT (n) = O π Θ eq (θ * ) -1 • δ -2 • | Θ ∁ | 2 • e -2(λ-α)n+2α 0 , (C.14)</formula><p>where O(•) only hides absolute constants. We conclude that when Θ is discrete and finite, the CoT prompting error decays exponentially to zero and it depends on Θ ∁ only through | Θ ∁ |. The upper bound in (C.14) establishes Theorem 5.7.</p><p>Step 3: Convergence rate for the continuous case. It remains to consider the case where Θ is continuous. Similar to the proof of Theorem C.1, we the α-cover with respect to the likelihood metric ∥ • ∥ Θ to discretize Θ ∁ at the cost of introducing an additional error.</p><p>For any α &gt; 0, let C(α) denote an α-covering of Θ ∁ with covering number N (α) and let {C(θ i )} θ∈N (α) denote the partition of Θ ∁ induced by C(α). We bound the discretization error due to the α-cover as follows:</p><formula xml:id="formula_169">log θ∈ Θ ∁ P prompt CoT (n) | θ π Θ eq (θ) dθ θ∈C(α) P prompt CoT (n) | θ π C n (θ) = log θ∈C(α) π C(θ) θ ′ ∈C(θ) P prompt CoT (n) | θ ′ π Θ eq (θ ′ ) /π(C(θ) dθ ′ θ∈C(α) P prompt CoT (n) | θ π C(θ) ≤ log max θ∈C(α) θ ′ ∈C(θ) P prompt CoT (n) | θ ′ π Θ eq (θ ′ ) /π C(θ) dθ ′ P prompt CoT (n) | θ ≤ log sup θ∈C(α),θ ′ ∈C(θ) P prompt CoT (n) | θ ′ P prompt CoT (n) | θ ≤ nα. (C.15)</formula><p>Here the first equality follows from decomposing the integral over Θ ∁ into a double integral using the partition structure. The second and third inequalities follow from the generalized mediant inequality. The last inequality is derived from the definition of C(α).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Now we have controlled the discretization error, combing with analysis from</head><p>Step 2, we obtain that the following inequality holds with probability at least 1 -δ:</p><formula xml:id="formula_170">KL P(y test = • | z test 0 , θ * ), P y test = • | prompt CoT (n) ≤ log 1 + e 2nα+2α 0 • θ∈ Θ ∁ π Θ eq (θ) P prompt CoT (n) | θ dθ P prompt CoT (n) | θ * π Θ eq (θ * ) ≤ log 1 + e 2nα+2α 0 • θ∈C(α) P prompt CoT (n) | θ π C n (θ) P prompt CoT (n) | θ * π Θ eq (θ * ) + nα ≤ log 1 + 1 -π Θ eq (θ * ) π Θ eq (θ * ) • exp -2n(λ -α) + 2α 0 + 2 log δ -1 N (α) + nα,</formula><p>where O(•) only hides absolute constants, and the randomness comes from the stochasticity of CoT prompts prompt CoT (n). The first inequality follows from (C.12), and the second inequality follows from (C.15). The final inequality is due to Lemma C.3. Therefore, we conclude that with probability at least 1 -δ we have</p><formula xml:id="formula_171">KL P(y test = • | z test 0 , θ * ), P y test = • | prompt CoT (n) = O δ -2 • π Θ eq (θ * ) -1 • N (α) 2 • e -2n(λ-α)+2α 0 + nα ,</formula><p>where O(•) omits absolute constants. Thus, we conclude the proof.</p><p>D Proofs of the Results in Section 5.2 D.1 Proof of Corollary 5.9</p><p>Proof. There are two notions of sample size in self-consistency CoT: the number of examples in CoT prompt n, and the number of reasoning paths K. These two notions have different roles. A large n ensures that the distribution of the perfectly pretrained LLM, P y test = • | prompt CoT (n) , approximates the desired distribution P y test = • | z test 0 , θ * . Whereas a large K ensures that the sample mode y * k approximates the population mode. In the following, we prove the corollary in two steps. We first show that the population mode of P y test = • | prompt CoT (n) coincides with y * when n is sufficiently large. Then we prove that y * K finds the population mode of P(y test = • | prompt CoT (n)) when K is sufficiently large. The final statistical error can be obtained by combining these two steps.</p><p>Step 1: Mode of P(y test = • | prompt CoT (n)) converges to y * . In the first step, we show that there exists n * such that, as long as n ≥ n * , the mode of P(y test = • | prompt CoT (n)) coincides with y * , the mode of P y test = • | z test 0 , θ * . This step leverages Theorem 5.5. Suppose there exists some ξ &gt; 0 such that</p><formula xml:id="formula_172">KL P(y test = • | z test 0 , θ * ), P y test = • | prompt CoT (n) ≤ ξ,</formula><p>Then by Pinsker's inequality, we obtain a bound on the TV distance,</p><formula xml:id="formula_173">TV (P(y test = • | z test 0 , θ * ), P y test = • | prompt CoT (n) ≤ 2ξ.</formula><p>Recall that we assume that y test belongs to a finite set Y. Then we have 2ξ ≥ TV (P(</p><formula xml:id="formula_174">y test = • | z test 0 , θ * ), P y test = • | prompt CoT (n) = 1 2 y∈Y P(y test = y | z test 0 , θ * ) -P y test = y | prompt CoT (n) ≥ 1 2 max y∈Y P(y test = y | z test 0 , θ * ) -P y test = y | prompt CoT (n) .</formula><p>Thus we can sandwich P(y test = • | prompt CoT (n)) for any y by</p><formula xml:id="formula_175">P(y test = y | z test 0 , θ * ) -2 2ξ ≤ P y test = y | prompt CoT (n) ≤ P(y test = y | z test 0 , θ * ) + 2 2ξ.</formula><p>We where the first inequality follows from (D.1), the second follows from the definition of ϵ, and the last one follows from (D.2). Hence, as long as ϵ -4 √ 2ξ &gt; 0, we ensure that y * is also the unique mode of P(y test = • | prompt CoT (n)). Now if we set the prompt size to be</p><formula xml:id="formula_176">n * = C • log |Θ ∁ |/π(θ * ) + log(1/ϵ) λ, (D.4)</formula><p>where C is a sufficiently large absolute constant. We now leverage Theorem 5.5 with a perfectly pretrained LLM. Specifically, setting δ = e -λn/2 in (C.7), we conclude that, with probability at least 1 -e -λn/2 , when n ≥ n * , we have</p><formula xml:id="formula_177">KL P(y test = • | z test 0 , θ * ), P y test = • | prompt CoT (n) ≤ ϵ 2 /128. (D.5)</formula><p>Combining (D.3) and (D.5), we conclude that, when n is sufficiently large such that n ≥ n * , with probability 1 -e -λn/2 ,</p><formula xml:id="formula_178">P y test = y * | prompt CoT (n) ≥ max y̸ =y * P y test = y | prompt CoT (n) + ϵ/2, (D.6)</formula><p>where n * is defined in (D.4). Thus, y * is also the mode of P(y test = y * | prompt CoT (n)) with high probability. Now we conclude Step 1.</p><p>Step 2: Sample mode y * K converges to population mode y * . In this step, we utilize concentration to show that y * K converges to y * when K is sufficiently large. Recall that we assume y test takes values in a finite set Y and also recall that we define an empirical distribution p K (y) = K -1 K i=1 1{y test,i = y} for all y ∈ Y. Thus, for any y ∈ Y, Kp K (y) is a binomial variable with distribution Bin(K, p , where p = P(y test = y | prompt CoT (n)). Thus by Bernstein's inequality for binomial distribution, for any t &gt; 0 we have</p><formula xml:id="formula_179">P p K (y) -P y test = y | prompt CoT (n) ≥ t ≤ 2 exp - 3Kt 2 6 • P y | prompt CoT (n) 1 -P(y | prompt CoT (n)) + 2t ≤ 2 exp - 6Kt 2 3 + 4t</formula><p>where the second inequality follows from the fact that p(1 -p) ≤ 1/4 for any p ∈ R. Now we set t = ϵ/4 and take a union bound over y ∈ Y to obtain that</p><formula xml:id="formula_180">P max y∈Y p K (y) -P y test = y | prompt CoT (n) ≥ ϵ/4 ≤ 2|Y| • exp - 3Kϵ 2 24 + 8ϵ . (D.7)</formula><p>Recall that y * K and y * are the modes of p K and P(y test = • | prompt CoT (n)), respectively. Also note that (D.6) implies that there is a gap of ϵ/2 between the mode of P(y test = • | prompt CoT (n)) and its second largest probability mass. If y * K ̸ = y * , there must exist some y ∈ Y such that |p K (y)</p><formula xml:id="formula_181">-P y test = y | prompt CoT (n) ≥ ϵ/4.</formula><p>Therefore, we can upper bound P y * K ̸ = y * | prompt CoT (n) using (D.7). Therefore, we conclude that with a perfectly pretrained LLM and a discrete and finite Θ, when is sufficiently large such that n ≥ n * , we have</p><formula xml:id="formula_182">P y * K ̸ = y * | prompt CoT (n) ≤ 2|Y| • exp - 3Kϵ 2 24 + 8ϵ</formula><p>with probability at least 1 -e -λn/2 . Here n * is define in (D.4). Thus we conclude the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2 Proof of Proposition 5.11</head><p>Proof. In ToT prompting, there are two notions of sample size: the number of examples in the CoT prompt n and the number of candidates generated at each step K. Additionally, there is a breadth limit parameter B, which controls the number of candidates that continue to the next step. For the ease of notation, we write z </p><formula xml:id="formula_183">= • | prompt h (n), t * h-1 ) approximates the true distribution P(z test h = • | t * h-1 , θ * ) for each h ∈ [H].</formula><p>A large K ensures that the optimal z * h appears in the samples for each h ∈ [H], which is then selected by BFS.</p><p>The proof involves two steps. First, we show that for any h ∈ [H],</p><formula xml:id="formula_184">p h = P(z test h = z * h | prompt h (n), t * h-1 ) is close to p * h = P(z test h = z * h | t * h-1 , θ * ) when n is large enough, where z * h is the optimal next step z * h = argmax z h V θ * (t * h-1 , z h ).</formula><p>Second, we demonstrate that a large K helps find the optimal trajectory t * H by iteratively sampling from the LLM-induced distribution P(z</p><formula xml:id="formula_185">test h = • | prompt h (n), t * h-1</formula><p>). Finally, we combine both arguments to present the statistical error for ToT prompting.</p><p>Step 1: LLM-induced probability p h approximates p * h for large n. In this step, we show p h approaches p * h as n increases, which is similar to Step 1 of the proof of Theorem 5.5. For each h ∈ [H], prompt h (n) contains the CoT examples in terms of the prediction in the h-th step. We want to lower bound the probability of outputting z * h by prompting a perfectly pretrained LLM. For simplicity, we assume Θ is finite and discrete. For each h ∈ [H], using Bayes rule, we write the posterior as</p><formula xml:id="formula_186">π(θ * | prompt h (n), t * h-1 ) = 1 + θ̸ =θ * P(prompt h (n) | θ)P(t * h-1 | θ)π(θ) P(prompt h (n) | θ * )P(t * h-1 | θ * )π(θ * ) -1</formula><p>.</p><p>Under Assumption 5.10, θ * maximizes P(t * h | θ) for each h ∈ [H]. Thus, we have</p><formula xml:id="formula_187">π(θ * | prompt h (n), t * h-1 ) ≥ 1 + θ̸ =θ * P(prompt h (n) | θ)π(θ) P(prompt h (n) | θ * )π(θ * ) -1 ≥ 1 + θ̸ =θ * π(θ) π(θ * ) • exp -2nH 2 P(t h | θ), P(t h | θ * ) + 2 log(δ -1 |Θ|) -1 ≥ 1 + 1 -π(θ * ) π(θ * ) • exp -2nλ h + 2 log(δ -1 |Θ|) -1 (D.8)</formula><p>with probability at least 1 -δ. Here the second inequality follows from Lemma C.3 in the proof of Theorem 5.5 and the third inequality follows from Assumption 5.10. Thus we derive a lower bound for p h as</p><formula xml:id="formula_188">p h = θ∈Θ P(z * h | t * h-1 , θ)π(θ | t * h-1 , prompt h (n)) ≥ P(z * h | t * h-1 , θ * )π(θ * | t * h-1 , prompt h (n)) ≥ p * h • 1 + 1 -π(θ * ) π(θ * ) • exp -2nλ h + 2 log(δ -1 |Θ|) -1</formula><p>.</p><p>with probability at least 1 -δ with respect to the randomness of the CoT prompt. The first inequality follows from omitting terms corresponding to θ ̸ = θ * . The second inequality is due to (D.8). Now we use the fact that (1</p><formula xml:id="formula_189">+ x) -1 ≥ 1 -x for x ∈ [0, 1] to obtain that p * h -p h ≤ p * h • 1 -π(θ * ) π(θ * ) • exp -2nλ h + 2 log(δ -1 |Θ|) . (D.9)</formula><p>Therefore, we conclude that p * h -p h , the difference in the probabilities evaluated at z * h for LLM-induced distribution and the true distribution, decreases exponentially with the number of examples n. Recall that we define λ * = min h∈[H] λ h . For any ϵ ∈ (0, 1), we define</p><formula xml:id="formula_190">n * = 1 λ * 2 log H|Θ| + log (1 -π(θ * ))/π(θ * ) + log(1/ϵ) . (D.10)</formula><p>Then by taking δ = e -nλ * /2 and combining (D.9), we conclude that with probability at least 1 -e -nλ * /2 , when n ≥ n * , p h ≥ p * h • (1 -ϵ) holds for every h ∈ [H]. Thus we conclude Step 1.</p><p>Step 2: Large K improves the selection of z * h . For any h ∈ [H], let</p><formula xml:id="formula_191">C h = B b=1 K i=1 1{(t b h-1 , z b,i h ) = t * h }</formula><p>denote the number of candidates in T h that match the optimal partial history t * h at step h ∈ [H], where we define T h in (5.4). Recall that we set B = 1. Therefore, each C h , h ∈ [H] is a binomial random variable, where</p><formula xml:id="formula_192">C 1 ∼ Bin(K, p 1 ), C h ∼ Bin(min{B = 1, C h-1 }K, p h ) for 2 ≤ h ≤ H.</formula><p>The algorithm outputs t * H if and only if C h ≥ 1 for all h ∈ [H]. The following gives the probability of outputting the optimal trajectory from the search tree:</p><formula xml:id="formula_193">P t H = t * H | prompt CoT (n) = P C 1 , • • • , C H ≥ 1 | prompt CoT (n) ≥ H h=1 P C h ≥ 1 | prompt h (n), t * h-1 = H h=1 1 -(1 -p h ) K ≥ 1 - H h=1 1 -p h K . (D.11)</formula><p>The last inequality is because m i=1 (1</p><formula xml:id="formula_194">-x i ) ≥ 1 -m i=1 x i for x i ∈ [0, 1].</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Combining (D.11) and the conclusion of</head><p>Step 1, we conclude that with a perfectly pretrained LLM and a discrete and finite Θ, ToT prompting using BFS with B = 1 incurs the following statistical error with probability at least 1 -e -nλ * /2 :</p><formula xml:id="formula_195">P t H ̸ = t * H | prompt CoT (n) ≤ H h=1 1 -p * h + p * h ϵ K .</formula><p>Here n is sufficiently large such that n ≥ n * where n * is defined in (D.10) and ϵ ∈ (0, 1) is an arbitrary number. Therefore, we conclude the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3 Proof of Corollary 5.13</head><p>Proof. The main idea of selection-inference prompting is to break down each step in vanilla CoT into two separate stages: selection and inference. We decompose each task θ into two components: θ se and θ in , with underlying distributions P(τ h = • | t h-1 , θ se ) and P(z h = • | τ h , θ in ), respectively. We follow and modify the proof for vanilla CoT in Appendix C.2 to derive the statistical rate of SI prompting. The proof consists of two steps. We first derive an upper bound for the KL divergence using a ratio of two integrals. Then we bound such a ratio using the separation among the probability distributions with different parameters in Θ.</p><p>Step 1: Deriving an upper bound for the KL divergence. We invoke Proposition C.2 to derive an upper bound for the KL divergence as follows:</p><formula xml:id="formula_196">KL P(y test = • | z test 0 , θ * ), P SI y test = • | S se (n), S in (n), z test 0 ≤ log 1 + Θ ∁ P S se (n), S in (n), z test 0 θ π(θ)dθ Θeq(θ * ) P S se (n), S in (n), z test 0 θ ′ π(θ ′ )dθ ′ ≤ log 1 + Θ ∁ P S se (n), S in (n), z test 0 θ π(θ) P S se (n), S in (n), z test 0 θ * π(θ * ) . (D.12)</formula><p>The first inequality is obtained by applying Proposition C.2 with substituting prompt CoT (n) by (S se (n), S in (n), z test 0 ), and in the second inequality we exclude all terms corresponding to θ ̸ = θ * in the denominator. We can directly apply Proposition C.2 since this proposition only requires the prompt to be generated from P(• | θ * ), but does not assume a specific statistical dependency relationship in the prompt. Here P SI (y test | S se (n), S in (n), z test 0 ) is the marginal distribution of y test according to (5.7).</p><p>Step 2: Statistical rate for discrete and finite Θ. The key to analyzing (D.12) is to derive an upper bound of the likelihood ratio</p><formula xml:id="formula_197">P S se (n), S in (n), z test 0 θ /P S se (n), S in (n), z test 0 θ * .</formula><p>Recall that we define S se (n) and S in (n) in (5.6). By construction, the likelihood of a single piece of trajectory can be decomposed as</p><formula xml:id="formula_198">P S se (1), S in (1) θ = P(z 0 | θ) H h=1 P τ h θ se , t h-1 • H j=1 P z j θ in , τ j . (D.13)</formula><p>Note that the n reasoning paths are independent conditioning on θ. We decompose the likelihood ratios into a sum of independent terms according to (D.13) and then apply Lemma H.2. With probability at least 1 -δ, we have</p><formula xml:id="formula_199">1 2 log P(S se (n), S in (n) | θ) P(S se (n), S in (n) | θ * ) = 1 2 n i=1 log P(z i 0 | θ) P(z i 0 | θ * ) + H h=1 log P τ i h θ se , t i h-1 P τ i h θ * se , t i h-1 + H j=1 log P z i j θ in , τ i j P z i j θ * in , τ i j ≤ n i=1 log E θ * P(z i 0 | θ) P(z i 0 | θ * ) 1/2 + H h=1 n k=1 log E θ * P(τ k h θ se , t k h-1 ) P(τ k h θ * se , t k h-1 ) 1/2 + H j=1 n m=1 log E θ * P z m j θ in , τ m j P z m j θ * in , τ m j 1/2 + log((2H + 1)δ -1 ). (D.14)</formula><p>Here the first equality follows from summing over the decomposition of likelihood ratios as shown in (D.13), and inequality follows is obtained by applying Lemma H.2 to each sum.</p><p>Using (D.14) and the fact that x -1 ≥ log(x), we further obtain that</p><formula xml:id="formula_200">1 2 log P(S se (n), S in (n) | θ) P(S se (n), S in (n) | θ * ) ≤ n i=1 E θ * P(z i 0 | θ) P(z i 0 | θ * ) 1/2 -1 + H h=1 E θ * P(τ i h θ se , t i h-1 ) P(τ i h θ * se , t i h-1 ) 1/2 -1 + H j=1 E θ * P z i j θ in , τ i j P z i j θ * in , τ i j 1/2 -1 + log (2H + 1)δ -1 = -n H h=1 E θ * H 2 P(τ h | θ * , t h-1 ), P(τ h | θ, t h-1 ) + E θ * H 2 P(z h | θ * , τ h ), P(z h | θ, τ h ) -nH 2 P(z 0 | θ * ), P(z 0 | θ) + log (2H + 1)δ -1 . (D.15)</formula><p>Here the final line follows from the definition of Hellinger distance. Now we apply Assumption 5.12 to derive an upper bound of the likelihood ratio using the constant λ SI = λ S + λ I + λ q , replacing the constant λ from Theorem 5.5. Combing (D.15) and Assumption 5.12, we have that when Θ is discrete and finite,</p><formula xml:id="formula_201">P S se (n), S in (n), z test 0 | θ P S se (n), S in (n), z test 0 | θ *</formula><p>≤ exp -2nλ SI + 2 log (2H + 1)δ -1 with probability at least 1 -δ.</p><p>Therefore, we have that when Θ ∁ is discrete and finite, then with probability at least 1 -δ,</p><formula xml:id="formula_202">KL P(y test = • | z test 0 , θ * ), P SI y test = • | S se (n), S in (n), z test 0 = O π(θ * ) -1 • δ -2 • |Θ ∁ | 2 • e -2λ SI n ,</formula><p>where O(•) omits only absolute constants. Therefore, we conclude the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2 Proof of Corollary E.2</head><p>Proof. To simplify the notation, let</p><formula xml:id="formula_203">E θ * denote E prompt CoT (n)∼P(• | θ * )</formula><p>. First, we compute the difference of KL divergences of two index sets J and J ′ on a fixed task θ * :</p><formula xml:id="formula_204">E θ * KL P(y test = • | z test 0 , θ * ), P y test = • | prompt J (n) -E θ * KL P(y test = • | z test 0 , θ * ), P y test = • | prompt J ′ (n) = E θ * E y test ∼P(• | z test 0 ,θ * ) log P y test | prompt J ′ (n) P y test | prompt J (n) . (E.1)</formula><p>Next, we take expectation of (E.1) with respect to θ * ∼ π to obtain</p><formula xml:id="formula_205">E π E θ * E y test ∼P(• | z test 0 ,θ * ) log P y test | prompt J ′ (n) P y test | prompt J (n) = L×L * P prompt J ′ (n) log P y test | prompt J ′ (n) P y test | prompt J (n) • Θ π θ * | prompt J ′ (n) P(y test | prompt J ′ (n), θ * )dθ * dy test dprompt J ′ (n). (E.2)</formula><p>Applying the Bayes' rule, we have</p><formula xml:id="formula_206">L Θ π θ * | prompt J ′ (n) • P(y test | prompt J ′ (n), θ * ) • log P y test | prompt J ′ (n) P y test | prompt J (n) dθ * dy test = E y test ∼P(• | prompt J ′ (n)) log P y test | prompt J ′ (n) P y test | prompt J (n) . (E.3)</formula><p>Thus, by (E.3) and interchanging the order of integration in (E.2), we have</p><formula xml:id="formula_207">E π E θ * E y test ∼P(• | z test 0 ,θ * ) log P y test | prompt J ′ (n) P y test | prompt J (n) = E prompt J ′ (n)∼P E y test ∼P(• | prompt J ′ (n)) log P y test | prompt J ′ (n) P y test | prompt J (n) = E P KL P y test = • | prompt J ′ (n) , P y test = • | prompt J (n) ≥ 0. (E.4)</formula><p>Therefore, the expectation of the difference between the two KL divergences in (E.2) can be expressed as (E.4), another KL divergence and thus is nonnegative for any number of samples n. Therefore, we conclude that on average, truncated CoT methods with steps J ′ ⊆ [H -1] is no worse than truncated CoT methods with fewer intermediate steps J ⊂ J ′ . Since vanilla ICL corresponds to the special case where J = ∅, we conclude that on average, CoT is no worse than vanilla ICL. This completes the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Proof and Auxiliary Results of Section 6</head><p>In this section, we prove the results in Section 6. In particular, in Section F.1 we prove Proposition 6.3, in Section F.2 we introduce the details of the approximation error analysis for pretraining, and in Section F.4 we prove Corollary 6.6.</p><p>F.1 Proof of Proposition 6.3</p><p>In the following, we prove Proposition 6.3 under the generalized multi-step latent variable model introduced in Section A, which contains the model in (4.2) as a special case.</p><p>Proof. In this proof, we adopt the PAC-Bayes framework <ref type="bibr" target="#b34">(McAllester, 1998;</ref><ref type="bibr" target="#b4">Alquier, 2021)</ref> to decompose the error and control each component. This proof consists of two steps. We first decompose the pretraining error into three parts using the PAC-Bayes framework, and then control each term to conclude the proof. Our proof is adapted from <ref type="bibr" target="#b73">Zhang et al. (2023a)</ref>, which analyze the generalization error for pretraining an LLM based with ICL data, i.e., H = 1. We explain the structure of the proof in detail and highlight the similarities and differences from <ref type="bibr" target="#b73">Zhang et al. (2023a)</ref>.</p><p>Step 1: Error decomposition using the PAC-Bayes framework. We fist decompose the pretraining error</p><formula xml:id="formula_208">E D TV P(• | S), P ρ (• | S)</formula><p>2 to prepare for further analysis. Recall that the pretraining dataset with N trajectories with T examples is D N,T = {(S t,ℓ h , z t,ℓ h )} H,T,N h=0,t=1,ℓ=1 , where z t,ℓ h ∼ P(• | S t,ℓ h ), and S t,ℓ h+1 = S t,ℓ h ∪ {z t,ℓ h }. Under the construction of D N,T under the general model in (A.1), the training data admits a sequential structure. For any h ∈ {0, . . . , H}, t ∈ [T ] and ℓ ∈ [N ], we let F t,ℓ h denote a σ-algebra defined as</p><formula xml:id="formula_209">F t,ℓ h = σ-algebra z t ′ ,ℓ ′ h ′ : ℓ &lt; ℓ, or ℓ ′ = ℓ, t ′ &lt; t, or ℓ ′ = ℓ, t ′ = t, h ′ &lt; h ,</formula><p>which is the σ-algebra generated by all the random variables in D N,T appearing before z t,ℓ h . Moroever, we define a sequence of ghost samples as D N,T = {( S t,ℓ h , z t,ℓ h )} H,T,N h=0,t=1,ℓ=1 , where z t,ℓ h ∼ P(• | S t,ℓ h ), and S t,ℓ h = S t,ℓ h . Here z t,ℓ h is independent of z t,ℓ h and all random variables in D N,T generated later than S t,ℓ h . In the following, we use this ghost sample and Donsker-Varadhan representation to decompose the error. Donsker-Varadhan representation <ref type="bibr" target="#b31">(MacKay, 2003)</ref> states that, for any distribution P, Q ∈ P LLM and for any function g :</p><formula xml:id="formula_210">P LLM → R such that E ρ∼Q [exp g(ρ) ] &lt; ∞, we have E ρ∼P [g(ρ)] ≤ KL(P, Q) + log E ρ∼Q [exp g(ρ) ]. (F.1)</formula><p>To proceed, we choose Q ∈ P LLM independent of both the dataset D N,T and ghost dataset D N,T and P ∈ P LLM to be potentially dependent on the dataset D N,T but independent of the newly sampled { z t,ℓ h } in the ghost dataset. To simplify the notation, we omit the subscripts and write the datasets as D and D respectively. In the sequel, we use E D or E D to denote the expectation with respect to the joint distribution of D and D, respectively. We set the function g as g</p><formula xml:id="formula_211">(ρ) = L(ρ, D) -log E D [exp(L(ρ, D)) | D], where L(ρ, D) = - 1 4 N ℓ=1 T t=1 H h=0 log P( z t,ℓ h | S t,ℓ h ) P ρ ( z t,ℓ h | S t,ℓ h ) . (F.2)</formula><p>Moreover, for any ℓ ∈ [N ], t ∈ [T ], and h ∈ {0, . . . , H}, we let L ℓ,t,h (ρ, D) denote the partial sum of L(ρ, D) with the last term being -1/4 • (log P( z t,ℓ h | S t,ℓ h ) -log P ρ ( z t,ℓ h | S t,ℓ h )). We note that L(ρ, D) itself is a random variable where the randomness stems from both ρ and D.</p><p>Exponentiating both sides of (F.1) and taking expectations with respect to D on both sides, we have</p><formula xml:id="formula_212">E D exp E ρ∼P L(ρ, D) -log E D [exp(L(ρ, D))] -KL(P, Q) ≤ E D E ρ∼Q [exp L(ρ, D) -log E D [exp(L(ρ, D)) | D] ] = E ρ∼Q E D [exp L(ρ, D) -log E D [exp(L(ρ, D))] | D ] = E ρ∼Q E D exp(L(ρ, D)) E D [exp(L(ρ, D)) | D]</formula><p>,</p><p>where in the first equality we exchange the order of expectations due to the independence between the dataset D and the prior Q. By the construction of L(ρ, D) in (F.2), we have</p><formula xml:id="formula_213">E D [exp(L(ρ, D)) | D] = N ℓ=1 T t=1 H h=0 E z t,ℓ h exp -1/4 • log P( z t,ℓ h | S t,ℓ h ) -log P ρ ( z t,ℓ h | S t,ℓ h ) .</formula><p>Notice that this is a random variable that is measurable under F N,T,H . Moreover, conditioning on F N,T,H , we have</p><formula xml:id="formula_214">E D exp(L(ρ, D)) F N,T,H = exp L N,T,H-1 (ρ, D) • E z T,N H exp -1/4 • log P(z T,N H | S T,N H ) -log P ρ (z T,N H | S T,N H ) .</formula><p>Since z T,N H and z T,N H have the same conditional distribution, we obtain that</p><formula xml:id="formula_215">E D exp(L(ρ, D)) E D [exp(L(ρ, D)) | D] F N,T,H = exp L N,T,H-1 (ρ, D) E D exp(L N,T,H-1 (ρ, D) | D]</formula><p>.</p><p>Then, using the tower property, we similarly have</p><formula xml:id="formula_216">E D E D exp(L(ρ, D)) E D [exp(L(ρ, D)) | D] F N,T,H F N,T,H-1 = E D exp L N,T,H-1 (ρ, D) E D exp(L N,T,H-1 (ρ, D) | D] F N,T,H-1 = exp L N,T,H-2 (ρ, D) E D exp(L N,T,H-2 (ρ, D) | D] . (F.3)</formula><p>Recursively apply conditional expectations to (F.3) with respect to the filtration {F ℓ,t,h }, we obtain that</p><formula xml:id="formula_217">E D exp(L(ρ, D)) E D [exp(L(ρ, D)) | D] = 1.</formula><p>Therefore, we have</p><formula xml:id="formula_218">E D exp E ρ∼P L(ρ, D) -log E D [exp(L(ρ, D))] -KL(P, Q) ≤ 1.</formula><p>Applying the Chernoff bound to it, we obtain a high probability bound as follows. With probability at least 1 -δ, we have</p><formula xml:id="formula_219">-E ρ∼P log E D exp L(ρ, D) ≤ -E ρ∼P L(ρ, D) + KL(P, Q) + log 1 δ . (F.4)</formula><p>Now we separately bound the left-hand side and the right-hand side of (F.4). Similar to the derivation in <ref type="bibr" target="#b73">Zhang et al. (2023a)</ref>, for the left-hand side of (F.4), using the definition of L(ρ, D) and Cauchy-Schwarz inequality, we have</p><formula xml:id="formula_220">log E D exp L(ρ, D) | D (F.5) = log E D exp - 1 4 N ℓ=1 T t=1 H h=0 log P( z t,ℓ h | S t,ℓ h ) P ρ ( z t,ℓ h | S t,ℓ h ) D = log E D exp - 1 4 N ℓ=1 T t=1 H h=0 log P( z t,ℓ h | S t,ℓ h ) P ρ ( z t,ℓ h | S t,ℓ h ) + log P ρ ( z t,ℓ h | S t,ℓ h ) P ρ ( z t,ℓ h | S t,ℓ h ) D ≤ 1 2 log E D exp - 1 2 N ℓ=1 T t=1 H h=0 log P( z t,ℓ h | S t,ℓ h ) P ρ ( z t,ℓ h | S t,ℓ h ) D + 1 2 log E D exp - 1 2 N ℓ=1 T t=1 H h=0 log P ρ ( z t,ℓ h | S t,ℓ h ) P ρ ( z t,ℓ h | S t,ℓ h ) D ,</formula><p>where the last inequality follows from Cauchy-Schwarz inequality. To see this, note that for two random variables X and Y , Cauchy-Schwarz inequality implies that</p><formula xml:id="formula_221">E[exp((X + Y )/2)] ≤ E[exp(X)] • E[exp(Y )]. According to the definition of Hellinger distance, we have 1-H 2 (Q 1 , Q 2 ) = x q 1 (x)q 2 (x)dx = E x∼Q 2 q 1 (x)/q 2 (x)</formula><p>. Therefore, we can rewrite the first term on the right-hand side of (F.5) using Hellinger distance as follows,</p><formula xml:id="formula_222">1 2 log E D exp - 1 2 N ℓ=1 T t=1 H h=0 log P( z t,ℓ h | S t,ℓ h ) P ρ ( z t,ℓ h | S t,ℓ h ) D = 1 2 log E D N ℓ=1 T t=1 H h=0 P ρ ( z t,ℓ h | S t,ℓ h ) P( z t,ℓ h | S t,ℓ h ) D , = 1 2 N ℓ=1 T t=1 H h=0 log 1 -H 2 (P ρ (• | S t,ℓ h ), P(• | S t,ℓ h )) . (F.6)</formula><p>Due to the fact that log(1 -x) ≤ -x for x ∈ [0, 1), we further upper bound (F.6) as follows, RHS of (F.6) ≤ -1 2</p><formula xml:id="formula_223">N ℓ=1 T t=1 H h=0 H 2 (P ρ (• | S t,ℓ h ), P(• | S t,ℓ h )) ≤ - 1 4 N ℓ=1 T t=1 H h=0 TV 2 P(• | S t,ℓ h ), P ρ (• | S t,ℓ h ) , (F.7)</formula><p>where the second line follows from the fact that 2H</p><formula xml:id="formula_224">2 (Q 1 , Q 2 ) ≥ TV 2 (Q 1 , Q 2 ). Applying (F.7)</formula><p>to the left-hand side of (F.4), we thus have</p><formula xml:id="formula_225">-E ρ∼P log E D exp L(ρ, D) | D ≥ 1 4 N ℓ=1 T t=1 H h=0 TV 2 P(• | S t,ℓ h ), P ρ (• | S t,ℓ h ) (F.8) - 1 2 E ρ∼P log E D exp - 1 2 N ℓ=1 T t=1 H h=0 log P ρ ( z t,ℓ h | S t,ℓ h ) P ρ ( z t,ℓ h | S t,ℓ h ) D .</formula><p>Next, we upper bound the right-hand side of (F.4). For any ρ ′ ∈ P LLM , we have</p><formula xml:id="formula_226">-E ρ∼P L(ρ, D) + KL(P, Q) + log( 1 δ ) (F.9) = 1 4 N ℓ=1 T t=1 H h=0 log P(z t,ℓ h | S t,ℓ h ) P ρ ′ (z t,ℓ h | S t,ℓ h ) + 1 4 N ℓ=1 T t=1 H h=0 E ρ∼P log P ρ ′ (z t,ℓ h | S t,ℓ h ) P ρ (z t,ℓ h | S t,ℓ h ) + KL(P, Q) + log( 1 δ ) ≤ 1 4 N ℓ=1 T t=1 H h=0 log P(z t,ℓ h | S t,ℓ h ) P ρ ′ (z t,ℓ h | S t,ℓ h ) + 1 4 N ℓ=1 T t=1 H h=0 E ρ∼P log P ρ (z t,ℓ h | S t,ℓ h ) P ρ (z t,ℓ h | S t,ℓ h ) + KL(P, Q) + log( 1 δ ),</formula><p>where the last inequality holds by noting that ρ maximizes the likelihood function.</p><p>We next choose ρ ′ as the projection of P (in terms of the KL divergence) onto the space of all parameterized learnable models {P ρ | ρ ∈ P LLM }, i.e.,</p><formula xml:id="formula_227">ρ ′ = argmin ρ * ∈P LLM E S∼D KL P(•|S)∥P ρ * (•|S) .</formula><p>Combining inequalities (F.8) and (F.9), we thus upper bound the desired pretraining error as a sum of a few terms as follows 1 4</p><formula xml:id="formula_228">N ℓ=1 T t=1 H h=0 TV 2 P(• | S t,ℓ h ), P ρ (• | S t,ℓ h ) ≤ 1 2 E ρ∼P log E D exp - 1 2 N ℓ=1 T t=1 H h=0 log P ρ (z t,ℓ h | S t,ℓ h ) P ρ (z t,ℓ h | S t,ℓ h ) D (I.i) (F.10) + 1 4 N ℓ=1 T t=1 H h=0 E ρ∼P log P ρ (z t,ℓ h | S t,ℓ h ) P ρ (z t,ℓ h | S t,ℓ h ) (I.ii) + 1 4 N ℓ=1 T t=1 H h=0 log P(z t,ℓ h | S t,ℓ h ) P ρ ′ (z t,ℓ h | S t,ℓ h ) (II) + KL(P, Q) (III) + log 1 δ .</formula><p>Here the The first two errors (I.i) and (I.ii) represent the fluctuation error due to the randomness of ρ ∼ P , (II) is the approximation error that characterizes the discrepancy between the true distribution P and its best approximator P ρ ′ , and (III) is the KL divergence between P and Q. Note that the left-hand side of (F.10) can be written as</p><formula xml:id="formula_229">N T • (H + 1)/4 • E S∼D TV 2 P(• | S), P ρ (• | S) .</formula><p>With the error decomposition in (F.10), we conclude Step 1. In the following, we will specify distributions P and Q.</p><p>Step 2: Control each term in the decomposition of pretraining error. In this step, we control each term in the error decomposition (F.10). Our first step is to control the fluctuation errors (I.i) and (I.ii), which describe the log density ratio between P ρ and P ρ . The errors (I.i) and (I.ii) are small when ρ is close to ρ. Therefore we control these two terms by setting the support of P to be a neighborhood around ρ. Specifically, for each weight matrix and residual link specified by</p><formula xml:id="formula_230">ρ = W softmax , {W d ff,1 , W d ff,2 , {W Q,d i , W K,d i , W V,d i } η i=1 , γ d 1 , γ d 2 } D d=1 ,</formula><p>we construct a ball with a radius shrinking at rate 1/(N T (H + 1)). More specifically, we define</p><formula xml:id="formula_231">P = B S • D d=1 B M (d) • B F (d) • B R (d), (F.11)</formula><p>where we define the balls around each weight matrix in each layer d as</p><formula xml:id="formula_232">B S = Unif B(W softmax , r S , ∥ • ∥ 1,2 ) , B R (d) = Unif B(γ d 1 , r (d) γ,1 , | • |) • Unif B(γ d 2 , r (d) γ,2 , | • |) , B F (d) = Unif B(W d ff,1 , r (d) F,1 , ∥ • ∥ F ) • Unif B(W d ff,2 , r (d) F,2 , ∥ • ∥ F ) , B M (d) = η i=1 Unif B(W Q,d i , r (d) V , ∥ • ∥ F ) • Unif B(W Q,d i , r (d) Q , ∥ • ∥ F ) • Unif B(W Q,d i , r (d) K , ∥ • ∥ F ) .</formula><p>The ball around center x with radius r is defined as B(x, r, ∥ • ∥) = {y | ∥x -y∥ ≤ r}. And Unif(S) denotes uniform distribution over the set S. Finally, we specify the radius as follows,</p><formula xml:id="formula_233">r (d) K = r (d) Q = R -1 η -1 (1 + B 2 F ) -1 B -2 M α -1 d /(N T (H + 1)), r (d) F,1 = r (d) F,2 = R -1 B -1 F α -1 d /(N T (H + 1)), r (d) V = R -1 η -1 (1 + B 2 F ) -1 α -1 d /(N T (H + 1)), r (d) γ,1 = R -1 (1 + B 2 F ) -1 α -1 d /(N T (H + 1)), r (d) γ,2 = R -1 α -1 d /(N T (H + 1)), r S = τ B -1 s /(N T (H + 1)),</formula><p>where</p><formula xml:id="formula_234">α d = 2 τ B S (1 + B 2 F ) 1 + ηB M (1 + 4B 2 M ) D-d .</formula><p>Under this assignment of P , we can control (I.i) and (I.ii) by invoking the following lemma from <ref type="bibr" target="#b73">Zhang et al. (2023a)</ref>.</p><p>Lemma F.1. We set the distribution P to be (F.11), which is a uniform distribution over a neighborhood around ρ with radius proportional to 1/N T (H + 1). Under Assumptions 6.1 and 6.2, we have</p><formula xml:id="formula_235">1 2 E ρ∼P log E D exp - 1 2 N ℓ=1 T t=1 H h=0 log P ρ (z t,ℓ h | S t,ℓ h ) P ρ (z t,ℓ h | S t,ℓ h ) D + 1 4 N ℓ=1 T t=1 H h=0 E ρ∼P log P ρ (z t,ℓ h | S t,ℓ h ) P ρ (z t,ℓ h | S t,ℓ h ) = O(1).</formula><p>Proof. See Appendix F.2 by <ref type="bibr" target="#b73">Zhang et al. (2023a)</ref> for a detailed proof.</p><p>This lemma quantifies how P ρ changes when ρ is getting closer to ρ. In the following, we briefly outline the proof and refer readers to the original work by <ref type="bibr" target="#b73">Zhang et al. (2023a)</ref> for more details. The proof consists of two steps. The first step is to control the TV distance between P ρ and P ρ using the differences between the layer parameters specified in ρ and ρ. The second step sets ρ ∼ P , where the distribution P in (F.11) is supported on a neighborhood around ρ. Then for any ρ ∈ supp(P ), we can control the log density ratio between P ρ and P ρ using the radius defined in (F.11) as follows:</p><formula xml:id="formula_236">log P ρ (z t,ℓ h | S t,ℓ h )/P ρ (z t,ℓ h | S t,ℓ h ) = O 1/(N T (H + 1)) (F.12)</formula><p>for any (z t,ℓ h , S t,ℓ h ). Therefore, we conclude that the fluctuation error (I) has a rate of O(1). Next, we control error (II) using the following lemma.</p><p>Lemma F.2. Under Assumptions 6.1 and 6.2, with probability at least 1 -δ, we have</p><formula xml:id="formula_237">1 N T (H + 1) N ℓ=1 T t=1 H h=0 log P(z t,ℓ h | S t,ℓ h ) P ρ ′ (z t,ℓ h | S t,ℓ h ) -E S t,ℓ h KL P(• | S t,ℓ h ) | P ρ ′ (• | S t,ℓ h ) ≤ b * 1 2N log T (H + 1) δ . (F.13)</formula><p>Proof. See Appendix G.3.1 for details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>This lemma controls error (II).</head><p>A key part of the proof is to derive the log-density bound</p><formula xml:id="formula_238">log P(z | S) -log P ρ (z | S) ≤ b * = log max{c -1 0 , 1 + |L| exp(B S /τ )}, (F.14)</formula><p>which provides the explicit form of b * mentioned earlier in Assumption 5.1. The proof involves applying Hoeffding's inequality, along with the log-density bound in (F.14), to the left-hand side of (F.13). Furthermore, we control (III), the KL divergence between P and Q, using the following lemma obtained from <ref type="bibr" target="#b73">Zhang et al. (2023a)</ref>. To make sure that supp(P ) ⊆ supp(Q), we set Q to be uniformly distributed over P LLM . More specifially, we have</p><formula xml:id="formula_239">Q = B ′ S • D d=1 B ′ M (d) • B ′ F (d) • B ′ R (d), (F.15)</formula><p>where we define the balls around each weight matrix in each layer d as</p><formula xml:id="formula_240">B ′ F (d) = 2 i=1 Unif B(0, B F , ∥ • ∥ F ) , B ′ M (d) = 3η i=1 Unif 0, B M , ∥ • ∥ F ) , B ′ S = Unif B(0, B S , ∥ • ∥ 1,2 ) , B ′ R (d) = 2 i=1 Unif B(1/2, 1/2, | • |) . Lemma F.3. Let D = D 2 • r • (d F + d k + r) + r • |L| and B = τ -1 RhB S B 2 F B 3 M .</formula><p>Let distributions P and Q be defined as in (F.11) and (F.15), respectively. We have that under Assumptions 6.1 and 6.2,</p><formula xml:id="formula_241">KL(P, Q) = O D log(1 + N T H B) .</formula><p>Proof. See Equation (F.9) in Appendix F.2 of <ref type="bibr" target="#b73">Zhang et al. (2023a)</ref> for a detailed proof. This Lemma is proved by directly computing the KL divergence between two uniform distributions P and Q, where supp(P ) ⊆ supp(Q). The calculation can be found in Appendix F.2 of <ref type="bibr" target="#b73">Zhang et al. (2023a)</ref>.</p><p>Applying Lemmas F.1, F.2, and F.3 to the three errors in (F.10), we have that with probability at least 1 -δ,</p><formula xml:id="formula_242">1 N T (H + 1) N ℓ=1 T t=1 H h=0 TV P(• | S t,ℓ h ), P ρ (• | S t,ℓ h ) (F.16) ≤ 1 N T (H + 1) N ℓ=1 T t=1 H h=0 TV 2 P(• | S t,ℓ h ), P ρ (• | S t,ℓ h ) 1/2 ≤ O √ b * N 1/4 log T H δ + inf ρ ′ ∈P LLM 1 N T H N ℓ=1 T t=1 H h=0 E S t,ℓ h KL P(• | S t,ℓ h ) | P ρ ′ (• | S t,ℓ h ) (II) + 1/(N T H) (I) + D N T H log(1 + N T H B) (III) + 1/(N T H) log(1/δ) , ≤ O √ b * N 1/4 log T H δ + inf ρ ′ ∈P LLM 1 N T H N ℓ=1 T t=1 H h=0 E S t,ℓ h KL P(• | S t,ℓ h ) | P ρ ′ (• | S t,ℓ h ) + D N T H log(1 + N T H B) ,</formula><p>where the first line follows from Cauchy-Schwarz inequality, the second line follows from upper bounding the three errors in (F.10) using Lemmas F.1, F.2, and F.3. The last line drops two terms that are dominated by the rest.</p><p>In the final step, we will change the left-hand side of (F.16) to its expectation. We control the difference between (F.16) and its expectation using the following lemma.</p><p>Lemma F.4. Under Assumptions 6.1 and 6.2, with probability at least 1 -δ, we have</p><formula xml:id="formula_243">1 N T (H + 1) N ℓ=1 T t=1 H h=0 E S t,ℓ h TV P(• | S t,ℓ h ), P ρ (• | S t,ℓ h ) -TV P(• | S t,ℓ h ), P ρ (• | S t,ℓ h ) = O 1 √ N D log(1 + N T H B) + log T H δ .</formula><p>Proof. See Appendix G.3.2 for details.</p><p>This lemma follows from establishing uniform convergence between the TV distances and their expectations that hold for any distribution P . Adding Lemma F.4 to (F.16), we obtain the rate for pretraining error:</p><formula xml:id="formula_244">E S∼D TV P(• | S), P ρ (• | S) = O √ b * N 1/4 log T H δ + inf ρ ′ ∈P LLM 1 N T H N ℓ=1 T t=1 H h=0 E S t,ℓ h KL P(• | S t,ℓ h ) | P ρ ′ (• | S t,ℓ h ) + 1 √ N D log(1 + N T H B) + log T H δ + D N T H log(1 + N T H B) = O √ b * N 1/4 log T H δ + inf ρ ′ ∈P LLM 1 N T H N ℓ=1 T t=1 H h=0 E S t,ℓ h KL P(• | S t,ℓ h ) | P ρ ′ (• | S t,ℓ h ) + 1 √ N D log(1 + N T H B) ,</formula><p>where the final line follows from dropping the last term in the second line, which is dominated by the rest. Therefore, we conclude the proof.</p><p>F.2 Formal Statement of Proposition 6.4</p><p>In this section, we formally state Proposition 6.4 and provide its proof. For simplicity, we derive the approximation error bound for reasoning steps of dimension one, i.e., we regard L as a subset of R. Our method can be readily generalized to higher-dimensional cases <ref type="bibr">(Elbrächter et al., 2021)</ref>. In this proof, we construct networks with specific parameters such that the KL divergence between the target distribution P and its best transformer neural network approximation P ρ decays exponentially as the network depth increases. We let T be the maximal number of examples included in the prompt. Thus, L = T (H+1) is the largest number of reasoning steps included in the prompt. We let S t h = {Υ t-1 , z t 0:(h-1) } denote the collection of t -1 examples of reasoning paths and a partial trajectory of length h of the t-th example. Here h ∈ {0, . . . , H}. Note that the desired transformer neural network takes each S t h as the input and outputs an element in the probability distribution over L as the conditional distribution of z t h . That is, the transformer takes a sequence of reasoning steps as the input and outputs a probability distribution.</p><p>Since each position of the input is indexed by (t, h), to simplify the notation, we use L ′ (t, h) = (t -1)(H + 1) + h to denote the length of S t h . For any t, t ′ ∈ [T ] and h, h ′ ∈ {0, . . . , H}, we write</p><formula xml:id="formula_245">(t ′ , h ′ ) &lt; (t, h) if and only if L ′ (t ′ , h ′ ) = (t ′ -1)(H + 1) + h ′ &lt; L ′ (t, h). (F.17)</formula><p>That is, (t ′ , h ′ ) &lt; (t, h) if and only if z t ′ h ′ appears earlier than z t h .</p><p>In the sequel, we fix t ∈ [T ] and 0 ≤ h ≤ H and focus on the problem of approximating the conditional distribution of z t h . We abbreviate L ′ (t, h) as L ′ when the meaning is clear from the context. The target distribution is denoted by a function g * h : </p><formula xml:id="formula_246">L L ′ → R |L| , i.e., g * h (S t h ) = P(z t h = • | S t h ).</formula><formula xml:id="formula_247">g * h (S t h ) = w * h 1 L ′ t-1 i=1 H j=0 ψ * h (z i j ) + h-1 j ′ =0 ψ * h (z t j ′ ) . (F.18)</formula><p>In particular, if h = 0, the second summation</p><formula xml:id="formula_248">h-1 j ′ =0 ψ * h (z t j ′ ) is set to zero. Let w * h,i denote the i-th component of w * h for all i ∈ [|L|]. In the following, we let S ∞ ([-B, B], R) denote the set of real-valued smooth functions on [-B, B] equipped with the ℓ ∞ -norm ∥f ∥ ∞ = sup x∈[-B,B] |f (x)|.</formula><p>We define S B as the set of smooth functions with bounded derivatives:</p><formula xml:id="formula_249">S B = f ∈ S ∞ ([-B, B], R) | f (n) ∞ ≤ C S • n! for all n ∈ N + , and ∥f ∥ ∞ ≤ C A ,</formula><p>where f (n) is the n-th order derivative of f , C S is a constant, and N + is the set of positive integers. Here S B contains functions whose high-order derivatives grow moderately fast in magnitude. We impose some regularity assumptions on functions {g * h } H h=0 as follows.</p><p>Assumption F.5. We assume that there exists B, C A &gt; 0 such that for any h ∈ {0, . . . , H}, we have</p><formula xml:id="formula_250">ψ * h , τ log w * h,i ∈ S B for i ∈ [|L|],</formula><p>where τ is the temperature of the LLMs and w * h,i is the i-th entry of w * h in (F.18). Moreover, without loss of generality, we assume C S = 1.</p><p>This assumption states that the target functions {g * h } H h=0 are sufficiently smooth in the sense that all functions appearing in the factorization in (F.18) are smooth. We establish the approximation error in the following proposition.</p><p>Proposition F.6 (Formal Statement of Proposition 6.4). Let S t h = (Υ t-1 , {z t j } h-1 j=0 ) be the sequence of reasoning steps that includes t -1 examples of reasoning paths Υ t-1 and the first h -1 steps of the t-th example {z t j } h-1 j=0 . Let D denote a sufficiently large integer, consider the parameter class P LLM in (6.1) with</p><formula xml:id="formula_251">d F ≥ 18|L| + 4, B S ≥ (C A + 1) • |L|, B M ≥ max{8 log(8T H), B 2 + (H + 1) 2 + 1}, and</formula><formula xml:id="formula_252">B F ≥ C F • B 2 + H 2 + C 2 A • |L| • 16 D ′ • |L| 3/2 ,</formula><p>where D ′ = (D -C p log(3H))/(H + 1), C F , C p &gt; 0 are absolute constants, and C A is from Assumption F.5. Under Assumptions 6.1, 6.2 and F.5, there exists a transformer with at most O(D) transformer blocks and parameter ρ * ∈ P LLM satisfying max</p><formula xml:id="formula_253">S t h ∈L * KL P(z t h = • | S t h ), P ρ * (z t h = • | S t h ) = O exp - D -C log(2H))/H 1/4</formula><p>5B , for all t ∈ [T ] and h ∈ {0, . . . , H}, where C &gt; 0 is a absolute constant. The integer H is the length of a reasoning trajectory, B is the parameter from Assumption F.5, and |L| is the alphabet size of the output distribution. We note that O(•) is with respect to the asymptotic regime where D goes to infinity.</p><p>This proposition shows that the approximation error decays exponentially to zero as D increases. The proof is based on an explicit construction of a transformer neural network that estimates {g * h } H h=0 altogether. The transformer architecture follows the one described in Appendix G.1. In particular, the transformer has H + 1 submodules that approximate each g * h separately. Besides, we assume C S = 1 in Assumption F.5 only to simplify the presentation. Our approximation result can be modified for a general C S by changing the constants in the upper bound correspondingly.</p><p>The proof of this proposition is technical and lengthy. We present a detailed proof in Appendix F.3 and give an overview as follows.</p><p>Overview of the Proof of Proposition F.6. Note that the transformer takes S t h as the input and outputs a probability distribution over L, where h ∈ {0, . . . , H} and t ∈ [T ]. We fix some arbitrary (t, h) and consider the problem of predicting z t h . As introduced in Appendix G.1, in the transformer architecture, the input sequence is first embedded in an Euclidean space and then passed through a series of transformer blocks. Then the output goes through a softmax output layer to generate a probability distribution. Intuitively, when predicting z t h using S t h , we need to first extract the step-index h and then apply an approximation of g * h . To achieve this goal, our transformer includes an extraction module NN J followed by H + 1 approximation and selection modules</p><formula xml:id="formula_254">{G h ′ , F h ′ } H h ′ =0 .</formula><p>Here NN J adds the desired step-index, i.e., h, to all the L ′ (t, h) locations. The approximation module G h ′ approximates the target distribution g * h ′ for all h ′ ∈ {0, . . . , H}. Selection modules {F h ′ } H h ′ =0 are used to select the particular approximation module G h with step-index h. The output of the final selection module F H is then passed to a softmax layer, which produces the output distribution. We list the components of the transformer architecture as follows. Also see Figure <ref type="figure" target="#fig_11">10</ref> for an illustration.</p><p>• Input embedding: Given a prompt S t h ∈ L * , we construct an input embedding to prepare for further processing of the input, which is defined as</p><formula xml:id="formula_255">X (0) NN in (F.24). Here X (0)</formula><p>NN is a sequence of vectors of length L ′ (t, h), where each vector has length 3, including the value of the reasoning step and its step-index.</p><p>• Extraction module NN J : The extraction module NN J extracts the step-index h from the last reasoning step z t h-1 and copy it to all previous reasoning steps {z</p><formula xml:id="formula_256">t ′ h ′ } (t ′ ,h ′ )&lt;(t,h-1) .</formula><p>Specifically, this module takes input embedding X</p><p>NN ∈ R L ′ ×3 in (F.24) and outputs a vector sequence of length L ′ (t, h), where each vector is in R 2+2|L| . In each vector indexed by (t ′ , h ′ ), the first entry is the value of the reasoning step z t ′ h ′ , and the second entry is approximately equal to h, the step-index of the desired output z t h . The remaining 2|L| entries of each vector are all set to zero. The output of NN J is then fed into a sequence of H + 1 approximation and selection modules.</p><p>• Approximation module G h ′ : For any h ′ ∈ {0, . . . , H}, G h ′ computes an embedding of S t h , denoted by embed h ′ (S t h ), which is a vector-valued function in |L|. In particular, embed h ′ (S t h ) is used to approximate the target distributions g * h ′ (S t h ) after a softmax transformation. Each G h ′ is a mapping that maps a sequence of L ′ = L ′ (t, h) vectors in R 2+2|L| to a vector sequence of the same shape, i.e., a function between R L ′ ×(2+2|L|) to itself. Here G h ′ only changes the columns with indices in {3, . . . , 2 + 2|L|} and sets them to embed h ′ (S t h ) ∈ R L ′ ×|L| , where embed h ′ is a matrix-valued mapping that maps</p><formula xml:id="formula_258">S t h ∈ R L ′ ×1 to a matrix in R L ′ ×|L| . • Selection module F h ′ : For any h ′ ∈ {0, . . . , H}, F h ′ checks if its index h ′ matches</formula><p>the extracted index h. When viewing each F h ′ as a matrix-valued mapping from R L ′ ×(2+2|L|) to itself, it only changes the last |L| columns of the matrix and uses them as a "memory". In particular, it approximately adds</p><formula xml:id="formula_259">embed h ′ (S t h ) • 1{h ′ = h}</formula><p>to the memory and passes it to the subsequent modules. As a result, after the last selection module, F H , the last |L| columns of the output matrix are given by</p><formula xml:id="formula_260">H h ′ =0 embed h ′ (S t h ) • 1{h ′ = h} ≈ embed h (S t h</formula><p>). Thus, by combining the approximation and selection modules, we eventually obtain embed h (S t h ) approximately. • Output softmax layer: Finally, we pass the output of F H , embed h (S t h ), to a softmax function to produce the output distribution g * h (S t h ), which is closed to the desired output g * h (S t h ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.3 Proof of Proposition F.6</head><p>Before the formal proof, we would like to highlight that our construction in the proof is based on a slightly generalized version of the transformer structure in Section 6.1. We note that this slight generalization can be easily taken into account in the generalization error in Proposition 6.3. Here, we first define a single transformer block as follows, which takes</p><formula xml:id="formula_261">X ∈ R L×r as input and output Y ∈ R L×r . Z = NL mha(X, W mha ) + Xγ 1 , Y = NL ff(Z, W ff , b ff ) + Zγ 2 , (F.19)</formula><p>where γ 1 and γ 2 are diagonal matrices, and the fully connected feed-forward (FF) network ff is defined as</p><formula xml:id="formula_262">ff(X in , W ff , b ff ) = ReLU(X in W ff,1 + 1 ⊤ b ff,1 )W ff,2 + 1 ⊤ b ff,2 . (F.20)</formula><p>Compared to the FF layer in Section 6.1, this FF layer has two additional bias terms b ff,1 ∈ R d F and b ff,1 ∈ R r . Here, NL is the row-wise ℓ 2 -normalization layer, which is defined in (G.2) in Appendix G.1. This function projects each row of the input matrix into the unit ℓ 2 -ball. Moreover, the multi-head attention (MHA) layer mha is defined in (3.2). In particular, a transformer block can be viewed as a four-layer neural network, where both ff and mha have two neural network layers. In this proof, we use "module" to refer to a sequence of transformer blocks that achieves certain functionality. Throughout this proof, we construct a transformer neural network that includes an input embedding module, a sequence of transformer blocks, and the output softmax layer. Instead of counting the number of neural network layers in the transformer, we keep track of the number of transformer blocks.</p><p>In this proof, we often construct neural network components that are solely based on the MHA or FF layers. These layers themselves can be regarded as special cases of the transformer block, as shown below.</p><p>Multi-Head Attention Layer as a Transformer Block. Let W mha be the weight matrices of a MHA layer. To view mha(•, W mha ) as a single transformer block, we can set W ff and b ff as zero matrices and vectors respectively. Then ff in (F.20) becomes a zero function. We also set γ 1 = 0 and γ 2 = I in (F.19).</p><p>It remains to consider the normalization layer, which plays a role when row-wise ℓ 2norm of mha(X, W mha ) exceeds one. To handle this, we introduce a scaling trick as follows. When the input matrix X has bounded rows and W mha is bounded, we know that each XW V i ∈ R L×dv has bounded rows. We let B ≥ 1 be an upper bound on the ℓ 2 -norm of the rows of mha(X, W mha ) for all bounded input matrix X. Then we define another set of MHA parameters W mha as</p><formula xml:id="formula_263">{W Q i , W K i , W V i } η i=1 , where W V i = W V i /B.</formula><p>Thus, for any input matrix X, we have mha(X, W mha ) = mha(X, W mha )/B, whose row-wise ℓ 2 -norm is no more than one. Therefore, for any input matrix X and any weight matrix W of a proper size, we have</p><formula xml:id="formula_264">NL mha(X, W mha ) • W = mha(X, W mha )W, (F.21)</formula><p>where we set W = B • W . Here W is some weight matrix that is multiplied to the output of MHA layer, i.e., a weight matrix of the next layer. The equality in (F.21) shows that suppose our constructed neural network involves a softmax layer, we can scale the weight matrices to ensure that it is equivalent to a transformer block. Moreover, the norms of these matrices are scaled by a factor of B.</p><p>Fully Connected Layer as a Transformer Block. Similarly, consider a FF layer with parameters {W ff , b ff }. We set {W V i } η i=1 to be a zero matrix and thus mha(•, W mha ) becomes a zero function. We then set γ 1 = I and γ 2 = 0 in (F.19). Similarly, we can apply the scaling trick by multiplying W ff,2 and b ff,2 by 1/B for some parameter B. This ensures that the output of the FF layer in (F.20) has row-wise ℓ 2 -norm bounded by one, and thus the normalization NN(•) does not take effect. We can multiply the weight matrix in the subsequent layer by B and get the desired output.</p><p>Multi-Layer Perceptron as Transformer Blocks. The above argument can be extended to multi-layer perceptions (MLPs), i.e., a multi-layer feed-forward neural network. We can show that an MLP can be written as a composition of multiple transformer blocks. This is achieved by (i) setting W V i = 0 in the MHA layer and setting γ 1 = I and γ 2 = 0 and (ii) applying the scaling trick in each transformer block.</p><p>Specifically, we define an MLP as a composition of L feed-forward layers with parameters</p><formula xml:id="formula_265">{W ℓ ff , b d ff } ℓ∈[L]</formula><p>. Given the input matrix X 0 ∈ R L×r , the output of each layer is given by X</p><formula xml:id="formula_266">ℓ = ReLU(X ℓ-1 W ℓ ff + 1 ⊤ b ℓ ff ), ∀ℓ ∈ [L] (F.22)</formula><p>Here W ℓ ff and b ℓ ff are the weight matrix and bias vector of a proper dimension. We have the following result showing that a L-layer MLP can be represented as a transformer with L blocks.</p><p>Proposition F.7. We consider a row-wise fully-connected network defined in (F.22). Let X 0 ∈ R L×r denote the input of this network, and the intermediate outputs are given by (F.22). We assume there exist positive numbers {B ℓ , 0 ≤ ℓ ≤ L} such that ∥(X ℓ ) ⊤ ∥ 2,∞ ≤ B ℓ for all ℓ ∈ {0, . . . , L} with B ℓ ≥ 1. Consider a transformer with input Y 0 = X 0 . Let Y ℓ denote the output of the ℓ-th transformer block for all ℓ ≥ 1. Then, we can construct a transformer with L transformer blocks such that</p><formula xml:id="formula_267">Y ℓ = X ℓ /B ℓ for all ℓ ∈ [L]. Moreover, let {W ℓ ff , b ℓ ff } denote</formula><p>the parameters of FF layer of the ℓ-th transformer block. We have</p><formula xml:id="formula_268">W ℓ ff,1 = B ℓ-1 • W ℓ ff , W ℓ ff,2 = I/B ℓ , b ℓ ff,1 = b ℓ ff , b ℓ ff,2 = 0. (F.23)</formula><p>Suppose the weight matrices of a fully connected network with L layers have a maximum width d and maximum weight α, and the biases have maximum weight β. In that case, the magnitude of the intermediate output can increase at most exponentially with d • α. More specifically, by direct calculation, we have</p><formula xml:id="formula_269">B ℓ ≤ √ d • B 0 • (d • α) ℓ + β • ((d • α) ℓ -1)/(d • α -1) for ℓ ∈ [L].</formula><p>Proof. See Appendix G.4.1 for a detailed proof. Here W ℓ ff,2 in (F.23) is proportional to an identity matrix of a proper dimension, and b ℓ ff,2 is a zero vector. The details of the other parameters of the transformer can be found in the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.3.1 Rigorous Proof of Proposition F.6</head><p>Proof. Throughout this proof, we focus on the problem of approximating P(z t h = • | S t h ) for some fixed (t, h), which is denoted by g * h (S t h ). We write L ′ (t, h) as L ′ for simplicity. To prove this proposition, we first introduce the transformer architecture and then establish the desired approximation error. As outlined above, the transformer as five components. We first introduce the input embedding as follows.</p><p>Input embedding. For each reasoning step in S t h , we define the input embedding as</p><formula xml:id="formula_270">z t ′ ,(0) h ′ = (z t ′ h ′ , h ′ , 0) for (t ′ , h ′ ) &lt; (t, h -1), (z t h-1 , h -1, 1) for (t ′ , h ′ ) = (t, h -1). (F.24)</formula><p>Here the ordering between index tuples is specified in (F.17). Recall that L ′ (t ′ , h ′ ) is the index of the reasoning step z t ′ h ′ in S t h . In the embedding in (F.24), the first coordinate z t ′ h ′ is the content embedding, which stores the actual reasoning step. The second coordinate of z t ′ h ′ indicates the step-index of each reasoning step z t ′ h ′ , and the last coordinate specifies if it is the last reasoning step. This last coordinate acts as an indicator function because we want to extract the index h and approximate the target function g * h . The indicator function is 0 for all steps except the last reasoning step z t h-1 of S t h , which helps to locate the step-index h. Thus, the last two coordinates are the positional embedding which carries the positional information. In the sequel, we let X (0) NN ∈ R L ′ ×3 denote the embedding matrix, whose rows are the embedding vectors defined in (F.24).</p><p>Using X</p><p>NN as the input, we present the other components of the transformer as follows. Our construction is decomposed into five steps as follows.</p><p>• In Step 1, we design the extraction module NN J to extract the step-index h from the last reasoning step and copy it to all previous reasoning steps in the prompt S t h . This module takes X (0) NN as the input and outputs a matrix in R L ′ ×(2+2|L|) in (F.27). Specifically, the first column of the output matrix corresponds to the context embedding S t h . The entries of the second column are all approximately equal to h, the step-index of z t h . The rest of the 2|L| columns are all equal to zero vectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>• In</head><p>Step 2, for all h ∈ {0, . . . , H}, we construct the approximation modules G h that produce an approximation embedding embed h (S t h ), which is used to approximate the target distributions g * h (S t h ) after a softmax transformation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>• In</head><p>Step 3, for all h ∈ {0, . . . , H}, we build the selection module F h to check if the index of the current module, i.e., h, matches the extracted index h. This module approximately adds embed h (S t h ) • 1{ h = h} to a memory. As a result, F H outputs a desired output embed h (S t h ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>• In</head><p>Step 4, we combine the constructions introduced in the first three steps with a softmax output layer to complete the final transformer. Then we analyze the approximation error of the transformer network.</p><p>• Finally, we conclude the proof in Step 5 by verifying that the constructed transformer network belongs to the function class P LLM by verifying that the transformer parameters satisfy (6.1).</p><p>Step 1: Extract and copy step index h using module NN J . In this step, we construct the extraction module NN J to extract the step-index h from the input X</p><p>NN and copy it to each reasoning step z t ′ h ′ . Here X</p><p>NN is defined in (F.24). This step is achieved by four transformer submodules. In particular, NN J takes X (0) NN as the input and outputs</p><formula xml:id="formula_274">X (4) NN = (S t h , p h , 0) ∈ R L ′ ×(2+2|L|) .</formula><p>Here the first column is S t h which stores all the reasoning steps. The second column p h is close to h • 1 L ′ , which copies the step-index h to every reasoning step. Here 1 L ′ denotes an all-one vector in R |L| . The last 2|L| columns are all equal to zero vectors. More specifically, we let {NN J,a } a∈[4] denote the four submodules of NN J . We define X (a) NN = NN J,a (X (a-1) NN ) for all a ∈ [4] as the output matrices of each submodule. These matrices are in R L ′ ×3 for a ∈ [3] and in R L ′ ×(2+2|L|) for a = 4. We let z t ′ ,(a) h ′ , a ∈ [4], to denote the rows of these matrices. For any (t</p><formula xml:id="formula_275">′ , h ′ ) with (t ′ , h ′ ) &lt; (t, h -1), z t ′ h ′ is the L ′ (t ′ , h ′ + 1)-th element in S t h . Then, starting from z t ′ ,(0) h ′</formula><p>defined in (F.24), the L ′ (t ′ , h ′ + 1)-th rows of these matrices are given by</p><formula xml:id="formula_276">z t ′ ,(0) h ′ = (z t ′ h ′ , h ′ , 0) L ′ (t ′ , h ′ + 1)-th row of X (0) NN , ⇓ NN J,1 z t ′ ,(1) h ′ = (z t ′ h ′ , h ′ + 1, 0), L ′ (t ′ , h ′ + 1)-th row of X (1) NN , ⇓ NN J,2 z t ′ ,(2) h ′ = z t ′ h ′ , f product (h ′ + 1, 0), 0 , L ′ (t ′ , h ′ + 1)-th row of X (2) NN , ⇓ NN J,3 z t ′ ,(3) h ′ = z t ′ h ′ , f product (h ′ + 1, 0), 1 , L ′ (t ′ , h ′ + 1)-th row of X (3) NN , ⇓ NN J,4 z t ′ ,(4) h ′ = z t ′ h ′ , p t ′ h ′ , 0 , L ′ (t ′ , h ′ + 1)-th row of X<label>(4)</label></formula><p>NN .</p><p>Here f product (a, b) ≈ ab is a neural network that approximately implements the product operation using transformer blocks. We specify the construction of f product in Lemma G.1. We use p t ′ h ′ ≈ h for each (t ′ , h ′ ) &lt; (t, h -1) to copy the step index h to the embedding of each reasoning step z n ′ h ′ . Moreover, the last row, i.e., the L ′ (t, h)-th row of these matrices are</p><formula xml:id="formula_277">z t,(0) h-1 = (z t h-1 , h -1, 1) L ′ (t, h)-th row of X (0) NN , ⇓ NN J,1 z t,(1) h-1 = (z t h-1 , h, 1), L ′ (t, h)-th row of X (1) NN , ⇓ NN J,2 z t,(2) h-1 = z t h-1 , f product (h, 1), 1 , L ′ (t, h)-th row of X (2) NN , ⇓ NN J,3 z t,(3) h-1 = z t h-1 , f product (h, 1), 1/2 , L ′ (t, h)-th row of X (3) NN , ⇓ NN J,4 z t,(4) h-1 = z t h-1 , p t h-1 , 0 , L ′ (t, h)-th row of X (4) NN ,</formula><p>where p t h-1 is close to h. In the rest of Step 1, we prove present lemmas proving that NN J,1 , . . . , NN J,4 can be realized by FF or MHA layers. The first submodule NN J,1 adds an one to the second coordinate of each input vector. This operation can be realized by a FF layer exactly. As shown in the beginning of Appendix F.3, this can be realized by a single transformer block.</p><p>Lemma F.8 (Submodule NN J,1 ). There exists a FF layer NN J,1 such that</p><formula xml:id="formula_278">NN J,1 X (0) NN = X (1) NN , where z t ′ ,(1) h ′ = (z t ′ h ′ , h ′ + 1, 0) for (t ′ , h ′ ) &lt; (t, h -1), (z t h-1 , h, 1) for (t ′ , h ′ ) = (t, h -1).</formula><p>Moreover, the Frobenius norms of the weight matrices are bounded by √ B 2 + H 2 + 1 • √ 6. Thus, this function can represented as a single transformer block.</p><p>Proof. See Appendix G.4.2 for details.</p><p>Next, we aim to substitute the second coordinate of each z t ′ ,(1) h ′ with the product of itself and the third coordinate using NN J,2 . Namely, we aim to compute (h ′ + 1)</p><formula xml:id="formula_279">• 0 = 0 for (t ′ , h ′ ) &lt; (t, h -1) and h • 1 = h for (t ′ , h ′ ) = (t, h -1).</formula><p>The product operation can be approximately realized by a fully connected neural network with an arbitrarily small error. As a result, NN J,2 can be implemented by a composition of multiple transformer blocks.</p><p>Lemma F.9 (Submodule NN J,2 ). Let ϵ ′ ∈ (0, 1) be a desired accuracy level. There exists fully connected MLP NN J,2 with at most</p><formula xml:id="formula_280">C p • (log(H) + log(1/ϵ ′ )) layers such that NN J,2 X (1) NN = X (2) NN , where z t ′ ,(2) h ′ = z t ′ h ′ , f product (h ′ + 1, 0), 0 for (t ′ , h ′ ) &lt; (t, h -1), z t h-1 , f product (h, 1), 1 for (t ′ , h ′ ) = (t, h<label>-1), (F.25)</label></formula><p>Here C p is an absolute constant and f product : R × R → R is an approximation of the product operation in the sense that |f product (h, 1) -h| &lt; ϵ ′ , and |f product (h ′ + 1, 0) -0| &lt; ϵ ′ for each (t ′ , h ′ ) &lt; (t, h -1). Thus, by Proposition F.7, NN J,2 can be written as a composition of C p • (log(H) + log(1/ϵ ′ )) transformer blocks up to a scaling factor. Moreover, the Frobenius norms of the weight matrices are all bounded by</p><formula xml:id="formula_281">√ B 2 + H 2 + 6 • √ H 4 • 5 + 4.</formula><p>Proof. See Appendix G.4.3 for details.</p><p>The third submodule NN J,3 modifies the last coordinates of the input vectors by adding a -1/2 or 1, which is a simple linear operation and thus can be implemented by a FF layer.</p><p>Lemma F.10 (Submodule NN J,3 ). There exists a FF layer NN J,3 such that</p><formula xml:id="formula_282">NN J,3 X (2) NN = X (3) NN , where z t ′ ,(3) h ′ = z t ′ h ′ , f product (h ′ + 1, 0), 1 for (t ′ , h ′ ) &lt; (t, h -1), z t h-1 , f product (h, 1), 1/2 for (t ′ , h ′ ) = (t, h -1).</formula><p>Thus, as a single FF layer, NN J,3 can be represented by a single transformer block. The Frobenius norms of the weight matrices are bounded by B 2 + (H + 1) 2 + 1 • √ 6.</p><p>Proof. See Appendix G.4.4 for a detailed proof.</p><p>In addition to the weights constructed in Lemma F.10, the parameter W ff,1 in NN J,3 is first multiplied with a diagonal matrix to compensate the scaling factor in Lemma F.9. Finally, we use an attention layer to copy the step-index h of z t h to all previous steps. Moreover, we also use a residual link to ensure the first coordinate remains unchanged.</p><p>Lemma F.11 (Submodule NN J,4 ). Let ϵ ′ denote the error induced by f product (•) in Lemma F.9, then for any ϵ ∈ (2ϵ ′ , 1), there exists a submodule NN J,4 such that</p><formula xml:id="formula_283">NN J,4 X (3) NN = X (4) NN , where z t ′ ,(4) h ′ = (z t ′ h ′ , p t ′ h ′ , 0) for (t ′ , h ′ ) &lt; (t, h -1), (z t h-1 , p t h-1 , 0) for (t ′ , h ′ ) = (t, h -1), (F.26)</formula><p>where p t ′ h ′ is the approximation of the step-index h such that |p t ′ h ′ -h| &lt; ϵ for all (t ′ , h ′ ) ≤ (t, h-1). Moreover, NN J,4 is a transformer block with a single-head attention (MHA with η = 1). The parameters</p><formula xml:id="formula_284">{W Q , W K , W V } satisfying ∥W Q ∥ F = 8 log(T H/(ϵ -2ϵ ′ )), ∥W K ∥ F = 1, and ∥W V ∥ F = B 2 + (H + 1) 2 + 1,</formula><p>and the Frobenius norms of the weight matrices in FF layers are bounded by B 2 + (H + 1) 2 + 6 • √ 6.</p><p>Proof. See Appendix G.4.5 for details.</p><p>Setting ϵ = 1/4 and ϵ ′ = 1/16 in Lemma F.11 we conclude that we can use a transformer block NN J,4 to generate the output in (F.26). The weights of the transformer are bounded by 8 log(8T H) in the Frobenius norm. Moreover, for any (t ′ , h ′ ) ≤ (t, h -1), we have |p t ′ h ′ -h| &lt; ϵ = 1/4. Then we write the composition of the four submodules above as 2+2|L|) , where 0 ∈ R L ′ ×2|L| , (F.27)</p><formula xml:id="formula_285">NN J (S t h ) = (S t h , p h , 0) ∈ R L ′ ×(</formula><p>where we use</p><formula xml:id="formula_286">p h = (p 1 0 , • • • , p t h-1 ) ⊤ ∈ R L ′ ×1</formula><p>to denote the vector that is the second column of X (4) NN . The entries of p h are all close to h in the sense that</p><formula xml:id="formula_287">∥ p h -h • 1 L ′ ∥ ∞ &lt; 1/4. To summarize, in</formula><p>Step 1, we have successfully designed a transformer module NN J to extract the target step-index h from the last reasoning step of the prompt S t h , and approximately copy it to all previous reasoning steps in the prompt. We defer the summary of parameters of NN J to Step 5. This concludes Step 1.</p><p>We outline how NN J (S t h ) is processed by the subsequent transformer blocks. In Step 2 and 3 we construct modules G h and F h . Submodule G h approximates each target function g * h and submodule F h checks if the module index h matches the step index h. If h = h, it passes along the approximation produced by G h ; otherwise, it discards the output. In the final network, blocks (G h , F h ) H h=0 are chained sequentially. With a slight abuse of notation, we also use G h and F h to refer to their outputs, respectively. The input and output of each module are listed as follows:</p><formula xml:id="formula_288">• G h : R L ′ ×(2+2|L|) → R L ′ ×(2+2|L|) takes a matrix (S t h , p h , F h-1 [3], F h-1 [4]</formula><p>) as the input. The columns of this matrix have four components, where S t h and p h are the same as in (F.27). The last two components F h-1 [3] and F h-1 [4] ∈ R L ′ ×|L| are the third and fourth components of the output of the previous module F h-1 . If h = 0, the input matrix is (F.27), the output of the extraction module</p><formula xml:id="formula_289">NN J . The output of G h (S t h , p h , G h [3], G h [4]</formula><p>) keeps the first two and the last components unchanged and only changes the third component. That is, G h</p><formula xml:id="formula_290">[4] = F h-1 [4]</formula><p>, and G h [3] computes the approximation of g * h with the third component, which will be specified in Step 2.</p><formula xml:id="formula_291">• Similarly, F h : R L ′ ×(2+2|L|) → R L ′ ×(2+2|L|) takes vector (S t h , p h , G h [3], G h [4]) as the in- put and produces (S t h , p h , F h [3], F h [4]).</formula><p>Here, only the last component of columns is changed, i.e.,</p><formula xml:id="formula_292">F h [3] = G h [3]. The last component F h [4] is constructed iteratively via F h [4] ≈ G h [3] • 1{ h = h} + F h-1 [4].</formula><p>(F.28)</p><p>We will introduce how to use a transformer to implement (F.28) in Step 3.</p><p>Step 2: Construct approximation module G h that approximates g * h . In this step, we introduce the submodules G h : R L ′ ×(2+2|L|) → R L ′ ×(2+2|L|) to separately approximate each target function g * h in (F.18) for all h ∈ {0, . . . , H}. The input and output of each G h are given by</p><formula xml:id="formula_293">G h S t h , p h , F h-1 [3], F h-1 [4] = S t h , p h , 1 ⊤ L ′ embed h (S t h ), F h-1 [4] ,</formula><p>where embed h (•) is used for the approximation of the target distribution g * h (•) in (F.18). More specifically, for each h, we use g * h (•) = softmax embed h (•)/τ to denote the output distribution by passing embed h (•) through a softmax function with a temperature τ . We expect g * h (S t h ) ≈ g * h (S t h ) for all h ∈ {0, . . . , H}. Intuitively, we want each module G h to handle prompts at different steps during the testing stage. We summarize the construction of G h in the following proposition.</p><p>Proposition F.12. Let ϵ ψ , ϵ w ∈ (0, 1) be two accuracy levels. Under Assumptions 6.1, 6.2, and F.5, for any h ∈ {0, . . . , H}, there exists a module G h : R</p><formula xml:id="formula_294">L ′ ×(2+2|L|) → R L ′ ×(2+2|L|) such that G h (S t h , p h , F 3 , F 4 ) = (S t h , p h , embed h (S t h ), F 4 ).</formula><p>Here G h contains (D w + D ψ + 2) transformer blocks with</p><formula xml:id="formula_295">D ψ = 2C d B • log(1/ϵ ψ ) 2 + log B, D w = 2C d B • log(1/ϵ w ) 2 + log B, (F.29)</formula><p>where C d &gt; 0 is an absolute constant and B is the smoothness parameter appearing in Assumption F.5. We define a function g * h (S t h ) = softmax(embed h (S t h )/τ ) as the output distribution approximated by the network G h . Then g * h satisfies max</p><formula xml:id="formula_296">S t h ∈L * TV g * h (S t h ), g * h (S t h ) = O ϵ w + 256 Dw • ϵ ψ . (F.30)</formula><p>For each G h , the maximum width of the FF layers is 18|L| + 4, the maximum Frobenius norm of weight matrices is</p><formula xml:id="formula_297">C F • B 0 • 16 max{D ψ ,Dw} • |L| 3/2 ,</formula><p>where</p><formula xml:id="formula_298">B 0 = B 2 + H 2 + |L| • C 2 A and C F &gt; 1 is a absolute constant.</formula><p>Proof. See Appendix G.4.6 for a detailed proof.</p><p>This proposition states that for any h ∈ [H], we can use a transformer G h to approximate g * h (S t h ) accurately. The number of transformer blocks in G h is determined by the desired accuracy levels ϵ ψ and ϵ w . Note that the approximation accuracy grows exponentially in D w . This will not be a problem when D w is small compared to D ψ . To see this, we can rewrite the upper bound in (F.30) in terms of the depth of the transformer. Specifically, let D ψ and D w be two sufficiently large integers. Setting ) so that the second term in (F.32) becomes negligible compared to the first term. We will determine D ψ and D w to obtain the final error in Step 4.</p><formula xml:id="formula_299">ϵ ψ = exp -(D ψ -log B)</formula><p>Step 3: Construct selection module F h . In this step, we introduce a sequence of transformer modules {F h } H h=1 . Each F h is a mapping from R L ′ ×(2+2|L|) to itself, and its input and output are given by</p><formula xml:id="formula_300">F h (S t h , p h , G h [3], G h [4]) = S t h , p h , G h [3], f product G h [3], 1{ h = h} • 1 ⊤ L ′ + G h [4] . (F.33)</formula><p>That is, F h takes the output of G h as the input, and it keeps the first three components of the columns unchanged. The last component, i.e., the last |L| columns, are used a "memory". Note that</p><formula xml:id="formula_301">G h [4] = F h-1 [4]. The last component of the output of F h , F h [4]</formula><p>, can be written as</p><formula xml:id="formula_302">F h [4] = 1 ⊤ L ′ f product embed h (S t h ), 1{ h = h} + F h-1 [4], (F.34)</formula><p>where f product is a transformer that approximately implements the product operation. Thus, F h first checks if the module index h matches the target step index h, and then writes embed h (•) to the memory if h = h. Thus, by (F.34) we have</p><formula xml:id="formula_303">F H [4] = H h=0 1 ⊤ L ′ f product embed h (S t h ), 1{ h = h} ≈ 1 ⊤ L ′ embed h (S t h ). (F.35)</formula><p>To implement each F h , we starting from the input (S t h , p h , G h [3], G h [4]), denoted by X h,(0) F , we perform the following three steps:</p><p>(i) First, we use a sequence of transformer blocks to represent the indicator 1{ h = h}, and then append it to the end of X h,(0) F . Thus, we have</p><formula xml:id="formula_304">X h,(1) F = S t h , p h , G h [3], G h [4], 1{ h = h} • 1 ⊤ L ′ . (F.36)</formula><p>Here the indicator is obtained by feeding p h to a trapezoid-shaped function.</p><p>(ii) Then we feed X h,(1) F</p><p>to the product module introduced in Lemma G.1 to multiply each entry of G h [3] with 1{ h = h}. The resulting output is</p><formula xml:id="formula_305">X h,(2) F = S t h , p h , G h [3], G h [4], f product G h [3], 1{ h = h} • 1 ⊤ L ′ . (F.37) (iii) Finally, we pass X h,(2) F</formula><p>to a linear layer, which adds the last two components of X h,(2) F and obtain</p><formula xml:id="formula_306">X h,(3) F = S t h , p h , G h [3], f product G h [4], 1{ h = h} • 1 ⊤ L ′ + G h [4] .</formula><p>Details of (i). We present the details of these three steps as follows. We first focus on how to construct the indicator 1{h = h}. Recall that by Lemma F.11 we show that p h satisfies</p><formula xml:id="formula_307">∥ p h -h • 1 L ′ ∥ ∞ &lt; 1/4. Thus, each entry of p h is in (h -1/4, h + 1/4).</formula><p>For any h, we want to construct a neural network</p><formula xml:id="formula_308">f h : R → [0, 1] such that f h (x) = 1 if |x -h| ≤ 1/4 and f (x) = 0</formula><p>if |x -h| ≥ 3/4. Then applying f h to each entry of p h , we have</p><formula xml:id="formula_309">f h ( p h ) = 1{h = h} • 1 L ′ .</formula><p>Such a f h can be constructed by a trapezoid-shaped function, which has value one in [h -ϵ, h + ϵ], zero when |x -h| &gt; 1 -ϵ, and a linear function in between. Here we can set ϵ = 1/4. See Figure <ref type="figure" target="#fig_19">16</ref> for an illustration of two trapezoid-shape functions. The following lemma shows that such trapezoid-shaped functions can be implemented by a FF layer.</p><p>Lemma F.13 (Trapezoid module). For any h ∈ {0, . . . , H} and any ϵ ∈ (0, 1/2), we define a trapezoid-shaped function f h : R → [0, 1] as</p><formula xml:id="formula_310">f h (x) =      1 for |x -h| ≤ ϵ, -(|x -h| -ϵ)/(1 -2ϵ) + 1 for ϵ &lt; |x -h| ≤ 1 -ϵ, 0 otherwise.</formula><p>Then there exists a neural network f that is identical to f h . Moreover, f is a composition of two FF layers, each with no more than 10 neurons, and the entries of the weight matrices and bias vectors are bounded by H + 1 in magnitude.</p><p>Proof. See Appendix G.4.7 for details.</p><p>We apply this lemma with ϵ = 1/4 and apply f h to each entry of p h to obtain 1{h = h} • 1 L ′ ,which becomes the last component of X h,(1) F in (F.36). Moreover, to preserve the , we apply Lemma G.2, which implies that we can use a single FF layer to map X h,(0) F to its first three components. The number of neurons in this FF layer is bounded by 2(2 + 2|L|). Thus, we can concatenate these two networks and obtain a larger network that maps X h,(0) F to X h,(1) F</p><p>. Moreover, such a network has one FF layer, and the maximum width of the weight matrices is bounded by 10 + 2(2 + 2|L|). As a result, the weight matrices of this feed-forward neural network are all bounded by</p><formula xml:id="formula_311">B 2 + H 2 + 2|L| • (C A + 1) 2 • 10 + 2(2 + 2|L|) ≤ B 2 + H 2 + 2|L| • (C A + 1) 2 • (4 + 2 |L|) in terms of the Frobenius norm.</formula><p>Details of (ii). Then, to get X h,(2) F defined in (F.37), we pass X h,(1) F to the product module in Lemma G.1 to multiply G h [3] and 1{ h = h} • 1 ⊤ L ′ in an elementwise fashion. Similar to the implementation of NN J,2 introduced in Lemma F.9, the product module f product here can be implemented as an MLP. More concretely, recall that Assumption F.5 states that each τ • log w * h,i is bounded by C A in terms of the ℓ ∞ -norm. Then, as shown in the proof of Proposition F.12 in Appendix G.4.6, we have</p><formula xml:id="formula_312">embed h (S t h ) [i] -τ • log w h,i 1 L ′ (t,h-1) (i,j)=(1,0) ψ h (z i h )) ≤ ϵ w + 256 Dw • ϵ ψ (F.38) for all i ∈ [|L|].</formula><p>Here (embed h (S t h ))[i] is the i-th entry of embed h (S t h ), which is defined in the same way as in (G.23), but with f w h and ψ * h replaced by f w h and ψ * h . Besides, as we will show later, we set ϵ ψ and ϵ w as in (F.31) so that the right-hand side of (F.38) is much smaller than one. As a result, we have ∥embed h (S t h )∥ ∞ ≤ 1 + C A . Thus, combining this fact with Lemma G.1, we conclude that, there exists an MLP f h product such that for any ϵ p ∈ (0, 1),</p><formula xml:id="formula_313">∥f h product embed h (S t h ), 1{ h = h} -embed h (S t h ) • 1{ h = h}∥ ∞ &lt; ϵ p (F.39)</formula><p>and any h ∈ {0, . . . , H}, where f h product has at most D p = C p • (log(C A + 1) + log(1/ϵ p )) FF layers, where C p is an absolute constant. Moreover, we remark that the module f h product is different from the one constructed in Lemma F.9, and different for each 0 ≤ h ≤ H.</p><p>Moreover, we need to write such a product module as a composition of transformer blocks using Proposition F.7. At the same time, we adopt Lemma G.2 to preserve the first four components of X h,(1) F , namely S t h , p h , G h [3], G h [4] using FF layers. Specifically, we apply Proposition F.7 by setting the scaling factors</p><formula xml:id="formula_314">B 0 = B 2 + H 2 + 2 • |L| • (C A + 1) 2 , {B ℓ = B 2 0 + 5|L|} Dp-1 ℓ=1 , B Dp = B 2 0 + (C A + 1) 2 .</formula><p>As a result, the function that maps X </p><formula xml:id="formula_315">+ 5|L| • 2(2 + 2|L|) + (C A + 1) 4 • 5|L| ≤ C ′ • (C A + 1) 3 • |L| for some absolute constant C ′ .</formula><p>Details of (iii). Finally, we pass X h,(2) F through a linear layer to add</p><formula xml:id="formula_316">f product G h [3], 1{ h = h} • 1 ⊤ L ′ with G h [4]</formula><p>. Moreover, we adopt Lemma G.2 to preserve the first three components of X h,(2) F . Therefore, the maximum Frobenius norm of the weight matrix of this module is</p><formula xml:id="formula_317">B 2 + H 2 + 3 • |L| • (C A + 1) 2 • 2(2 + |L|) + 4|L|</formula><p>, where the first term results from the scaling trick, and the second term results form the linear operation.</p><p>Combining (i)-(iii). Combining these three steps above, we obtain the selection module F h shown in (F.33) and (F.34). By (F.35), the output of F H is given by</p><formula xml:id="formula_318">S t h , p h , 1 ⊤ L ′ embed H (S t h ), H h=1 f product embed h (S t h ), 1{ h = h} • 1 ⊤ L ′ . (F.40)</formula><p>For the ease of notation, we use embed(S t h ) to denote last component of (F.40), and we expect embed(S t h ) ≈ embed h (S t h ).</p><p>Finally, we calculate the depth and norms of the weight matrices of F h . According to Lemma F.13, the trapezoid module has a depth of 1 and a maximum Frobenius norm of</p><formula xml:id="formula_319">B 2 + H 2 + 2 • |L| • (C A + 1) 2 (4 + 2 |L|).</formula><p>The linear module also has a depth of 1 and a maximum Frobenius norm of B 2 + H 2 + 3 • |L| • (C A + 1) 2 (2 + 6|L|). The product module has a maximum Frobenius norm of C ′ • (C A + 1) 3 • |L| and depth D p , which will be determined in Step 4. Overall, each F h module has a depth of D f = D p + 2 and maximum Frobenius norm of weight matrices as C ′ • (C A + 1) 3 • |L|.</p><p>Step 4: Compute the approximation error. The last component of F H , embed(S t h ), is then fed into a softmax layer to generate the final output. In this step, we characterize the approximation error ∥g * h (S t h )softmax( embed(S t h )/τ )∥ 1 . Here g * h in (F.18) refers to the target distribution, and softmax( embed(S t h )/τ ) refers to the output distribution. First, we handle the error induced by the product operation in F h , which is established in (F.39). Let D p be an integer and we set ϵ p = (C A + 1) where the second inequality follows from (F.41). Therefore, combining Proposition F.12 and (F.42), for any prompt S t h with length L ′ ≤ L, the approximation error of the transformer is bounded by</p><formula xml:id="formula_320">g * h (S t h ) -softmax embed(S t h )/τ 1 ≤ g * h (S t h ) -softmax embed h (S t h )/τ 1 + softmax embed h (S t h )/τ -softmax embed(S t h )/τ 1 ≤ 2ϵ w + 2 • 256 Dw • ϵ ψ + 2Hϵ p /τ. (F.43)</formula><p>Here ϵ w and ϵ ψ are chosen as in (F.31) and ϵ p is defined above. Thus, we can equivalently write the approximation error in terms of the parameters D ψ , D w , and D p as</p><formula xml:id="formula_321">g * h (S t h ) -softmax embed(S t h )/τ 1 (F.44) = O exp -D w /B + exp -D ψ /B + 6 • D w + H/τ • exp -D p /C p + log(C A + 1) ,</formula><p>where we use (F.32), (F.43), and the definition of ϵ p . Here the number of transformer blocks of each module among {G j , F j } H j=1 satisfy D w = O(D w ), D ψ ≤ CD ψ , and D p ≤ D p , where C &gt; 0 is a absolute constant.</p><p>To get an explicit upper bound, we choose D ψ , D w , and D p properly to balance the three terms in the right-hand side of (F.44). Specifically, we require <ref type="bibr">(F.45)</ref> and let D g = (D ψ + D w + 2) + (D p + 2) denote a parameter that characterizes the total number of transformer blocks in each G h and F h together. Note that the actual total number of transformer blocks in G h and F h is O(D g ). To satisfy (F.45), we can set</p><formula xml:id="formula_322">D ψ ≥ 6 √ B • D w + D w 2 , (D p /C p -log(C A + 1)) 2 ≥ D w /B,</formula><formula xml:id="formula_323">D w = D g /(24 √ B), D p = C p √ 24B 3/4 • D g + C p • log(C A + 1), D ψ = D g -D w -D p -4. (F.46)</formula><p>Then for sufficiently large D g , we have that (F.44) is dominated by the first term:</p><formula xml:id="formula_324">g * h (S t h ) -softmax embed(S t h )/τ 1 = O exp -D w /B = O exp - D 1/4 g 5B , (F.47)</formula><p>where the second inequality follows from (F.46), the fact that √ 24 &lt; 5, and the relaxation of the exponent of B for notational clarity, assuming B ≥ 1.</p><p>Finally, we convert the ℓ 1 -norm upper bound in (F.47) into a bound in terms of the KL divergence. Let P ρ (• | S t h ) denote softmax embed(S t h )/τ , where ρ refers to the parameter that specifies the transformer we constructed in the first three steps of the proof, which consists of a step-index extraction module NN J , and H pairs of modules {G h , F h } H h=0 . We first note that if TV(P(• | S t h ), P ρ (• | S t h )) = ε with ε sufficiently small such that ε &lt; c 0 /2, where c 0 comes from Assumption 6.2. Under this lemma, we can bound the likelihood ratio</p><formula xml:id="formula_325">P(z = • | S t h )/P ρ (z = • | S t h ) for each z ∈ L by log c 0 c 0 + 2ϵ ≤ log P(• | S t h ) P ρ (• | S t h ) ≤ log c 0 + 2ϵ c 0 ≤ 2ϵ c 0 .</formula><p>Therefore we conclude that, when D g is sufficiently large, there exists a transformer with parameter ρ such that, for an any t ∈ [T ] and h ∈ {0, . . . H},</p><formula xml:id="formula_326">max S t h ∈L * KL P(• | S t h ), P ρ (• | S t h ) = O exp - D 1/4 g 5B ,</formula><p>where the number of transformer blocks is at most C • D g for some absolute constant C.</p><p>Step 5: Verify that the constructed transformer is in P LLM . Finally, we verify that the constructed transformer is in P LLM . To this end, for each module of the transformer, we explicitly characterize the width and norms of the weight matrices. Recall that the transformer contains an embedding module NN J , {G h , F h } H h=0 , and a softmax output layer.  Step 1: Control expected pretraining error under distribution shift. In this step, we evaluate E P CoT [err pre ], the expected pretraining error err pre defined in (5.2) with the expectation taken under prompt CoT (n) ∼ P CoT . We first decompose err pre into a sum of errors incurred in each reasoning step and then take expectations with respect to P CoT . We adopt the following lemma to decompose err pre into a sum of KL divergences.</p><formula xml:id="formula_327">B F ≥ C F • B 2 + H 2 + C 2 A • |L| • 16 D ′ • |L| 3/2 , where D ′ = (D -C p log(3H))/(H +</formula><p>Lemma F.14 (KL decomposition). Recall that prompt h CoT (n) = {Υ n , z test 0:h-1 } consists of n examples and the first h -1-th inferred steps for the testing example. Then we have</p><formula xml:id="formula_328">KL P(y test = • | prompt CoT (n)), P ρ (y test = • | prompt CoT (n)) (F.48) ≤ H h=1 E z test 1:h-1 ∼P(• | prompt CoT (n)) KL P(z test h = • | prompt h CoT (n)), P ρ (z test h = • | prompt h CoT (n)) .</formula><p>Proof. See Appendix G.5.1 for detailed proof.</p><p>This Lemma states that we can upper bound the pretraining error err pre by aggregating the pretraining error at each step of inference. We apply (F.14) with Lemma H.3 to convert each KL divergence in (F.48) into TV distances. Namely, we have</p><formula xml:id="formula_329">KL P(y test = • | prompt CoT (n)), P ρ (y test = • | prompt CoT (n)) (F.49) ≤ H h=1 E z test 1:h-1 ∼P(• | prompt CoT (n)) 2(3 + b * ) • TV P(z test h = • | prompt h CoT (n)), P ρ (z test h = • | prompt h CoT (n)) .</formula><p>The number b * comes from (F.14), which upper bounds the log density difference | log P(z|S)log P ρ (z|S) for any z ∈ L, S ∈ L * . We take the expectation of (F.49) with respect to prompt CoT (n) ∼ P CoT . Notice that different parts of prompt h</p><p>CoT (n) have different distributions:</p><formula xml:id="formula_330">z test 1:(h-1) ∼ P(• | prompt CoT (n)), z test 0 ∼ µ(• | Υ n ), Υ n ∼ P(• | θ * ).</formula><p>We take these expectations sequentially:</p><formula xml:id="formula_331">H h=1 E θ * E µ E z test 1:(h-1) ∼P(• | prompt CoT (n)) KL P(z test h = • | prompt h CoT (n)), P ρ (z test h = • | prompt h CoT (n)) ≤ H h=1 2(3 + b * ) • L * TV P(z test h = • | prompt h CoT (n)), P ρ (z test h = • | prompt h CoT (n)) (F.50) • π(θ * ) -1 • P(Υ n ) • κ • P(z test 0 = • | Υ n ) • P(z test 1:(h-1) = • | prompt CoT (n)) dprompt h CoT (n) = 2(3 + b * )κπ(θ * ) -1 H h=1 E P TV P(z test h = • | prompt h CoT (n)), P ρ (z test h = • | prompt h CoT (n))</formula><p>with probability at least 1 -δ. In the first inequality, we integrate over prompt h CoT (n) ∈ L * . This inequality is a result of (F.49), which transforms the KL distances into TV distances, and a change of distributions from P CoT to P. Notice that we have P(Υ n | θ * )π(θ * ) ≤ P(Υ n ) due to the discreteness of Θ. Also note that, by Assumption 6.5, µ(z test Next, we upper bound (F.50) using the analysis of pretraining error in Proposition 6.3. Recall that we introduce the notation E S∼D in (6.2), which involves an expectation over </p><formula xml:id="formula_332">= • | prompt h CoT (n)), P ρ (z test h = • | prompt h CoT (n)) (F.51) ≤ T t=1 H h=1 TV P(z t,1 h = • | S t,1 h ), P ρ (P(z t,1 h = • | S t,1 h ) ≤ T • (H + 1) • ∆ pre (N, T, δ),</formula><p>with probability with at least 1 -δ. Here the first inequality holds because the left-hand side is only a single term in the right-hand side summation with t = n, and the second inequality follows from Proposition 6.3, Combing (F.50) and (F.51), we upper bound the expectation of pretraining error as</p><formula xml:id="formula_333">E P CoT err pre (P, P ρ ; prompt CoT (n)) = E P CoT KL P(y test = • | prompt CoT (n)), P ρ (y test = • | prompt CoT (n)) ≤ 2(3 + b * )κ • π(θ * ) -1 • T (H + 1) • ∆ pre (N, T, δ), (F.52)</formula><p>with probability with at least 1 -δ. The randomness comes from the pretrained model P ρ . This concludes Step 1.</p><p>Step 2: Control expected prompting error under distribution shift. In this step, we evaluate the expected prompting error err prompt (5.1) with the expectation taken under prompt CoT (n) ∼ P CoT . Similar to Step 1, we first take the expectation of the shifted testing query z test 0 ∼ µ(• | prompt CoT (n)), followed by the expectation of the demonstrations Υ n ∼ P(• | θ * ).</p><p>We first compute the expected KL divergence with respect to the query</p><formula xml:id="formula_334">z test 0 ∼ µ(• | Υ n ): E z test 0 ∼µ(• | Υn) KL P(y test = • | z test 0 , θ * ), P(y test = • | prompt CoT (n)) ≤ E z test 0 ∼µ(• | Υn) log 1 + θ∈Θ ∁ P(Υ n , z test 0 | θ)π(θ) P(Υ n , z test 0 | θ * )π(θ * ) ≤ log 1 + E z test 0 ∼µ(• | Υn) θ∈Θ ∁ P(Υ n , z test 0 | θ)π(θ) P(Υ n , z test 0 | θ * )π(θ * ) . (F.53)</formula><p>The first inequality follows from Proposition C.2, and the second is due to Jensen's inequality. Next, we rewrite the expected likelihood ratio in (F.53) as</p><formula xml:id="formula_335">E z test 0 ∼µ(• | Υn) θ∈Θ ∁ P(Υ n , z test 0 | θ)π(θ) P(Υ n , z test 0 | θ * )π(θ * ) = θ∈Θ ∁ P(Υ n | θ) P(Υ n | θ * ) • E z test 0 ∼µ(• | Υn) P(z test 0 | θ)π(θ) P(z test 0 | θ * )π(θ * ) , (F.54)</formula><p>where ξ ∈ (0, 1), and the randomness comes from Υ n ∼ P(• | θ * ). By replacing x = log(1 + C(n) • ξ -2 ) in (F.59), for any x ∈ [0, ∞), we have</p><formula xml:id="formula_336">P E z test 0 ∼µ(• | Υn) KL P(y test = • | z test 0 , θ * ), P(y test = • | prompt CoT (n)) &gt; x ≤ C(n)/(exp(x) -1) 1/2 . (F.60)</formula><p>We provide an upper bound of the expected KL divergence by integrating the tail probability in (F.60) as follows,</p><formula xml:id="formula_337">E P CoT KL P(y test = • | z test 0 , θ * ), P(y test = • | prompt CoT (n)) = ∞ 0 P E z test 0 ∼µ(• | Υn) KL P(y test = • | z test 0 , θ * ), P(y test = • | prompt CoT (n)) &gt; x dx.</formula><p>Note that we can split the integration of x over [0, ∞) into two regions: [0, log(1 + C(n)) and [log(1 + C(n), ∞), where the probability in (F.60) is bounded by one in [0, log(1 + C(n)). Therefore, we have</p><formula xml:id="formula_338">E P CoT KL P(y test = • | z test 0 , θ * ), P(y test = • | prompt CoT (n)) ≤ log 1 + C(n) + ∞ log(1+C(n)) C(n)/(exp(x) -1) 1/2 dx = log 1 + C(n) + C(n) 1/2 • π -2 arctan C(n) 1/2 = O C(n) 1/2 = O π(Θ ∁ ) π(θ * ) • C(θ * ) • κ 1/2 • Θ ∁ • exp(-nλ) . (F.61)</formula><p>Here in the third line we plug in the closed-form exp(x) -1 -1/2 dx = 2 arctan exp(x) -1 .</p><p>When n is large, C(n) is sufficiently small. The second equality follows from the first-order Taylor approximations log(1 + u) ≈ u and arctan(u) ≈ u when u is close to zero. The convergence rate in the last line is dominated by the rate of C(n) 1/2 . Therefore, we control the rate of expected prompting error defined in (5.1) by applying (F.61):</p><formula xml:id="formula_339">E P CoT err prompt P, θ * , prompt CoT (n) = O Hb * π(Θ ∁ ) π(θ * ) C(θ * )κ 1/4 • Θ ∁ 1/2 • exp(-nλ/2) .</formula><p>(F.62) Combining (F.52) and (F.62), we have that under the Assumptions 5.4, 6.5, with probability at least 1 -δ,</p><formula xml:id="formula_340">E P CoT KL P(y test = • | z test 0 , θ * ), P ρ (y test = • | prompt CoT (n)) = O Hb * π(Θ ∁ ) π(θ * ) • C(θ * ) • κ 1/4 • Θ ∁ 1/2 • exp(-nλ/2) + κT Hπ(θ * ) -1 (1 + b * ) • ∆ pre (N, T, δ) ,</formula><p>where the first term corresponds to the prompting error err prompt (5.1), and the second corresponds to the pre-training error err pre (5.2). The randomness comes from the pretrained model P ρ . Therefore, we conclude the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G Supplemental Materials</head><p>This section consists of three subsections. The first Subsection G.1 gives a more detailed description of the pretraining process. The Subsections G.2 and G.3 prove the lemmas and propositions used in Sections C and F.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.1 Additional Details about Pretraining</head><p>In this section, we provide a detailed description of the pretraining process of autoregressive LLM. Specifically, we focus on pretraining with data sampled from the generalized model described in (A.1). An autoregressive LLM is a transformer that maps a reasoning step sequence S ∈ L * to a probability distribution for predicting the next reasoning step z ∈ L.</p><p>Transformer Architecture. We focus on a transformer with D transformer blocks stacked sequentially followed by a final softmax layer. Let X 0 ∈ R L×r denote the initial input embedding for the entire network, which contains both the content embedding and the positional encoding. The d-th block takes in X d-1 ∈ R L×r , produces X d ∈ R L×r , and feeds it to the next module until arriving at the last one. Each transformer block consists of four components: a MHA and a FF layer. Each component has a residual connection around it, followed by layer normalization, which prepares the raw output of the current layer to be forwarded as input toward the next layer. See Figure <ref type="figure">9</ref> for an illustration of the architecture.</p><p>Input Embedding. Specifically, the input of the transformer is a sequence of reasoning steps of length L, with each step taking values in L. Since the attention mechanism is permutation invariant but the sequential order matters in CoT reasoning, to encode an order, the transformer incorporates positional embeddings that map the positional information of each reasoning step into the Euclidean space. In addition, the values in L are also mapped to a vector space. Thus, the transformer first maps the input sequence of length L into a sequence of L vectors in R r , which involves both content and positional embedding.</p><p>Multi-Head Attention (MHA). We let X 0 ∈ R L×r denote the output after the embedding module, which is passed to D transformer blocks. Each block consists of a MHA layer, a FF layer, and two normalization layers. For any d ∈ [D], the parameters of the d-th transformer block are</p><formula xml:id="formula_341">ρ d = (W d mha , W d ff , γ d 1 , γ d 2 )</formula><p>. The weight matrices of the MHA layer are</p><formula xml:id="formula_342">W d mha = (W Q,d i , W K,d i , W V,d i ) η i=1</formula><p>, where η is the number of heads. Here,</p><formula xml:id="formula_343">W Q,d i ∈ R r×dq , W K,d i ∈ R r×d k , and W V,d i ∈ R r×dv convert the input X d-1 into</formula><p>queries, keys, and values, respectively. We set d v = r to ensure the MHA output is also in R r . Specifically, a MHA layer with η heads output a vector sequence given by (3.2). We denote the output of the d-th attention layer by X</p><formula xml:id="formula_344">d = mha(X d-1 , W d mha ) ∈ R L×r .</formula><p>In particular, for any ℓ ∈ [L], the ℓ-th vector of X d is given by</p><formula xml:id="formula_345">X d [ℓ] = η i=1 attn(q ℓ i , K i , V i ), (G.1)</formula><p>We concatenate all ρ d , d ∈ [D + 1], to form ρ, which parameterizes the whole pretraining network. We assume the parameters are bounded, i.e., we consider transformers in the following parameter space:</p><formula xml:id="formula_346">P LLM = ρ : ∥γ d 1 ∥ ∞ , ∥γ d 2 ∥ ∞ ≤ 1, ∥W Q,d i ∥ F , ∥W K,d i ∥ F , ∥W V,d i ∥ F ≤ B M , ∥W d ff,1 ∥ F , ∥W d ff,2 ∥ F ≤ B F , ∥W softmax ∥ 1,2 ≤ B S , ∀d ∈ [D], i ∈ [η], D ≥ C log(2H), η ≥ 1 ,</formula><p>where C &gt; 0 is a constant, B M , B F , B S are the upper bounds on the norms. We assume these bounds to be larger than 1.</p><p>Pretraining Dataset under the Generalized Model. We describe the pretraining dataset generated according to the generalized model in (A.1). We denote the pretraining dataset using D </p><formula xml:id="formula_347">= (Υ t-1,ℓ , {z t,ℓ j } h-1 j=0</formula><p>) to denote the sequence with all previous reasoning steps of the ℓ-th trajectory. Since LLMs make prediction autoregressively, we divide each trajectory into T (H + 1) pieces and collect all N independent trajectories and use D N,T = {(S t,ℓ h , z t,ℓ h )} H,T,N h=0,t=1,ℓ=1 to denote the pretraining dataset.</p><p>Maximum Likelihood Estimation (MLE). We obtain the pretrained LLM by minimizing the negative likelihood loss computed based on D N,T ,</p><formula xml:id="formula_348">ρ = argmin ρ∈P LLM - 1 N T (H + 1) N ℓ=1 T t=1 H h=0 log P ρ z t,ℓ h | S t,ℓ h (G.4)</formula><p>and set P LLM = P ρ . Here P ρ denotes the conditional distribution specified by the transformer with parameter ρ. We neglect the optimization issue and assume that the MLE in (G.4) can be obtained. We note that when the transformer class is sufficiently expressive, we expect that P LLM learns the conditional distribution of z t,ℓ h given S t,ℓ h , which is given in (4.5). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.2 Proofs of the</head><formula xml:id="formula_349">= • | prompt CoT (n) ≥ E θ∼q(θ) log P y test , θ | prompt CoT (n) q(θ) (G.5) = E q log P y test | θ, prompt CoT (n) π(θ)P prompt CoT (n) | θ q(θ)P prompt CoT (n) ,</formula><p>where we take expectation with respect to an arbitrary distribution q over Θ. The inequality follows from the evidence lower bound, and the equality follows from decomposing the numerator. Conditioning on any prompt CoT (n) consisting of n examples generated from the true distribution and a new testing input z test 0 , we compute the KL divergence with respect to the final output y test . For simplicity, we write E y test ∼P(• | z test 0 ,θ * ) as E θ * . Then, we have</p><formula xml:id="formula_350">KL P(y test = • | z test 0 , θ * ), P y test = • | prompt CoT (n) (G.6) ≤ E θ * log P(y test | z test 0 , θ * ) -E θ * E q log P y test | θ, prompt CoT (n) π(θ)P prompt CoT (n) | θ q(θ)P prompt CoT (n) = E θ * E θ∼q log P prompt CoT (n) q(θ) P(prompt CoT (n) | θ)π(θ) -E θ * E θ∼q log P y test | z test 0 , θ P(y test | z test 0 , θ * )</formula><p>.</p><p>The inequality follows from the definition of KL divergence and the lower bound in (G.5).</p><p>The equality follows from the fact that P(y test | θ, prompt CoT (n)) = P(y test | z test 0 , θ) under the model in (4.2) and rearranging terms. Now we set q as q(θ) ∝ 1{θ ∈ Θ eq (θ * )} • π θ | prompt CoT (n) , where π θ | prompt CoT (n) is the posterior distribution over Θ after observing the prompt. Note that this q assigns zero probability to any θ outside the equivalence class Θ eq (θ * ). By taking an expectation with respect to this q, we have</p><formula xml:id="formula_351">E θ * E θ∼q log P(y test | z test 0 , θ) -log P(y test | z test 0 , θ * ) = 0</formula><p>by the construction of the equivalence classes in Definition (5.3). Thus the KL divergence in (G.6) is further bounded as follows:</p><formula xml:id="formula_352">KL P y test = • | z test 0 , θ * , P y test = • | prompt CoT (n) ≤ E θ * E θ∼q log P prompt CoT (n) q(θ) P prompt CoT (n) | θ • π(θ) = E θ * E θ∼q log P prompt CoT (n) • π θ | prompt CoT (n) P prompt CoT (n) | θ π(θ) • Θeq(θ * ) π θ ′ | prompt CoT (n) dθ ′ = -log Θeq(θ * ) π θ | prompt CoT (n) dθ . (G.7)</formula><p>Here in the first equality, we plug in the closed form of q(θ), and in the second equality, we use the fact that</p><formula xml:id="formula_353">P prompt CoT (n) • π θ | prompt CoT (n) = P prompt CoT (n) | θ • π(θ)</formula><p>to cancel terms. We can interpret the last term in (G.7) as an integrated version of posterior contraction. Intuitively, a better CoT prompt yields a higher posterior concentration on Θ eq (θ * ), leading to a smaller upper bound. Finally, we plug in the closed-form expression of π</p><formula xml:id="formula_354">(θ | prompt CoT (n)), i.e., π θ | prompt CoT (n) = P prompt CoT (n) | θ π(θ) Θ P prompt CoT (n) | θ π(θ)dθ</formula><p>, in (G.7) and obtain that</p><formula xml:id="formula_355">KL P(y test = • | z test 0 , θ * ), P y test = • | prompt CoT (n) ≤ log Θ P prompt CoT (n) | θ π(θ)dθ Θeq(θ * ) P prompt CoT (n) | θ π(θ)dθ = log 1 + Θ ∁ P prompt CoT (n) θ π(θ)dθ Θeq(θ * ) P prompt CoT (n) θ π(θ)dθ .</formula><p>Here the equality follows from the definition of Θ ∁ . Therefore, we conclude the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.2.2 Proof of Lemma C.3</head><p>Proof of Lemma C.3. Recall that we define J i ⊆ [H -1] for each i ∈ [n], and we use S i J i to denote a truncated version of the i-th trajectory S i corresponding to the indices specified by</p><formula xml:id="formula_356">J i . Namely, S i J i = {z i 0 } ∪ {z j } j∈J i ∪ {z i H }.</formula><p>We begin by applying the previous Lemma H.2:</p><formula xml:id="formula_357">1 2 log P({S i J i } n i=1 | θ) P({S i J i } n i=1 | θ * ) = n i=1 1 2 log P(S i J i | θ) P(S i J i | θ * ) ≤ n i=1 log E θ * P(S J i | θ) P(S J i | θ * ) 1 2 + log(δ -1 ) ≤ n i=1 E θ * P(S J i | θ) P(S J i | θ * ) 1 2 -1 + log(δ -1 ) = - n i=1 H 2 P(S J i | θ * ), P(S J i | θ) + log(δ -1 ),</formula><p>with probability at least 1 -δ. The first inequality follows from Lemma H.2, and the second inequality follows from the fact that x -1 ≥ log(x). Putting both sides of the inequality into the exponential function, we have</p><formula xml:id="formula_358">P({S i J i } n i=1 | θ) P({S i J i } n i=1 | θ * ) ≤ exp -2 n i=1 H 2 P(S J i | θ * ), P(S J i | θ) + 2 log(δ -1 ) ,</formula><p>with probability at least 1 -δ.</p><p>Finally, to prove the second argument, by conditional independence, we have</p><formula xml:id="formula_359">1 2 log P({S i J i } n i=1 , z n+1 0 | θ) P({S i J i } n i=1 , z n+1 0 | θ * ) = 1 2 log P(z n+1 0 | θ) P(z n+1 0 | θ * ) + n i=1 1 2 log P(S i J i | θ) P(S i J i | θ * )</formula><p>.</p><p>The rest of the proof is exactly the same as above. Therefore, we conclude the proof.</p><p>G.3 Proofs of the Auxiliary Lemmas in Appendix F.1</p><formula xml:id="formula_360">G.3.1 Proof of Lemma F.2</formula><p>Proof. We first note that for any z ∈ L and S ∈ L * , we have</p><formula xml:id="formula_361">P ρ (z | S) ≥ 1 + |L| exp(B S /τ ), (G.8)</formula><p>which follows from the softmax layer in (G.3). Combining (G.8) with Assumption 6.2, we obtain the upper bound of the following log density bound:</p><formula xml:id="formula_362">log P(z | S) -log P ρ (z | S) ≤ b * = log max{c -1 0 , 1 + |L| exp(B S /τ )}. (G.9)</formula><p>Inequality (G.9) gives the specific form of the upper bound b * mentioned in Assumption 5.1. Next, we apply concentration to each fixed (t, h) with t ∈ [T ] and 0 ≤ h ≤ H. By Hoeffding's inequality and (G.9) we have</p><formula xml:id="formula_363">P N ℓ=1 log P(z t,ℓ h | S t,ℓ h ) P ρ ′ (z t,ℓ h | S t,ℓ h ) -E S t,ℓ h KL P(• | S t,ℓ h ), P ρ ′ (• | S t,ℓ h ) &gt; t ≤ 2 exp -t 2 2N (b * ) 2 .</formula><p>(G.10) Then (G.10) implies that, with probability at least 1 -δ, we have</p><formula xml:id="formula_364">1 N N ℓ=1 log P(z t,ℓ h | S t,ℓ h ) P ρ ′ (z t,ℓ h | S t,ℓ h ) -E S t,ℓ h KL P(• | S t,ℓ h ), P ρ ′ (• | S t,ℓ h ) ≤ b * 1 2N log 1 δ . (G.11) Applying union bound to (G.11) for all (t, h) with t ∈ [T ], 0 ≤ h ≤ H, we have that 1 N T (H + 1) • N ℓ=1 T t=1 H h=0 log P(z t,ℓ h | S t,ℓ h ) P ρ ′ (z t,ℓ h | S t,ℓ h ) -E S t,ℓ h KL P(• | S t,ℓ h ), P ρ ′ (• | S t,ℓ h ) ≤ b * 1 2N log T (H + 1) δ .</formula><p>Therefore, we conclude the proof of this lemma.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.3.2 Proof of Lemma F.4</head><p>Proof. Fix any (t, h) with t ∈ [T ] and 0 ≤ h ≤ H, we invoke Proposition H.5 by setting X ℓ = S t,ℓ h and f (S t,ℓ h ) = TV P(• | S t,ℓ h ), P ρ (• | S t,ℓ h ) , which gives us that with probability at least 1 -δ, <ref type="bibr">G.12)</ref> where P refers to the distribution defined in Lemma F.1, and Q is the uniform distribution over P LLM . The right-hand side of (G.12) follows from Proposition H.5 by setting b = 1 because the TV distance is always between 0 and 1.</p><formula xml:id="formula_365">1 N E ρ∼P N ℓ=1 E S t,ℓ h TV P(• | S t,ℓ h ), P ρ (• | S t,ℓ h ) -TV P(• | S t,ℓ h ), P ρ (• | S t,ℓ h ) ≤ 1 2 log 2 • N KL(P ∥ Q) + log 4 δ ,<label>(</label></formula><p>Next, we note that by the construction of P in (F.11), for ρ ∼ P , we have</p><formula xml:id="formula_366">TV P ρ (• | S t,ℓ h ), P ρ (• | S t,ℓ h ) ≤ 1 2 KL P ρ (• | S t,ℓ h ), P ρ (• | S t,ℓ h ) = O 1/ √ N T H , (G.13)</formula><p>for any S t h . The first line follows from the Pinsker's inequality, and the second line follows directly from (F.12).</p><p>Combining Lemma F.3, (G.12) and (G.13) with union bound across all (t, h), t ∈</p><formula xml:id="formula_367">[T ], 0 ≤ h ≤ H, we obtain 1 N T (H + 1) N ℓ=1 T t=1 H h=0 E S t,ℓ h TV P(• | S t,ℓ h ), P ρ (• | S t,ℓ h ) -TV P(• | S t,ℓ h ), P ρ (• | S t,ℓ h ) = O 1 √ N D log(1 + N T H B) + log T H δ .</formula><p>Therefore, we conclude the proof.  <ref type="bibr">(F.19)</ref>. Second, to avoid the influence of the normalization layers, we adopt the scaling trick by scaling the parameters of the FF layer to ensure the output matrix is bounded by one in terms of the row-wise ℓ 2 -norm. This normalizing scalar is then multiplied by the weight matrix of the next layer to ensure the output stays the same.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.4 Proofs of the</head><p>More rigorously, we define B 0 = B, which is an upper bound on ∥(X 0 ) ⊤ ∥ 2,∞ . Recall that we assume the intermediate values of the MLP satisfy ∥(</p><formula xml:id="formula_368">X d ) ⊤ ∥ 2,∞ ≤ B d for all d ∈ [L],</formula><p>where ∥(X d ) ⊤ ∥ 2,∞ is the row-wise maximum ℓ 2 -norm of X d . We will construct a transformer such that Y d = X d /B d for all d ∈ [L]. We prove this argument via induction.</p><p>We will verify the base case Y 1 = X 1 /B 1 later. For any d ≥ 2, suppose we have Y d-1 = X d-1 /B d-1 and let us consider the d-th transformer block and the d-th layer of the MLP. Note that X d is constructed by</p><formula xml:id="formula_369">X d-1 via (F.22) and Y d is derived from Y d-1 via Y d = NL(ff(Z, W ff , b ff ) + Z d γ2 , Z d = NL(mha(Y d-1 , W mha ) + Y d-1 γ1 (G.14)</formula><p>for some weight matrices W ff , b ff , W mha , γ 1 , and γ 2 to be determined. We set the number of heads of the MHA layer to be η = 1 and set the weight matrix of the values, W V , as a zero matrix. Moreover, we set γ 1 = I in (G.14). Thus, we have</p><formula xml:id="formula_370">Z d = NL mha(Y d-1 , W mha ) + Y d-1 γ 1 = NL(X d-1 /B d-1 ) = X d-1 /B d-1 . (G.15)</formula><p>Here the second equality follows from the induction assumption and the last equality follows from the fact that the ℓ 2 -norm of each row of X d-1 /B d-1 is bounded by one. Now we set</p><formula xml:id="formula_371">W ff = {W d ff • B d-1 , I/B d }, b d ff = {b d ff , 0},<label>and</label></formula><formula xml:id="formula_372">γ 1 = I in (G.14).</formula><p>That is, W ff,1 is proportional to W d ff of the MLP, W ff,2 is proportional to an identity matrix, b ff,1 is the same as b d ff of the MLP, b ff,2 is a zero vector, and γ 1 as an identity matrix. Then, by direct calculation we have</p><formula xml:id="formula_373">ff(Z d , W ff , b ff ) = ReLU(X d-1 W d ff + 1 ⊤ b d ff ) B d = X d /B d .</formula><p>As a result, in (G.14) we have</p><formula xml:id="formula_374">Y d = NL ff(X d-1 /B d-1 , W ff , b ff ) = NL(X d /B d ) = X d /B d , (G.16)</formula><p>where the first equality follows from (G.15) and the last equality follows from the fact that B d ≥ ∥(X d ) ⊤ ∥ 2,∞ . Also see Figure <ref type="figure" target="#fig_23">17</ref> for an illustration of each transformer block. It remains to verify the base case Y 1 = X 1 /B 1 . Recall that the transformer and the MLP have the same input. Thus we have Y 0 = X 0 . For d = 1, in (G.14) we set the value matrix of MHA to be zero and set γ 1 = I/B 0 . Since each row of X 0 /B 0 is in the unit ball with respect to the ℓ 2 -norm, we have In summary, we construct a transformer with L transformer blocks such that the final output Y L satisfies Y L = X L /B L . The weight matrices of these transformer blocks are given by {W </p><formula xml:id="formula_375">Z 1 = NL(Y 0 γ 1 ) = NL(X 0 /B 0 ) = X 0 /B 0 .</formula><formula xml:id="formula_376">Q,d 1 , W K,d 1 , W V,d 1 , W d ff,1 , W d ff,2 , b d ff,1 , b d ff,2 , γ d 1 , γ d 2 , } L d=1 , where W d ff,1 = B d-1 • W d ff , W d ff,2 = I/B d , b d ff,1 = b d ff , b d ff,2 = 0, W V,d mha = 0, γ d 2 =</formula><formula xml:id="formula_377">=   1 -1 0 0 0 0 0 0 1 -1 0 0 0 0 0 0 1 -1   ∈ R 3×6 .</formula><p>Note that fact that x = ReLU(x) -ReLU(-x) for any x ∈ R. For any vector of the form v = (a, b, 0) ⊤ in R 3 , by the direct computation we have</p><formula xml:id="formula_378">(W NN,(1) ff,2 ) ⊤ ReLU (W NN,(1) ff,1 ) ⊤ v + (b NN,(1) ff,1 ) ⊤ = (a, b + 1, 0) ⊤ .</formula><p>Thus, we have</p><formula xml:id="formula_379">X (1) NN = ff(X (0) NN , W NN,(1) ff , b NN,(1) ff ) = ReLU(X (0) NN W NN,(1) ff,1 + 1 T b NN,(1) ff,1 )W NN,(1) ff,2 ,</formula><p>which means that NN J,1 is realized by a FF layer. Here 1 ∈ R L ′ is the all-one vector. Next, we compute the scaling factors in Proposition F.7 to bypass the normalization layer in the FF layer. Note that both the input and output magnitude is bounded by √ B 2 + H 2 + 1. Absorbing the scaling factor into the weight matrix, we have max{∥W NN,(1)</p><formula xml:id="formula_380">ff,1 ∥ F , ∥W NN,(1) ff,2 ∥ F , ∥b NN,(1) ff,1 ∥ 2 , ∥b NN,(1) ff,2 ∥ 2 } ≤ √ B 2 + H 2 + 1 • √ 6.</formula><p>Furthermore, as introduced at the beginning of Appendix F.6, we can represent a feedforward layer using a transformer block. Thus, we conclude the proof.</p><p>G.4.3 Proof of Lemma F.9</p><p>Proof. We first show that the product operation can be well approximated by a fully connected neural network, which is stated in the following lemma.</p><p>Lemma G.1 (Product Operation as Neural Network). Let r ≥ 1 be an integer. There exists a constant C p &gt; 0 such that for any M and ϵ, there exists a multi-layer perceptron</p><formula xml:id="formula_381">f product : R r × R 1 → R r with D layers such that for any x 1 , • • • , x r , y ∈ [-M, M ], ∥f product x 1:r , y -(x 1 y, • • • , x r y)∥ ∞ ≤ ϵ.</formula><p>Moreover, the depth D satisfies D ≤ C p log(M ) + log(1/ϵ)), and the maximum number of the hidden neurons is bounded by 5r. Furthermore, the parameters</p><formula xml:id="formula_382">{W i ff,1 , b i } i∈[D] satisfies max{∥W i ff,1 ∥ ∞ , ∥b i ∥ ∞ } ≤ 1 for all i ∈ [D -1], and ∥W D ff,1 ∥ ∞ ≤ M 2 .</formula><p>Proof. See Appendix G.5.2 for details.</p><p>The output (F.25) can be achieved by applying the product operation construction from Lemma G.1 with r = 1, ensuring that |f product (h, 1) -h| &lt; ϵ ′ and |f product (h ′ + 1, 0) -0| &lt; ϵ ′ for all (t ′ , h ′ ) &lt; (t, h -1).</p><p>By setting r = 1, this lemma says that we can realize the product operation f product in (F.25) as the output of a fully connected network with an error at most ϵ ′ . Moreover, the depth of f product is at most C p (log(H) + log(1/ϵ ′ )) for some constant C, and the maximum number of hidden neurons is bounded by 5.</p><p>To implement this fully connected network using a transformer, we employ Proposition F.7 by setting scaling factors as B 0 = √ H 2 + 1 and B d = √ 5 for all d ∈ [D -1], and B D = H 2 . So far we have shown that we can use D transformer blocks to realize the product operation f product (x, y) ≈ xy. However, note that the target input of module NN J,2 is X</p><formula xml:id="formula_383">(1) NN ∈ R L ′ ×3 , and the target output is X (2) NN ∈ R L ′ ×3</formula><p>, where the product operation f product (x, y) only substitutes the second column of the output. Thus, we have to preserve the first and last columns of the input.</p><p>To realize the output in (F.25), we concatenate another MLP, denoted by f 1,3 to f product . MLP f 1,3 : R L ′ ×3 → R L ′ ×2 extracts the first and last columns of the input matrix. As we show in Lemma G.2, such an MLP exists and f 1,3 in fact can be written as a single FF layer. The Frobenius norms of the weight matrices are bounded by 2. The intuition behind Lemma G.2 is that we can write and x as ReLU(x) -ReLU(-x), which enables us to preserve desired columns of the input matrix using a FF layer.</p><p>Finally, to concatenate f 1,3 with f product , notice that these two MLPs might have different numbers of layers. This does not cause trouble because by Lemma G.2, we can further stack FF layers on top of f 1,3 that represents identity mappings. This will enable us to write f 1,3 as an MLP that has the same depth as f product . Then we can concatenate the weights of these two MLPs in a layer-wise fashion. Thus, the Frobenius norm of the weight matrices of the concatenated MLP is bounded by</p><formula xml:id="formula_384">√ B 2 + H 2 + 6 • √ H 4 • 5 + 4</formula><p>, where the first term in the multiplication comes from the scaling factors and the second term follows from the weight matrices. This concludes the proof of this lemma.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.4.4 Proof of Lemma F.10</head><p>Proof. This linear operation can be realized without error by setting which can be realized by setting bias term b NN,(2) = (0, 0, 0, 0, 1, -1) and b NN,(2) ff,2 = 0. We set the weight matrices as</p><formula xml:id="formula_385">W NN,(2) ff,1 =   1 -1 0 0 0 0 0 0 1 -1 0 0 0 0 0 0 -1/2 1/2   , W NN,(2) ff,2 =   1 -1 0 0 0 0 0 0 1 -1 0 0 0 0 0 0 1 -1   ⊤ .</formula><p>based on the i-th query and j-th key for all i, j ∈ [L ′ ]:</p><formula xml:id="formula_386">a(i, j) =                          e -α/2 e -α/2 + (L ′ -1) • e -α for i ̸ = L ′ and j = L ′ , e -α e -α/2 + (L ′ -1) • e -α</formula><p>for i ̸ = L ′ and j ̸ = L ′ , e -α/4 e -α/4 + (L ′ -1) • e -α/2 for i = L ′ and j = L ′ , e -α/2 e -α/4 + (L ′ -1) • e -α/2 for i = L and j ̸ = L ′ .</p><p>Using these attention scores to aggregate the value vectors, for any position (t ′ , h ′ ), we obtain the output of the attention layer:</p><formula xml:id="formula_387">(0, p t ′ h ′ , 0) = L ′ j=1 a L ′ (t ′ , h ′ ), j • v j , (G.18)</formula><p>where v j is the j-th row of the value matrix V , and</p><formula xml:id="formula_388">L ′ (t ′ , h ′ ) is the index of z t ′ h ′ in sequence S t h .</formula><p>By the construction of V in (G.17), the first and last coordinates of the attention output are both zero, and we let p t ′ h ′ in (G.18) to denote the nonzero coordinate. It remains to show that |p t ′ h ′ -h| is small for all (t ′ , h ′ ). For any ϵ &gt; 2ϵ ′ , we can choose α sufficiently large such that (L ′ -1) • max{e -α/2 , e -α/4 } + 2ϵ ′ &lt; ϵ. We separately consider the cases where (t ′ , h ′ ) &lt; (t, h -1) and (t ′ , h ′ ) = (t, h -1) as follows. For the first case, we have</p><formula xml:id="formula_389">|p t ′ h ′ -h| = e -α/2 e -α/2 + (L ′ -1) • e -α • f product (h, 1) -h + e -α e -α/2 + (L ′ -1) • e -α (t ′ ,h ′ )&lt;(t,h-1) f product (h ′ + 1, 0) (G.19) ≤ (L ′ -1) • e -α/2 + 2ϵ ′ &lt; ϵ.</formula><p>To see the second inequality, we note that using the fact that |f product (h, 1) -h| ≤ ϵ ′ and the fact that</p><formula xml:id="formula_390">e -α/2 e -α/2 + (L ′ -1) • e -α = 1 1 + (L ′ -1) • e -α/2 , we have by direct computation that e -α/2 e -α/2 + (L ′ -1) • e -α • f product (h, 1) -h ≤ |f product (h, 1) -h| 1 + (L ′ -1) • e -α/2 + (L ′ -1) • e -α/2 1 + (L ′ -1) • e -α/2 , which is bounded by (L ′ -1) • e -α/2 + ϵ ′ . Moreover, the second summation in (G.19) is bounded by ϵ ′ because e -α e -α/2 + (L ′ -1) • e -α (t ′ ,h ′ )&lt;(t,h-1) f product (h ′ + 1, 0) ≤ |f product (h ′ + 1, 0)| ≤ ϵ ′ .</formula><p>Combining the above two inequalities yields (G.19). Similarly, for (t ′ , h ′ ) = (t, h -1), we use the same argument to obtain that</p><formula xml:id="formula_391">|p n+1 h-1 -h| = e -α/4 e -α/4 + (L ′ -1) • e -α/2 • f product (h, 1) -h + e -α/2 e -α/4 + (L ′ -1) • e -α/2 (t ′ ,h ′ )&lt;(t,h-1) f product (h ′ + 1, 0) ≤ (L ′ -1) • e -α/4 + 2ϵ ′ &lt; ϵ.</formula><p>In conclusion, when selecting α such that (L ′ -1) • max{e -α/2 , e -α/4 } &lt; ϵ -2ϵ ′ , we have</p><formula xml:id="formula_392">|p t ′ h ′ -h| &lt; ϵ for all (t ′ , h ′ ). Since L ′ ≤ T • (H + 1), it suffices to choose α = 8 • log T H/(ϵ -2ϵ ′ ) .</formula><p>We conclude that we can construct an attention layer that takes z t ′ h ′ ,(2) ∈ R 3 as input and outputs (0, p t ′ h ′ , 0), such that |p t ′ h ′ -h| &lt; ϵ for any (t ′ , h ′ ) ≤ (t, h -1). Moreover, the norms of weight matrices satisfy</p><formula xml:id="formula_393">∥W Q ∥ F = 8 log T H/(ϵ -2ϵ ′ ) , ∥W K ∥ F = ∥W V ∥ F = 1.</formula><p>Note that this single-head attention is a special of MHA layer with η = 1. Finally, to show that such a layer can be implemented by a single transformer block defined in (F.19), we use a residual link by setting γ 1 = diag(1, 0, 0) ∈ R 3×3 in (F.19). This enables us to pass along the first coordinate z t ′ h ′ . Additionally, the FF module can append zeros to the input by taking W</p><formula xml:id="formula_394">NN,(1) ff,2 as W NN,(1) ff,1 =   1 -1 0 0 0 0 0 0 1 -1 0 0 0 0 0 0 1 -1   ∈ R 3×6 , W NN,(1) ff,2 ⊤ = W NN,(1) ff,1 0 ∈ R (2|L|+2)×6 , and b NN,(1) ff,1 = b NN,<label>(1)</label></formula><p>ff,2 = 0. As a result, for any (t ′ , h ′ ), the output of the transformer block is given by</p><formula xml:id="formula_395">z t ′ ,(4) h ′ = NL (z t ′ h ′ , p h ′ h ′ , 0) ∈ R 2+2|L| . Note that the ℓ 2 -norm of vector (z t ′ h ′ , p h ′ h ′ , 0) is bounded by a constant because |p h ′ h ′ | ≤ H<label>and</label></formula><p>L is regarded as a compact subset of R. Thus, we can additionally apply the scaling trick introduced in Appendix F.3 to bypass the normalization layer NL(•). To implement this scaling trick, we need only to scale W V by a constant factor B 2 + (H + 1) 2 + 1, which affects the magnitude of the transformer weight matrices by a constant factor. Now we conclude the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.4.6 Proof of Proposition F.12</head><p>Proof. This proof is structured in three steps. In Step 1, we provide a high-level overview of the network embed h (•), aiming for softmax(embed h (S ), F 4 ) as the output. We show how to modify the weight matrices in embed h (•) to construct G h . This technique is applied repeatedly in the proof found in Appendix F.3, with similar approaches being used in related cases.</p><p>Step 1: High-level structure of each embed h (•). The module </p><formula xml:id="formula_396">G h takes input (S t h , p h , F 3 , F 4 ) ∈ R L ′ ×(2+2|L|) as the input and outputs (S t h , p h , 1 ⊤ L ′ embed h (S t h ), F 4 ) ∈ R L ′ ×(2+2|L|) . Note</formula><formula xml:id="formula_397">* h (S t h ) = w * h 1 L ′ (t,h-1) (i,j)=(1,0) ψ * h (z i j ) ≈ softmax f w h 1 L ′ (t,h-1) (i,j)=(1,0) ψ * h (z i j ) τ = g * h (S t h ), (G.20)</formula><p>where (t,h-1) (i,j)=(1,0) means that we sum over all reasoning steps before z t h . Here f w h in (G.20) approximates the function τ • log w * h . In Step 2, we apply Lemma H.7 to separately bound the error induced by f w h and ψ * h using the universal approximation property of the fullyconnected networks. Finally, in Step 3 we combine everything and construct the G h as a composition of transformer blocks.</p><p>In the sequel, we introduced the rationale behind the construction of ψ * h , and f w h describe how to implement them using transformer blocks.</p><p>• Approximate ψ * h using an MLP. We construct ψ * h : R → R as a fully-connected MLP with D ψ layers and each layer has no more than 16 neurons, where D ψ will be specified later. The construction directly follows from Lemma H.7, which is a neural network approximation result established in <ref type="bibr">Elbrächter et al. (2021)</ref>. As shown in Proposition F.7, such a fully connected network can be regarded as a composition of D ψ transformer blocks. In particular, as shown in the proof of Proposition F.7, in each transformer block as in (F.19), we can set the value matrices in the MHA layers to be zero, set γ 1 as an identity matrix, and set γ 2 to a zero matrix. This reduces the transformer block to a feed-forward layer, combined with normalization. We can apply the scaling trick introduced in Appendix F.3 to bypass the normalization layer. This enables us to represent each layer of the MLP using a transformer block.</p><p>• Realize the average module. After having ϕ * h , we need to compute 1/L ′ • (t,h-1) (i,j)=(1,0) ψ * h (z i j )</p><p>using a transformer block. This can be achieved by having a single-head attention layer with W Q = W K = 0, and W V = 1. To see this, observe that when W Q = W K = 0, all the attention scores become 1/L ′ and thus the attention layer becomes an average. where 256 appears because it is the total number of parameters in each layer of f w h,i . Here, the first inequality follows from the triangle inequality. In the second inequality, we employ Lemma H.6, which states that softmax(•) is Lipschitz continuous with parameter 2 in terms of the ℓ 1 -ℓ ∞ norm pair. To bound the first term, we combine Lemma H.6 and (G.21), which shows that the first term is no more than 2ϵ w . To bound the second term, we note that fact that each f i h,i , as a D w -layer MLP, is a Lipschitz continuous function in terms of the ℓ ∞ -norm. The Lipschitz parameter is bounded by 256 Dw because the vectorized ℓ 1 -norm of the weight matrix in each layer is bounded by 256, which is a result of Lemma H.7. Then we combine Lemma H.6, Lipschitzness of f w h , and (G.21) to obtain</p><formula xml:id="formula_398">softmax f w h 1 L ′ (t,h-1) (i,j)=(1,0) ψ * h (z i j ) -softmax f w h 1 L ′ (t,h-1) (i,j)=(1,0) ψ * h (z i j ) 1 ≤ 2 • max i∈[|L|] f w h,i 1 L ′ (t,h-1) (i,j)=(1,0) ψ * h (z i j ) -f w h 1 L ′ (t,h-1) (i,j)=(1,0) ψ * h (z i j ) ≤ 2 • 256 Dw • 1 L ′ (t,h-1) (i,j)=(1,0) ψ * h (z i j ) - 1 L ′ (t,h-1) (i,j)=(1,0) ψ * h (z i j ) ∞ &lt; 2 • 256 Dw • ϵ ψ ,</formula><p>In summary, for any given ϵ w and ϵ ψ , there exist MLPs ψ * h and { f w h,i } i∈ <ref type="bibr">[|L|]</ref> such that g * h (S t h )softmax embed h (S t h )/τ 1 ≤ 2ϵ w + 2 • 256 Dw • ϵ ψ .</p><p>These MLPs have at most D ψ and and D w layers respectively, where D ψ and D w are defined in (G.22). In each layer, there are 16 neurons and the weights are all in [-1, 1].</p><p>In conclusion, the above analysis can be extended to bound the error g * h (S t h )softmax(embed h (S t h )/τ ) 1 for all h ∈ {0, . . . , H} using the same upper bound. This means that softmax(embed h (S t h )/τ ) serves as an estimator for g * h (•) with uniform precision across all h. This generalization is possible because the analysis in (G.24) is based solely on the Lipschitz continuity of softmax and the approximation errors established in (G.21). Therefore, the same error bounds apply when substituting any h for h in g * h (•) and embed h (•), ensuring that the error analysis is valid for any h ∈ {0, . . . , H}.</p><p>Step 3: Construct the transformer module G h . In the previous step, we construct embed h (S t h ) that approximates g * h (S t h ). However, the actual input of the transformer module G h is (S t h , p h , F 3 , F 4 ) ∈ R L ′ ×(2+2|L|) and the expected output is (S t h , p h , 1 ⊤ L ′ embed h (S t h ), F 4 ) ∈ R L ′ ×(2+2|L|) . In this final step, we explicitly construct the transformer module G h that preserves S t h , p h , F 4 and substitute 1 ⊤ L ′ embed h (S t h ) in the third column. To achieve such a goal, we need to first show that embed h (•) can be implemented by transformer blocks. Then we need to show that these transformer blocks can be put in a larger transformer with the desired input-output relationship. To achieve the first goal, we apply Proposition F.7 separately to the approximation modules ψ * h and ( f w h,1 , • • • , f w h,|L| ), and connect them with the average module that is realized by a single-head attention layer. In particular, we apply the scaling trick in Proposition F.7 to bypass the normalization layers in the transformer blocks. We specify the expression of these scaling at the end of our proof.</p><p>Our next step is to adjust weight matrices in each transformer block to preserve S t h , p h , F 4 and substitute embed h (S t h ) in the third column. To this end, we introduce the notion of a residual ReLU module, which is an FF layer that only keeps some desired columns of the input matrix. Then we can concatenate embed h (•) with a residual ReLU module to achieve the desired functionality. Proof. See Appendix G.5.3 for details.</p><p>To show that we can fuse embed h (•) with a residual ReLU module that preserves the submatrix (S t h , p h , F 4 ) ∈ R L ′ ×(2+|L|) through each FF layer, it suffices to show that a residual ReLU module can work together with a feed-forward layer and a MHA layer. The reason is that embed h (•) is a composition of D ψ FF layers, an attention layer, and D w FF layers. If each layer of embed h (•) can be added to a larger network which keeps (S t h , p h , F 4 ) ∈ R L ′ ×(2+|L|) unchanged, then we can apply this argument to all layers of embed h (•) and obtain the desired network. Thus, in the following, we focus only on a FF layer and a MHA layer.</p><p>Notice that permutation of the columns can be achieved by a linear FF layer. It suffices to put columns corresponding to embed h (•) to the first |L| columns. That is, we can study whether the transformation (X, S t h , p h , F 4 ) -→ (X ′ , S t h , p h , F 4 ), can be achieved by a transformer block, where X ∈ R L ′ ×d for some d, and X ′ is obtained by X through an FF or MHA layer. For ease of presentation, we denote (S t h , p h , F 4 ) by a matrix Y ∈ R L ′ ×d ′ and study this problem with abstraction, where d ′ is the number of columns in Y . Then we consider X ′ = ff((X, Y ), W ff , b ff ) or X ′ = mha((X, Y ), W mha ).</p><p>First, we assume X ′ = ff((X, Y ), W ff , b ff ), where W ff = (W ff,1 , W ff,2 ) and b ff = (b ff,1 , b ff,2 ). Using Lemma G.2, we will construct weights W ff , b ff such that (X ′ , Y ) = ff((X, Y ), W ff , b ff ). In particular, we apply Lemma G. Here in W ff,1 we add d ′ all-zero rows below W ff,1 to construct a valid matrix. As defined in (F.20), we can directly calculate the FF layer with parameters W ff and b ff and have</p><formula xml:id="formula_399">ff (X, Y ), W ff , b ff ) = ReLU (X, Y )W ff,1 + 1 ⊤ b ff,1 W ff,2 + 1 ⊤ b ff,2 = ReLU((X, Y )W ff,1 + 1 ⊤ b ff,1 ), ReLU((X, Y )W Y ff,1 ) W ff,2 + 1 ⊤ b ff,2 = (X ′ , Y ).</formula><p>Therefore, we construct an FF layer such that we change X to X ′ and keep Y unchanged.</p><p>It remains to consider the case where X ′ = mha((X, Y ), W mha ). We show that (X ′ , Y ) can be implemented by a transformer block starting from (X, Y ). When X ′ = mha(X, W mha ), we can augment the three matrices of W mha by adding zeros such that (X ′ , 0) = mha((X, Y ), W mha ), where W mha is obtained from W mha by adding zeros, and 0 is a zero matrix that has the shape as Y . Then, with a generalized residual link, we have (X ′ , Y ) = mha((X, Y ), W mha ) + (X ′ , Y ) 0 0 0 I , where the 2 × 2 block matrix plays the same role as γ 1 in (F.19). Therefore, we conclude that an FF and MHA layer that maps X to X ′ can be augmented to a layer that maps (X, Y ) to (X ′ , Y ). Now we apply this argument recursively for embed h (•). The input matrix is (F 3 , S t h , p h , F 4 ) and the desired output is (1 ⊤ L ′ embed h (S t h ), S t h , p h , F 4 ). In particular, embed h (S t h ) is an MLP of S t h that consists of D ψ + D w FF layers in total and a MHA layer. Thus, we can apply the above argument with X = F 3 , Y = (S t h , p h , F 4 ) and X ′ being the intermediate outputs of embed h (•). We conclude that such a mapping can be implemented by a transformer with D ψ + D w + 1 blocks.</p><p>Finally, we need to permute (1 ⊤ L ′ embed h (S t h ), S t h , p h , F 4 ) to (S t h , p h , 1 ⊤ L ′ embed h (S t h , F 4 ), which can be achieved by another linear layer. Therefore, the desired G h can be implemented by D ψ + D w + 2 transformer blocks.</p><p>We compute the scaling factors from Proposition F.7 when implementing each approximation module f w h,i and f w h = { f w h,i } |L| i=1 using transformers blocks. Furthermore, we conclude this proof by commenting on the width and norm of weight matrices of G h .</p><p>In the construction of G h , we first note that since we approximate each coordinate of the output distribution individually using f w h,i for i ∈ [|L|], we horizontally stack the weight matrices for each f w h,i at corresponding layers. Therefore, we derive an upper bound of the hidden layer size as d F ≤ 16|L| + 4 + 2|L|, where 16|L| follows from the transformer implementation of f w h , and 2(2 + |L|) follows from the preservation of columns S t h , p h , F 4 . To implement ψ * h using transformer blocks while preserving the inputs, we apply Proposition F.7 by setting the scaling factors {B ψ ℓ } D ψ ℓ=0 as</p><formula xml:id="formula_400">B ψ 0 = B ψ D ψ = B 2 + H 2 + 2|L|(C A + 1) 2 , B ψ ℓ = d F • ((B ψ 0 + 1) • 16 ℓ ) 2 + B 2 + H 2 + |L|(C A + 1) 2 , for ℓ ∈ [D ψ -1],</formula><p>where B ψ 0 normalizes the input (S t h , p h , F 3 , F 4 ) row-wisely, each B ψ ℓ keeps the intermediate outputs in a unit ball. Finally, B ψ D ψ follows since C A upper bounds the magnitude of ψ * h by Assumption F.5, which controls the magnitude of each row in ψ * h (S t h ). This scaling is absorbed into the average module realized via a MHA layer. Next, we consider the transformer implementation of f  This function can be expressed as the sum of two triangular-shaped functions h 1 (x) + h 2 (x), where we define h 1 and h 2 as</p><formula xml:id="formula_401">h 1 (x) = 1 -(|x -h| -ϵ)/(1 -2ϵ) for |x -h| ≤ 1 -ϵ, 0 else, h 2 (x) = (|x -h| -ϵ)/(1 -2ϵ) for |x -h| ≤ ϵ, 0 else.</formula><p>Here h 1 is nonzero when |x -h| ≤ 1 -ϵ with h 1 (h) = (1 -ϵ)/(1 -2ϵ), h 1 (h + 1 -ϵ) = h 1 (h -1 + ϵ) = 0. Thus h 1 is a triangle pointing upwards. Similarly h 2 is a triangle pointing downwards with h 2 (h) = -ϵ/(1 -2ϵ) and h 2 (h -ϵ) = h 2 (h + ϵ) = 0. See Figure <ref type="figure" target="#fig_28">18</ref> for an illustration of f h , h 1 , and h 2 . Furthermore, both h 1 and h 2 are piecewise linear functions with four linear pieces, and holds for any three random variables (X 1 , X 2 , X 3 ) with two joint distributions P and P. Then according to data processing inequality, we have that </p><formula xml:id="formula_402">= • | prompt h CoT (n)) .</formula><p>Therefore, we conclude the proof. First, we define A 0,1 = (1, -1, 1, -1)/(2M ) and A 0,2 = (1, -1, -1, 1)/(2M ). Then for each i ∈ [r], we define A 1,i ∈ R (r+1)×4 by setting the i-th row as A 0,1 , (r + 1)-th row as A 0,2 and fill the rest with zero. For example, A 1,1 = (A 0,1 , 0 (r-1)×4 , A 0,2 ) ⊤ ∈ R (r+1)×4 . We stack the matrices {A 1,i } r i=1 horizontally to form A 1 = (A 1,1 , • • • , A 1,r ) ∈ R (r+1)×4r . Then we define the bias b 1 = 0 ∈ R 4r .</p><p>To define {A i , b i } m+2 i=2 , we first define {A ′ ℓ , b ′ ℓ } m+1 ℓ=2 by letting</p><formula xml:id="formula_403">A ′ 2 =     1 1 1 0 0 1 1 1 0 0 0 0 -1 1 1 0 0 -1 1 1     ∈ R 4×5 , b ′ 2 = 0 -1/2 0 0 -1/2 , A ′ ℓ = 1 2M       1/2 1/2 -1/2 0 0 -1 -1 1 0 0 0 0 1 0 0 0 0 1/2 1/2 1/2 0 0 -1 -1 -1       ∈ R 5×5 , b ′ ℓ = 0 -2 3-2ℓ 0 0 -2 3-2ℓ ,</formula><p>for 3 ≤ ℓ ≤ m + 1. Finally, we define A ′ m+2 = (-1/2, 1, 1, 1/2, -1) ⊤ ∈ R 5×1 and b ′ m+2 = 0 ∈ R. Next, for any i ∈ {2, . . . , m + 2}, we define A i and b i as</p><formula xml:id="formula_404">A i = diag(A ′ i , • • • , A ′ i r ), b i = b ′ i , • • • , b ′ i r ,</formula><p>where each A i is obtained by constructing a block-diagonal matrix with A ′ i being the diagonal blocks, and each b i is obtained by stacking b ′ i horizontally for r times. Therefore we have A 2 ∈ R 4r×5r , A m+2 ∈ R 5r×r , and A ℓ ∈ R 5r×5r for 3 ≤ ℓ ≤ m + 1. Besides, we have b i ∈ R 1×5r for each 2 ≤ ℓ ≤ m + 1, and b m+2 = 0 ∈ R r . Note that in the construction of Proposition III.3 by <ref type="bibr">Elbrächter et al. (2021)</ref>, a scalar multiplication module is used to restore the normalization 1/2M introduced by the first weight matrix A 1 . In our approach, we instead scale the last weight matrix A m+2 by M 2 , thereby eliminating the need for a separate scalar multiplication module.</p><p>Note that by setting r = 1, we recover the exact construction by Elbrächter et al. ( <ref type="formula">2021</ref>) in Proposition III.3. We use f 1 : R → R to denote such a network, which is an MLP with parameters {A ′ i , b ′ i } m+2 i=1 , where |f (x i , y) -x i y| &lt; ϵ.</p><formula xml:id="formula_405">A ′ 1 = (A 0,1 , A 0,2 ) ⊤ ∈ R 2×4 , b ′ 1 = 0 ∈ R 4 ,</formula><p>The depth of f is no more than C(log M + log(1/ϵ)), and the maximum dimension of the hidden neurons is 5r. The maximum magnitude of the intermediate weight matrices is bounded by 1, i.e., max{∥A i ∥ ∞ , ∥b i ∥ ∞ } ≤ 1 for all i ∈ [m + 1]. Additionally, ∥A m+2 ∥ ∞ ≤ M 2 due to the direct scaling, which replaces the scalar multiplication module.</p><p>To bound the Frobenius norms of weight matrices and bias vectors in f , by direct computation, we have max</p><formula xml:id="formula_406">ℓ∈[m+2] {∥A ℓ ∥ F , ∥b ℓ ∥ F } = max{∥A 2 ∥ F , ∥A m+2 ∥ F } = max{ √ 12r, H 2 • √ 5r}.</formula><p>Finally, we compute the row-wise ℓ 2 -norm for each intermediate output</p><formula xml:id="formula_407">X ℓ = ReLU X ℓ-1 A d + 1 ⊤ b ℓ , ∀ℓ ∈ [m + 2],</formula><p>where the initial input is given by X 0 = (x 1 , • • • , x r , y) ∈ R r+1 . Given that each entry of X ℓ-1 lies within the interval [0, 1], and noting that by construction ∥1A ℓ + b ℓ ∥ ∞ ≤ 1, for ℓ ≥ 3, we conclude that each entry of X ℓ is also within [0, 1]. We can calculate X 1 as:</p><formula xml:id="formula_408">X 1 = 1 2M</formula><p>• ReLU (x 1 + y, -(x 1 + y), x 1 -y, -(x 1 -y), . . . , -(x r -y)) .</p><p>Since x 1 , . . . , x r , y ∈ [-M, M ], each coordinate of X 1 is in [0, 1]. Direct computation shows that</p><formula xml:id="formula_409">X 2 = ReLU 1 2M • (|x 1 + y|, |x 1 + y|, |x 1 + y| -|x 1 -y|, |x 1 -y|, |x 1 -y|, • • • , |x r -y|)</formula><p>-(0, 1/2, 0, 0, 1/2, • • • , 1/2) , thus each coordinate of X 2 is in [0, 1]. By induction, this implies that for each intermediate output X ℓ (with ℓ ∈ [m + 1]), every element remains within [0, 1]. In conclusion, we have that</p><formula xml:id="formula_410">∥X 0 ∥ 2,∞ = ∥(x 1 , • • • , x r , y)∥ 2 ≤ M √ r + 1, ∥X ℓ ∥ 2,∞ ≤ √ 5r, for any ℓ ∈ [m + 1], ∥X m+2 ∥ 2,∞ ≤ H 2 .</formula><p>The first line follows from the direct calculation, and the second line holds because the maximum hidden embedding size is 5r, thus a row in X ℓ has length at most 5r. These upper bounds on the ℓ 2 -norm of X ℓ will be used when implementing this fully connected network under a transformer. The total number of layers of this fully connected network is C p • (log M + log(1/ϵ)), where C p is an absolute constant. Now we conclude the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.5.3 Proof of Lemma G.2</head><p>Proof. In this proof, we first construct a pair of weight matrices (W ′ ff,1 , W ′ ff,2 ) such that the output matrix keeps columns in J and set the other columns to a zero vector. Thus, the output matrix is in R m×r . Then we modify (W ′ ff,1 , W ′ ff,2 ) to form another pair of weight matrices (W ff,1 , W ff,2 ) such that the FF layer truncates the zero columns generates the desired output.</p><p>Since a = ReLU(a) -ReLU(-a) for any a ∈ R, by defining W 1 = W ⊤ 2 = (1, -1) ∈ R 1×2 , we have ReLU(aW 1 )W 2 = a. Setting W ′ 1 = W ′ 2 = 0 ∈ R, we send a to zero by ReLU(aW ′ 1 )W ′ 2 = 0.</p><p>For each i ∈ [r], define W 1,i = 1{i ∈ J }•W 1 +1{i / ∈ J }•W ′ 1 and W 2,i = 1{i ∈ J }•W 2 +1{i / ∈ J } • W ′ 2 . For any i ∈ [r], using {W 1,i , W 2,i } as the weight matrices of a FF layer to process the i-th column X :,i , the output is 1{i ∈ J } • X :,i . Now we put these matrices in the diagonal blocks of W ′ ff,1 and W ′ ff,2 to form</p><formula xml:id="formula_411">W ′ ff,1 = diag(W 1,i , • • • , W 1,r ) ∈ R r×(r+|J |) , W ′ ff,2 = diag(W 2,i , • • • , W 2,r ) ∈ R (r+|J |)×r .</formula><p>By direct calculation, we have ReLU(XW ′ ff,1 )W ′ ff,2 = X :,1 • 1{1 ∈ J } + 0 m×1 , . . . , X :,r • 1{r ∈ J } + 0 m×1 ∈ R m×r .</p><p>This output has the same shape as the input X. To get the final result, This output keeps the dimension as m×r. To get the final result, we define W ff,1 ∈ R r×2|J | by removing all all-zero columns from W ′ ff,1 , and W ff,2 ∈ R 2|J |×|J | by removing all all-zero rows and columns from W ′ ff,2 . These are the submatrices of W ′ ff,1 and W ′ ff,2 used to process columns X :,i 's with i ∈ J . As a result, W ff,1 and W ff,2 have only 2|J | nonzero entries, taking values in {-1, 0, 1}. Moreover, we have ReLU(XW ff,1 )W ff,2 = X :,i∈J ∈ R m×|J | , and the norms of the these weight matrices are ∥W ff,1 ∥ F = 2|J | and ∥W ff,2 ∥ F = 2|J |.</p><p>Vanilla ICL prompts produce incorrect reasoning steps like using time zones, indicating a propensity to misinterpret prompts without clear guidance. Thus, it is challenging for vanilla ICL to realize that longitudes are the key to solving the CityEquations talk.</p><p>Furthermore, the errors incurred by Partially Informative CoT-(a) typically involve rounding and sign issues, particularly in reasoning steps related to longitudes. For example, in the failed case mentioned above, the longitude of New York is given as -74 instead of 74. This suggests that while the prompts enable GPT-4 to associate the problem with the cities' longitudes, it sometimes struggles to handle the signs correctly. Additionally, the failure of Partially Informative CoT-(b) often results from the use of irrelevant information about the cities, such as demographic data, in the computation. In the failure example of Partially Informative CoT-(c) mentioned earlier, GPT-4 lists demographic data for both cities and the longitude of Mexico City, using both to compute the answer. Partially Informative CoT-(d) includes demographic data and longitudes for both cities in the intermediate reasoning steps. However, the final answer is based solely on the computation involving the longitudes. In the failure case above, the arithmetic formula incorrectly switches the minuend and subtrahend and rounds the longitude of Moscow to 37 inaccurately. This suggests that while GPT-4 correctly identifies that the longitudes are the only useful information for solving the task, it struggles with using this information correctly. By comparing Partially Informative CoT-(b) through CoT-(d), we observe that adding more relevant information to the CoT prompts improves GPT-4's performance on the CityEquation task.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>( a )</head><label>a</label><figDesc>What are the statistical estimators constructed by CoT and its variants? (b) What are the statistical properties of these estimators? (c) How does the transformer architecture enable the LLMs to learn these estimators? (d) Does CoT prompting always outperform vanilla ICL?</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: An illustration of CoT and vanilla ICL.Figure (a) shows the CoT prompt and the corresponding output of ChatGPT (GPT-3.5-turbo-16k). The intermediate reasoning is shown in red. The output of ChatGPT follows the pattern in the prompt, which consists of a reasoning step, followed by the desired answer. Figure (b) shows the result of using the corresponding vanilla ICL prompt, which includes of two input-output pairs. In this case, ChatGPT fails to provide the correct answer.Figure (c) illustrates a general pipeline of CoT prompting with n demonstration examples. Each example includes an input question, H -1 intermediate reasoning steps, and the final answer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Figure 1: An illustration of CoT and vanilla ICL.Figure (a) shows the CoT prompt and the corresponding output of ChatGPT (GPT-3.5-turbo-16k). The intermediate reasoning is shown in red. The output of ChatGPT follows the pattern in the prompt, which consists of a reasoning step, followed by the desired answer. Figure (b) shows the result of using the corresponding vanilla ICL prompt, which includes of two input-output pairs. In this case, ChatGPT fails to provide the correct answer.Figure (c) illustrates a general pipeline of CoT prompting with n demonstration examples. Each example includes an input question, H -1 intermediate reasoning steps, and the final answer.</figDesc><graphic coords="13,86.04,72.00,439.93,212.42" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: An illustration of the multi-step latent-variable model defined in (4.2). According to this graphical model, for any h ≥ 1, each step z i h of i-th example depends on the previous steps {z i ℓ } ℓ&lt;h and the hidden concept θ * .</figDesc><graphic coords="14,118.80,72.00,374.41,163.67" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: An instantiation of the model in (4.2) in the context of arithmetic problems. Here θ * stands for "solving an arithmetic problem with intermediate steps", and z i 0 describes a new arithmetic problem generated independently from any other demonstrations. Then each subsequent step, z i 1 , z i 2 , and y i , depends on both the previous steps and the hidden task θ * .</figDesc><graphic coords="15,86.04,72.00,439.93,184.75" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: An illustration of the SC-CoT prompting method. This method creates the final answer y * K based on two steps. First, we sample K i.i.d. reasoning paths {z test,i 0:H } K i=1 given the CoT prompt, and then report y * K by a majority vote based on {y test,i = z test,i H } K i=1 .</figDesc><graphic coords="25,86.04,72.00,439.93,215.93" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: An illustration of ToT with BFS selection. Here BFS samples K = 3 i.i.d. reasoning steps based on the prompt and selects B = 2 partial histories. The red ovals represent the pruned nodes, and the green ovals represent the active nodes. At each step h ∈ [H], K candidates are generated per active node, and thus there are KB candidates. Then B nodes survive the selection according to the value function V θ * (•). At the final step h = H, we select a single best candidate and output the corresponding chain as t H .</figDesc><graphic coords="27,95.40,72.00,421.20,261.57" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: An example generated from the hierarchical model in (5.5). The query z 0 (blue box) includes a question and all necessary information to answer it. The first selection step τ 1 (top yellow box) picks two sentences from z 0 indicating which teams Alice and Bob are in.The first inference step z 1 (top green box) uses the information from τ 1 to infer that Alice and Bob are on the same team. Together, τ 1 and z 1 form a single reasoning step. In the second selection step, τ 2 (bottom yellow box) selects information from t 1 = {z 0 , z 1 }, with the first sentence of τ 2 from z 1 and the second from z 0 . The second inference step z 2 (bottom green box) answers the question using only the information provided in τ 2 .</figDesc><graphic coords="29,79.02,158.68,453.96,247.23" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: An illustration of the graphical model of SI prompting. In SI prompting, each step in CoT is divided into two substeps: selection and inference. The selection module retrieves relevant facts from the context t h-1 , and the inference module derives inference z h based on these selected facts τ h .</figDesc><graphic coords="30,76.68,72.00,458.64,176.08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>. The errors made by informative CoT are due to rounding errors. Moreover, compared to vanilla ICL, version (a) of PI CoT includes a piece of relevant information (longitude of London), which significantly helps the reasoning. Comparing vanilla ICL to versions (b)-(d), we see that including more relevant information in the intermediate reasoning steps improves the accuracy. In particular, version (b)-(d) include the first one to three steps of the same reasoning steps, where the first reasoning step is a piece of irrelevant information. Version (b) of PI COT, only including such irrelevant information, performs drastically worse than vanilla ICL. This observation supports our remark that CoT does not always outperforms vanilla ICL. Note that here the intermediate steps added in the version (b) of PI CoT are true facts.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>Figure 9: An illustration of a transformer with depth D. The left-hand side shows the network with D sequentially stacked transformer blocks followed by a final softmax layer. The right-hand side zooms in on a single transformer block, which comprises a MHA layer and a FF layer, connected by normalization layers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: An illustration of the transformer constructed for proving Proposition 6.4. After the input embedding, the first module NN J extracts the step-index h from S t h and copy it to all previous positions. Then, the transformer go through H +1 submodule pairs {G h , F h } H h=0 , where each pair is designed to approximate the conditional distribution associated with a specific step h ∈ [H]. The output of F H goes through a softmax output layer to get the final output distribution, which is close to the target distribution P(• | S t h ).</figDesc><graphic coords="39,76.68,72.00,458.65,252.05" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: Statistical errors of vanilla ICL and two CoT estimators on the task of learning 2-layer MLPs. The estimators are constructed by prompting the pretrained transformer models with n demonstration examples. In Figure (a), the MSE of the three estimators decreases rapidly as n increases. In Figure (b) we plot the logarithm of MSE, which follows a strong linear trend for n ≤ 20. Beyond n = 20, the MSE is dominated by the pretraining error and therefore the linear trend stops. Furthermore, the model trained with "CoT 1 ", using intermediate steps z = ReLU(W x), shows a significant improvement over vanilla ICL, while the "CoT 2 " model, using z ′ = v, only shows a slight improvement. In both (a) and (b), we also plot the standard deviation computed based on 10 random experiments.</figDesc><graphic coords="43,315.37,81.96,210.60,148.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head></head><label></label><figDesc>Figure 12: Statistical errors of vanilla ICL and two CoT estimators on the task of learning decision trees. The estimators are constructed by prompting the pretrained transformer models with n demonstration examples. In (a), we plot the mean-squared errors of these two estimators against the number of demonstrations. The error decreases faster with incontext examples for models trained with CoT data compared to those trained with vanilla ICL. In Figure (b) we plot the logarithm of the MSE, which reveals a linear trend in errors for both models when n ≤ 65.</figDesc><graphic coords="44,77.45,76.73,215.28,155.51" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 14 :</head><label>14</label><figDesc>Figure 14: An illustration of the generalized multi-step latent-variable model (A.1) with the fixed task θ * . Compared to the simple model (4.2), this model allows each step z i j of i-th example to depend on earlier examples via u i j , which depends on all latent variables previously generated. Therefore, the examples {s i } n i=1 do not have to be i.i.d.</figDesc><graphic coords="55,95.40,72.00,421.19,228.66" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head></head><label></label><figDesc>θ dθ. (B.6) Note that the n examples and testing query z test 0 in prompt CoT (n) are not i.i.d under the generalized model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head></head><label></label><figDesc>can upper bound the marginal log density ratio log(P(y test = y | prompt CoT (n))/P ρ (y test = y | prompt CoT (n))) by aggregating the density ratio at each step. More specifically, we have log P(y test = y | prompt CoT (n))</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head></head><label></label><figDesc>e H•b * , where the first line follows directly from the chain rule of conditional probability, and the second line follows from Assumption 5.1. Combining (C.2) and (C.3), we have log P(y test = y | prompt CoT (n)) -log P ρ (y test = y | prompt CoT (n)) ≤ Hb * for all y ∈ L. Thus, we can upper bound the integral in the right-hand side of (C.1) by 2 TV P(y test = • | z test 0 , θ * ), P(y test = • | prompt CoT (n)) • Hb * .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head></head><label></label><figDesc>two distributions P(• | θ) and P(• | θ * ), where P(• | θ * ) is the ground truth distribution. The upper bound is related to the Hellinger distance between them.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Figure 16 :</head><label>16</label><figDesc>Figure16: An illustration of two trapezoid-shaped functions f h (blue) and f h+1 (green), i.e., f h with h = h and h = h + 1. Observe that for any x ∈ [h -ϵ, h + ϵ], f h (x) = 1 if and only if h = h, and f h (x) = 0 for any other h ̸ = h. Thus, when we apply each f h to entries of p h , when ϵ = 1/4, we have 1{h = h}. Each entry of p h falls within the highlighted region on the x-axis.</figDesc><graphic coords="107,83.70,72.00,444.59,246.59" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head></head><label></label><figDesc>by a composition of D p transformer blocks. Each block has at most 5|L|+2(2+2|L|) neurons in the FF layer, where 5|L| are used for the product operation, and 2(2+2|L|) are used for preserving the input of X h,(1) F . According to Lemma G.1, the Frobenius norm of weight matrices are bounded by B 2 0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head>0=</head><label></label><figDesc>• | Υ n ) can be bounded by P(z test 0 = • | Υ n ) by introducing an additional factor κ. Consequently, (F.50) shifts the expectation under P CoT towards P by scaling some constants.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_22"><head>N</head><label></label><figDesc>i.i.d. documents, each has T examples. Since prompt CoT (n) only has n examples and T ≥ n + 1 and the N documents in D N,T are i.i.d., we have H h=1 E P TV P(z test h</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_23"><head>Figure 17 :</head><label>17</label><figDesc>Figure 17: An illustration of the d-th transformer block, compared with the d-th fully connected layer. (a) The left figure shows the d-th transformer block, where the MHA block is omitted by setting the value weight matrix to zero. Since the input is scaled down before each normalization layer, these two layers act as identity maps. As a result, the only "active" component in this transformer block is the feed-forward layer. (b) The right figure depicts the d-th fully connected layer. The output of the d-th transformer block matches the output of this fully connected layer up to a scaling factor B d .</figDesc><graphic coords="125,130.50,72.00,351.01,304.57" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_24"><head></head><label></label><figDesc>0 for all d ∈ [L]. Moreover, we have γ 1 1 = I/B 0 and γ d 1 = I for all d ≥ 2. Finally, to recover X L , it suffices to multiply B L • I to Y L . Therefore, we conclude the proof of this proposition.G.4.2 Proof of Lemma F.8Proof. Consider the FF layer defined in (F.20). We set the bias terms as b</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_25"><head></head><label></label><figDesc>Lemma G.2 (Residual ReLU module). Let X ∈ R m×r denote the input, and J ⊆ [r] denote a set of indices. Then there exists a FF layer with weight matrices W ff,1 ∈ R r×(2|J |) and W ff,2 ∈ R (2|J |)×|J | such that the output matrix only keeps those columns with indices i ∈ J . Specifically, we haveReLU(XW ff,1 )W ff,2 = X :,i∈J ∈ R m×|J | .Moreover, these weight matrices satisfy where ∥W ff,1 ∥ F = 2|J | and ∥W ff,2 ∥ F = 2|J |.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_26"><head></head><label></label><figDesc>2 to (X, Y ) with J = {d + 1, . . . , d + d ′ }, where J refers to the column indices corresponding to Y . Then there exist weight matrices W Y ff,1 and W Y ff,2such thatY = ReLU (X, Y )W Y ff,1 )W Y ff,2 . Notice that W Y ff,1 has size (d + d ′ ) × (2d ′ ) and W Y ff,2 has size 2d ′ × d ′ . Whereas W ff,1 has d rows. Now we define W ff,1 = W ff,1 W Y ff,1 , W ff,2 = W ff1 = (b ff,1 , 0), b ff,2 = (b ff,2 , 0).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_27"><head></head><label></label><figDesc>w h . Similar to the implementation of ψ * h , we apply Proposition F.7 by setting the scaling factors {B w ℓ } Dw ℓ=0 asB w 0 = B w Dw = B 2 + H 2 + 2|L|(C A + 1) 2 , B w ℓ = d F • ((B w 0 + 1) • 16 ℓ ) 2 + B 2 + H 2 + |L|(C A + 1) 2 , for ℓ ∈ [D w -1]. Finally, we compute the maximum network weight for the module G h as max 0≤i≤D ψ ,0≤j≤Dw{B ψ i , B w j } • d 2 F + 2(2 + |L|) ≤ C F • B 0 • 16 max{D ψ ,Dw} • |L| 3/2 ,whereB 0 = B 2 + H 2 + |L| • C 2 A and C F &gt; 1 is a absolute constant.G.4.7 Proof of Lemma F.13Proof. Recall that for each h ∈ [H] and ϵ ∈ (0, 1/2), we want to construct a MLP to that implements the trapezoid-shaped functionf h (xh| ≤ ϵ, 1 -(|x -h| -ϵ)/(1 -2ϵ) for ϵ &lt; |x -h| ≤ 1 -ϵ, 0 otherwise.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_28"><head>Figure 18 :</head><label>18</label><figDesc>Figure18: An illustration of the trapezoid functions f h (x) as the sum of two functions h 1 (x) and h 2 (x). Intuitively, h 1 (x) is a large upward-pointing triangle, and h 2 (x) is a smaller downward-pointing triangle that mirrors h 1 (x) within the interval x ∈ [-ϵ, ϵ]. The downward slope of h 2 (x) cancels out the upward slope of h 1 (x) within this interval, resulting in a flat region. Therefore, the sum of the two functions creates a trapezoid shape.</figDesc><graphic coords="137,83.70,72.00,444.60,302.69" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_29"><head>G</head><label></label><figDesc>.5.2 Proof of Lemma G.1Proof. In this proof, we extend PropositionIII.3 in Elbrächter et al. (2021)  to construction a sequence of FF modules such that f : R r × R → R r , where we want f (x 1:r , y) ≈ (x 1 y, • • • , x r y). For simplicity, we denote the input asX 0 = (x 1 , • • • , x r , y) ∈ R r+1. By leveraging the construction by Elbrächter et al. (2021), we define a set of matrices {A i , b i } m+2 i=1 as follows.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_30"><head></head><label></label><figDesc>and {A ′ i , b ′ i } m+2 i=2 are defined above. By the construction of the weight matrices{A ′ i , b ′ i } m+2 i=1, the MLP with these parameters yields a vector-valued mappingf : R r × R → R such that f (x, y) = f 1 (x 1 , y), • • • , f 1 (x r , y) , ∀x ∈ R r , y ∈ R. (G.27)As shown in Proposition III.3 in Elbrächter et al. (2021), when the depth of f 1 , i.e., m+2, is bounded by C p (log M + log(1/ϵ)) for some constant C p &gt; 0, f 1 is a good approximator of the product operation in the sense that|f 1 (a, b) -ab| &lt; ϵ for any a, b ∈ [-M, M ]. Therefore, f constructed in (G.27) using weight matrices {A i , b i } m+2 i=1 satisfies ∥f (x, y) -(x 1 y, • • • , x r y)∥ ∞ = max i∈[r]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="20,86.04,72.00,439.93,281.55" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="36,153.90,72.00,304.20,300.83" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="45,83.70,72.00,444.59,268.43" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="56,83.70,188.11,444.60,245.92" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>The distribution of the final answer y test = z test Summary of the three sources of errors in CoT prompting.</figDesc><table><row><cell cols="2">Error Sources Description</cell><cell></cell></row><row><cell cols="2">Pretraining error Statistical error of the pretrained LLM</cell><cell></cell></row><row><cell cols="3">Prompting Error Combination of query error and in-context error</cell></row><row><cell>Query error</cell><cell>Distributional shift between testing query z test 0</cell><cell>and n prompt examples</cell></row><row><cell cols="3">In-context error Statistical error of inferring θ  (z test 1 , • • • , z test H-1 ) of CoT. To evaluate the statistical error of such an estimator, we consider</cell></row><row><cell cols="2">the Kullback-Leibler (KL) divergence</cell><cell></cell></row></table><note><p>H induced by the LLM via CoT reasoning is P LLM (y test = • | prompt CoT (n)), which is given by marginalizing out the intermediate steps * based on the n prompt examples</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>H•d k as the features of the first h -1 steps of the i-th example, and we pad (H -h) zero vectors to ensure that {k i h } h∈[H] live in the same Euclidean space. Moreover, ϕ : R H•d k → R d ϕ is another feature mapping that maps k i h to some Euclidean space, where θ * ∈ R dv×d ϕ is a linear operator.</figDesc><table><row><cell></cell><cell>The simplified</cell></row><row><cell cols="2">model in (4.6) thus postulates that k i h and v(z i h ) satisfy a kernelized linear model. Moreover,</cell></row><row><cell>h we assume the noise ϵ i</cell><cell>i.i.d.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Average accuracy of the five prompting methods on the CityEquation task. The results are based on 200 random testing instances. "PI CoT" stands for the partially informative chain of thought method.Transformer Architecture. We let T F(D, η, r, d F , d k , d v ) denote the class of transformers that maps a sequence of reasoning steps to a probability distribution over L, where the input sequence is embedded in R r , followed by D sequentially stacked transformer blocks and a final softmax layer that outputs a distribution. Here the embedding contains both token and positional embedding. Moreover, each of the D transformer blocks includes a multi-</figDesc><table><row><cell>Vanilla</cell><cell>Informative</cell><cell>PI CoT-(a)</cell><cell>PI CoT-(b)</cell><cell>PI CoT-(c)</cell><cell>PI CoT-(d)</cell></row><row><cell>ICL</cell><cell>CoT</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>59.5%</cell><cell>81.5%</cell><cell>70.5%</cell><cell>2.5%</cell><cell>66%</cell><cell>80%</cell></row></table><note><p>head attention (MHA) layer with η parallel heads and a fully connected feedforward (FF) layer. The embedding dimensions of the queries, keys, and values in MHA in (3.2) are d k , d k , and d v , respectively.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>. Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. arXiv preprint arXiv:2211.12588. Chu, Z., Chen, J., Chen, Q., Yu, W.,He, T., Wang, H., Peng, W., Liu, M., Qin, B. and  Liu, T. (2023). A survey of chain of thought reasoning: Advances, frontiers and future. arXiv preprint arXiv:2309.15402.Dai, D., Sun, Y., Dong, L., Hao, Y., Ma, S., Sui, Z. andWei, F. (2023). Why can gpt learn in-context? language models secretly perform gradient descent as meta-optimizers. In Findings of the Association for Computational Linguistics: ACL 2023.In this section, we propose a generalized multi-step latent-variable model that removes the i.i.d requirement in the model defined in (4.2). This generalized model captures (i) the evolving relationships among examples, and (ii) the multi-step reasoning framework of CoT. The rationale behind (i) is that LLMs are pretrained on trillions of reasoning steps from documents and articles from the internet<ref type="bibr" target="#b38">(OpenAI, 2023)</ref>. The examples in the pretraining data associated with the same task are often not i.i.d. For example, imagine composing a sequence of examples of the concept of "animals", we typically begin with familiar examples such as cats and dogs before progressing to less conventional ones like panthers and meerkats.To capture both (i) and (ii), we propose to model the joint distribution of the latent variable θ * ∈ Θ, the n examples {s j } n j=1 , and the test instance z test 0:H as follows:</figDesc><table><row><cell>A A Generalized Multi-Step Latent Variable Model</cell></row><row><cell>Creswell, A., Shanahan, M. and Higgins, I. (2022). Selection-inference: Exploiting large</cell></row><row><cell>language models for interpretable logical reasoning. arXiv preprint arXiv:2205.09712.</cell></row><row><cell>Foster, D. J., Kakade, S. M., Qian, J. and Rakhlin, A. (2021). The statistical complexity of</cell></row><row><cell>interactive decision making. arXiv preprint arXiv:2112.13487.</cell></row><row><cell>Fu, D., Chen, T.-Q., Jia, R. and Sharan, V. (2023). Transformers learn higher-order opti-</cell></row><row><cell>mization methods for in-context learning: A study with linear models. arXiv preprint</cell></row><row><cell>arXiv:2310.17086.</cell></row></table><note><p><p><p>Devlin, J., Chang, M.-W.,</p>Lee, K. and Toutanova, K. (2018)</p>. BERT: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805. Dong, Q., Li, L., Dai, D., Zheng, C., Wu, Z., Chang, B., Sun, X., Xu, J. and Sui, Z. (2022). A survey on in-context learning. arXiv preprint arXiv:2301.00234. Dziri, N., Lu, X., Sclar, M., Li, X. L., Jiang, L., Lin, B. Y., Welleck, S., West, P., Bhagavatula, C., Le Bras, R. et al. (2024). Faith and fate: Limits of transformers on compositionality. Advances in Neural Information Processing Systems, 36. Edelman, B. L., Goel, S., Kakade, S. and Zhang, C. (2022). Inductive biases and variable creation in self-attention mechanisms. In International Conference on Machine Learning. PMLR. Elbrächter, D., Perekrestenko, D., Grohs, P. and Bölcskei, H. (2021). Deep neural network approximation theory. IEEE Transactions on Information Theory, 67 2581-2623. Erhan, D., Courville, A., Bengio, Y. and Vincent, P. (2010). Why does unsupervised pretraining help deep learning? In Proceedings of the thirteenth international conference on artificial intelligence and statistics. JMLR Workshop and Conference Proceedings. Feng, G., Gu, Y., Zhang, B., Ye, H., He, D. and Wang, L. (2023). Towards revealing the mystery behind chain of thought: a theoretical perspective. arXiv preprint arXiv:2305.15408.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Both k i h and v i h are random variables depending on the latent variable θ * . We define random variables K and V as uniform mixtures of {k i h } h∈[H] and {v(z i h )} h∈[H] , respectively, with h ∼ Unif([H]). That is, with probability with 1/H, (K, V) has the same distribution as (k i h , v(z i h )) for any h ∈ [H]. Under the model in (4.6), V and K are linked via a linear model, i.e., E[</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>i h ) and k i h with h ∼ Unif[H]. Therefore, (B.12) and (B.15) hold for each fixed h ∈ [H]. Specifically, note that the random variables corresponding to the same step h ∈ [H] across different examples i ∈ [n] are i.i.d. That is, for any fixed h, k i h , v(z i h ) n i=1 is a sequence of i.i.d samples. Therefore, we can apply Lemma H.1 to each j ∈ [H]. More specifically, we can apply Lemma H.1 to each sample mean with fixed step index h by setting B</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>plug in different values of y in the above inequality and obtain max y̸ =y * P y test = y | prompt CoT (n) ≤ max Recall the definition of ϵ in Assumption 5.8. Combining (D.1) and (D.2) we have P y test = y * | prompt CoT (n) ≥ P(y test = y * | z test 0 , θ * ) -2 2ξ ≥ max y̸ =y * P(y test = y | z test 0 , θ * ) + ϵ -2 2ξ</figDesc><table><row><cell>≥ max y̸ =y  *  P y test = y | prompt CoT (n) + ϵ -4 2ξ,</cell><cell>(D.3)</cell></row><row><cell>y̸ =y  *  P(y test = y | z test 0 , θ  *  ) + 2 2ξ,</cell><cell>(D.1)</cell></row><row><cell>P y test = y  test 0 , θ  *  ) -2 2ξ.</cell><cell>(D.2)</cell></row></table><note><p>* | prompt CoT (n) ≥ P(y test = y * | z</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>That is, for any z ∈ L, the z-th entry of g * Theorem 2 in<ref type="bibr" target="#b71">Zaheer et al. (2017)</ref> proves that any permutation invariant function of a function admits a factorization structure. In particular, there exist w * h : R → R |L| and ψ * h : L → R for all h ∈ {0, . . . , H} and t ∈ [T ] such that</figDesc><table><row><cell>h (S t h ) is equal to</cell></row><row><cell>P(z t h = z | S t h ). Functions {g  *  h } H h=0 are the target functions and we want to construct a</cell></row><row><cell>single transformer that approximates all of them.</cell></row><row><cell>Function class containing {g  *  h } H h=0 . Under the general model introduced in Appendix A,</cell></row><row><cell>by the Bayes' rule, function g  *  h (•) is invariant to permutations of the L(t, h) reasoning steps</cell></row><row><cell>in S t h .</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>O exp -D w /B + exp -D ψ /B + 6 • D w (F.32) for all h ∈ {0, . . . , H}. Here we use the fact that log(256) &lt; 6. Moreover, (F.29) implies that the number of transformer blocks satisfies D ψ ≤ C d • D ψ and D w ≤ C d • D w for some absolute constant C d . Note that D ψ and D w can be chosen arbitrarily. We can set D ψ = O(D</figDesc><table><row><cell>/B</cell><cell>and</cell><cell>ϵ w = exp -(D w -log B)/B</cell><cell>(F.31)</cell></row><row><cell cols="3">in Proposition (F.12), we know that there exists {G h } H h=0 such that</cell><cell></cell></row><row><cell cols="4">max S t h ∈L  *  TV g  *  h (S t h ), g  *  h (S t h ) = 2</cell></row><row><cell></cell><cell></cell><cell></cell><cell>w</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head></head><label></label><figDesc>• exp(-D p /C p ) in (F.39), by triangle inequality we conclude that embed(S t h )embed h (S t h ) ∞ &lt; Hϵ p . (F.41) Moreover, with number of transformer blocks used to implemnt f product , D p , satisfies D p ≤ D p . Since softmax is Lipschitz continuous, as shown in Lemma H.6, we conclude that</figDesc><table><row><cell>softmax embed(S t h )/τ -softmax embed h (S t h )/τ 1</cell><cell></cell></row><row><cell>≤ 2 embed(S t h )/τ -embed h (S t h )/τ ∞ ≤ 2Hϵ p /τ,</cell><cell>(F.42)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>•</head><label></label><figDesc>Module NN J . The module NN J consists of four submodules, which are two linear modules, a product module realized via FF layers, and a MHA module. The two linear modules in Lemmas F.8 and F.10 have Frobenius norm of weight matrices upper bounded by B 2 + (H + 1) 2 In conclusion, the module NN J has total depth of at most D j = C p log(3H)+3 for some constant C p &gt; 0, the Frobenius norms for FF layers are upper bounded by B 2 + (H + 1) 2 + 6 • √ H 4 • 5 + 4 and those for MHA layers are upper bounded by max{8 log(8T H), B 2 + (H + 1) 2 + 1}. • Module G h and F h . The parameter constraint for any G h is specified in Proposition F.12. According to Step 3, each F h has maximum Frobenius norm of weight matrices as C ′ • (C A +1) 3 •|L|. The depth of each submodule pair G h and F h is specified in (F.46). Let D denote a sufficient large integer. Consider any parameter class P LLM in (6.1) with B S ≥ (C A + 1) • |L|, B M ≥ max{8 log(8T H), B 2 + (H + 1) 2 + 1}, d F ≥ 4 + 18|L| and</figDesc><table><row><cell>+ 1 • of at most C(log(H) + log(16)) layers with maximum Frobenius norm √ 6. According to Lemma F.9, the product module consists √ B 2 + H 2 + 6 • √ H 4 • 5 + 4. According to Lemma F.11, the extraction module has the maximum</cell></row><row><cell>Frobenius norm of weight matrices as max{8 log(8T H), B 2 + (H + 1) 2 + 1} for the MHA layer and B 2 + (H + 1) 2 + 6 • √ 6 for the FF layer.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head></head><label></label><figDesc>1), C p , C F &gt; 0 are absolute constants, and C A comes Proof. Recall that Lemma 5.2 decomposes the CoT error err CoT into err pre and err prompt . This proof consists of two steps. We first control the expected pretraining error E P CoT [err pre ] under the distribution P CoT with OOD queries. Then we consider the prompting error E P CoT [err prompt ] in the second step.</figDesc><table><row><cell cols="4">from Assumption F.5. Then under Assumptions 6.1,6.2, and F.5, there exists a transformer</cell></row><row><cell cols="2">with at most O(D) transformer blocks and parameter ρ  *  ∈ P LLM such that</cell><cell></cell><cell></cell></row><row><cell>max S t h ∈L  *  KL P(z t h = • | S t h ), P ρ  *  (z t h = • | S t h ) = O exp -</cell><cell>D -C log(2H))/H 5B</cell><cell>1/4</cell><cell>,</cell></row><row><cell cols="4">for any t ∈ [T ], 0 ≤ h ≤ H, where C &gt; 0 is some absolute constant. Therefore, we conclude</cell></row><row><cell>the proof.</cell><cell></cell><cell></cell><cell></cell></row><row><cell>F.4 Proof of Corollary 6.6</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head></head><label></label><figDesc>N,T , which consists of N independent trajectories with T examples in each trajectory. For each trajectory ℓ ∈ [N ], we first sample a task θ * Conditioning on this task, we sequentially generate T examples {s k,ℓ } T k=1 according to the model (A.1), i.e., we iteratively generate the next reasoning step z t,ℓ h ∼ P(• | S t,ℓ h ), where we use S t,ℓ h</figDesc><table><row><cell>ℓ</cell><cell>i.i.d ∼ π.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head></head><label></label><figDesc>Proof of Proposition C.2. Using any distribution q over Θ, we bound the loglikelihood log P y test = • | prompt CoT (n) by log P y test</figDesc><table><row><cell>Auxiliary Results in Appendix C.2</cell></row><row><cell>G.2.1 Proof of Proposition C.2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head></head><label></label><figDesc>Proof. In this proof, we aim to show that a sequence of L transformer blocks defined in (F.19) can exactly represent the MLP up to a scaling. Recall that the intermediate layers of the MLP are denoted by {X d } d∈[L]  . In the following, we let Y 0 = X 0 , and let Y d denote the output of the d-th transformer block for all d ∈ [L]. Our goal is to construct the transformer blocks such that Y L is exactly equal to X L up to a constant factor.Our construction is based on two key ideas. First, as shown in Appendix F.3, in each transformer block, we can set the MHA layer to a zero function and only keep the FF layer and normalization layer in</figDesc><table><row><cell>Auxiliary Lemmas in Appendix F.2</cell></row><row><cell>G.4.1 Proof of Proposition F.7</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head></head><label></label><figDesc>t h )/τ ) ≈ g * h (S t h ), where g * h (•) is the target distribution. In Step 2, we provide a detailed construction of embed h (•) by approximating ψ * h and τ log w * h,i for each i ∈ [|L|]. Finally in Step 3, we apply the embed h (•) approximation to construct each module G h , which takes (S t h , p h , F 3 , F 4 ) as the input and produces (S t h , p h , 1 ⊤ L ′ embed h (S t h</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head></head><label></label><figDesc>that ψ * h is a univariate function and ω * h : R → R |L| . Note that the key functionality of the module G h is to use S t h to produce embed h . For any h ∈ [H], we want to construct networks w *</figDesc><table><row><cell>h and</cell></row><row><cell>ψ  *  h approximating functions w  *  h and ψ  *  h in (F.18) separately. More specifically, we want to</cell></row><row><cell>construct a network g  *  h such that</cell></row><row><cell>g</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20"><head>•</head><label></label><figDesc>Approximate w * h using an MLP. Note that w * h takes values in R |L| . We let w * h,i denote its i-th entry for all i ∈ [|L|]. We leverage Lemma H.7 to approximate each τ • log w * h,i using a fully connected MLP f w h,i with D ω layers, where each layer has at most 16 neurons. Here D ω will be specified later. Similar to ψ h , such an MLP can be implemented by a composition of D w transformer blocks. Step 2: Approximate ψ * h and w * h using MLPs. In this step, we employ the universal approximation properties of fully connected networks to construct MLPs that approximate ψ * h and w * h,i for all i ∈ [|L|]. The technical tool we leverage is Lemma H.7, obtained from Elbrächter et al. (2021), which shows that MLP functions can approximate sufficiently smooth functions. Specifically, under Assumption F.5, by Lemma H.7, for any desired accuracy levels ϵ ψ and ϵ w , there exist MLPs ψ * h and {f w h,i } i∈[|L|] such that ∥ ψ * h -ψ * h ∥ ∞ &lt; ϵ ψ , ∥ f w h,i -τ • log w * h,i ∥ ∞ &lt; ϵ w for all i ∈ [|L|], (G.21)where ψ * h has D ψ layers and each f w h,i has at most D w layers. Here we haveD ψ = 2C d B • log(1/ϵ ψ ) 2 + log B, D w = 2C d B log(1/ϵ w ) 2 + log B, (G.22)where C d &gt; 0 is an absolute constant and B is the parameter appearing in Assumption F.5. Moreover, each layer has at most 16 neurons and all the neural network weights are bounded by one in magnitude, i.e., each entry of the weight matrices is bounded in[-1, 1]. By this construction, define an embedding vector embed h (S t h ) ∈ R |L| as denotes the vector-valued mapping whose entries are { f w h,i } i∈[|L|] . Next, we feed embed h (S t h ) into the softmax layer and obtain an estimator of g * h (S t h ). For any prompt S t h with length L ′ ≤ L, the ℓ 1 -approximation error is bounded by g * h (S t h )softmax embed h (S t h )/τ 1</figDesc><table><row><cell>embed h (S t h ) = f w h h ) -softmax f w h (S t h 1 L ′ + softmax f w h 1 L ′ (t,h-1) L ′ 1 (t,h-1) t-1 i=1 (i,j)=(1,0) ψ  *  j=0 H h (z i j ) ψ  *  h (z i j ) + 1 (i,j)=(1,0) ψ  *  h (z i j ) -softmax f w h-1 j ′ =0 ψ  *  h (z t j ′ ) , h 1 L ′ (t,h-1) (i,j)=(1,0) h ≤ g  *  where f w ≤ 2ϵ w + 2 • 256 Dw • ϵ ψ ,</cell><cell>ψ  *  h (z i j )</cell><cell>(G.23) 1 (G.24)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_21"><head></head><label></label><figDesc>KL P(z test 1:H = • | prompt CoT (n)), P ρ (z test 1:H = • | prompt CoT (n)) (G.26) ≥ KL P(z test H = • | prompt CoT (n)), P ρ (z test H = • | prompt CoT (n)) .Notice that y test = z test H . Combing (G.25) with (G.26), we have thatKL P(y test = • | prompt CoT (n)), P ρ (y test = • | prompt CoT (n)) ∼P(• | prompt CoT (n)) KL P(z test h = • | prompt h CoT (n)), P ρ (z test h</figDesc><table><row><cell>H</cell><cell></cell></row><row><cell>≤</cell><cell>E z test 1:h-1</cell></row><row><cell>h=1</cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Both the CoT and vanilla ICL prompts are tested on ChatGPT (GPT-3.5-turbo-16k) with the temperature set to zero. See Section I.1 for the details.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>≤ C • H • (L -1 σ 2 /λ) -1 d ϕ (B.15)</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgement</head><p><rs type="person">Zhuoran Yang</rs> acknowledges the support of <rs type="funder">NSF</rs> under the award <rs type="grantNumber">DMS-2413243</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_M3GhC9T">
					<idno type="grant-number">DMS-2413243</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Combing the results from the previous two steps <ref type="bibr">(B.20)</ref>  for some absolute constant C. Therefore, we conclude the proof. Note that this proof assumes ϕ is finite-dimensional, but it also holds for infinite-dimensional ϕ. More specifically, by replacing the trace in (B.14) with the effective dimension of ϕ, we can still balance the rate of σ 2 /λ to ensure (B.20) goes to zero.</p><p>E Proofs and Auxiliary Results of Section 5.3</p><p>E.1 Proof of a Generalized Version of Proposition 5.14</p><p>In the following, we generalize Proposition 5.14 to handle comparisons of different truncated CoT methods, which covers vanilla ICL as a special case. For simplicity, we assume zero pretraining error and input query does not have a distributional shift, i.e., P LLM = P and z test 0 ∼ P(• | θ * ). Before presenting the result, we state the following regularity assumption.</p><p>Assumption E.1. For a fixed CoT prompt prompt CoT (n), we define truncated CoT prompts with fixed intermediate step indices as prompt J (n) = {z i 0 , y i } n i=1 ∪{z i j } j∈J ∪{z test 0 } with fixed index set J ⊆ [H -1], where prompt CoT (n) = {z i 0 , • • • , z i H } n i=1 ∪ {z test 0 }. We assume that P y test | prompt J (n) is a mixture of P y test | θ, z test 0 aggregated with respect to the posterior of θ based on truncated CoT demonstrations. Specifically, the density P y test | θ, z test 0 is obtained by marginalizing the omitted intermediate steps z i / ∈J from the joint distribution induced by the CoT model (4.2).</p><p>The truncated CoT method recovers CoT by setting J = [H -1] and vanilla ICL by setting J = ∅. This assumption ensures that the estimators induced by different truncated CoT methods are comparable. This can be achieved by training LLMs using a truncated dataset and then prompted via the truncated CoT. Specifically, each J , we define D J as a truncated version of the CoT data D obtained by omitting intermediate steps with indices j / ∈ J . Then we can pretrain an LLM using D J using the same MLE loss as in (4.3), and then prompt the learned model using prompt J (n). Assumption E.1 requires LLMs to process different truncated CoT prompts by making posterior inferences based on their respective pretraining data. This is the case when the LLM is pretrained using D J , following a similar argument as in Section 4.3. Based on this assumption, we establish a hierarchy of CoT methods in terms of statistical error.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Corollary E.2 (Comparison of CoT Methods</head><p>We notice that such an inequality only holds in an average case by taking an expectation with respect to θ * ∼ π.</p><p>where the equality follows from the independence between Υ n and z test 0 conditioning on any task θ, which enables us to exchange θ∈Θ ∁ with the expectation with respect to µ(• | Υ n ). According to Lemma C.3 and Assumption 5.4, we have P(Υ n | θ)/P(Υ n | θ * ) ≤ exp(-2nλ + 2 log( ξ-1 )) with probability at least 1 -ξ for any θ ∈ Θ ∁ . Setting ξ = ξ/|Θ| and taking a union bound, we therefore have RHS of (F.54</p><p>which holds with probability at least 1 -ξ with respect to the randomness comes from</p><p>Next we upper bound each term in the summation in (F.55). By changing the probability measure and Cauchy-Schwarz inequality, we have</p><p>where the second line follows from the Cauchy-Shwarz inequality. The final line follows because</p><p>which results from Assumption 6.5. Besides, we define C(θ * ) as</p><p>Thus, combining (F.55), (F.56), (F.57), with probability at least 1 -ξ, we have RHS of (F.55</p><p>and C(θ * ) is defined in (F.57). Applying (F.58) to (F.53) gives us the following tail probability bound:</p><p>where the query, key, and value of the i-th head are</p><p>Here attn in (G.1) is the softmax attention defined in (3.1). First Residual Link and Normalization. The raw output of MHA layer is then passed through a residual link with diagonal weight matrix γ d 1 ∈ R r×r and a normalization layer NL(•), resulting in the intermediate output</p><p>Here the multiplication of γ d 1 should be understood as a columnwise operation for all r ′ ∈ [r]. Note that each of the r vector of X d-1 is in R L , which is mapped to another vector in R L by scaling with</p><p>For the ease of analysis, we adopt the normalization function that maps each row of the input into the unit ℓ 2 -ball as follows.</p><p>[NL(X)] i,: = X i,:</p><p>Another popular normalization function is layer normalization <ref type="bibr" target="#b69">(Xiong et al., 2020)</ref>, which standardizes the vectors of X d + X d-1 γ d 1 by subtracting the mean and dividing by the variance.</p><p>Feed Forward (FF) Layer. The FF layer is parameterized by Second Residual Link and Normalization. The output of FF layer is then passed through a second residual link with weight γ d 2 ∈ R r×r and a normalization layer NL(•). This concludes the d-th transformer block and the resulting output is given by</p><p>Softmax Output Layer After processing through all D transformer blocks, the output X D is fed into a softmax layer to generate the probability distribution of the next reasoning step. This softmax layer is parameterized by ρ D+1 = (W softmax , τ ), where 0 &lt; τ ≤ 1 is the temperature parameter and W softmax ∈ R r×|L| is the weight matrix. The softmax layer takes X D ∈ R L×r and produces the output distribution</p><p>where 1 ∈ R L is an all-one vector. Here softmax(•) is the softmax function which maps a vector in R |L| to a distribution over L.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>For any vector of the form</head><p>Thus, c ′ = 1 if c = 0 and c ′ = 1/2 if c = 1. Therefore, we have</p><p>which means that NN J,3 can be realized by a single FF layer. Moreover, the weight matrices and bias vectors are all bounded by B 2 + (H + 1) 2 + 1• √ 6 in terms of the Frobenius norm. Noticing that a FF layer can be represented by a single transformer block, we conclude the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.4.5 Proof of Lemma F.11</head><p>Proof. In this proof, we first construct an attention layer that takes z t ′ ,(2) h ′ ∈ R 3 as the input and outputs (0, p t ′ h ′ , 0) for any index (t ′ , h ′ ). Here p t ′ h ′ ≈ h for any (t ′ , h ′ ). Next, we explain how to maintain the first coordinate of z</p><p>Finally, we show that this network module can be implemented by a single transformer block.</p><p>To construct the attention layer, we introduce an auxiliary parameter α to control the precision of p t ′ h ′ . Recall that the sequence S t h has length</p><p>NN . We define the attention matrices</p><p>Here α &gt; 0 is a parameter to be determined later. With the input X</p><p>(3) NN ∈ R L ′ ×3 , the queries, keys, and values are given by</p><p>where we f product approximates the product operation in the sense that |f product (x, y) -xy| ≤ ϵ ′ for any x, y ∈ [-H, H]. Using (G.17), we compute that the softmax attention score a(i, j)</p><p>thus can be written as a sum of four ReLU functions. In particular, we can write h 1 as</p><p>which can be verified by direct calculation. Thus, this function can be written as a single feed-forward layers with parameters</p><p>Similarly, we can write h 2 as</p><p>which can be written as a feed-forward layer with parameters</p><p>Finally, by directly concatenating the corresponding weight matrices for h 1 (•) and h 2 (•), we can implement the function f h using a single feedforward (FF) layer. The width of the weight matrix in this FF layer is at most 10, and the magnitude of the weights is bounded by H + 1. Thus, we conclude the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.5 Proofs of the Remaining Auxiliary Lemmas</head><p>In the following, we prove the remaining auxiliary lemmas, which include Lemma F.14 used in the proof of Corollary 6.6, and Lemmas G.1 and G.2 used in the proofs in Appendix G.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.5.1 Proof of Lemma F.14</head><p>Proof. Let prompt CoT (n) denote a fixed prompt, then according to the chain rule of KL divergence, we have that</p><p>Here the chain rule of KL divergence states that</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H Technical Lemmas</head><p>Finally, in this appendix, we lay out the helper lemmas used in the proofs in previous appendices. These lemmas are directly obtained from existing works and we provide the references to their proofs.</p><p>Lemma H.1 (Proposition 2 in <ref type="bibr" target="#b13">Caponnetto and De Vito (2007)</ref>). Let (Ω, ν) be a probability space and ξ be a random variable on Ω taking value in a real separable Hilbert space H. We assume that there exists constants B, σ &gt; 0 such that</p><p>Then, it holds with probability at least 1 -δ that</p><p>Lemma H.2 (Theorem A.4 in Foster et al. ( <ref type="formula">2021</ref>)). For any sequence of real random variables {X i } 1≤i≤n that adapts to a filtration {F i } 1≤i≤n , then for any m ≤ n, with probability at least 1 -δ,</p><p>Lemma H.3 (Lemma I.10 in <ref type="bibr" target="#b73">Zhang et al. (2023a)</ref>). Let b = sup x log(p(x)/q(x)). We have that </p><p>Let Q be a probability distributionover F. With probability at least 1 -δ, we have</p><p>simultaneously for any distribution P on F.</p><p>Lemma H.6 (Corollary A.7 in Edelman et al. ( <ref type="formula">2022</ref>)). For any two vectors x, y ∈ R r ,</p><p>Lemma H.7 (Lemma A.6 in Elbrächter et al. ( <ref type="formula">2021</ref>)). For a, b ∈ R with a &lt; b, define</p><p>There exists a constant C &gt; 0 such that for all a, b ∈ R with a &lt; b, f ∈ S [a,b] , and ε ∈ (0, 1/2), there is a fully connected network Ψ f such that</p><p>where the depth of the network is upper bounded by</p><p>the width of the network is upper bounded by 16, and the maximum weight of the weight matrices is bounded by one. In particular, applying Propsotion F.7, we compute the magnitude of each intermediate output as follows. For any input x ∈ [a, b], let X ℓ denote the output of the ℓ-th layer of the neural network, we have X ℓ ≤ 4 b + 1) • 16 ℓ .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I Supplementary Information about Prompts</head><p>This section provides supplementary examples for the area code experiment in Section 4.1 and the CityEquation experiment in Section 5.3.</p><p>I.1 Details of the Area Code Experiment in in Section 4.1</p><p>The following includes the experimental details of testing the ChatGPT(gpt-3.5-turbo-16k with the temperature set to zero) on the area code task using vanilla ICL and CoT methods, respectively. The output of ChatGPT is colored in red.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The area code experiment with ICL prompt</head><p>Input prompt: Q: The US=?</p><p>A: The answer is 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Q: France=?</head><p>A: The answer is 66. Q: Japan=? A:</p><p>LLM output:</p><p>The answer is 100.</p><p>As we can see, the answer provided by ChatGPT is wrong: it should be 162 instead of 100</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The area code experiment with CoT prompt</head><p>Input prompt: Q: The US=? A: The US has area code 1, so the answer is 2. Q: France=? A: France has area code 33, so the answer is 66. Q: Japan=? A:</p><p>LLM output: Japan has area code 81, so the answer is 162.</p><p>The output follows the same pattern shown in the demonstrations: it starts with stating the area code and computes the final answer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.2 Details of the CityEquation Experiment</head><p>This section provides the experimental details for the city arithmetic experiment discussed in Section 5.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Example prompts</head><p>We provide example prompts (2-shot) for vanilla ICL, informative CoT, Partially informative CoT-(a), Partially informative CoT-(c), and uninformative CoT methods, respectively. To highlight the differences among these five methods, we color code the useful intermediate steps in blue and the irrelevant intermediate steps in red for better comparison.</p><p>Example prompts for Vanilla ICL Q: "Mumbai + Sydney" A: The answer is 224.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Q: "New York + Seoul"</head><p>A: The answer is 53. Q: "Paris + Beijing" A: Concisely explain your steps and write your answer as an integer in the last sentence starting with "The answer is".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Example prompts for Informative CoT</head><p>Q: "Mumbai + Sydney" A: Using the longitudes of cities, the equation "Mumbai + Sydney" translates as "Mumbai" = 73, "Sydney" = 151. Here the longitudes of the western hemisphere are negative numbers. And we round the coordinates to the nearest integer. This gives the result. The answer is 224. Q: "New York + Seoul" A: Using the longitudes of cities, the equation "New York + Seoul" translates as "New York" = -74, "Seoul" = 127. Here the longitudes of the western hemisphere are negative numbers. And we round the coordinates to the nearest integer. This gives the result. The answer is 53. Q: "Paris + Beijing" A: Concisely explain your steps and write your answer as an integer in the last sentence starting with "The answer is'/.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Example prompts for Partially informative CoT-(a)</head><p>Q: "Mumbai + Sydney" A: Mumbai has longitude: 73. The answer is 224. Q: "New York + Seoul" A: New York has longitude: -74. The answer is 53. Q: Paris + Beijing A: Concisely explain your steps and write your answer as an integer in the last sentence starting with "The answer is".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Example prompts for Partially informative CoT-(b)</head><p>Q: "Mumbai + Sydney" A: The greater Mumbai area has around 20 million residents, predominantly of South Asian ethnicity. The median age is 31 years. Sydney has a population of around 5.3 million people, with a breakdown of 58% White, 34.2% Asian, and 2.6% Aboriginal/-Torres Strait Islander. The remaining percentages include others. The median age is 36 years. The answer is 224. Q: "New York + Seoul" A: With a population of approximately 8.4 million people, New York is diverse. Around 32.1% identify as White, 29.1% as Hispanic, 24.3% as Black, 14.1% as Asian, with the rest being a mixture of Native American, Pacific Islander, and other ethnicities. The median age is 36 years. Seoul has a population of around 9.7 million, predominantly Korean, with a median age of 41 years. The answer is 53. Q: "Paris + Beijing" A: Concisely explain your steps and write your answer as an integer in the last sentence starting with 'The answer is'. Q: "Paris + Beijing" A: Concisely explain your steps and write your answer as an integer in the last sentence starting with "The answer is".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Example prompts for</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Failure cases</head><p>We present examples of failure cases for vanilla ICL, informative CoT, Partially informative CoT-(a), Partially informative CoT-(c), and uninformative CoT methods. The ground truth is written in blue, while the incorrect answer produced by GPT-4 is in red.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Failed example for Vanilla ICL</head><p>Testing query: Q: "Sao Paulo -Cairo", Ground truth: -78, LLM output: "The questions seem to be asking for the difference in time zones between two cities. Sao Paulo, Brazil is in the Brasilia Time Zone, which is GMT -3. Cairo, Egypt is in the Eastern European Time Zone, which is GMT +2. To find the difference, subtract the time zone of Sao Paulo from that of Cairo: -3 -(+2) = -5. The answer is -5."</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Failed example for Informative CoT</head><p>Testing query: Q: "Moscow + New York", Ground truth: -36, LLM output: "Using the longitudes of cities, the equation "Moscow + New York" translates as "Moscow" = 37, "New York" = -74. Here the longitudes of the western hemisphere are negative numbers. And we round the coordinates to the nearest integer. This gives the result. The answer is -37."</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Failed example for Partially informative CoT-(a)</head><p>Testing query: Q: "Sao Paulo -New York", Ground truth: 27, LLM output: "Let's consider the geographic coordinates that specify their east-west or north-south positions on the Earth's surface. Sao Paulo is located at approximately 46 degrees west, and New York is located at approximately 74 degrees west. If we subtract the longitude of Sao Paulo from that of New York, we get -28. The answer is -28."</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Failed example for Partially informative CoT-(b)</head><p>Testing query: Q: "Sydney -New York", Ground truth: 225, LLM output: "Sydney has a population of around 5.3 million people, with a breakdown of 58% White, 34.2% Asian, and 2.6% Aboriginal/Torres Strait Islander. The remaining percentages include others. The median age is 36 years. New York City has a population of approximately 8.4 million people, with a diverse population that includes White (42.7%), Hispanic (29.1%), Black (24.3%), and Asian (14%) individuals. The median age is 36 years. The answer is -2."</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Failed example for Partially informative CoT-(c)</head><p>Testing query: Q: "Mexico City + Beijing", Ground truth: 17, LLM output: "Mexico City has a population of approximately 21.7 million in its metropolitan area. The majority of residents are of Mestizo (mixed Indigenous and European) descent, with a significant number of Indigenous peoples, and a smaller number of European and other ethnicities. The median age is 27 years. Beijing, with a population of around 21.5 million, is predominantly Han Chinese, with a median age of 37.6 years. Mexico City has longitude: -99. The answer is -77."</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Failed example for Partially informative CoT-(d)</head><p>Testing query: Q: "Moscow -Tokyo", Ground truth: -102, LLM output: "Moscow, the capital of Russia, has a population of approximately 12.5 million people, predominantly of Russian ethnicity. The median age is 40 years. Tokyo, the capital of Japan, has a population of around 14 million in its 23 special wards, predominantly of Japanese ethnicity. The median age in Tokyo is 45.5 years. Moscow has longitude: 37. Tokyo has longitude: 140. Subtracting Moscow's longitude from Tokyo's gives us 140 -37. The answer is 103." Upon closer inspection of GPT-4 outputs, incorrect results from informative CoT are solely due to rounding. That is, when rounding the coordinates to the nearest integer, GPT-4 makes an error. In particular, in the failed example shown above, the longitude of Moscow is 37.6, which should be rounded to 38 instead of 37. However, such a rounding error is the only error source. Therefore, with informative CoT, GPT-4 in fact understands that extracting the longitudes is the key to solving the CityEquation task.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Achiam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Adler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ahmad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Akkaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">L</forename><surname>Aleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Altenschmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Altman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Anadkat</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.08774</idno>
		<title level="m">Gpt-4 technical report</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Unified pre-training for program understanding and generation</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">U</forename><surname>Ahmad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-W</forename><surname>Chang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.06333</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Transformers learn to implement preconditioned gradient descent for in-context learning</title>
		<author>
			<persName><forename type="first">K</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Daneshmand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2306.00297</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">What learning algorithm is in-context learning? investigations with linear models</title>
		<author>
			<persName><forename type="first">E</forename><surname>Akyürek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Schuurmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2211.15661</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Alquier</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.11216</idno>
		<title level="m">User-friendly introduction to pac-bayes bounds</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><surname>Anthropic</surname></persName>
		</author>
		<title level="m">The claude 3 model family: Opus, sonnet, haiku. View in Article</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">42</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<title level="m">Layer normalization</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2306.04637</idno>
		<title level="m">Transformers as statisticians: Provable in-context learning with in-context algorithm selection</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Joint measures and cross-covariance operators</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Baker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions</title>
		<imprint>
			<date type="published" when="1973">1973</date>
			<publisher>American Mathematical Society</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Graph of thoughts: Solving elaborate problems with large language models</title>
		<author>
			<persName><forename type="first">M</forename><surname>Besta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Blach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kubicek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gerstenberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Podstawski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Gianinazzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gajda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lehmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Niewiadomski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Nyczyk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">38</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">T</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">A</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1877" to="1901" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Optimal rates for the regularized least-squares algorithm</title>
		<author>
			<persName><forename type="first">A</forename><surname>Caponnetto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>De Vito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations of Computational Mathematics</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">What can transformers learn in-context? a case study of simple function classes</title>
		<author>
			<persName><forename type="first">S</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Valiant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="30583" to="30598" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Hahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.07971</idno>
		<title level="m">A theory of emergent in-context learning as implicit structure induction</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Friedman</surname></persName>
		</author>
		<title level="m">The elements of statistical learning: data mining, inference, and prediction</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">From words to actions: Unveiling the theoretical underpinnings of llm-driven autonomous systems</title>
		<author>
			<persName><forename type="first">J</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2405.19883</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Using pre-training can improve model robustness and uncertainty</title>
		<author>
			<persName><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mazeika</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Bayesian model averaging: a tutorial (with comments by m. clyde, david draper and ei george, and a rejoinder by the authors</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Hoeting</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Madigan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Raftery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">T</forename><surname>Volinsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistical science</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="382" to="417" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Stolfo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bosselut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sachan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.14491</idno>
		<title level="m">Towards a mechanistic interpretation of multi-step reasoning capabilities of language models</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Qin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2311.05232</idno>
		<title level="m">A survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.05249</idno>
		<title level="m">-context convergence of transformers</title>
		<imprint>
			<date type="published" when="2023">2023b</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.09960</idno>
		<title level="m">A latent space theory for emergent abilities in large language models</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Self-generated in-context learning: Leveraging auto-regressive language models as a demonstration generator</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2206.08082</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<title level="m">Auto-encoding variational bayes</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Slivkins</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2403.15371</idno>
		<title level="m">Can large language models explore in-context? arXiv preprint</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Lanham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Radhakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Denison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hernandez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Durmus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hubinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kernion</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2307.13702</idno>
		<title level="m">Measuring faithfulness in chain-ofthought reasoning</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Transformers as algorithms: Generalization and stability in in-context learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Ildiz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Papailiopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Oymak</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2023">2023a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Dissecting chain-of-thought: A study on compositional in-context learning of mlps</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sreenivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Giannou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Papailiopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Oymak</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.18869</idno>
		<imprint>
			<date type="published" when="2023">2023b</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Information theory, inference and learning algorithms</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Mackay</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
			<publisher>Cambridge university press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Text and patterns: For effective chain of thought, it takes two to tango</title>
		<author>
			<persName><forename type="first">A</forename><surname>Madaan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yazdanbakhsh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2209.07686</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">One step of gradient descent is provably the optimal in-context learner with one layer of linear self-attention</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mahankali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">B</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2307.03576</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Some pac-bayesian theorems</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Mcallester</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the eleventh annual conference on Computational learning theory</title>
		<meeting>the eleventh annual conference on Computational learning theory</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">The expresssive power of transformers with chain of thought</title>
		<author>
			<persName><forename type="first">W</forename><surname>Merrill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sabharwal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.07923</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Artetxe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hajishirzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.12837</idno>
		<title level="m">Rethinking the role of demonstrations: What makes in-context learning work? arXiv preprint</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Olsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Elhage</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Nanda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Dassarma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2209.11895</idno>
		<title level="m">-context learning and induction heads</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<author>
			<persName><forename type="first">R</forename><surname>Openai</surname></persName>
		</author>
		<idno>arxiv 2303.08774</idno>
	</analytic>
	<monogr>
		<title level="m">Gpt-4 technical report</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Training language models to follow instructions with human feedback</title>
		<author>
			<persName><forename type="first">L</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wainwright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Slama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="27730" to="27744" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ismayilzada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Peyrard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Borges</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bosselut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>West</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Faltings</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.01904</idno>
		<title level="m">Refiner: Reasoning feedback on intermediate representations</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<author>
			<persName><forename type="first">Phuong</forename></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hutter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename></persName>
		</author>
		<idno type="arXiv">arXiv:2207.09238</idno>
		<title level="m">Formal algorithms for transformers</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Why think step by step? reasoning emerges from the locality of experience</title>
		<author>
			<persName><forename type="first">B</forename><surname>Prystawski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Goodman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="page">36</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">An introduction to hidden markov models</title>
		<author>
			<persName><forename type="first">L</forename><surname>Rabiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Juang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ieee assp magazine</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="4" to="16" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenAI blog</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<author>
			<persName><forename type="first">O</forename><surname>Rubin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Herzig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Berant</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.08633</idno>
		<title level="m">Learning to retrieve prompts for in-context learning</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Sahoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Saha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mondal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chadha</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.07927</idno>
		<title level="m">A systematic survey of prompt engineering in large language models: Techniques and applications</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Language models are greedy reasoners: A systematic formal analysis of chain-of-thought</title>
		<author>
			<persName><forename type="first">A</forename><surname>Saparov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.01240</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Language models are multilingual chain-ofthought reasoners</title>
		<author>
			<persName><forename type="first">F</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Suzgun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Freitag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Srivats</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vosoughi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">W</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.03057</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">An information-theoretic approach to prompt engineering without ground truth labels</title>
		<author>
			<persName><forename type="first">T</forename><surname>Sorensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Rytting</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Shaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J</forename><surname>Rogers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Delorey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Khalil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Fulda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wingate</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.11364</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Roformer: Enhanced transformer with rotary position embedding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<date type="published" when="2023">2023. 127063</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Large language models are in-context semantic reasoners rather than symbolic reasoners</title>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.14825</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><surname>Team</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Anil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Borgeaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-B</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Soricut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schalkwyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hauth</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2312.11805</idno>
		<title level="m">Gemini: a family of highly capable multimodal models</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Tonmoy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zaman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Rawte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chadha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2401.01313</idno>
		<title level="m">A comprehensive survey of hallucination mitigation techniques in large language models</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Stone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Albert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Almahairi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Babaei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Bashlykov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bhargava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bhosale</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2307.09288</idno>
		<title level="m">Llama 2: Open foundation and finetuned chat models</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Why can large language models generate correct chain-of-thoughts</title>
		<author>
			<persName><forename type="first">R</forename><surname>Tutunov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Grosnit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ziomek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bou-Ammar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.13571</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Transformers learn in-context by gradient descent</title>
		<author>
			<persName><forename type="first">Von</forename><surname>Oswald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Niklasson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Randazzo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Sacramento</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mordvintsev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vladymyrov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Towards understanding chain-of-thought prompting: An empirical study of what matters</title>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2212.10001</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.14160</idno>
		<title level="m">Label words are anchors: An information flow perspective for understanding in-context learning</title>
		<imprint>
			<date type="published" when="2023">2023a</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Self-consistency improves chain of thought reasoning in language models</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Schuurmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chowdhery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.11171</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Large language models are implicitly topic models: Explaining and finding good demonstrations for in-context learning</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">Y</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2301.11916</idno>
		<imprint>
			<date type="published" when="2023">2023b</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">What do position embeddings learn? an empirical study of pre-trained language model positional encoding</title>
		<author>
			<persName><forename type="first">Y.-A</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-N</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.04903</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Guu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.01652</idno>
		<title level="m">Finetuned language models are zero-shot learners</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Chain-of-thought prompting elicits reasoning in large language models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Schuurmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="24824" to="24837" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<author>
			<persName><forename type="first">N</forename><surname>Wies</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shashua</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.07895</idno>
		<title level="m">The learnability of in-context learning</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">How many pretraining tasks are needed for in-context learning of linear regression</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Braverman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">L</forename><surname>Bartlett</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.08391</idno>
		<imprint>
			<date type="published" when="2023">2023a</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Analyzing chainof-thought prompting in large language models via gradient-based feature attributions</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Badrinath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lakkaraju</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2307.13339</idno>
		<imprint>
			<date type="published" when="2023">2023b</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">An explanation of in-context learning as implicit bayesian inference</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Raghunathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.02080</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">On layer normalization in the transformer architecture</title>
		<author>
			<persName><forename type="first">R</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">Tree of thoughts: Deliberate problem solving with large language models</title>
		<author>
			<persName><forename type="first">S</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Shafran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Narasimhan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.10601</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kottur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ravanbakhsh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Poczos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<title level="m">Deep sets. Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.07830</idno>
		<title level="m">Hellaswag: Can a machine really finish your sentence? arXiv preprint</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.19420</idno>
		<title level="m">What and how does in-context learning learn? bayesian model averaging, parameterization, and generalization</title>
		<imprint>
			<date type="published" when="2023">2023a</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<title level="m" type="main">Igniting language intelligence: The hitchhiker&apos;s guide from chainof-thought reasoning to language agents</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gerstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2311.11797</idno>
		<imprint>
			<date type="published" when="2023">2023b</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<title level="m" type="main">Automatic chain of thought prompting in large language models</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.03493</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Karypis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.00923</idno>
		<title level="m">Multimodal chain-of-thought reasoning in language models</title>
		<imprint>
			<date type="published" when="2023">2023c</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Schärli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Scales</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Schuurmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Bousquet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.10625</idno>
		<title level="m">Least-to-most prompting enables complex reasoning in large language models</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Rethinking pre-training and self-training</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="3833" to="3845" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
