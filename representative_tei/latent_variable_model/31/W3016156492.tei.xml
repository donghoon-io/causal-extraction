<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Correlated Multi-armed Bandits with a Latent Random Source</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2019-01-29">29 Jan 2019</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Samarth</forename><surname>Gupta</surname></persName>
							<email>samarthg@andrew.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University Pittsburgh</orgName>
								<address>
									<postCode>15213</postCode>
									<region>PA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Gauri</forename><surname>Joshi</surname></persName>
							<email>gaurij@andrew.cmu.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">Carnegie Mellon University Pittsburgh</orgName>
								<address>
									<postCode>15213</postCode>
									<region>PA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Osman</forename><surname>Yağan</surname></persName>
							<email>oyagan@andrew.cmu.edu</email>
							<affiliation key="aff2">
								<orgName type="institution">Carnegie Mellon University Pittsburgh</orgName>
								<address>
									<postCode>15213</postCode>
									<region>PA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Correlated Multi-armed Bandits with a Latent Random Source</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2019-01-29">29 Jan 2019</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:1808.05904v2[stat.ML]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-28T23:13+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We consider a novel multi-armed bandit framework where the rewards obtained by pulling the arms are functions of a common latent random variable. The correlation between arms due to the common random source can be used to design a generalized upper-confidencebound (UCB) algorithm that identifies certain arms as non-competitive, and avoids exploring them. As a result, we reduce a K-armed bandit problem to a C + 1-armed problem, where C + 1 includes the best arm and C competitive arms. Our regret analysis shows that the competitive arms need to be pulled O(log T ) times, while the non-competitive arms are pulled only O(1) times. As a result, there are regimes where our algorithm achieves a O(1) regret as opposed to the typical logarithmic regret scaling of multi-armed bandit algorithms. We also evaluate lower bounds on the expected regret and prove that our correlated-UCB algorithm achieves O(1) regret whenever possible.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Multi-armed Bandits. The multi-armed bandit (MAB) framework is a special case of reinforcement learning <ref type="bibr" target="#b24">(Sutton and Barto, 1998)</ref> where actions do not change the system state. At each time step we obtain a reward by pulling one of K arms which have unknown reward distributions, and the objective is to maximize the cumulative reward. The seminal work of <ref type="bibr" target="#b14">Lai and Robbins (Lai and Robbins, 1985)</ref> proposed the upper confidence bound (UCB) arm-selection algorithm, and studied its fundamental limits in terms of bounds on regret. Subsequently, multi-armed bandit algorithms <ref type="bibr" target="#b7">(Bubeck et al., 2012;</ref><ref type="bibr" target="#b9">Garivier and Cappé, 2011)</ref> have been used in numerous applications including medical diagnosis <ref type="bibr" target="#b27">(Villar et al., 2015)</ref>, system testing <ref type="bibr" target="#b25">(Tekin and Turgay, 2017)</ref>, scheduling in computing systems <ref type="bibr" target="#b19">(Nino-Mora, 2009;</ref><ref type="bibr" target="#b13">Krishnasamy et al., 2016;</ref><ref type="bibr" target="#b12">Joshi, 2016)</ref>, and web optimization <ref type="bibr" target="#b29">(White, 2012;</ref><ref type="bibr" target="#b1">Agarwal et al., 2009)</ref> among others. A drawback of the classical model is that it assumes independent rewards from the arms, which is typically not true in practice.</p><p>Related Work. Motivated by this shortcoming, several variants of the multi-armed bandit framework have been proposed in recent years. A class of variants relevant to our work is contextual bandits <ref type="bibr" target="#b31">(Zhou, 2016;</ref><ref type="bibr" target="#b2">Agrawal and Goyal, 2013;</ref><ref type="bibr" target="#b0">Agarwal et al., 2014;</ref><ref type="bibr" target="#b21">Sakulkar and Krishnamachari, 2016;</ref><ref type="bibr" target="#b22">Sen et al., 2017)</ref>, where in each round we observe a contextual vector that provides side information about the reward of each arm. Instead of receiving side information, correlated multi-armed bandits exploit the inherent correlation between the rewards of arms arising due to a structural relationship between the arms, or a set of common parameters shared between them. Some recent works <ref type="bibr" target="#b20">(Pandey et al., 2007;</ref><ref type="bibr" target="#b28">Wang et al., 2018;</ref><ref type="bibr" target="#b11">Hoffman et al., 2014;</ref><ref type="bibr" target="#b30">Yahyaa and Drugan, 2015;</ref><ref type="bibr" target="#b23">Srivastava et al., 2015;</ref><ref type="bibr" target="#b18">Mersereau et al., 2009;</ref><ref type="bibr" target="#b3">Atan et al., 2015;</ref><ref type="bibr" target="#b8">Combes et al., 2017)</ref> have studied the correlated multi-armed bandit problem. Many of these works consider specific types of correlation such as clusters of arms <ref type="bibr" target="#b20">(Pandey et al., 2007;</ref><ref type="bibr" target="#b28">Wang et al., 2018)</ref> and Gaussian or invertible reward functions <ref type="bibr" target="#b3">(Atan et al., 2015)</ref> that depend on a constant hidden parameter vector θ <ref type="bibr" target="#b30">(Yahyaa and Drugan, 2015;</ref><ref type="bibr" target="#b3">Atan et al., 2015;</ref><ref type="bibr" target="#b8">Combes et al., 2017;</ref><ref type="bibr" target="#b17">Maillard and Mannor, 2014;</ref><ref type="bibr" target="#b16">Lattimore and Munos, 2014)</ref>. We consider latent random variable X, instead of constant parameter θ. Some recent papers <ref type="bibr" target="#b5">(Bresler et al., 2014)</ref> study the regret of such latent source models for collaborative filtering, with rewards belonging to the set {-1, 0, +1}. Instead of maximizing regret, <ref type="bibr" target="#b10">(Gupta et al., 2018)</ref> considers the same model as this paper, but with the objective of learning the distribution of the latent random variable X.</p><p>Main Contributions. We consider a novel correlated multi-armed bandit model with a latent random source X, and we allow the rewards to be arbitrary functions of X, as described in Section 2. In Section 3, we propose the C-UCB algorithm, which is a fundamental generalization of the classic UCB algorithm. The C-UCB algorithm uses observed rewards to generate pseudo-reward estimates of other arms, and restricts the exploration to the arms that are deemed (empirically) competitive. Regret analysis in Section 4 shows that after T rounds of sampling, the C-UCB algorithm achieves an expected regret of C • O(log T ) + O(1), where C ∈ {0, . . . , K -1} denotes the number of arms that are competitive with respect to the optimal arm. Thus, when the correlation between the rewards results in C being equal to 0, C-UCB achieves constant regret scaling with T , which is an order-wise improvement over standard bandit algorithms like UCB. We also find a lower bound on expected regret and show that the proposed algorithm achieves bounded regret whenever possible. Simulation results in Section 5 show that our C-UCB algorithm outperforms the vanilla UCB algorithm that does not exploit the correlation between arms.</p><p>Applications. Unlike the classic MAB model that considers arms with independent rewards, our framework captures several applications where the rewards of arms k = 1, . . . , K depend on a common source of randomness. For example, the response to K possible advertisements/products can depend on a latent variable X that represents the social/economic condition of a customer. Similarly, the reward for using one of the K possible encoding/routing strategies in a wireless communication network may depend on the current state X of a time-varying channel.</p><p>Through controlled experiments or supervised learning approaches, we can learn the reward function g k (•) for each possible value of X. While it is possible to find the mappings g k (x) for a small control group with different x's, learning the distribution F X of a large population is likely to be difficult and costly; e.g., imagine a company willing to expand to a new region/country with an unknown demographic, and trying to identify the best products/ads. Similarly, in a communication network, it may not be efficient/possible to obtain the channel state information at every node and at every time instant. In this setting, our framework will help obtain larger cumulative reward. In particular, instead of the correlation-agnostic MAB framework, our approach will leverage the previously learned correlations to reduce the regret. Also, unlike contextual bandits where a personalized recommendation is given after observing the context x, our framework identifies a single recommendation that appeals to a large population where these contexts are hidden. Consider a latent random variable X whose probability distribution is unknown. The random variable can be either discrete or continuous. For discrete X, we denote the sample space by W = {x 1 , x 2 , . . . x J }, and use p j to denote the probability Pr(X = x j ) such that J j=1 p j = 1. For continuous X, f X (x) denotes the probability density function of X over x ∈ R.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Problem Formulation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">System Model and Regret Definition</head><formula xml:id="formula_0">X g 1 (X) g 2 (X) g K (X) a r m 1 a r m 2 a r m K Figure 1:</formula><p>Due to the latent nature of X, it is not possible to draw direct samples of X and infer its unknown probability distribution. Instead, indirect samples can be obtained by choosing one of K arms in each round t, where K is finite and fixed. Arm k is associated with a reward function g k (X). If we take action k t ∈ {1, 2 . . . , K} in time slot t, we obtain the reward g kt (x t ) where x t is an i.i.d. realization of X as shown in Figure <ref type="figure">1</ref>. The functions g 1 (X), g 2 (X) . . . g K (X) are assumed to be known. Assume that there is a unique optimal arm k * that gives the maximum expected reward, that is,</p><formula xml:id="formula_1">k * = arg max k∈{1,2,...,K} E [g k (X)] = arg max k∈{1,2,...K} µ k ,<label>(1)</label></formula><p>where µ k denotes the mean reward of arm k. Let ∆ k µ k * -µ k be defined as the suboptimality gap of arm k with respect to the optimal arm k * . We also assume that the reward functions are bounded within an interval of size B, that is, (max x∈W g k (x)-min x∈W g k (x)) ≤ B for all arms k ∈ {1, . . . , K}. We do not make any other assumptions such as the functions g 1 , . . . g K being invertible. And indeed our problem framework and algorithm is most interesting when the reward functions are not invertible.</p><p>Our objective is to sequentially pull arms k 1 , . . . , k t in order to maximize the cumulative reward. After T rounds, the cumulative reward is T t=1 g kt (x t ). Maximizing the cumulative reward is equivalent to minimizing the cumulative regret which is defined as follows.</p><p>Definition 1 (Cumulative Regret). The cumulative regret Reg(T ) after T rounds is defined as</p><formula xml:id="formula_2">Reg(T ) T t=1 (g k * (x t ) -g kt (x t )) (2)</formula><p>where x t is an i.i.d. realization of X that is not directly observed; we only observe g kt (x t ).</p><p>Thus, our goal is to design an algorithm to choose an arm k t at every round t so as to minimize expected Reg(T ). Note that we do not know the number of rounds T beforehand, and aim to minimize Reg(T ) for all T .</p><p>Remark 1 (Connection to Classical Multi-armed Bandits). Although we consider a scalar random variable X for brevity, our framework and algorithm can be generalized to a latent random vector X = (X 1 , X 2 , . . . X m ), as we explain in the supplementary material. The classical multi-armed bandit framework with independent arms is a special case of this generalized model when X = (X 1 , X 2 , . . . X K ) where X i are independent random variables and g k (X) = X k for k ∈ {1, 2, . . . , K}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Utilizing Correlation Between the Arms: Intuition and Examples</head><p>In the classical multi-armed bandit framework there is a trade-off between exploring more arms to improve the estimates of their rewards, and exploiting the current best arm in order to maximize the cumulative reward. The sub-optimal arms have to be pulled Θ(log T ) times each, resulting in a Θ(log T ) cumulative regret as shown in the seminal work <ref type="bibr" target="#b14">(Lai and Robbins, 1985)</ref>. In our new framework, since the reward functions g 1 , . . . g K are correlated through the common hidden random variable X, pulling one arm can give information about the distribution of X, which in turn can help estimate the reward from other arms. These pseudo-rewards (defined formally in Section 3) can allow us to declare certain arms as non-competitive (defined formally in Section 3) and pull them only O(1) times. As a result, a K-armed bandit problem is reduced to a C + 1-armed bandit problem, where C ∈ {0, 1, . . . , K -1} is the number of competitive arms. Let us consider some examples to gain intuition on how arms are deemed non-competitive.</p><p>Example 1 (All Reward Functions are Invertible). Suppose that all the reward functions g 1 , . . . g K are invertible. Then, if we obtain a reward r by pulling arm k in slot t, it can be mapped back to a unique realization x = g -1 k (r) of the latent random variable X. Using this realization, we can generate pseudo-samples g (x) from any other arm = k. This renders all sub-optimal arms non-competitive and obviates the need to explore them. As a result, a pure-exploitation strategy is optimal and it gives O(1) regret. In fact, it suffices to have only the function g k * (x) corresponding to the optimal arm to be invertible to deem all other arms as non-competitive and to achieve O(1) regret; see Section 4 for details. To understand the intuition behind declaring arms as non-competitive for general reward functions, consider the two-arm example below.</p><formula xml:id="formula_3">g 1 (X) x 1 x 2 x 3 x 1 x 2 x 3 g 2 (X) 2 1.5 1 0 0</formula><p>Example 2 (Identifying Non-competitive Arms). Consider two-armed bandit problem with reward functions g 1 and g 2 respectively, as shown in Figure <ref type="figure" target="#fig_0">2</ref>. Suppose arm 1 is pulled 10 times, out of which we observe reward 1 three times, and 2 seven times, such that the empirical reward is μ1 = p1 + 2(p 2 + p3 ) = 1.7</p><p>(3) Using (3), we can estimate the distribution (p 1 , p 2 , p 3 ) of X to be p1 = 0.3 and p2 + p3 = 0.7. It is not possible to use this to estimate the reward of arm 2 since we only know the sum p2 + p3 . However, we can find an upper bound on the empirical reward of arm 2 as follows.</p><formula xml:id="formula_4">μ2 = 1.5p 1 + 0p 2 + 1.5p 3 (4) ≤ 1.5p 1 + max(0, 1.5)(p 2 + p3 ) = 1.5<label>(5)</label></formula><p>Since the upper bound on arm 2's reward (which we refer to as its pseudo-reward) is less than arm 1's empirical reward, we consider arm 2 as empirically non-competitive with respect to arm 1 and do not pull it until it becomes empirically competitive again.</p><p>In Section 3 below we formalize the idea of competitive and non-competitive arms and propose a correlated upper confidence bound (C-UCB) algorithm. In Section 4 we give upper and lower bounds on the regret of the proposed algorithm, and show that the regret is similar to that of UCB with just C + 1 arms instead of K arms, where C is the number of competitive arms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">C-UCB: The Proposed Correlated-UCB Algorithm</head><p>Our algorithm to choose an arm in each round in the correlated multi-armed bandit framework is a fundamental generalization of the upper confidence bound (UCB1) algorithm presented in <ref type="bibr" target="#b4">(Auer et al., 2002)</ref>. In round t, the UCB1 algorithm chooses the arm that maximizes the upper confidence index I k (t) which is defined as</p><formula xml:id="formula_5">I k (t) = μk (t) + B 2 log t n k (t) ,<label>(6)</label></formula><p>where μk (t) is the empirical mean of the rewards received from arm k until round t, and n k (t) is the number of times arm k is pulled till round t. The second term causes the algorithm to explore arms that have been pulled only a few times (small n k (t)). Recall that we assume all rewards to be bounded within an interval of size B. When the index t is implied by context, we abbreviate μk (t) and I k (t) to μk and I k respectively in the rest of the paper. Also, we use the terms UCB1, UCB, and classic UCB interchangeably to refer to the UCB1 algorithm proposed in <ref type="bibr" target="#b4">(Auer et al., 2002)</ref>.</p><p>In correlated MAB framework, the rewards observed from one arm can help estimate the rewards from other arms. Our key idea is to use this information to reduce the amount of exploration required. We do so by evaluating the empirical pseudo-reward of every other arm with respect to an arm k, as we saw in Example 2. If this pseudo-reward is smaller than empirical reward of arm k, then arm is considered to be empirically non-competitive with respect to arm k, and we do not consider it as a candidate in the UCB1 algorithm.</p><p>The notions of pseudo-reward and empirical competitiveness of arms are defined in Section 3.1 and Section 3.2 below, and in Section 3.3 we describe how we modify the UCB1 algorithm. The pseudo-code of our algorithm is presented in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Pseudo-Reward of Arm with respect to Arm k</head><p>The pseudo-reward of arm with respect to arm k is an artificial sample of arm 's reward generated using the reward observed from arm k. It is defined as follows.</p><p>Definition 2 (Pseudo-Reward). Suppose we pull arm k and observe reward r. Then the pseudo-reward of arm with respect to arm k is</p><formula xml:id="formula_6">s ,k (r) max x:g k (x)=r g (x).<label>(7)</label></formula><p>The pseudo-reward s ,k (r) gives the maximum possible reward that could have been obtained from arm , given the reward observed from arm k. In Example 2, if we observe a reward of r = 2 from arm 1, X could have been either x 2 or x 3 . Then the pseudo-reward of arm 2 is s 2,1 = 1.5 which is the maximum of g 2 (x 2 ) and g 2 (x 3 ). The pseudo-reward definition also applies to continuous X, and it can be directly extended to a latent random vector X = (X 1 , . . . X m ) as well as explained in the supplementary material.</p><p>Definition 3 (Empirical and Expected Pseudo-Reward). After t rounds, arm k is pulled n k (t) times. Using these n k (t) reward realizations, we can construct the empirical pseudo-reward φ ,k (t) for each arm with respect to arm k as follows.</p><formula xml:id="formula_7">φ ,k (t) t τ =1 1 kτ =k s ,k (r t ) n k (t) , ∈ {1, . . . , K} \ {k}. (<label>8</label></formula><formula xml:id="formula_8">)</formula><p>The expected pseudo-reward of arm with respect to arm k is defined as</p><formula xml:id="formula_9">φ ,k E [s ,k (g k (X))] .<label>(9)</label></formula><p>Note that the empirical pseudo-reward φ ,k (t) is defined with respect to arm k and it is only a function of the rewards observed by pulling k. It may be possible to get a more accurate estimate of arm 's reward by combining the observations from all other arms. However, we consider this rough estimate, and it is sufficient to reduce K-armed bandit problem to a C + 1 armed problem, as we show in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Competitive and Non-competitive arms with respect to Arm k</head><p>Using the pseudo-reward estimates defined above, we can classify each arm = k as competitive or non-competitive with respect the arm k. To this end, we first define the notion of the pseudo-gap.</p><p>Definition 4 (Pseudo-Gap). The pseudo-gap ∆ ,k of arm with respect to arm k is defined as</p><formula xml:id="formula_10">∆ ,k µ k -φ ,k ,<label>(10)</label></formula><p>i.e., the difference between expected reward of arm k and the expected pseudo-reward of arm with respect to arm k.</p><p>From the definition of pseudo-reward, it follows that the expected pseudo-reward φ ,k is greater than or equal to the expected reward µ from arm . Thus, a positive pseudo-gap ∆ ,k &gt; 0 indicates that it is possible to classify arm as sub-optimal using only the rewards observed from arm k (with high probability as the number of pulls for arm k gets large); thus, arm needs not be explored. Such arms are called non-competitive, as we define below.</p><formula xml:id="formula_11">Algorithm 1 C-UCB Correlated UCB Algorithm 1: Input: Reward Functions {g 1 , g 2 . . . g K } 2: Initialize: n k = 0, I k = ∞ for all k ∈ {1, 2, . . . K} 3: for each round t do 4:</formula><p>Find k max = arg max k n k (t -1), the arm that has been pulled most times until round t -1 5:</p><p>Initialize the empirically competitive set A = {1, 2, . . . , K} \ {k max }.</p><p>6:</p><formula xml:id="formula_12">for k = k max do 7: if μk max &gt; φk,k max then 8:</formula><p>Remove arm k from the empirically competitive set: A = A {k} Apply UCB1 over arms in A ∪ {k max } by pulling arm k t = arg max k∈A∪{k max } I k (t -1)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>12:</head><p>Receive reward r t , and update n kt = n kt + 1 13:</p><p>Update Empirical reward: μkt (t) = μk t (t-1)(n k t (t)-1)+rt n k t (t)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>14:</head><p>Update the UCB Index:</p><formula xml:id="formula_13">I kt (t) = μkt + B 2 log t n k t 15:</formula><p>Compute pseudo-rewards for all arms k = k t : s k,kt (r t ) = max x:g k t (x)=rt g k (x).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>16:</head><p>Update empirical pseudo-rewards for all k = k t : φk,kt (t) = τ :kτ =kt s k,kτ (r τ )/n kt 17: end for Definition 5 (Competitive and Non-Competitive arms). An arm is said to be noncompetitive if its pseudo-gap with respect to the optimal arm k * is positive, that is, ∆ ,k * &gt; 0.</p><p>Similarly, an arm is said to be competitive if ∆ ,k * &lt; 0. The unique best arm k * has ∆k * ,k * = 0 and is not counted in the set of competitive arms.</p><p>Since the distribution of X is unknown, we can not find the pseudo-gap of each arm and thus have to resort to empirical estimates based on observed rewards. In our algorithm, we use a noisy notion of the competitiveness of an arm defined as follows. Note that since the optimal arm k * is also not known, empirical competitiveness of an arm is defined with respect to each of the other arms k = .</p><p>Definition 6 (Empirically Competitive and Non-Competitive arms). An arm is said to be "empirically non-competitive with respect to arm k at round t" if its empirical pseudo-reward is less than the empirical reward of arm k, that is, μk (t) -φ ,k (t) &gt; 0. Similarly, an arm = k is deemed empirically competitive with respect to arm k at round t, if μk (t) -φ ,k (t) ≤ 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Modified UCB1 Algorithm to Eliminate Non-Competitive Arms</head><p>The central idea in our correlated UCB algorithm is that after pulling the optimal arm k * sufficiently large number of times, the non-competitive (and thus sub-optimal) arms can be classified as empirically non-competitive with increasing confidence, and thus need not be explored. As a result, the non-competitive arms will only be pulled only O(1) times. However, the competitive arms cannot be discerned as sub-optimal by just using the rewards observed from the optimal arm, and have to be explored Θ(log T ) times each. Thus, we are able to reduce a K-armed bandit to a C + 1-armed bandit problem, where C is the number of competitive arms.</p><p>Using this idea, our C-UCB algorithm proceeds as follows. After every round t, we maintain values for empirical reward, μk (t), and the UCB1 index I k (t) for each arm k. These empirical estimates are based on the n k (t) samples of rewards that have been observed for k till round t. In addition to this, we maintain empirical pseudo-reward of arm with respect to arm k, φ ,k (t), for all pairs of arms ( , k). In each round t, the algorithm performs the following steps:</p><p>1. Select arm k max = arg max k n k (t -1), that has been pulled the most until round t -1.</p><p>2. Identify the set A of arms that are empirically competitive with respect to arm k max .</p><p>3. Pull the arm k t ∈ {A ∪ k max } with the highest UCB1 index I k (t -1) (defined in ( <ref type="formula" target="#formula_5">6</ref>)).</p><p>4. Update the empirical pseudo-rewards s ,kt for all , the empirical reward φ ,kt (t), and the UCB1 indices of all arms based on the observed reward r t .</p><p>In step 1, we choose the arm that has been pulled the most number of times because we have the maximum number of reward samples from this arm. Thus, it is likely to most accurately identify the non-competitive arms. This property enables the proposed algorithm to achieve an O(1) regret contribution from non-competitive arms as we show in Section 4 below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Regret Analysis and Bounds</head><p>We now characterize the performance of the C-UCB algorithm by analyzing the expected value of the cumulative regret (Definition 1). The expected regret can be expressed as</p><formula xml:id="formula_14">E [Reg(T )] = K k=1 E [n k (T )] ∆ k ,<label>(11)</label></formula><p>where</p><formula xml:id="formula_15">∆ k = E [g k * (X)] -E [g k (X)] = µ k * -µ k</formula><p>is the sub-optimality gap of arm k with respect to the optimal arm k * , and n k (T ) is the number of times arm k is pulled in T slots.</p><p>For the regret analysis, we assume without loss of generality that the reward functions g k (X) satisfy 0 ≤ g k (X) ≤ 1 for all k ∈ {1, 2, . . . K}. Note that the C-UCB algorithm does not require this condition on g k (X), and the regret analysis can also be generalized to any bounded reward functions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Instance-Dependent Bounds</head><p>Most works on multi-armed bandits derive two types of bounds on expected regret: instancedependent and worst case bounds, depending on whether or not the minimum sub-optimality gap ∆ min goes to 0 with the total number of rounds T . Our instance-dependent bounds assume that the minimum gap ∆ min = min k ∆ k remains strictly positive as the number of rounds T → ∞, which is generally true in practice. Worst-case bounds are required when ∆ min can be arbitrarily small for large T . We derive both these bounds for the correlated-UCB algorithm. We use the standard Landau notation in the results, where all asymptotic statements are for large T . The proofs of all the results presented below are deferred to the supplement.</p><p>In order to bound E [Reg(T )] in (11), we can analyze the expected number of times suboptimal arms are pulled, that is, E [n k (T )], for all k = k * . Theorem 1 and Theorem 2 below show that E [n k (T )] scales as O(1) and O(log T ) for non-competitive and competitive arms respectively. Recall that a sub-optimal arm is said to be non-competitive if its pseudo-gap ∆k,k * &gt; 0, and competitive otherwise.</p><p>Theorem 1 (Expected Pulls of a Non-competitive Arm). If the pseudo-gap ∆k,k * ≥ 2 2K log t 0 t 0 , and the sub-optimality gap ∆ min ≥ 4 K log t 0 t 0 for some constant t 0 &gt; 0 then</p><formula xml:id="formula_16">E [n k (T )] ≤ Kt 0 + K(K -1) T t=Kt 0 3 t K -2 + T t=1 t -3 , (<label>12</label></formula><formula xml:id="formula_17">) = O(1). (<label>13</label></formula><formula xml:id="formula_18">)</formula><p>Theorem 2 (Expected Pulls of a Competitive Arm). Expected number of times a competitive arm is pulled can be bounded as</p><formula xml:id="formula_19">E [n k (T )] ≤ 8 log(T ) ∆ 2 k + 1 + π 2 3 + T t=1 t exp - t∆ 2 min 2K ,<label>(14)</label></formula><formula xml:id="formula_20">= O(log T ) if ∆ min = min k ∆ k &gt; 0.<label>(15)</label></formula><p>Substituting the bounds on E [n k (T )] derived in Theorem 1 and Theorem 2 into (11), we get the following upper bound on expected regret.</p><p>Theorem 3 (Upper Bound on Expected Regret). If the minimum sub-optimality gap ∆ min ≥ 4 K log t 0 t 0 , and the pseudo-gap of non-competitive arms ∆k,k * ≥ 2 2K log t 0 t 0 for some constant t 0 &gt; 0, then the expected cumulative regret of the C-UCB algorithm is</p><formula xml:id="formula_21">E [Reg(T )] ≤ k∈C ∆ k U (c) k (T ) + k ∈{1,...,K}\{C∪k * } ∆ k U (nc) k (T ),<label>(16)</label></formula><formula xml:id="formula_22">= C • O(log T ) + O(1),<label>(17)</label></formula><p>where C ⊆ {1, . . . , K} \ {k * } is set of competitive arms with cardinality C, U Remark 2. If the set of competitive arms C is empty (i.e., the number of competitive arms C = 0), then our algorithm will lead to (see ( <ref type="formula" target="#formula_22">17</ref>)) an expected regret of O(1), instead of the typical O(log T ) regret scaling in classic multi-armed bandits. A simple case where C is empty is when the reward function g k * (X) corresponding to the arm k * is invertible. This is because, for all sub-optimal arms = k * , the pseudo-gap ∆ ,k * = ∆ &gt; 0, resulting in those arms being non-competitive. The set C can be empty in more general cases where none of the arms are invertible. Then, our algorithm still achieves an expected regret of O(1).</p><p>Remark 3. For the UCB1 algorithm <ref type="bibr" target="#b4">(Auer et al., 2002)</ref>, the first sum in ( <ref type="formula" target="#formula_21">16</ref>) is taken over all arms. In this sense, our C-UCB algorithm is able to reduce a K-armed bandit problem to a C + 1-armed bandit problem.</p><p>Next, we present a lower bound on the expected regret E [Reg(T )]. Intuitively, if an arm is competitive, it can not be deemed sub-optimal by only pulling the optimal arm k * infinitely many times. This indicates that exploration is necessary for competitive arms. The proof of this bound closely follows that of the 2-armed classical bandit problem <ref type="bibr" target="#b14">(Lai and Robbins, 1985)</ref>; i.e., we construct a new bandit instance under which a previously sub-optimal arm becomes optimal without affecting reward distribution of any other arm.</p><p>Theorem 4 (Lower Bound on Expected Regret). For any algorithm that achieves a subpolynomial regret,</p><formula xml:id="formula_23">lim T →∞ inf E [Reg(T )] log(T ) ≥    max k∈C ∆ k D(f R k ||f Rk ) if C &gt; 0, 0 if C = 0.<label>(18)</label></formula><p>Here f R k is the reward distribution of arm k, which is linked with f X since R k = g k (X). The term f Rk represents the reward distribution of arm k in the new bandit instance where arm k becomes optimal and distribution f R k * is unaffected. The divergence term represents "the amount of distortion needed in f X to make arm k optimal", and hence captures the problem difficulty in the lower bound expression.</p><p>Remark 4. From Theorem 3, we see that whenever C &gt; 0, our proposed algorithm achieves O(log T ) regret matching the lower bound given in Theorem 4 order-wise. Also, when C = 0, our algorithm achieves O(1) regret. Thus, our algorithm achieves bounded regret whenever possible, i.e., when C = 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Worst Case Bound on Expected Regret</head><p>Our instance-dependent bounds assumed that the minimum gap ∆ min ≥ 4 K log t 0 t 0 for some t 0 &gt; 0, with a similar assumption on the pseudo-gap. We now present an upper bound the on expected regret without this assumption, when ∆ k can scale with T and become arbitrarily small as T → ∞.</p><p>Theorem 5 (Worst Case Expected Regret). In the worst case, the expected regret of the C-UCB algorithm is O( T log(T )).</p><p>Note that this worst case regret bound is the same as that obtained for the UCB1 algorithm <ref type="bibr" target="#b4">(Auer et al., 2002)</ref> when the arms are independent. This demonstrates that our algorithm can achieve the same order-wise worst case regret as classic UCB.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Simulation Results</head><p>We now present simulation results for the case where X is a discrete random variable (simulations for continuous X and random vector X are shown in the supplement). We consider the reward functions g 1 (X), g 2 (X) and g 3 (X) shown in Figure <ref type="figure" target="#fig_3">3</ref> for all simulation plots. However, the probability distribution P X = (p x 1 , p x 2 , . . . p x 5 ) of X is different for each of the following cases given below. For each case, Figure <ref type="figure" target="#fig_5">4</ref> shows the cumulative regret versus the number of rounds. The cumulative regret is averaged over 500 simulation runs, and for each run we use the same reward realizations for both the C-UCB and the vanilla UCB1 algorithms. Case 1: No competitive arms. Here, we set P X = (0.1, 0.2, 0.25, 0.25, 0.2). For this probability distribution, arm 1 is optimal, and arms 2 and 3 are non-competitive. Since both arm 2 and arm 3 are noncompetitive, our result from Theorem 1 suggests that regret of C-UCB algorithm should not scale with the number of rounds T . This is supported by our simulation results as well. We see in Figure <ref type="figure" target="#fig_5">4a</ref> that the proposed C-UCB algorithm achieves a constant regret and is significantly superior to the UCB1 algorithm as it is able to exploit the correlation of rewards between the arms.</p><p>Case 2: One competitive arm. Let P X = (0.25, 0.17, 0.25, 0.17, 0.16) which results arm 3 being optimal. Arm 1 is non-competitive while arm 2 is competitive. We expect from our results that number of pulls of arm 1 should not scale with T , while the number of pulls for arm 2 can scale with the T . This phenomenon can be seen in Figure <ref type="figure" target="#fig_5">4b</ref>. The regret of C-UCB algorithm is much smaller than the UCB1 algorithm as C-UCB algorithm is not exploring arm 1. However, the regret scales with the number of rounds T as it is necessary to explore Arm 2.</p><p>Case 3: Two competitive arms. In the last scenario, we set P X = (0.05, 0.3, 0.3, 0.05, 0.3). For this distribution, arm 3 is optimal and arms 1 and 2 are both competitive. Since both arms are competitive, exploration is necessary for both arms. Therefore, as we see in Figure <ref type="figure" target="#fig_5">4c</ref>, the regret obtained under C-UCB and UCB1 are similar and scale with the number of rounds T .   <ref type="figure" target="#fig_3">3</ref>, the cumulative regret of C-UCB is smaller than vanilla-UCB1 in all the three cases above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Concluding Remarks</head><p>This work studies a correlated multi-armed bandit (MAB) framework where the rewards obtained by pulling the K different arms are functions of a common latent random variable X.</p><p>We propose the C-UCB algorithm which achieves significant regret-reduction over the classic UCB. In fact, C-UCB is able to achieve a constant (instead of the standard logarithmic) regret in certain cases. A key idea behind the success of this algorithm is that correlation helps us use reward samples from one arm to generate pseudo-rewards from other arms, thus obviating the need to explore them. We believe that this idea is applicable more broadly to several other sequential decision-making problems. Ongoing work includes generalization of other multi-armed bandit algorithms such as Thompson sampling <ref type="bibr" target="#b2">(Agrawal and Goyal, 2013)</ref>, and understanding the scaling of regret with respect to the number of arms K. Instead of the deterministic reward functions g i (X), we also plan to consider random reward variables Y i , such that the conditional distribution p(Y i |X) is known.</p><p>Appendix B. Simulations for Continuous X and Random Vector X</p><p>In this section we obtained cumulative regret by averaging over 100 simulation runs, for each run we use the same reward realizations for both the C-UCB and UCB1 ( <ref type="bibr" target="#b4">(Auer et al., 2002)</ref>) algorithm. We show these results for continuous X and random vector X.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Continuous Random Variable</head><p>We consider the reward functions g 1 (X), g 2 (X) and g 3 (X) as shown in Figure <ref type="figure" target="#fig_6">5</ref>. Arm 1 corresponds to a Gaussian reward function</p><formula xml:id="formula_24">g 1 (x) = 1 2 √ 2πσ 2 exp -(x-µ) 2 2σ 2</formula><p>, with µ = 0.5 and σ = 0.2. Arm 2 corresponds to g 2 (x) = 1 -exp(-5λx), with λ = 0.5. Arm 3 corresponds to a uniform reward function with g 3 (x) = 0.5. Depending on the distribution of random variable X, we can have different scenarios. For this simulation, we considered three cases with distribution of X as Beta(4, 4), Beta(2, 5) and Beta(1, 5) respectively. Distribution of X for these three cases is shown in Figure <ref type="figure" target="#fig_7">6</ref>.</p><p>Case 1: X ∼ Beta(4, 4). For this case arm 1 is the optimal arm, and arms 2 and 3 are non-competitive. As a result, the regret of C-UCB algorithm does not scale with the number of rounds. Observe that in Figure <ref type="figure" target="#fig_9">7a</ref> the regret of C-UCB algorithm is very small; this is because the pseudo-gap of arms 2 and 3 with respect to arm 1 in this setting are large and hence sub-optimal arms are pulled very few times as they are easily identified as sub-optimal through pulls of Arm 1. This also demonstrates a case where sub-optimal arms are non-nompetitive even though the optimal arm is non-invertible.</p><p>Case 2: X ∼ Beta(2, 5). In this scenario arm 1 is the optimal arm, arm 2 is competitive and arm 3 is non-competitive. Due to this, C-UCB algorithm still explores arm 2. As evident in Figure <ref type="figure" target="#fig_9">7b</ref>, C-UCB clearly outperforms the UCB1 algorithm. This is because C-UCB algorithm explores only arm 2, while UCB1 explores both arm 1 and arm 2.</p><p>Case 3: X ∼ Beta(1, 5). In this case, arm 3 is the optimal arm. Since pulls of arm 3 provide no information about reward from Arm 1 and Arm 2, both Arm 1 and Arm 2 are Competitive. Due to this C-UCB algorithm explores both the arms and has a performance very similar to the UCB1 algorithm as shown in Figure <ref type="figure" target="#fig_9">7c</ref>.   (b) Arm 1 is optimal, arm 2 is competitive and arm 3 is noncompetitive.</p><p>(c) Arm 3 is optimal and arm 1, 2 are competitive. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Latent Random Vector X</head><p>We now consider a case where we have a random vector X = (X 1 , X 2 ). In our setting X 1 , X 2 have a support of {-1, 0, 1}. We consider two arms with g 1 (X) = X 1 + X 2 and g 2 (X) = X 1 -X 2 . In this example s 2,1 (r) &gt; g 1 (r) only if the observed reward r = 2, which corresponds to the case where the realization (X 1 , X 2 ) can be identified as (1, 1). Similarly s 1,2 (r) &gt; g 2 (r) only if the observed reward r = 2, which corresponds to the realization (1, -1).</p><p>Depending on the distribution of X, suboptimal arm can be competitive or non-competitive.</p><p>Case 1: Suboptimal arm is Competitive. We consider a case where P X = P X 1 P X 2 , with P X 1 = {0.3, 0.4, 0.3} and P X 2 = {0.38, 0.22, 0.4}. In this scenario Arm 1 is optimal and sub-optimality gap of arm2 is ∆ 2 = 0.04. Since the probability mass on (1, 1) is small, Arm 2 is Competitive. Due to this, we see in Figure <ref type="figure" target="#fig_11">8a</ref> that regret of the C-UCB algorithm scales with number of rounds T and has a performance very similar to the UCB1 algorithm.</p><p>Case 2: Suboptimal arm is Non-Competitve We consider the distribution P X with P X (1, -1) = 0.48, P X (1, 1) = 0.5 and P X (x 1 , x 2 ) = 0.0028 for all other x 1 , x 2 . In this scenario, arm 1 is optimal and arm 2 is sub-optimal with suboptimality gap ∆ 2 = 0.04. Since probability mass at (1, 1) is high, it is possible to infer sub-optimality of arm 2 using reward samples of arm 1. We see this effect in Figure <ref type="figure" target="#fig_11">8b</ref>.  Lemma 1 (Standard result used in bandit literature). If μk,n k (t) denotes the empirical mean of arm k by pulling arm k n k (t) times through any algorithm and µ k denotes the mean reward of arm k, then we have</p><formula xml:id="formula_25">Pr μk,n k (t) -µ k ≥ , τ 2 ≥ n k (t) ≥ τ 1 ≤ τ 2 s=τ 1 exp -2s 2 .</formula><p>Proof. Let Z 1 , Z 2 , ...Z t be the reward samples of arm k drawn separately. If the algorithm chooses to play arm k for m th time, then it observes reward Z m . Then the probability of observing the event μk,n k (t) -µ k ≥ , τ 2 ≥ n k (t) ≥ τ 1 can be upper bounded as follows,</p><formula xml:id="formula_26">Pr μk,n k (t) -µ k ≥ , τ 2 ≥ n k (t) ≥ τ 1 = Pr n k (t) i=1 Z i n k (t) -µ k ≥ , τ 2 ≥ n k (t) ≥ τ 1 (19) ≤ Pr τ 2 m=τ 1 m i=1 Z i m -µ k ≥ , τ 2 ≥ n k (t) ≥ τ 1 (20) ≤ Pr τ 2 m=τ 1 m i=1 Z i m -µ k ≥ (21) ≤ τ 2 s=τ 1 exp -2s 2 . (<label>22</label></formula><formula xml:id="formula_27">)</formula><p>Lemma 2 (From Proof of Theorem 1 in <ref type="bibr" target="#b4">(Auer et al., 2002)</ref>). Let I k (t) denote the UCB index of arm k at round t, and µ k = E [g k (X)] denote the mean reward of that arm. Then, we have</p><formula xml:id="formula_28">Pr(µ k &gt; I k (t)) ≤ t -3 .</formula><p>Observe that this bound does not depend on the number n k (t) of times arm k is pulled. UCB index is defined in equation ( <ref type="formula" target="#formula_5">6</ref>) of the main paper.</p><p>Proof. This proof follows directly from <ref type="bibr" target="#b4">(Auer et al., 2002)</ref>. We present the proof here for completeness as we use this frequently in the paper.</p><formula xml:id="formula_29">Pr(µ k &gt; I k (t)) = Pr µ k &gt; μk,n k (t) + 2 log t n k (t)<label>(23)</label></formula><formula xml:id="formula_30">≤ t m=1 Pr µ k &gt; μk,m + 2 log t m (24) = t m=1 Pr μk,m -µ k &lt; - 2 log t m (25) ≤ t m=1 exp -2m 2 log t m (26) = t m=1 t -4 (27) = t -3 . (<label>28</label></formula><formula xml:id="formula_31">)</formula><p>where ( <ref type="formula">24</ref>) follows from the union bound and is a standard trick (Lemma 1) to deal with random variable n k (t). We use this trick repeatedly in the proofs. We have (26) from the Hoeffding's inequality.</p><formula xml:id="formula_32">Lemma 3. Let E 1 I k &gt;I k * be the expected number of times I k (t) &gt; I k * (t) in T rounds.</formula><p>Then, we have</p><formula xml:id="formula_33">E 1 I k &gt;I k * = T t=1 P r(I k &gt; I k * ) ≤ 8 log(T ) ∆ 2 k + 1 + π 2 3 .</formula><p>The proof follows the analysis in Theorem 1 of <ref type="bibr" target="#b4">(Auer et al., 2002)</ref>. The analysis of P r(I k &gt; I k * ) is done by conditioning on the event that Arm k has been pulled 8 log(T )</p><formula xml:id="formula_34">∆ 2 k . Conditioned on this event, P r(I k (t) &gt; I k * (t)|n k (t)) ≤ t -2 .</formula><p>Lemma 4 (Theorem 2 (Lai and Robbins, 1985)). Consider a two armed bandit problem with reward distributions Θ = {f R 1 (r), f R 2 (r)}, where the reward distribution of the optimal arm is f R 1 (r) and for the sub-optimal arm is f R 2 (r), and <ref type="bibr" target="#b14">(Lai and Robbins, 1985)</ref>), then for any policy that achieves sub-polynomial regret, we have</p><formula xml:id="formula_35">E [f R 1 (r)] &gt; E [f R 2 (r)]; i.e., arm 1 is optimal. If it is possible to create an alternate problem with distributions Θ = {f R 1 (r), fR 2 (r)} such that E fR 2 (r) &gt; E [f R 1 (r)] and 0 &lt; D(f R 2 (r)|| fR 2 (r)) &lt; ∞ (equivalent to assumption 1.6 in</formula><formula xml:id="formula_36">lim inf T →∞ E [n 2 (T )] log T ≥ 1 D(f R 2 (r)|| fR 2 (r))</formula><p>.</p><p>Proof. Proof of this is derived from the analysis done in <ref type="bibr">(Lattimore)</ref>. We show the analysis here for completeness. A bandit instance v is defined by the reward distribution of arm 1 and arm 2. Since policy π achieves sub-polynomial regret, for any instance v, E v,π [(Reg(T ))] = O(T p ) as T → ∞, for all p &gt; 0.</p><p>Consider the bandit instances</p><formula xml:id="formula_37">Θ = {f R 1 (r), f R 2 (r)}, Θ = {f R 1 (r), fR 2 (r)},</formula><p>where</p><formula xml:id="formula_38">E [f R 2 (r)] &lt; E [f R 1 (r)] &lt; E fR 2 (r) .</formula><p>The bandit instance Θ is constructed by changing the reward distribution of arm 2 in the original instance, in such a way that arm 2 becomes optimal in instance Θ without changing the reward distribution of arm 1 from the original instance.</p><p>From divergence decomposition lemma (derived in <ref type="bibr">(Lattimore)</ref>), it follows that</p><formula xml:id="formula_39">D(P Θ,Π ||P Θ ,Π ) = E Θ,π [n 2 (T )] D(f R 2 (r)|| fR 2 (r)).</formula><p>The high probability Pinsker's inequality (Lemma 2.6 from <ref type="bibr" target="#b26">(Tsybakov, 2008)</ref>, originally in <ref type="bibr" target="#b6">(Bretagnolle and Huber, 1979)</ref>) gives that for any event A,</p><formula xml:id="formula_40">P Θ,π (A) + P Θ ,π (A c ) ≥ 1 2 exp -D(P Θ,π ||P Θ ,π ) ,</formula><p>or equivalently,</p><formula xml:id="formula_41">D(P Θ,π ||P Θ ,π ) ≥ log 1 2(P Θ,π (A) + P Θ ,π (A c ))</formula><p>.</p><formula xml:id="formula_42">If arm 2 is suboptimal in a 2-armed bandit problem, then E [Reg(T )] = ∆ 2 E [n 2 (T )] . Expected regret in Θ is E Θ,π [Reg(T )] ≥ T ∆ 2 2 P Θ,π n 2 (T ) ≥ T 2 ,</formula><p>Similarly regret in bandit instance Θ is</p><formula xml:id="formula_43">E Θ ,π [Reg(T )] ≥ T δ 2 P Θ ,π n 2 (T ) &lt; T 2 , since suboptimality gap of arm 1 in Θ is δ. Define κ(∆ 2 , δ) = min(∆ 2 ,δ)</formula><p>2 . Then we have,</p><formula xml:id="formula_44">P Θ,π n 2 (T ) ≥ T 2 + P Θ ,π n 2 (T ) &lt; T 2 ≤ E Θ,π [Reg(T )] + E Θ ,π [Reg(T )] κ(∆ 2 , δ)T .</formula><p>On applying the high probability Pinsker's inequality and divergence decomposition lemma stated earlier, we get</p><formula xml:id="formula_45">D(f R 2 (r)|| fR 2 (r))E Θ,π [n 2 (T )] ≥ log κ(∆ 2 , δ)T 2(E Θ,π [Reg(T )] + E Θ ,π [Reg(T )]) (29) = log κ(∆ 2 , δ) 2 + log(T ) -log(E Θ,π [Reg(T )] + E Θ ,π [Reg(T )]).<label>(30)</label></formula><p>Since policy π achieves sub-polynomial regret for any bandit instance, E Θ,π [Reg(T )] + E Θ ,π [Reg(T )] ≤ γT p for all T and any p &gt; 0, hence,</p><formula xml:id="formula_46">lim inf T →∞ D(f R 2 (r)|| fR 2 (r)) E Θ,π [n 2 (T )] log T ≥ 1 -lim sup T →∞ E Θ,π [Reg(T )] + E Θ ,π [Reg(T )] log T + lim inf T →∞ log κ(∆ 2 ,δ) 2 log T (31) = 1.<label>(32)</label></formula><p>Hence, lim inf</p><formula xml:id="formula_47">T →∞ E Θ,π [n 2 (T )] log T ≥ 1 D(f R 2 (r)|| fR 2 (r)) .</formula><p>Appendix D. Lemmas Required to Prove Theorems 1, 2, 3, and 5</p><p>Lemma 5. Define E 1 (t) to be the event that arm k * is empirically non-competitive in round t + 1, then,</p><formula xml:id="formula_48">Pr(E 1 (t)) ≤ t exp -t∆ 2 min 2K ,</formula><p>where ∆ min = min k ∆ k , the gap between the best and second-best arms.</p><p>Proof. We analyze the probability that arm k * is empirically non competitive by conditioning on the event that arm k * is not pulled for maximum number of times till round t. Analyzing this expression gives us,</p><formula xml:id="formula_49">Pr(E 1 (t)) = Pr(E 1 (t), n k * (t) = max k n k (t)) (33) = k =k * P r(E 1 (t), n k (t) = max k n k (t)) (34) ≤ max k Pr(E 1 (t), n k (t) = max k n k (t)) (35) = max k Pr(μ k &gt; φk * ,k , n k (t) = max k n k (t)) (36) ≤ max k Pr μk &gt; φk * ,k , n k (t) ≥ t K (37) = max k Pr t τ =1 1 {kτ =k} r τ n k (t) &gt; t τ =1 1 {kτ =k} s k * ,k (r τ ) n k (t) , n k (t) ≥ t K (38) = max k Pr t τ =1 1 {kτ =k} (r τ -s k * ,k (r τ )) n k (t) &gt; 0, n k (t) ≥ t K (39) = max k Pr t τ =1 1 {kτ =k} (r τ -s k * ,k (r τ )) n k (t) -(µ k -φ k * ,k ) &gt; φ k * ,k -µ k , n k (t) ≥ t K (40) ≤ max k Pr t τ =1 1 {kτ =k} (r τ -s k * ,k (r τ )) n k (t) -(µ k -φ k * ,k ) &gt; ∆ k , n k (t) ≥ t K (41) ≤ max k t exp -t∆ 2 k 2K (42) = t exp -t∆ 2 min 2K ,<label>(43)</label></formula><p>Here ( <ref type="formula">36</ref>) follows from the fact that in order for arm k * to be empirically non-competitive, empirical mean of arm k should be more than empirical pseudo-reward of arm k * with respect to arm k. Inequality (37) follows since n k (t) being more than t K is a necessary condition for n k (t) = max k n k (t) to occur. We have (41) as s k * ,k is more than µ k * . We have (42) from the Hoeffding's inequality, as we note that rewards {r τ -s k * ,k (r τ ) : τ = 1, . . . , t, k τ = k} form a collection of i.i.d. random variables each of which is bounded between [-1, 1] with mean (µ k -φ k * ,k ). The term t before the exponent in (42) arises as the random variable n k (t) can take values from t/K to t (Lemma 1). Lemma 6. If ∆ min ≥ 4 K log t 0 t 0 for some constant t 0 &gt; 0, then,</p><formula xml:id="formula_50">Pr(k t+1 = k, n k (t) ≥ s) ≤ 3t -3 for s &gt; t 2K , ∀t &gt; t 0 .</formula><p>Proof. By noting that k t+1 = k corresponds to arm k having the highest index among the set of arms that are not empirically non-competitive (denoted by A), we have,</p><formula xml:id="formula_51">Pr(k t+1 = k, n k (t) ≥ s) = Pr(I k (t) = arg max k ∈A I k (t), n k (t) ≥ s) (44) ≤ Pr(E 1 (t) ∪ (E c 1 (t), I k (t) &gt; I k * (t)) , n k (t) ≥ s) (45) ≤ Pr(E 1 (t), n k (t) ≥ s) + Pr(E c 1 (t), I k (t) &gt; I k * (t), n k (t) ≥ s) (46) ≤ t exp -t∆ 2 min 2K + Pr (I k (t) &gt; I k * (t), n k (t) ≥ s) .<label>(47)</label></formula><p>Here E 1 (t) is the event described in Lemma 5. If arm k * is not empirically non-competitive at round t, then arm k can only be pulled in round t + 1 if I k (t) &gt; I k * (t), due to which we have (45). Inequalities ( <ref type="formula">46</ref>) and ( <ref type="formula" target="#formula_51">47</ref>) follow from union bound and Lemma 5 respectively.</p><p>Moreover, if ∆k,k * ≥ 2 2K log t 0 t 0 for some constant t 0 &gt; 0. Then,</p><formula xml:id="formula_52">Pr(k t+1 = k, n k * (t) = max k n k ) ≤ t -3 ∀t &gt; t 0 .</formula><p>Proof. We now bound this probability as,</p><formula xml:id="formula_53">Pr(k t+1 = k, n k * = max k n k ) = Pr μk * (t) &lt; φk,k * (t), I k (t) = max k I k (t), n k * (t) = max k n k (t) (61) ≤ Pr μk * (t) &lt; φk,k * (t), n k * (t) = max k n k (t) (62) ≤ Pr μk * (t) &lt; φk,k * (t), n k * (t) ≥ t K (63) ≤ Pr t τ =1 1 {kτ =k * } r τ n k * (t) &lt; t τ =1 1 {kτ =k * } s k,k * (r τ ) n k * (t) , n k * (t) ≥ t K (64) = Pr t τ =1 1 {kτ =k * } (r τ -s k,k * ) n k * (t) -(µ k * -φ k,k * ) &lt; -∆k,k * , n k * ≥ t K (65) ≤ t exp -t ∆2 k,k * 2K (66) ≤ t -3 ∀t &gt; t 0 .<label>(67)</label></formula><p>Here, (65) follows from the Hoeffding's inequality as we note that rewards {r τ -s k,k * (r τ ) : τ = 1, . . . , t, k τ = k} form a collection of i.i.d. random variables each of which is bounded between [-1, 1] with mean (µ k -φ k,k * ). The term t before the exponent in (65) arises as the random variable n k (t) can take values from t/K to t (Lemma 1). Step (67) follows from the fact that ∆k,k * ≥ 2 2K log t 0 t 0 for some constant t 0 &gt; 0.</p><formula xml:id="formula_54">Lemma 8. If ∆ min ≥ 4 K log t 0 t 0 for some constant t 0 &gt; 0, then, Pr n k (t) &gt; t K ≤ 3K t K -2 ∀t &gt; Kt 0 .</formula><p>Proof. We expand Pr n k (t) &gt; t K as,</p><formula xml:id="formula_55">Pr n k (t) ≥ t K = Pr n k (t) ≥ t K | n k (t -1) ≥ t K Pr n k (t -1) ≥ t K + Pr k t = k, n k (t -1) = t K -1 (68) ≤ Pr n k (t -1) ≥ t K + Pr k t = k, n k (t -1) = t K -1 (69) ≤ Pr n k (t -1) ≥ t K + 3(t -1) -3 ∀(t -1) &gt; t 0 .<label>(70)</label></formula><p>Here, (70) follows from Lemma 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>This gives us</head><p>Pr</p><formula xml:id="formula_56">n k (t) ≥ t K -Pr n k (t -1) ≥ t K ≤ 3(t -1) -3 , ∀(t -1) &gt; t 0 . Now consider the summation t τ = t K Pr n k (τ ) ≥ t K -Pr n k (τ -1) ≥ t K ≤ t τ = t K 3(τ -1) -3 .</formula><p>This gives us,</p><formula xml:id="formula_57">Pr n k (t) ≥ t K -Pr n k t K -1 ≥ t K ≤ t τ = t K 3(τ -1) -3 . Since Pr n k t K -1 ≥ t K = 0, we have, Pr n k (t) ≥ t K ≤ t τ = t K 3(τ -1) -3 (71) ≤ 3K t K -2 ∀t &gt; Kt 0 .<label>(72)</label></formula><p>Appendix E. Proofs of Instance Dependent Bounds (Theorem 1,2,3)</p><formula xml:id="formula_58">Proof of Theorem 1 We bound E [n k (T )] as, E [n k (T )] = E T t=1 1 {kt=k} (73) = T -1 t=0 Pr(k t+1 = k) (74) = Kt 0 t=1 Pr(k t = k) + T -1 t=Kt 0 Pr(k t+1 = k) (75) ≤ Kt 0 + T -1 t=Kt 0 Pr(k t+1 = k, n k * (t) = max k n k (t))+ T -1 t=Kt 0 k =k * Pr(n k (t) = max k n k (t)) Pr(k t+1 = k|n k (t) = max k n k (t))<label>(76)</label></formula><formula xml:id="formula_59">≤ Kt 0 + T -1 t=Kt 0 Pr(k t+1 = k, n k * (t) = max k n k (t))+ T -1 t=Kt 0 k =k * Pr(n k (t) = max k n k (t))<label>(77)</label></formula><formula xml:id="formula_60">≤ Kt 0 + T -1 t=Kt 0 t -3 + T t=Kt 0 k =k * Pr n k (t) ≥ t K (78) ≤ Kt 0 + T t=1 t -3 + K(K -1) T t=Kt 0 3 t K -2 . (<label>79</label></formula><formula xml:id="formula_61">)</formula><p>Here, (78) follows from Lemma 7 and (79) follows from Lemma 8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proof of Theorem 2</head><p>For any suboptimal arm k</p><formula xml:id="formula_62">= k * , E [n k (T )] ≤ T t=1 Pr(k t = k) (80) ≤ T t=1</formula><p>Pr(E 1 (t) ∪ (E c 1 (t),</p><formula xml:id="formula_63">I k &gt; I k * ))<label>(81)</label></formula><p>≤ T t=1</p><p>Pr(E 1 (t)) + Pr(E c 1 (t), I k (t -1) &gt; I k * (t -1)) (82)</p><formula xml:id="formula_64">E [n k (T )] ≤ T t=1</formula><p>Pr(E 1 (t)) + Pr(E c 1 (t), I k (t -1) &gt; I k * (t -1))</p><formula xml:id="formula_65">≤ T t=1</formula><p>Pr(E 1 (t)) + Pr(I k (t -1) &gt; I k * (t -1)) Therefore, if these are the only two arms in our problem, then from Lemma 4, lim</p><formula xml:id="formula_67">T →∞ inf E [n k (T )] log T ≥ 1 D(f R k (r)||f Rk (r))</formula><p>.</p><p>Moreover, if we have more K -1 sub-optimal arms, instead of just 1, then lim</p><formula xml:id="formula_68">T →∞ inf E =k * n (T ) log T ≥ 1 D(f R k (r)||f Rk (r))</formula><p>. </p><p>Proof. Expanding µ k -µ k gives us</p><formula xml:id="formula_70">µ k -µ k = µ k -µ 1 - k =2 (µ -µ -1 ) (94) = ∆ k - k =2 (µ -µ -1 ) (95) = ∆ k 1 - k =2 (µ -µ -1 ) ∆ k (96) ≥ ∆ k 1 - 3 α (97) = γ∆ k .<label>(98)</label></formula><p>Here, (97) follows from the fact that ∆ k ≥ α K log T T . Since ≤ k , we also have,</p><formula xml:id="formula_71">µ k -µ ≥ µ k -µ k ≥ γ∆ k ∀ ≤ k . Lemma 13. If ∆ k &gt; α K log T T</formula><p>for some α &gt; 3K, then</p><formula xml:id="formula_72">E [n k (T )] ≤ β log T ∆ 2 k</formula><p>, for some β &gt; 0.</p><p>Proof. From Lemma 11 there exists an arm ( ≤ k) such that µ -µ -1 ≥ 3 K log T T .</p><p>Denote k to be the minimum such that µ -µ -1 ≥ 3 K log T T . Then we have,</p><formula xml:id="formula_73">E [n k (T )] ≤ T t=1</formula><p>Pr(k t = k) (99)</p><formula xml:id="formula_74">≤ T t=1</formula><p>Pr (E c 1 (t), I k &gt; I 1 ) (E 1 (t), E c 2 (t), I k &gt; I 2 ) . . .  </p><formula xml:id="formula_75">(</formula><formula xml:id="formula_76">≤ K 8 log T (γ∆ k ) 2 + 1 + π 2 3 + K T t=1 t -2 (108) ≤ K 8 log T (γ∆ k ) 2 + 1 + π 2 3 + K 1 + π 2 3 (109) ≤ β log T ∆ 2 k for some β &gt; 0,<label>(110)</label></formula><p>where (105) follows from Lemma 3. We have (106) from Lemma 12. Inequality (107) follows from Lemma 10 and (109) follows from the fact that ∞ t=1 t -2 = 1 + π 2 3 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proof of Theorem 5</head><p>From Lemma 13, we have E [n k (T )] &gt; β log(T )</p><formula xml:id="formula_77">∆ 2 k if ∆ k &gt; ∆ = 3K K log T T</formula><p>for some β &gt; 0. Using this we can write,  <ref type="bibr">(115)</ref> </p><formula xml:id="formula_78">E [Reg(T )] = k =k * ∆ k E [n k (T )] (111) = k:∆ k &lt;∆ ∆ k E [n k (T )] + k:∆ k &gt;∆ ∆ k E [n k (T )]<label>(112</label></formula></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Example of two arms.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>is the upper bound on E [n k (T )] for competitive arms given in (14), and U (nc) k (T ) is the upper bound for non-competitive arms given in (12).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Reward Functions used for the simulation results presented in Figure 4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure4: For the reward functions in Figure3, the cumulative regret of C-UCB is smaller than vanilla-UCB1 in all the three cases above.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Reward Functions used for the simulation results presented in Figure 7.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Distribution of X for the three cases of simulation results presented in Figure 7.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>(a) Arm 1 is optimal, other two are non-competitive.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Simulation results for continuous X.</figDesc><graphic coords="16,90.00,96.84,142.54,87.15" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>Sub-optimal arm is non-competitive.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Simulation results for latent vector X.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>84) follows from Lemma 5. We have (85) from the definition of E n I k &gt;I k * (T ) in Lemma 3, and (86) follows from Lemma 3.Proof of Theorem 3: Follows directly by combining the results on Theorem 1 and Theorem 2.Note that such a construction of P X (x) does not change the reward distribution of Arm k * . Moreover E Rk ≥ (1 -)φ k,k * (since rewards are always non-negative). Since ∆k,k * &lt; 0 we can always choose &gt; 0 such that (1-)φ k,k * -E [R k * ] &gt; 0 and subsequently, E Rk -E [R k * ] &gt; 0.Proof of Theorem 4Let arm k be a Competitive sub-optimal arm, i.e ∆k,k * &lt; 0. Since ∆k,k * &lt; 0, From Lemma 9, it is possible to change the distribution of R k such that E Rk &gt; E [R k * ] and reward distribution of arm k * is unaffected, i.e f Rk * (r) = f R k * (r). Moreover, by our construction of f Rk (r) in Lemma 9, D(f R k * (r)||f Rk (r)) &lt; ∞.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head></head><label></label><figDesc>R k ||f Rk ) .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>E 1 (t)E 2 (t) . . . E c k-1 (t), I k &gt; I k-1 ) (E 1 (t), E 2 (t) . . . E k-1 (t)) &gt; I 1 ) + Pr E 1 (t) Pr I k &gt; I 2 |E 1 (t) + . . . Pr E 1 (t), E 2 (t), . . . E k-2 (t) Pr I k &gt; I k-1 |E 1 (t), E 2 (t) . . . E k-2 (t) +</figDesc><table><row><cell></cell><cell>(100)</cell></row><row><cell>T</cell><cell></cell></row><row><cell>≤</cell><cell>Pr(I</cell></row><row><cell>t=1</cell><cell></cell></row></table><note><p><p><p>k Pr E 1 (t), E 2 (t) . . . E k-1 (t)</p>(101)</p>(102)</p></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Appendix A. Continuous X and Random Vector X = (X 1 , X 2 , . . . X m )</p><p>Observe that our algorithm depends on the functions g i (X) through the evaluation of pseudorewards (see Definition 2). For discrete X, the set {x : g k (x) = r} is a discrete set with a finite number of elements. Hence, it is easy to evaluate max {x:g k (x)=r} g (x) for any arm = k. For continuous X, if {x : g k (x) = r} is a finite union of continuous sets, and if g (x) has finite stationary points, then it is possible to evaluate g (x) for x that lie at the boundary of continuous sets and at stationary points lying within these sets. Therefore, it is possible to compute max {x:g k (x)=r} g (x).</p><p>The algorithm and the regret analysis is also applicable to more general random sources, such as a latent random vector X = (X 1 , X 2 , . . . X m ). For example, if X = (X 1 , X 2 ) is a random variable, and g 1 (X) = X 1 + 0.1X 2 and g 2 (X) = X 2 + 0.1X 1 . Then evaluating the pseudo-reward of arm 2 with respect to arm 1 on observing reward r reduces to solving an optimization problem</p><p>where, W 1 , W 2 are support of X 1 and X 2 respectively.</p><p>As mentioned in Remark 1, this also captures the case of classical multi-armed bandit problem, if X = (X 1 , X 2 , . . . X n ), where X i are independent random variables and g k (X) = X k for k ∈ {1, 2, . . . K}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix C. Standard Results from Previous Works</head><p>Fact 1 (Hoeffding's inequality). Let Z 1 , Z 2 . . . Z n be i.i.d random variables bounded between [a, b] : a ≤ Z i ≤ b, then for any δ &gt; 0, we have</p><p>We now bound the second term in (47).</p><p>≤ 2t -3 for all t &gt; t 0 .</p><p>(57)</p><p>We have (48) holds because of the fact that P (A) = P (A|B)P (B)+P (A|B c )P (B c ), Inequality (50) follows from Lemma 2. From the definition of I k (t) we have (52). Inequality (55) follows from Hoeffding's inequality and the term t before the exponent in (42) arises as the random variable n k (t) can take values from s to t (Lemma 1). Inequality (57) follows from the fact that s &gt; t 2K and ∆ k ≥ 4 K log t 0 t 0 for some constant t 0 &gt; 0. Plugging this in the expression of Pr(</p><p>Here, (60) follows from the fact that ∆ min ≥ 2 2K log t 0 t 0 for some constant t 0 &gt; 0.</p><p>Lemma 7. If for a suboptimal arm k = k * , ∆k,k * &gt; 0, then,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix F. Lower bound proofs</head><p>For these proofs we define R k = g k (X) and Rk = g k ( X), where f X (x) is the probability density function of random variable X and f X (x) is the probability density function of random variable X.</p><p>Lemma 9. If arm k is competitive, i.e., ∆k,k * &lt; 0, then there exists f X (x) such that</p><p>Proof. Informally the statement means that if there exists an arm k such that Pseudo-Gap of arm k with respect to arm k * is less than 0, then it is possible to change the distribution of random variable X from f X (x) to f X (x) such that reward distribution of arm k * remains unchanged, but arm k becomes better than k * in terms of expected reward. We now prove this statement for the case when X is a discrete random variable. A similar argument can be made to generalize the result for continuous X. If P X is the original distribution of X, we show how to create a distribution</p><p>Let B denote the set of values taken by g k * (X), then for all r ∈ B, we define P X (x) as</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix G. Proof of Worst Case Regret Bound</head><p>In this section, without loss of generality we assume that Arm 1 is optimal, and µ 1 &gt; µ 2 &gt; µ 3 &gt; µ 4 . . . &gt; µ K . Correspondingly, we define the event E i (t) to denote that arm i was empirically non-competitive in round t + 1. Note that this notation is consistent with the definition of E 1 (t) in Lemma 5.</p><p>Lemma 10.</p><p>Proof. We expand Pr(E 1 (t), E 2 (t) . . . E (t)) as,</p><p>Here, (90) follows from the fact that arm 1, 2 . . . can all be empirically non-competitive with respect to arms + 1, + 2 . . . K only. Analysis done in the proof of Lemma 5 gives us (92). </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Taming the monster: A fast and simple algorithm for contextual bandits</title>
		<author>
			<persName><forename type="first">Alekh</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Satyen</forename><surname>Kale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Langford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lihong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">E</forename><surname>Schapire</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning (ICML)</title>
		<meeting>the International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1638" to="1646" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Explore/exploit schemes for web content optimization</title>
		<author>
			<persName><forename type="first">Deepak</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bee-Chung</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pradheep</forename><surname>Elango</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Data Mining, 2009. ICDM&apos;09. Ninth IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Thompson sampling for contextual bandits with linear payoffs</title>
		<author>
			<persName><forename type="first">Shipra</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Navin</forename><surname>Goyal</surname></persName>
		</author>
		<ptr target="http://dl.acm.org/citation.cfm?id=3042817.3043073" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning (ICML)</title>
		<meeting>the International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Global multi-armed bandits with hölder continuity</title>
		<author>
			<persName><forename type="first">Cem</forename><surname>Onur Atan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mihaela</forename><surname>Tekin</surname></persName>
		</author>
		<author>
			<persName><surname>Van Der Schaar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AISTATS</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Finite-time analysis of the multiarmed bandit problem</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>Auer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolo</forename><surname>Cesa-Bianchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Fischer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="235" to="256" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A latent source model for online collaborative filtering</title>
		<author>
			<persName><forename type="first">Guy</forename><surname>Bresler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devavrat</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Estimation des densitiés: risque minimax. z. für wahrscheinlichkeitstheorie und verw</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bretagnolle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Huber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Geb</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="199" to="137" />
			<date type="published" when="1979">1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Regret analysis of stochastic and nonstochastic multi-armed bandit problems</title>
		<author>
			<persName><forename type="first">Sébastien</forename><surname>Bubeck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolo</forename><surname>Cesa-Bianchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends in Machine Learning</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="122" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Minimal exploration in structured stochastic bandits</title>
		<author>
			<persName><forename type="first">Richard</forename><surname>Combes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Magureanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Proutière</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The kl-ucb algorithm for bounded stochastic bandits and beyond</title>
		<author>
			<persName><forename type="first">Aurélien</forename><surname>Garivier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Cappé</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th annual Conference On Learning Theory</title>
		<meeting>the 24th annual Conference On Learning Theory</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="359" to="376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Active distribution learning from indirect samples</title>
		<author>
			<persName><forename type="first">Samarth</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gauri</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Osman</forename><surname>Yağan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.05334</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">On correlation and budget constraints in model-based bandit optimization with application to automatic machine learning</title>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bobak</forename><surname>Shahriari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nando</forename><surname>Freitas</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v33/hoffman14.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<publisher>AISTATS</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="365" to="374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Efficient Redundancy Techniques to Reduce Delay in Cloud Systems</title>
		<author>
			<persName><forename type="first">Gauri</forename><surname>Joshi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016-06">June 2016</date>
		</imprint>
		<respStmt>
			<orgName>Massachusetts Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Regret of queueing bandits</title>
		<author>
			<persName><forename type="first">Subhashini</forename><surname>Krishnasamy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajat</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ramesh</forename><surname>Johari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjay</forename><surname>Shakkottai</surname></persName>
		</author>
		<idno>CoRR, abs/1604.06377</idno>
		<ptr target="http://arxiv.org/abs/1604.06377" />
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Asymptotically efficient adaptive allocation rules</title>
		<author>
			<persName><forename type="first">Tze</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lai</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Herbert</forename><surname>Robbins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in applied mathematics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="4" to="22" />
			<date type="published" when="1985">1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">Tor</forename><surname>Lattimore</surname></persName>
		</author>
		<ptr target="http://banditalgs.com/2016/09/30/instance-dependent-lower-bounds/" />
		<title level="m">Instance dependent lower bounds</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Bounded regret for finite-armed structured bandits</title>
		<author>
			<persName><forename type="first">Tor</forename><surname>Lattimore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rémi</forename><surname>Munos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="550" to="558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Latent bandits</title>
		<author>
			<persName><forename type="first">Odalric-Ambrym</forename><surname>Maillard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shie</forename><surname>Mannor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="136" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A structured multi-armed bandit problem and the greedy policy</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Mersereau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Rusmevichientong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">N</forename><surname>Tsitsiklis</surname></persName>
		</author>
		<idno type="DOI">10.1109/TAC.2009.2031725</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Automatic Control</title>
		<idno type="ISSN">0018-9286</idno>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2787" to="2802" />
			<date type="published" when="2009-12">Dec 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Stochastic scheduling</title>
		<author>
			<persName><forename type="first">J</forename><surname>Nino-Mora</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Encyclopedia of Optimization</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="3818" to="3824" />
		</imprint>
	</monogr>
	<note>2 edition</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Multi-armed bandit problems with dependent arms</title>
		<author>
			<persName><forename type="first">Sandeep</forename><surname>Pandey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deepayan</forename><surname>Chakrabarti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deepak</forename><surname>Agarwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
		<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="721" to="728" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Stochastic contextual bandits with known reward functions</title>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Sakulkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bhaskar</forename><surname>Krishnamachari</surname></persName>
		</author>
		<idno>CoRR, abs/1605.00176</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Identifying best interventions through online importance sampling</title>
		<author>
			<persName><forename type="first">Rajat</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthikeyan</forename><surname>Shanmugam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandros</forename><forename type="middle">G</forename><surname>Dimakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjay</forename><surname>Shakkottai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">stat</title>
		<imprint>
			<biblScope unit="volume">1050</biblScope>
			<biblScope unit="issue">9</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Correlated multiarmed bandit problem: Bayesian algorithms and regret analysis</title>
		<author>
			<persName><forename type="first">Vaibhav</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Reverdy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naomi</forename><forename type="middle">Ehrich</forename><surname>Leonard</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1507.01160</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2015-07">July 2015</date>
		</imprint>
	</monogr>
	<note>math.OC</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Reinforcement Learning: An Introduction</title>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">G</forename><surname>Barto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Multi-objective contextual multi-armed bandit problem with a dominant objective</title>
		<author>
			<persName><forename type="first">Cem</forename><surname>Tekin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eralp</forename><surname>Turgay</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.05655</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Introduction to Nonparametric Estimation</title>
		<author>
			<persName><forename type="first">B</forename><surname>Alexandre</surname></persName>
		</author>
		<author>
			<persName><surname>Tsybakov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISBN</title>
		<imprint>
			<biblScope unit="volume">0387790519</biblScope>
			<biblScope unit="page">9780387790510</biblScope>
			<date type="published" when="2008">2008</date>
			<publisher>Springer Publishing Company</publisher>
		</imprint>
	</monogr>
	<note>Incorporated, 1st edition</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Multi-armed bandit models for the optimal design of clinical trials: benefits and challenges</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sofía</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Villar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Bowden</surname></persName>
		</author>
		<author>
			<persName><surname>Wason</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistical science: a review journal of the Institute of Mathematical Statistics</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">199</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Regional multi-armed bandits</title>
		<author>
			<persName><forename type="first">Zhiyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruida</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cong</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Bandit algorithms for website optimization</title>
		<author>
			<persName><forename type="first">John</forename><surname>White</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>Reilly Media, Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Correlated gaussian multi-objective multi-armed bandit across arms algorithm</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Q</forename><surname>Yahyaa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Drugan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Symposium Series on Computational Intelligence</title>
		<imprint>
			<date type="published" when="2015-12">Dec 2015</date>
			<biblScope unit="page" from="593" to="600" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">A survey on contextual multi-armed bandits</title>
		<author>
			<persName><forename type="first">Li</forename><surname>Zhou</surname></persName>
		</author>
		<idno>1508.03326 [cs.LG</idno>
		<ptr target="https://arxiv.org/abs/1508.03326" />
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
