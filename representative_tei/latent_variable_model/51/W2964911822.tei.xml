<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Zero-shot Learning with Many Classes by High-rank Deep Embedding Networks</title>
				<funder ref="#_UHr4UFQ">
					<orgName type="full">National Postdoctoral Program for Innovative Talents</orgName>
				</funder>
				<funder ref="#_kKTCpC5">
					<orgName type="full">National Key R&amp;D Program of China</orgName>
				</funder>
				<funder ref="#_VdQQDNZ">
					<orgName type="full">China Postdoctoral Science Foundation</orgName>
				</funder>
				<funder ref="#_GtYE73r">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yuchen</forename><surname>Guo</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Automation</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Guiguang</forename><surname>Ding</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Software</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jungong</forename><surname>Han</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">WMG Data Science</orgName>
								<orgName type="institution">University of Warwick</orgName>
								<address>
									<settlement>Coventry</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hang</forename><surname>Shao</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Zhejiang Future Technology Institute (Jiaxing)</orgName>
								<address>
									<settlement>Zhejiang</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xin</forename><surname>Lou</surname></persName>
							<affiliation key="aff4">
								<orgName type="institution">Chinese PLA General Hospital</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Qionghai</forename><surname>Dai</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Automation</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Zero-shot Learning with Many Classes by High-rank Deep Embedding Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-28T22:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Zero-shot learning (ZSL) is a recently emerging research topic which aims to build classification models for unseen classes with knowledge transferred from auxiliary seen classes. Though many ZSL works have shown promising results on small-scale datasets by utilizing a bilinear compatibility function, the ZSL performance on large-scale datasets with many classes (say, ImageNet) is still unsatisfactory. We argue that the bilinear compatibility function is a low-rank approximation of the true compatibility function such that it is not expressive enough especially when there are a large number of classes because of the rank limitation. To address this issue, we propose a novel approach, termed as High-rank Deep Embedding Networks (GREEN), for ZSL with many classes. In particular, we propose a feature-dependent mixture of softmaxes as the image-class compatibility function, which is a simple extension of the bilinear compatibility function, but yields much better results. It utilizes a mixture of non-linear transformations with featuredependent latent variables to approximate the true function in a high-rank way, thus making GREEN more expressive. Experiments on several datasets including ImageNet demonstrate GREEN significantly outperforms the state-of-the-art approaches.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The aim of zero-shot learning is to recognize concepts that are never seen during training <ref type="bibr" target="#b9">[Xian et al., 2017]</ref>. It is very useful in real-world applications because of the following three reasons. Firstly, there are potentially unlimited concepts in practice such that it is very expensive to collect sufficient labeled samples for all of them <ref type="bibr" target="#b5">[Lampert et al., 2014]</ref>. Secondly, new concepts emerge every data and it is almost impossible to retrain a model every time a new concept pops up. Thirdly, the objects or concepts "in the wild" follow a long-tail distribution such that many concepts have very limited visual samples for training <ref type="bibr" target="#b1">[Changpinyo et al., 2016]</ref>.</p><p>Generally speaking, ZSL can be formulated as a crossmodality matching problem equipped with a compatibility function F (x, y; W ) where x ∈ R p is the feature vec- tor of an image such as deep features <ref type="bibr" target="#b4">[He et al., 2016]</ref>, y ∈ R q is the feature vector of a concept such as class attributes <ref type="bibr" target="#b1">[Farhadi et al., 2009]</ref> or word2vec representations <ref type="bibr" target="#b6">[Socher et al., 2013]</ref>, and W ∈ R p×q is the parameter of the function F . The bilinear compatibility function defined as F (x, y; W ) = xW y is widely utilized <ref type="bibr" target="#b0">[Akata et al., 2016;</ref><ref type="bibr">2015;</ref><ref type="bibr">Kodirov et al., 2017;</ref><ref type="bibr" target="#b5">Norouzi et al., 2013;</ref><ref type="bibr" target="#b6">Socher et al., 2013;</ref><ref type="bibr" target="#b9">Xian et al., 2016;</ref><ref type="bibr">Zhang and Saligrama, 2015]</ref>. Since there is no labeled samples for unseen classes for training, some auxiliary seen classes related to the unseen ones which have many labeled samples are utilized for training. In particular, given an image-class pair (x, y) from seen classes, the parameter W is learned with the objective that increases F (x, y; W ) for a positive pair and decreases it for a negative pair. Since the seen classes and unseen classes are related (e.g., they are all animal species), the parameter W trained with seen classes can be applied to the unseen ones. Then given any image x and an class y, the compatibility can be directly computed by F (x, y; W ) and the prediction of a test image x is given based on its compatibility to each unseen class, e.g., by choosing the class with the largest response.</p><p>For small-scale datasets with only a few classes, such as AwA <ref type="bibr" target="#b5">[Lampert et al., 2014]</ref> which just has 50 classes in total, bilinear compatibility function has yielded promising results, which has been demonstrated in massive number of ZSL literatures <ref type="bibr" target="#b0">[Akata et al., 2016;</ref><ref type="bibr">Guo et al., 2017b;</ref><ref type="bibr">Kodirov et al., 2017;</ref><ref type="bibr">Zhang and Saligrama, 2015]</ref>. However, when dealing with large-scale datasets with many classes, like <ref type="bibr">ImageNet [Russakovsky et al., 2015]</ref> which has thousands of classes, the performance is still unsatisfactory <ref type="bibr" target="#b9">[Xian et al., 2017]</ref>. Since the latter is the case of real-world scenario where we wish to apply ZSL, it is necessary to investigate how to improve ZSL accuracy when there are many classes. For demonstration, we plot the current state-of-the-art ZSL accuracy w.r.t. the number of classes in the test set, as shown in Figure <ref type="figure" target="#fig_0">1</ref>. When there are 50 test classes, the accuracy is around 60% <ref type="bibr" target="#b1">[Changpinyo et al., 2016]</ref>. But it drops below 10% when there are more than one thousand test classes. Moreover, the test accuracy keeps decreased until it reaches less than 1%, when the test classes are increased to more than ten thousands <ref type="bibr" target="#b9">[Xian et al., 2017]</ref>.</p><p>If we regard the model learning as a function approximation problem, i.e., we try to approximate a true compatibility function F * by F , the complexity of F becomes crucial. A simple model may lead to under-fitting while a complicated model may result in over-fitting <ref type="bibr">[Bishop and others, 2006]</ref>. When dealing with small-scale datasets, the bilinear compatibility function seems complicated enough. However, when there are a large number of classes (say, 10 thousand), its complexity seems too low to approximate F * . We argue that this is caused by the rank limitation from a matrix factorization perspective. Because the bilinear compatibility function is a (relatively) low-rank approximation, it seems too simple to handle the complicated situation when there are many classes. From this point of view, it is necessary to improve the complexity of the function to handle large-scale datasets.</p><p>On the other hand, ZSL for a large-scale dataset is different from the one for a small-scale dataset. In particular, in small datasets, it only needs to recognize coarse-grained classes like in AwA <ref type="bibr" target="#b5">[Lampert et al., 2014]</ref>, or fine-grained classes from one root category like CUB <ref type="bibr" target="#b7">[Wah et al., 2011]</ref>. However, for a large-scale dataset like ImageNet, there are many root categories and fine-grained sub-categories, like tens of kinds of dogs and birds. In this scenario, a model has to capture macro characteristics to distinguish between root categories, such as bird and dog, and micro ones to distinguish between fine-grained classes, such as "Labrador Retriever" and "Golden Retriever". Obviously, using a simple bilinear model to handle a large-scale dataset seems unreasonable <ref type="bibr" target="#b9">[Xian et al., 2016]</ref> such that a more expressive model is required.</p><p>The success of bilinear function for ZSL motivates us to investigate it deeper. Inspired by <ref type="bibr" target="#b11">[Yang et al., 2018]</ref>, we propose a novel approach, termed as High-rank Deep Embedding Networks (GREEN), for ZSL with many classes. Inspired by the latent variable models <ref type="bibr" target="#b1">[Bishop, 1998]</ref>, GREEN adopts a mixture of sotmaxes together with featuredependent latent variables. By the weighted combination of softmaxes, the complexity of the model is improved such that it breaks the rank limitation suffered by the bilinear model, which makes GREEN effective for ZSL with many classes from the theoretical perspective. On the other hand, the latent variables can be regarded as a coarse clustering of data which groups images with similar macro characteristics, such as views or backgrounds into one branch, followed by a softmax focusing on the micro characteristics to distinguish between them, making GREEN more powerful and flexible than the bilinear model. In summary, the contributions are below.</p><p>1. We notice that the widely used bilinear compatibility function suffers from rank limitation so that it performs poorly for ZSL with many classes. We propose novel Highrank Deep Embedding Networks (GREEN) for ZSL with many classes. It adopts a mixture of softmaxes with featuredependent latent variables. GREEN is capable of keeping the formulation and training simple while resulting in a highcomplexity model to handle large-scale datasets.</p><p>2. We develop a shallow version which utilizes given features and a deep version which can be trained in an end-to-end manner. Both versions can be trained efficiently.</p><p>3. We carry out extensive experiments for ZSL with many classes, including ImageNet. The experimental results demonstrate that GREEN outperforms the state-of-the-art ZSL approaches, which validates its effectiveness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Preliminaries</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Notations</head><p>ZSL problem is described as follows. There are two disjoint class sets C s = {c s 1 , ..., c s ks } and C u = {c u 1 , ..., c u ku } with C s ∩ C u = ∅, denoted as seen classes and unseen classes respectively. Each image is represented by an image feature vector x ∈ R p and each class is represented by a label feature vector y ∈ R q . There is a training set</p><formula xml:id="formula_0">D tr = {(x i , y i )} ns i=1</formula><p>where the each class feature y i corresponds to a seen class from C s . A compatibility function F (x, y; W ) between image and class features is trained based on the training set. Then, it is applied to a test sample from unseen classes C u in the conventional ZSL setting, or C s ∪ C u in the generalized ZSL setting. The classification is performed by selecting the class which has the largest compatibility to the test sample.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Related Works</head><p>The bilinear compatibility function is widely utilized:</p><p>F (x, y; W ) = xW y (1) There are many representative works with it, including DE-VISE <ref type="bibr">[Frome et al., 2013]</ref>, ALE <ref type="bibr" target="#b0">[Akata et al., 2016]</ref>, SJE <ref type="bibr" target="#b0">[Akata et al., 2015]</ref>, SAE <ref type="bibr">[Kodirov et al., 2017]</ref>, ESZSL <ref type="bibr" target="#b6">[Romera-Paredes and Torr, 2015]</ref>, and many other approaches <ref type="bibr">[Fu et al., 2015b;</ref><ref type="bibr">2015a;</ref><ref type="bibr" target="#b3">Guo et al., 2016;</ref><ref type="bibr">Zhang and Saligrama, 2016]</ref>. The basic idea is to build a cross-modality matching function between image feature space and class feature space, which can be achieved by using the labeled data in D tr . Since the class features from C s and C u are from the same feature space like the word2vec space and C s and C u are related, e.g., they are all animal species, the function F trained with C s can be applied to C u , which has been demonstrated in many ZSL literatures. To learn the function F , many different loss functions are considered. For example, triplet loss <ref type="bibr" target="#b0">[Akata et al., 2015]</ref>, ranking loss <ref type="bibr" target="#b0">[Akata et al., 2016]</ref>, Euclidean loss [Romera-Paredes and Torr, 2015] and cross-entropy loss <ref type="bibr" target="#b8">[Wu et al., 2018]</ref>. Bilinear compatibility function is shown to be simple and effective for ZSL.</p><p>There are also some other ideas for ZSL. For example, as seminal works of ZSL, DAP and IAP <ref type="bibr" target="#b1">[Farhadi et al., 2009;</ref><ref type="bibr" target="#b5">Lampert et al., 2014]</ref> consider to recognize the attributes from images and compare them to the class attributes. <ref type="bibr">CMT [Socher et al., 2013]</ref> embeds image features into the class feature space for distance measure. CONSE <ref type="bibr" target="#b5">[Norouzi et al., 2013]</ref> utilizes convex combination of semantic embeddings to compute the conditional probability. SYNC <ref type="bibr" target="#b1">[Changpinyo et al., 2016]</ref>    <ref type="bibr">(GREEN)</ref>. Given an image, a (trainable or not) feature extractor is applied to produce its p-dimensional image feature. Then the feature is used to compute the feature-dependent latent weight factors where each factor controls the weight of one branch for the final output. For each branch, a d-dimensional branch-specific image feature is generated based on the image feature, and then together with the class features, the branch softmax is computed based on the bilinear compatibility function. At last, the softmaxes from each branch is mixed, producing the final output. Benefiting from the latent factors and mixture of softmaxes, GREEN is capable of yielding high-rank output, which is more powerful than simple bilinear compatibility function.</p><p>mapping from class feature space to a model space. SSZS-L <ref type="bibr">[Guo et al., 2017a]</ref> synthesizes samples based on the reconstructed distribution of unseen classes. STZSL <ref type="bibr">[Guo et al., 2017b]</ref> transfers similar training samples to unseen classes based on the image-class similarity. Although they do not explicitly adopt bilinear compatibility function in Eq. ( <ref type="formula">1</ref>) as the final classification model, Eq. ( <ref type="formula">1</ref>) still acts as an important part in their algorithms.</p><p>3 High-rank Deep Embedding Networks</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Rank Limitation</head><p>Suppose there are n images and k classes, given the parameter matrix W , we can construct an image-class compatibility matrix F ∈ R n×k , where each element F ic = F (x i , y c ; W ) = x i W y c . If the ground-truth compatibility matrix for these image-class pairs is F ∈ R n×k , we have</p><formula xml:id="formula_1">F = XW Y ≈ F (2)</formula><p>where X ∈ R n×p is the image feature matrix by stacking x i and Y ∈ R c×q is the class feature matrix by stacking y c . Interestingly, this formulation is essentially a matrix factorization problem where the matrices W , X, and Y (if learnable) are learned by some algorithms so that the factorized matrix XW Y can approximate F as precise as possible. Suppose the rank of F and F are r and r respectively. Obviously, to precisely factorize F into XW Y , the ranks should satisfy the condition r ≥ r or at least r is close to r. From the definition of these matrices, we can observe that r ≤ min(p, q) and r ≤ k. For a small dataset with only tens of classes, r is usually small such that the condition is always true. In this case, the bilinear compatibility function can well approximate the true function. However, for a large-scale dataset with large k, r could be very large because the real-world dataset is complicated and it is reasonable to assume the true compatibility matrix is high-rank. However, the widely used class features, such as word2vec representation <ref type="bibr">[Mikolov et al., 2013a;</ref><ref type="bibr">2013b]</ref>, have only hundreds of dimensions, or even tens of dimensions. Since r ≤ q, the rank of F is much smaller than the rank of F, making the approximation imprecise. Due to the rank limitation, the bilinear function seems to "underfit" datasets with many classes, yielding poor performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">GREEN</head><p>To address the issues above, in this paper we propose a novel model using a simple extension of bilinear compatibility function, termed as high-rank deep embedding networks (GREEN). The framework is summarized in Figure <ref type="figure" target="#fig_1">2</ref>. In particular, given an image feature x, instead of utilizing the simple bilinear compatibility function in Eq. ( <ref type="formula">1</ref>), we propose to use a mixture of softmaxes with feature-dependent latent variables to compute the compatibility,</p><formula xml:id="formula_2">F G (x, y c ; W ) = B b=1 ω x,b exp(x b W b y c ) c∈Cs exp(x b W b y c) (3)</formula><p>where B is the number of branches and ω x,b is the featuredependent latent weight factor which controls the contribution of the b-th branch to the output, defined as follows,</p><formula xml:id="formula_3">ω x,b = exp(xu b ) B b=1 exp(xu b)<label>(4)</label></formula><p>where u b ∈ R p is the weight parameter. For each branch, the branch-specific feature </p><formula xml:id="formula_4">x b = ReLU (xV b ) ∈ R d where V b ∈ R p×d is</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Discussion</head><p>GREEN leads to high-rank approximation. As discussed above, bilinear function suffers from rank limitation such that it underfits the dataset which has many classes. GREEN is a simple extension of bilinear function, but results in a highrank compatibility matrix. In particular, the compatibility matrix F G produced by GREEN is as follows:</p><formula xml:id="formula_5">F G = B b=1 Ω b exp(X b W b Y -Λ b J ns,ks )<label>(7)</label></formula><p>where .., n s ), and J ns,ks is a n s × k s matrix whose elements are all 1. Obviously, F G is a non-linear combination of image features and class features by the sum-exp function. Since Λ is a diagonal matrix, the rank d G is no longer limited by the feature dimensionality p or q, making it arbitrarily high-rank. Please refer to <ref type="bibr" target="#b10">[Yang et al., 2017]</ref> for proof. In the extreme case where B = 1, F G degenerates to F after a log transformation and a simple row-wise shift. Therefore, GREEN results in a higher-rank matrix than the bilinear model, which is able to approximate complicated datasets with many classes more precisely. Due to its improved expressiveness, we can expect GREEN to obtain better (or at least equal) results compared to the simple bilinear function. GREEN keeps the model simple. Based on the rank limitation r ≤ min(p, q), there is a straightforward solution to problem. One can significantly increase the dimensionality of image and class features, i.e., p and q. However, when p and q get too large, the bilinear model becomes very complicated since W has p × q parameters, which is likely to overfit the training set. In addition, the number of semantic classes is potentially unlimited, it is almost impossible to keep increasing p and q when there are more classes are taken into consideration. Both issues make the model quite complicated. On the other hand, GREEN utilizes mixture of softmaxes and non-linear transformations, which is capable of approximating arbitrarily high-rank matrix, and keeping the model simple at the same time. In particularly, GREEN needs only p × b parameters to compute the feature-dependent latent weight factor, and B × p × d parameters in total. Since B is usually small (e.g., 16), the complexity is controlled. In addition, because GREEN do not suffer from rank limitation any longer, it becomes possible to reduce p and q, especially p by the branch-specific feature, to compensate for the increase of model parameters caused by the mixture structure. In this way, GREEN keeps as simple as the original bilinear compatibility model while being more expressive and powerful.</p><formula xml:id="formula_6">Ω b = diag(ω x1,b , ..., ω xn s ,b ), X b ∈ R ns×d</formula><p>Relation to existing works. As discussed above, GREEN is an simple and effective extension of the bilinear function, which address the rank limitation problem when dealing with many classes. We also notice that there are some works considering similar mixture structure of bilinear functions. One is LATEM <ref type="bibr" target="#b9">[Xian et al., 2016]</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiment 4.1 Setting</head><p>There are many widely used benchmarks for evaluating ZS-L approaches. In this paper, we focus on ZSL with many classes. Therefore, many of them with only tens of classes, like AwA and aPY <ref type="bibr" target="#b1">[Farhadi et al., 2009]</ref> We use the widely used 1, 000 classes with about 1.3 images as the training set. There are another about 20k classes with about 14 million samples utilized as the test set. To comprehensively evaluate on ImageNet, we consider different subsets of the test set, including classes that are 2-hops (denoted as 2H, 1, 509 classes) and 3-hops (3H, 7, 678 classes) away from the 1, 000 seen classes, the most popular 500 (M500), 1k (M1K), and 5k (M5k) classes, and the least popular 500 (L500), 1k (L1K), and 5k (L5K) classes. For evaluation, we use average per-class top-1 accuracy <ref type="bibr" target="#b9">[Xian et al., 2017]</ref>:</p><formula xml:id="formula_7">acc = 1 k u c∈Cu #correct predictions in c #samples in c<label>(9)</label></formula><p>For each image, we use the ResNet-101 pretrained on ImageNet-1k as the feature extractor, which yields 2, 048dimensional image features. For SUN dataset, we use the   <ref type="bibr">et al., 2016]</ref>. For fair comparison, we use the features provided by <ref type="bibr" target="#b9">[Xian et al., 2017]</ref> as input.</p><p>GREEN can take feature vectors as input like many other ZSL approaches, which can be regarded as fixing the feature extractor in Figure <ref type="figure" target="#fig_1">2</ref>. We denote this version as GREEN-S(hallow). In addition, it is simple to combine GREEN with deep convolutional networks, and thus it can use raw images as input and train (finetune) the feature extractor, which is denoted as <ref type="bibr">GREEN-D(eep)</ref>. For both versions, we set the number of branches as B = 16. Since there are multiple branches, we can reduce the dimensionality of the branch feature x b to reduce computational burden. In particular, we set d = 256. To minimize the loss function in Eq. ( <ref type="formula">5</ref>), we use mini-batch based stochastic gradient descent algorithm. The batch size is 128 and we train the model for 100k iterations. The initial learning rate is 0.01 and then 0.001 at the 70k-th iteration. For GREEN-D, we use ResNet-101 as the backbone. We implement GREEN in TensorFlow 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Benchmark Comparison</head><p>We summarize the comparison on SUN and ImageNet (including its subsets) in Table <ref type="table" target="#tab_3">1</ref>. In Figure <ref type="figure" target="#fig_4">3</ref>, we show the results on ImageNet with different metrics. From the results, we can observe that GREEN outperforms the other ZSL ap-1 <ref type="url" target="https://www.tensorflow.org/">https://www.tensorflow.org/</ref> proaches with significant margin, which demonstrates the effectiveness of GREEN for ZSL with many classes. We have the following important observations based on the results.</p><p>Firstly, GREEN-S, which uses the same image features as many non-deep baseline approaches, shows observable improvement. This is a clear evidence of the superiority of GREEN framework. As discussed above, GREEN utilizes a mixture of softmaxes with feature-dependent latent weight factors to address the rank limitation problem suffered by the simple bilinear model, which is capable of approximating the true compatibility matrix in a high-rank manner. From the results, we can observe that the simple formulation of GREEN can indeed approximate the true compatibility more precisely.</p><p>Secondly, GREEN-D, which finetunes the feature extractor, improves significantly over GREEN-S. This phenomenon is reasonable since the finetuning results in better image features. However, some baseline approaches are based on deep networks too, such as CONSE and DEVISE. They do not show comparable performance. This phenomenon demonstrates that the mixture of softmaxes and the objective function of GREEN is more effective for ZSL, especially when there are many classes. In addition, SYNC is one of the best baseline approaches in all. Its nonlinear compatibility function seems work well for ZSL. However, since it is very complicated, it seems difficult to combine it with deep networks, while the simple loss function of GREEN can be combined with deep networks easily, making GREEN more powerful.</p><p>Thirdly, we show the relative improvement of GREEN-S over the best result achieved by baseline approaches. Here we  can observe that the improvement becomes larger with more test classes. There are macro characteristics to distinguish between "dog" and "bird", and micro characteristics to distinguish between fine-grained classes like "Golden Retriever" and "Labrador Retriever". When there are just a few classes, one simple linear more is likely to handle both. However, when there are a large number of classes, it is unreasonable to handle them at the same time. The mixture structure of GREEN can model the hierarchical structure classes where the latent weight factors can be regarded as coarse clustering of data which distinguish between root classes and each branch focuses on fine-grained classes. The results demonstrate the superiority of GREEN for ZSL with many classes, which makes it more practical for real-world applications.</p><p>Fourthly, LATEM, which uses max operation to bring nonlinearity into compatibility function, does not perform well. As discussed above, with max operation, only one branch contributes to the final decision, which also suffers from rank limitation to some extent. GREEN uses soft combinations using all branches, making it more expressive, which seems more reasonable and suffers less from the rank limitation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">The Effect of Mixture</head><p>To further verify the effectiveness of GREEN, we conduct another experiment, which change the number of branches, i.e., B, and evaluate the performance of GREEN-D. When B = 1, GREEN-D degenerates to the simple bilinear model. The results w.r.t. B are shown in Figure <ref type="figure" target="#fig_5">4</ref>. We can observe that the performance increases significantly with larger B, which shows the importance of the mixture of softmaxes. When there are many classes, it is unreasonable to handle them by a simple bilinear model. With more branches in the mixture, the model can focuses on more aspects of data, such as different background or views, and then capture the micro information in each branch. Besides, although the sum-exp function can theoretically result in arbitrarily high rank, the rank of the generated compatibility function in practice is still limited by the complexity of the model due to the existence of model regularization. With more branches in the mixture, the model is more expressive, leading to higher-rank approximation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper we focus on ZSL with many classes. In particular, we notice that the widely used bilinear compatibility function works well on small-scale datasets, but fails in largescale datasets with many classes like ImageNet. We argue that this is due to the rank limitation problem based on a matrix factorization perspective. To address this issue, we propose a novel approach, termed as High-rank Deep Embedding Networks (GREEN). GREEN utilizes a mixture of softmaxes as the image-class compatibility function, which is a simple extension of bilinear function, but is able to approximate the true function in a high-rank manner by a mixture of nonlinear transformations with feature-dependent latent variables. GREEN is very simple and expressive. It can be combined with deep networks as well. Extensive experiments on benchmarks including ImageNet demonstrate GREEN significantly outperforms the state-of-the-arts for ZSL with many classes.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The ZSL accuracy drops significantly with more classes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure2: The basic framework of high-rank deep embedding networks(GREEN). Given an image, a (trainable or not) feature extractor is applied to produce its p-dimensional image feature. Then the feature is used to compute the feature-dependent latent weight factors where each factor controls the weight of one branch for the final output. For each branch, a d-dimensional branch-specific image feature is generated based on the image feature, and then together with the class features, the branch softmax is computed based on the bilinear compatibility function. At last, the softmaxes from each branch is mixed, producing the final output. Benefiting from the latent factors and mixture of softmaxes, GREEN is capable of yielding high-rank output, which is more powerful than simple bilinear compatibility function.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>the fully connected parameter, and W b ∈ R d×q is the compatibility parameter for the b-th branch. One can verify F (x, y c ; W ) ≥ 0 and c F (x, y c ; W ) = 1 which indicates that we can regard it as the conditional probability on each training class. Therefore we can train the model like training with conventional softmax by cross entropy loss function as follows, L CE = -ns i=1 c∈Cs I(y c = y i ) log F G (x, y c; W ) (5) Minimizing L CE is achieved by the gradient descent algorithm, which gives us the most important model parameters {u b } B b=1 for computing the latent weight factor, {V b } B b=1 for computing branch-specific feature, and {W b } B b=1 as the compatibility parameter for each branch. With the model F G , the prediction for a test image x is given as follows, c(x) = argmax c F G (x, y c ; W ) (6)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>is the branch-specific feature matrix by stacking training features of the b-th branch, Λ b = diag(log c∈Cs exp(x i,b W b y c), i = 1, .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The top-1, top-5, and top-10 accuracy on ImageNet</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: The effect of the number of branches (B) on GREEN-D.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>builds synthesized classifiers based on a</figDesc><table><row><cell></cell><cell>Class</cell><cell></cell></row><row><cell></cell><cell>Feature</cell><cell></cell></row><row><cell></cell><cell>Image</cell><cell></cell></row><row><cell></cell><cell>Feature</cell><cell></cell></row><row><cell>……</cell><cell>Weight</cell><cell></cell></row><row><cell></cell><cell>Factor</cell><cell></cell></row><row><cell></cell><cell></cell><cell>Output</cell></row><row><cell></cell><cell></cell><cell>Softmax</cell></row><row><cell>Feature</cell><cell></cell><cell></cell></row><row><cell>Extractor</cell><cell></cell><cell>Branch</cell></row><row><cell>Image</cell><cell>Feature Branch</cell><cell>Softmax</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>It utilize max operation to bring in nonlinearity. GREEN is different from LATEM in three folds. Firstly, due to the max operation, only one component contribute to the final decision in LATEM. As discussed above, with only one component, the model suffers from rank limitation. GREEN uses soft combinations such that all branches contribute to the final decision, making it more expressive. Secondly, LATEM uses triplet loss for training while GREEN directly uses softmax based cross entropy loss. Obviously, training with cross entropy loss is much easier and more efficient than with triplet loss. We can expect better performance with better optimization. Thirdly, LATEM observed the limitation of linear function but did not point out the reason. We theoretically demonstrate that the problem lies in the rank limitation and propose GREEN to effectively address it. Because of these advantages, GREEN is more powerful for ZSL with many classes.</figDesc><table><row><cell cols="3">whose compatibility function is</cell></row><row><cell>F (x, y; W ) = max b=1,...,B</cell><cell>xW b y</cell><cell>(8)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 :</head><label>1</label><figDesc>Zero-shot performance comparison on benchmarks.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">SUN</cell><cell>2H</cell><cell>3H</cell><cell>M500</cell><cell>M1k</cell><cell>M5k L500 L1k</cell><cell>L5k ALL</cell></row><row><cell></cell><cell cols="2">CONSE [Norouzi et al., 2013]</cell><cell cols="2">38.8</cell><cell>7.63</cell><cell cols="2">2.18 12.33</cell><cell>8.31</cell><cell>3.22</cell><cell>3.53 2.69 1.05 0.95</cell></row><row><cell></cell><cell cols="2">CMT [Socher et al., 2013]</cell><cell cols="2">39.9</cell><cell>2.88</cell><cell>0.67</cell><cell>5.10</cell><cell>3.04</cell><cell>1.04</cell><cell>1.87 1.08 0.33 0.29</cell></row><row><cell></cell><cell cols="2">LATEM [Xian et al., 2016]</cell><cell cols="2">55.3</cell><cell>5.45</cell><cell cols="2">1.32 10.81</cell><cell>6.63</cell><cell>1.90</cell><cell>4.53 2.74 0.76 0.50</cell></row><row><cell></cell><cell></cell><cell>ALE [Akata et al., 2016]</cell><cell cols="2">58.1</cell><cell>5.38</cell><cell cols="2">1.32 10.40</cell><cell>6.77</cell><cell>2.00</cell><cell>4.27 2.85 0.79 0.50</cell></row><row><cell></cell><cell cols="2">DEVISE [Frome et al., 2013]</cell><cell cols="2">56.5</cell><cell>5.25</cell><cell cols="2">1.29 10.36</cell><cell>6.68</cell><cell>1.94</cell><cell>4.23 2.86 0.78 0.49</cell></row><row><cell></cell><cell></cell><cell>SJE [Akata et al., 2015]</cell><cell cols="2">53.7</cell><cell>5.31</cell><cell>1.33</cell><cell>9.88</cell><cell>6.53</cell><cell>1.99</cell><cell>4.93 2.93 0.78 0.52</cell></row><row><cell cols="3">ESZSL [Romera-Paredes and Torr, 2015]</cell><cell cols="2">54.5</cell><cell>6.35</cell><cell cols="2">1.51 11.91</cell><cell>7.69</cell><cell>2.34</cell><cell>4.50 3.23 0.94 0.62</cell></row><row><cell></cell><cell cols="2">SYNC [Changpinyo et al., 2016]</cell><cell cols="2">56.3</cell><cell>9.26</cell><cell cols="4">2.29 15.83 10.75 3.42</cell><cell>5.83 3.52 1.26 0.96</cell></row><row><cell></cell><cell cols="2">SAE [Kodirov et al., 2017]</cell><cell cols="2">40.3</cell><cell>4.89</cell><cell>1.26</cell><cell>9.96</cell><cell>6.57</cell><cell>2.09</cell><cell>2.50 2.17 0.72 0.56</cell></row><row><cell></cell><cell></cell><cell>GREEN-S</cell><cell cols="7">61.2 10.54 2.41 17.33 12.51 4.44</cell><cell>6.60 5.09 1.61 1.57</cell></row><row><cell></cell><cell></cell><cell>GREEN-D</cell><cell cols="7">65.6 12.38 3.41 18.80 14.47 5.38</cell><cell>7.71 6.07 2.30 2.06</cell></row><row><cell></cell><cell>20</cell><cell></cell><cell></cell><cell>50</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>60</cell></row><row><cell></cell><cell></cell><cell>CONSE</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>CONSE</cell><cell></cell><cell>CONSE</cell></row><row><cell></cell><cell></cell><cell>CMT</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>CMT</cell><cell></cell><cell>CMT</cell></row><row><cell>top 1 accuracy (%)</cell><cell>5 10 15</cell><cell>LATEM ALE DEVISE SJE ESZSL SYNC SAE GREEN-S GREEN-D</cell><cell>top 5 accuracy (%)</cell><cell>10 20 30 40</cell><cell></cell><cell></cell><cell cols="2">LATEM ALE DEVISE SJE ESZSL SYNC SAE GREEN-S GREEN-D</cell><cell>top 10 accuracy (%)</cell><cell>10 20 30 40 50</cell><cell>LATEM ALE DEVISE SJE ESZSL SYNC SAE GREEN-S GREEN-D</cell></row><row><cell></cell><cell>0</cell><cell></cell><cell></cell><cell>0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0</cell></row><row><cell></cell><cell>2H</cell><cell>3H M500 M1k M5k L500 L1k L5k ALL</cell><cell></cell><cell>2H</cell><cell cols="4">3H M500 M1k M5k L500 L1k L5k ALL</cell><cell>2H</cell><cell>3H M500 M1k M5k L500 L1k L5k ALL</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence </p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>This work was supported by the <rs type="funder">National Key R&amp;D Program of China</rs> (No. <rs type="grantNumber">2018YFC0807500</rs>), <rs type="funder">National Natural Science Foundation of China</rs> (No. <rs type="grantNumber">61571269</rs>), <rs type="funder">National Postdoctoral Program for Innovative Talents</rs> (No. <rs type="grantNumber">BX20180172</rs>), and the <rs type="funder">China Postdoctoral Science Foundation</rs> (No. <rs type="grantNumber">2018M640131</rs>).</p><p>Email: yuchen.w.guo@gmail.com. Corresponding author: <rs type="person">Guiguang Ding</rs>, dinggg@tsinghua.edu.cn.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_kKTCpC5">
					<idno type="grant-number">2018YFC0807500</idno>
				</org>
				<org type="funding" xml:id="_GtYE73r">
					<idno type="grant-number">61571269</idno>
				</org>
				<org type="funding" xml:id="_UHr4UFQ">
					<idno type="grant-number">BX20180172</idno>
				</org>
				<org type="funding" xml:id="_VdQQDNZ">
					<idno type="grant-number">2018M640131</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Evaluation of output embeddings for fine-grained image classification</title>
		<author>
			<persName><surname>Akata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<editor>
			<persName><forename type="first">Florent</forename><surname>Akata</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Zaïd</forename><surname>Perronnin</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Cordelia</forename><surname>Harchaoui</surname></persName>
		</editor>
		<editor>
			<persName><surname>Schmid</surname></persName>
		</editor>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006">2015. 2015. 2016. 2016. 2006. 2006</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note>IEEE Trans. Pattern Anal. Mach. Intell.</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Synthesized classifiers for zero-shot learning</title>
		<author>
			<persName><forename type="first">;</forename><surname>Bishop</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><surname>Bishop</surname></persName>
		</author>
		<author>
			<persName><surname>Changpinyo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1998">1998. 1998. 2016. 2016. 2009. 2009. 2013. 2013</date>
			<biblScope unit="page" from="1778" to="1785" />
		</imprint>
	</monogr>
	<note>NIPS</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Zero-shot object recognition by semantic manifold distance</title>
		<author>
			<persName><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Tao Xiang, and Shaogang Gong</title>
		<imprint>
			<date type="published" when="2015">2015. 2015. 2015</date>
		</imprint>
	</monogr>
	<note>CVPR</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Transductive zero-shot recognition via shared model space learning</title>
		<author>
			<persName><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence</title>
		<meeting>the Twenty-Sixth International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2016">2016. 2016. 2017. 2017. 2017</date>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="3277" to="3290" />
		</imprint>
	</monogr>
	<note>Zero-shot learning with transferred samples</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Elyor Kodirov, Tao Xiang, and Shaogang Gong. Semantic autoencoder for zero-shot learning</title>
		<author>
			<persName><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016">2016. 2016. 2017</date>
		</imprint>
	</monogr>
	<note>CVPR</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName><surname>Lampert</surname></persName>
		</author>
		<idno>CoRR, abs/1312.5650</idno>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Patterson and Hays</publisher>
			<date type="published" when="2012">2014. 2014. 2013. 2013. 2013. 2013. 2013. 2013. 2012. 2012</date>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
	<note>CVPR</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">An embarrassingly simple approach to zeroshot learning</title>
		<author>
			<persName><forename type="first">Romera-Paredes</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Torr</forename><forename type="middle">;</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">S</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><forename type="middle">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei-Fei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2013">2015. 2015. 2015. 2013. 2013</date>
		</imprint>
	</monogr>
	<note>NIPS</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">The Caltech-UCSD Birds-200-2011 Dataset</title>
		<author>
			<persName><surname>Wah</surname></persName>
		</author>
		<idno>CNS-TR-2011-001</idno>
		<imprint>
			<date type="published" when="2011">2011. 2011</date>
		</imprint>
		<respStmt>
			<orgName>California Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Global semantic consistency for zero-shot learning</title>
		<author>
			<persName><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Latent embeddings for zero-shot classification</title>
		<author>
			<persName><surname>Xian</surname></persName>
		</author>
		<idno>ab- s/1707.00600</idno>
	</analytic>
	<monogr>
		<title level="m">Bernt Schiele, and Zeynep Akata. Zero-shot learning -A comprehensive evaluation of the good, the bad and the ugly</title>
		<imprint>
			<publisher>CoRR</publisher>
			<date type="published" when="2016">2016. 2016. 2017. 2017</date>
		</imprint>
	</monogr>
	<note>CVPR</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Breaking the softmax bottleneck: A highrank RNN language model</title>
		<author>
			<persName><forename type="first">Yang</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Zhang and Saligrama, 2015] Ziming Zhang and Venkatesh Saligrama. Zero-shot learning via semantic similarity embedding</title>
		<author>
			<persName><forename type="first">Yang</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Ziming Zhang and Venkatesh Saligrama</title>
		<meeting><address><addrLine>Saligrama</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2018. 2018. 2015. 2016. 2016</date>
		</imprint>
	</monogr>
	<note>CVPR</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
