<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MECD: Unlocking Multi-Event Causal Discovery in Video Reasoning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2024-12-27">27 Dec 2024</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Tieyuan</forename><surname>Chen</surname></persName>
							<email>tieyuanchen@sjtu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Huabin</forename><surname>Liu</surname></persName>
							<email>huabinliu@sjtu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tianyao</forename><surname>He</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yihang</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chaofan</forename><surname>Gan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiao</forename><surname>Ma</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Lenovo Research</orgName>
								<address>
									<region>AI</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Cheng</forename><surname>Zhong</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Lenovo Research</orgName>
								<address>
									<region>AI</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yang</forename><surname>Zhang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Lenovo Research</orgName>
								<address>
									<region>AI</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yingxue</forename><surname>Wang</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Academic of Electronics and Information Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hui</forename><surname>Lin</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Academic of Electronics and Information Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Weiyao</forename><surname>Lin</surname></persName>
							<email>wylin@sjtu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">MECD: Unlocking Multi-Event Causal Discovery in Video Reasoning</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2024-12-27">27 Dec 2024</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2409.17647v4[cs.CV]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-14T19:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Video causal reasoning aims to achieve a high-level understanding of video content from a causal perspective. However, current video reasoning tasks are limited in scope, primarily executed in a question-answering paradigm and focusing on short videos containing only a single event and simple causal relationships, lacking comprehensive and structured causality analysis for videos with multiple events. To fill this gap, we introduce a new task and dataset, Multi-Event Causal Discovery (MECD). It aims to uncover the causal relationships between events distributed chronologically across long videos. Given visual segments and textual descriptions of events, MECD requires identifying the causal associations between these events to derive a comprehensive, structured event-level video causal diagram explaining why and how the final result event occurred. To address MECD, we devise a novel framework inspired by the Granger Causality method, using an efficient maskbased event prediction model to perform an Event Granger Test, which estimates causality by comparing the predicted result event when premise events are masked versus unmasked. Furthermore, we integrate causal inference techniques such as front-door adjustment and counterfactual inference to address challenges in MECD like causality confounding and illusory causality. Experiments validate the effectiveness of our framework in providing causal relationships in multi-event videos, outperforming GPT-4o and VideoLLaVA by 5.7% and 4.1%, respectively.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Video causal reasoning aims to achieve a high-level understanding and analysis of video content from a causal perspective. Video Question Answering (VQA) <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref> represents one of the most prominent tasks in causal reasoning, where models are tested on their causal ability to understand video content through causal questions such as explanations, predictions, and counterfactual assumptions. Recently, some studies have sought to move beyond the single QA task, attempting to construct more complex and challenging video reasoning tasks and methodologies. For example, CLEVRER <ref type="bibr" target="#b4">[5]</ref>, V-CDN <ref type="bibr" target="#b5">[6]</ref> and CATER <ref type="bibr" target="#b6">[7]</ref> explored more difficult causal reasoning tasks in virtual scenes by constructing object-aware features or using graph neural networks. Neural-symbolic paradigm AAR <ref type="bibr" target="#b7">[8]</ref> and LMLN <ref type="bibr" target="#b8">[9]</ref> extended to derive inference rules by symbolizing data. VAR <ref type="bibr" target="#b9">[10]</ref> and BiGED <ref type="bibr" target="#b10">[11]</ref> extended to daily video causal reasoning by introducing causality during prediction.</p><p>However, current video causal reasoning tasks are still limited in scope (primarily QA-based) and mainly focus on short videos containing only a single event or a few events. Most importantly, they cannot provide a comprehensive and structured causal representation for multi-event video reasoning, which is typically required in real-world scenarios. For instance, in traffic surveillance videos, it is necessary to cross-analyze events happening at different times to determine which events, or combinations of events, led to the final traffic accident event.</p><p>To address this gap, we set up a new task: Multi-Event Causal Discovery (MECD), which aims to uncover causal relationships among events that distribute chronologically in long videos. As illustrated in Fig. <ref type="figure" target="#fig_0">1</ref>, given multiple chronologically arranged event segments in a video along with their corresponding textual descriptions (Fig. <ref type="figure" target="#fig_15">1(a)</ref>), MECD requires identifying causal associations between these events to derive a comprehensive and structured event-level causal diagram (Fig. <ref type="figure" target="#fig_15">1(b)</ref>), indicating why and how the final result event happens. Meanwhile, we contribute a new dataset for the training and evaluation of MECD by collecting long-form videos involving multiple events and manually annotating real causal relations between events for them. However, to our knowledge, no available solutions can directly comprehend causal relationships at the event level, necessitating the development of a new framework to tackle this complex task.</p><p>To this end, we draw inspiration from the Granger Causality Method <ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref> for solution, which is widely used in traditional causal discovery for low-dimensional time-series data (e.g., stock prices, weather patterns). The main idea is that temporal causality can be effectively estimated by predictive ability. Specifically, applied to videos, if Event A occurs prior to Event B, we consider A to be a cause of B only if A could facilitate the prediction of B. We term this criterion the Event Causality Test. However, compared to simple low-dimensional data, the inputs of MECD involve much more complex modalities, including both visual and textual content, which may introduce bias in the estimation of causality using such a predictive paradigm. Specifically, we observe that directly applying Event Causality Test to video causal discovery presents two main problems:</p><p>(1) Causality confounding indicates that the original causal relationships between events are disrupted or interfered with by other relay or adjacent events. Such confounding stems from the fact that many causal relationships flow through an intermediary event that acts as a bridge. As shown in Fig. <ref type="figure" target="#fig_15">1(c</ref>), event "submitting the paper" serves as a necessary bridge between "taking the test" and "obtaining a grade." In this case, this bridge event might be mistakenly regarded as the only cause of the result event, while another cause, "taking the test," is overlooked. However, the bridge event can only occur if "taking the test" happens first. Therefore, we cannot identify the real causality between events that linked by such bridges following a simple predictive criterion, and eliminating such confounding is thus crucial for an accurate causal discovery.</p><p>(2) Illusory Causality, which includes illusory temporal and existence causality. Illusory temporal causality exists when events exhibit a close correlation in temporal distribution. Such correlation may mislead the test of real causality. As shown in Fig. <ref type="figure" target="#fig_15">1(d)</ref>, the event "adding oil when cooking" often occurs before "adding vegetables to stir-fry," but there is no real causality between them. As for illusory existence causality, it occurs when some objects in early events may serve as necessary existence conditions of a later event. For instance (Fig. <ref type="figure" target="#fig_15">1(e)</ref>), consider determining the causal relation between "a large brown dog enters the room" (at the start of the video) and "the dog runs towards the camera." (at the end of the video). Although the presence of the dog in the former event is a prerequisite for the subsequent event, it does not directly cause the dog to rush towards the camera.</p><p>Building upon the preceding discussion, we introduce a novel framework to tackle MECD. This framework executes the Event Granger Test via an efficient mask-based event prediction model. It deduces the causality of a premise event by comparing the predicted features of the result event when the premise is either masked or unmasked. Furthermore, to mitigate the challenges of causality confounding and illusory causality discussed earlier, we integrate two additional causal inference techniques-front-door adjustment <ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref> and counterfactual inference <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19]</ref>-into our framework. Specifically, these techniques compensate for or remove the causal effects of previous or subsequent adjacent bridge events to eliminate confounding. Simultaneously, they address the issue of illusory causality through the incorporation of an extra chain of thought <ref type="bibr" target="#b19">[20]</ref><ref type="bibr" target="#b20">[21]</ref><ref type="bibr" target="#b21">[22]</ref> and existence-only descriptions during inference. Extensive experiments validate the effectiveness of our proposed framework in predicting structured causal relationships for given long-form videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Video causal reasoning Many tasks in the past have tried to carry out causal reasoning in videos. Among these, the most common task is Video Question Answering (VQA) <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref>, aiming to give a reasonable answer to the question, methods such as SeViLA and LocAns <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref> made abductions based on the result, they grounded a single reason in previous time. However, VQA does not extend to abduct multiple reasons, merely creating a single causal link from reason to result.</p><p>Many tasks were based on VQA task for further causal reasoning attempts. CLEVRER <ref type="bibr" target="#b4">[5]</ref>, CATER <ref type="bibr" target="#b6">[7]</ref> and V-CDN <ref type="bibr" target="#b5">[6]</ref> explored causal reasoning based on physics and other basic laws in virtual scenes. However, these tasks haven't been committed to extending to the general video causal reasoning. AAR <ref type="bibr" target="#b7">[8]</ref> and LMLN <ref type="bibr" target="#b8">[9]</ref> symbolized data and derived inference rules using external knowledge. However, they can only reason within a defined symbol domain. The most similar VAR <ref type="bibr" target="#b9">[10]</ref> predicted explanation events with premise events, and the causality was introduced during its prediction process. However, firstly it hasn't been committed to discovering the complete causal diagram. Besides, there is no explicit utilization of causal methods which constrains its ability.</p><p>All tasks above are for causal reasoning in short videos, while ours aims to handle long-duration videos. Besides, most of these are coarse video-level tasks, ours is more fine-grained event-level reasoning. Additionally, we want to establish a whole causal diagram rather than a single causal link. In conclusion, all these tasks haven't been committed to discovering causality among complex multi-event videos. Consequently, there exists a necessity need for a more comprehensive task.</p><p>Causal discovery in low-dimensional temporal data Traditional causal discovery methods of simple temporal data are mainly divided into three categories. Constraint-based methods use conditional independence tests to identify causal relations <ref type="bibr" target="#b22">[23]</ref><ref type="bibr" target="#b23">[24]</ref><ref type="bibr" target="#b24">[25]</ref>. Score-based methods search through the space of all possible causal structures to optimize a specified metric <ref type="bibr" target="#b25">[26]</ref><ref type="bibr" target="#b26">[27]</ref><ref type="bibr" target="#b27">[28]</ref>. The Granger Causality method discovers causal relations by calculating the degree to which the earlier occurred event contributes to the prediction of the latter occurred event <ref type="bibr" target="#b28">[29]</ref><ref type="bibr" target="#b29">[30]</ref><ref type="bibr" target="#b30">[31]</ref>. The constraint-based and score-based methods require stringent assumptions about data distribution, making them less suitable for video data. The Granger Causality methods are more suitable yet face challenges when applied directly to video data, our method reaches better performance by utilizing causal inference methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Benchmark</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">MECD task settings</head><p>Our Multi-Event Causal Discovery (MECD) task is designed to test the ability of causal discovery in multi-event videos. Given a video E that contains chronologically organized N events, E := {e 1 , . . . , e N }, the task aims at determining whether any previous event e n (n &lt; N ) has a causal The word cloud is also summarized for video types. In (a2), the left chart indicates the impact of positions of events on their causality where we find the second last event tends to be more significant; while the right chart plots the number of events in videos.</p><p>relation with the last one (i.e., e N ). Specifically, an event e n = {v n , c n } consists of a video clip v n and the corresponding caption c n . Without loss of generality, relations of previous events to the last one can be expressed as r = [r 1 , . . . , r N -1 ], where r k (k &lt; N ) is set to "1" to indicate the existence of e k 's causal relation with e N , and "0" otherwise. Notably, this setting is generalizable to causal relations of any of two events as long as we cut off the video and treat the latter one as the last event.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">MECD task dataset</head><p>Data Source The Multi Events Causal Discovery (MECD) task contains videos with multiple events and intricate causal relationships. The ActivityNet Captions dataset <ref type="bibr" target="#b31">[32]</ref> is built on ActivityNet v1.3 which includes 20k 120-second YouTube untrimmed videos. We carefully reorganize the ActivityNet Captions dataset and select 1,105 lifestyle videos that span diverse scenarios. We call this new dataset as MECD dataset, where 806 and 299 videos are randomly split for training and testing, respectively. Specifically, each video in the MECD dataset contains 4 to 11 events, with a minimum of 2 premise events exhibiting causal relations with the last one. Fig. Dataset Annotation The annotations of MECD dataset include 4 attributes. The "duration", "sentence", and "timestamps" attributes in annotations remain the same as the ActivityNet Captions dataset. Specifically, in the context of our task, a new attribute "relation" is introduced. To obtain this attribute, relations among events are firstly annotated by GPT-4 API <ref type="bibr" target="#b32">[33]</ref>, and subsequently refined by five human annotators. Through a cross-annotation process, gt labels are determined by the majority of the annotators' causal relation choices, thus mitigating potential inaccuracies and subjective biases to a certain extent. We also present the impact of positions of events on their causality and number of events in videos in Fig. <ref type="figure" target="#fig_1">2</ref> (a2), annotation pipeline is in Appendix Sec. B.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Methodology</head><p>In this section, we present our Video Granger Causality Model (VGCM), as shown in Fig. <ref type="figure" target="#fig_2">3</ref>. This model establishes the global connections across all events, and deduces the causality of a premise event by comparing the output features when it is masked or not, under the concept of the Event Causality Test. However, masking out an event may lead to the problem of confounding and illusion.</p><p>In this context, we further utilize causal inference methods to address these by compensating or removing the effect of previous or subsequent causal events to mitigate the confounding meanwhile during inference the extra chain of thoughts and existence-only descriptions relieve the illusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">VGCM: Video Granger Causality Model</head><p>Building upon the Granger Causality method introduced in <ref type="bibr" target="#b33">[34]</ref><ref type="bibr" target="#b34">[35]</ref><ref type="bibr" target="#b35">[36]</ref>, our core motivation for constructing VGCM is Event Causality Test: To compare the prediction result of the last event using all the premise events with or without a certain event in it. If the results exhibit obvious divergence, it indicates that the current premise event is causally related to the result event. Two data streams V p and V m k serve as input, video and text embeddings are concatenated after being separately embedded. The VGCM incorporates two classifiers, the caption head takes the unmasked stream to accomplish the event-predicting task, while the relation head discovers the causal relations with two embedding streams.</p><p>We design VGCM to take in both the video clips and the captions to maximize information utilization. As illustrated in Fig. <ref type="figure" target="#fig_2">3</ref>, our proposed VGCM is a multi-modal transformer-based structure with a video encoder and caption encoder, and a multi-modal decoder with causal relation head to discover causal relations through the predicting process and the comparison of predicting results.</p><p>Based on this, we denote E p as the set of all the premise events E p := E \ e N , and E m k := E p \ e k as the event set where the premise event e k (k &lt; N ) is masked. Notably, we mask the event e k by setting all zeros to its video clip v k and assign constant characters to the caption c k .</p><p>Following <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b36">[37]</ref><ref type="bibr" target="#b37">[38]</ref><ref type="bibr" target="#b38">[39]</ref>, we firstly pretrain a video encoder Φ pre under an action recognition task to extract the features of the video clips. We essentially create two paths, one for the unmasked event set E p (orange path in Fig. <ref type="figure" target="#fig_2">3</ref>) while the other for the set with one event (i.e., e k ) masked E m k (green path in Fig. <ref type="figure" target="#fig_2">3</ref>). The video clips and captions are first separately encoded using Enc V and Enc C to obtain compact features, then their features are sent to a multi-modal decoder Dec that shares weights for both paths to fuse the information. Afterward, several model heads are employed for feature comparison and loss measurement. V p and C p are the video clip and caption matrix concatenated from all premise events set E p , similarly,</p><formula xml:id="formula_0">V m k and C m k are from E m k . F p = Enc V (Φ pre (V p )), O p = [Dec(Cat(F p , Enc C (C p ))] N -1 F m k = Enc V (Φ pre (V m k )), O m k = [Dec(Cat(F m k , Enc C (C m k ))] N -1<label>(1)</label></formula><p>where Enc V and Enc T represent the encoder module of video clips and captions, respectively. Dec is a multi-modal decoder that shares weights for both paths. Cat indicates the concatenate operation, and [-] N -1 indicates the (N -1)-th slice at dimension 0. F p and F m k are features after encoding, and O p and O m k are the output features, which are then used for comparison of difference. Incorporating both visual and linguistic representations, the decoder conducts cross-modal reasoning and leverages the context from the unmasked premise events to posit a meaningful representation of the most likely explanatory result event.</p><p>Subsequently, the feature O p deduced from the unmasked events is sent to the caption head for prediction. Additionally, in order to compare the difference of the prediction result, O p , O m k are directed to the relation head for causal relation discovery. The result event e N is encoded the same way as e k to get feature F N = Enc V (Φ pre (v N )) and the output O N = Dec(Cat(F N , Enc C (C N )), O N is aggregated for reasoning (red path in Fig. <ref type="figure" target="#fig_2">3</ref>). The relation head consists of a semantic query module and a self-enhancement module, where outputs are concatenated and then passed through the cross-reasoning layer g r for further interaction. Last but not least, the auxiliary similarity is measured between O p and O m k as a supplement to the output information of the relation head. After the reasoning process, the prediction output of the causal relation rk can be represented by:</p><formula xml:id="formula_1">rk = g r (Cat(Φ C att (Cat(O m k , O N ), Cat(O p , O N )), Φ I att (Cat(O m k , O N ))))</formula><p>(2) where Φ C att represents cross-attention, Φ I att represents self-attention, g r represents linear layer. The training objective consists of two main directions as previously discussed:</p><p>To reconstruct the textual and visual representation of the result event e N , we introduce caption loss and reconstruction loss, respectively. Caption loss L C ensures an accurate prediction of the result caption ĉN given all the premise events E p . Simultaneously, visual reconstruction loss L V forces the encoder to "imagine" a representation of the result video clip vN that better aligns with the original representation v N . These losses allow the model to predict visual and textual representations that are close to the original representations, which better supports our method of inferring causal relations by comparing the results of the two-stream predictions.</p><p>For the objective of causal discovery, we introduce causal relation loss and an auxiliary semantics similarity loss. Causal relation loss L R supervised the output relations rk . Meanwhile, the semantics similarity loss L S is introduced to guarantee the semantics similarity of result event prediction under the presence or absence of a causal-relation-free premise event. The complete loss function is:</p><formula xml:id="formula_2">L = L C (c N , ĉN ) + λ R L R (r k , rk ) + λ V L V (F p N , F N ) + λ S sign(r k )L S (O m k , O p )<label>(3)</label></formula><p>where λ R , λ V , and λ S are weights for trade off. L C and L R are the cross-entropy losses, L V and L S are the mse losses, F p N is the N-th slice of F p , which represents the encoded feature of e N .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Causal Inference in VGCM</head><p>In Sec. 4.1, we employ the concept of Granger Causality to design our VGCM model under the principle of Event Causality Test which may, however, introduce causality confounding and illusory. Below we introduce these issues in detail, as well as how we manage to solve the problems.</p><p>Causality confounding is a phenomenon where the original causal relations across events are impacted due to modification (i.e., masking) of some intermediate events (i.e., e k ). Existing disentangled representation learning works <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b40">41]</ref> disentangled different attributes of a variable by supervising high-order distribution under strict assumptions but failed in disentangling different variables.</p><p>Specifically, when e k is masked for the comparison in VGCM, the causal relations between e k 's adjacent events and the last event are impacted, leading to a confounding of causal effects. Notably, for brevity, we only employ e k 's previous one event e k-1 and its subsequent one event e k+1 for analysis, but the same analysis also applies to all the previous or subsequent events. To be specific, there exist two distinct kinds of confounding when e k is absent: 1) Causal effects of e k-1 to e N may be lost, as its connection to e N is built upon e k , (green path in Fig. <ref type="figure" target="#fig_3">4</ref> (a1)). 2) Causal effects of e k+1 to e N may be redundant, as e k must be a necessary prior of its causality, (red path in Fig. <ref type="figure" target="#fig_3">4</ref> (a1)).</p><p>Illusory causality is another issue that may lead to some spatial or temporal misunderstandings, including illusory temporal and existence causality. 1) Illusory temporal causality is the situation that events could have tight temporal ordering, but they in fact have no causal relations. 2) Additionally, illusory existence causality occurs when an object introduced in the premise event is a necessary condition for the result, but the premise event does not semantically serve as a reason. Notably, we find that illusory in multi-event videos is much more significant than two independent events, which also tends to be exacerbated by causality confounding.</p><p>Overall, causality confounding and illusory causality both bring difficulties for relation modeling of events in videos. Notably, these two issues are coupled in that causality confounding tends to exacerbate illusory causality by misallocating attention to temporal ordering and causal effect. Therefore, illusory causality can be partially relieved by solving the problem of causality confounding.</p><p>When considering taking the illusory causality, the chain of thoughts <ref type="bibr" target="#b19">[20]</ref><ref type="bibr" target="#b20">[21]</ref><ref type="bibr" target="#b21">[22]</ref> has been shown in LLMs to lead the model to logical thinking which is similar to human thought process, the chain of thoughts T cot[e k-1 :e N ] provides a step-by-step process of reasoning the e N from e k-1 . Specifically, T cot[e k-1 :e N ] is obtained using GPT-4 API <ref type="bibr" target="#b32">[33]</ref> by feeding it with e k-1 , e N along with a prompt asking it to provide the probable reasoning chain. We consider utilizing it in causal inference to eliminate the attention bias on temporal correlations introduced by non-causal temporal knowledge.</p><p>Besides, as the illusory existence causality is caused by the objects' correlation between the events, we address this influence by keeping objects in the green path in Fig. <ref type="figure" target="#fig_2">3</ref> the same as those in the orange path. We introduce an alternative event e 0 k = {v 0 k , c 0 k } of e k to briefly recaps the objects in e k . Specifically, c 0 k is obtained using GPT-4 API <ref type="bibr" target="#b32">[33]</ref> by feeding it with c k along with a prompt asking it to extract the objects from c k and organize them as the sentence such as "There are objects A, B and C.". Consequently, we opt to employ c 0 k to approximate e 0 k in our VGCM model while omitting v 0 k , as c 0 k is sufficient already to convey the information of objects. By providing e 0 k , all the necessary objects are still available in this path, thus effectively mitigating the illusory existence causality, facilitating the model to focus more on essential and causality-related semantic information.</p><p>To tackle the issues above, we introduce two causal inference methods: the front-door adjustment <ref type="bibr">[42]</ref> for the missing causal effect of e k-1 and counterfactual inference <ref type="bibr">[42]</ref> for the redundant causal effect of e k+1 . Meanwhile, the chain of thoughts T cot[e k-1 :e N ] and the descriptions of existence c 0 k are also provided to carefully address illusory causality, which in turn mitigates confounding.</p><p>We establish a causality diagram in Fig. <ref type="figure" target="#fig_3">4</ref> (a2) for an improved elaboration. On masking e k , the causality confounding that requires compensation F C or removement F R can be expressed as:</p><formula xml:id="formula_3">F C = P (e N |e k ) -P (e N |do(e k )), F R = P (e N |e k+1 ) -P (e N |do(e k+1 ))<label>(4)</label></formula><p>where P (e N |e k ) and P (e N |e k+1 )represents the process by which we predict e N from e k and e k+1 in the orange path in Fig. <ref type="figure" target="#fig_2">3</ref>, and do(•) represents do-operation in causal inference <ref type="bibr" target="#b14">[15]</ref> that cuts off the causal relation between the event and its causes.</p><p>We aggregate the subsequent events e k+1 , the current event e k and the chain of thoughts T cot[e k-1 :e N ] using a linear layer g do for aggregation and the cross-attention and self-attention, according to the study in <ref type="bibr" target="#b41">[43,</ref><ref type="bibr" target="#b16">17]</ref>, P (e N |do(e k )) can be implemented as:</p><formula xml:id="formula_4">P (e N |do(e k )) = g do ((Cat(Φ C att (e k , e k+1 , e k+1 ), Φ I att (e k , e k , e k ), Enc c (T cot[e k-1 :e N ] ))),<label>(5)</label></formula><p>Here, we re-use the cross-attention Φ C att and the self-attention Φ I att modules as in (2) to cut off the causal effect from e k-1 to e k through do-operation, e k only interacts with subsequent events in predicting e N . Then the missing causal effect F C can be compensated since the causal-view operation and illusory temporal causality can be suppressed at the same time with the introduction of the chain of thoughts. Similarly, the redundant causal effect F R can be removed by applying counterfactual intervention, then P (e N |do(e k+1 )) can be represented by:</p><formula xml:id="formula_5">P (e N |do(e k+1 )) = P (e N |e k+1 )[P (e k+1 |e k ) -P (e k+1 |e 0 k )],<label>(6)</label></formula><p>P (e N |do(e k+1 )) effectively cuts off the redundant causal effect between e k+1 and e N for the reason that the causes of e k+1 are replaced with counterfactual description e 0 k , then the illusory existence causality can be suppressed at the same time.</p><p>To refine the originally decoded feature O m k from the path with premise events masking:</p><formula xml:id="formula_6">O ′m k = O m k -Dec(F C ) + Dec(F R )<label>(7)</label></formula><p>where O ′m k is the refined feature that replaces O m k for further deduction of the model. With the refinement feature O ′m k , our VGCM model effectively compensates the connections between e k-1 and e N that were originally lost due to the removal of e k , and effectively removes the redundant causal effect between e k+1 and e N as well.  5 Experiments</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Main results</head><p>Implementation details. including the pretraining process, detailed architecture of VGCM, and hyper-parameters settings can be found in Appendix Sec. A due to space constraints.</p><p>Baselines. We mainly compared our model with basic multi-modal models such as baseline model Videobert <ref type="bibr" target="#b48">[50]</ref> and widely used CLIP-L <ref type="bibr" target="#b49">[51]</ref> and the most similar reasoning model VAR <ref type="bibr" target="#b9">[10]</ref>. Besides, we also conduct experiments on powerful LLM, including GPT-4 <ref type="bibr" target="#b32">[33]</ref> and Gemini-Pro <ref type="bibr" target="#b42">[44]</ref>. VLLM utilized for comparison includes widely accepted GPT4-o <ref type="bibr" target="#b32">[33]</ref>, VideoLLaVA <ref type="bibr" target="#b47">[49]</ref>, MiniGPT-4 <ref type="bibr" target="#b44">[46]</ref>, Video-llama <ref type="bibr" target="#b45">[47]</ref>, VideoChat2 <ref type="bibr" target="#b46">[48]</ref> and MiniGPT4-video <ref type="bibr" target="#b43">[45]</ref>. Specifically, LLMs and VLLMs are conducted under the few shot setting (In-Context Learning) following the causal discovery tasks in NLP <ref type="bibr" target="#b50">[52]</ref><ref type="bibr" target="#b51">[53]</ref><ref type="bibr" target="#b52">[54]</ref>, additionally, we reported the performance of fine-tuned VideoLLaVA and VideoChat2.</p><p>Metrics. We utilize the top-1 accuracy of the output causal relation chains with respect to the final event to evaluate the model's capability in causal discovery. Although our VGCM is designed to discover the causal relations leading to the final event, when truncating the video during inference and redefining the final event as the new result, VGCM can generate a comprehensive causal diagram for the entire video without introducing additional training. Consequently, in addition to the primary metric accuracy, we introduce Structural Hamming Distance (SHD) <ref type="bibr" target="#b53">[55,</ref><ref type="bibr" target="#b54">56]</ref> as a supplementary metric. SHD measures the degree of matching between comprehensive causal graphs by summing the number of incorrect causal relations. In the MECD test set, the average number of causal relations in video causal graphs is 12.31, and a lower Ave SHD value of the test set indicates better performance.</p><p>Results. We report the quantitative results in Tab. 1. Our VGCM without causal inference reaches an accuracy of 66.9%, demonstrating basic reasoning capabilities. Furtherly, the complete VGCM reaches a better performance with an accuracy of 71.2%, outperforming the GPT-4, GPT4-o, finetuned VideoLLaVA <ref type="bibr" target="#b47">[49]</ref> by 11.6%, 5.7%, and 4.1%. Additionally, we explored the effect of altering the input format of the two modalities in Appendix Sec. C.1, indicating that VGCM is not dependent on the input format. The results compared with the two metrics indicate that for most models, accuracy is already adequate to represent their causal discovery capabilities. However, the additional metric Ave SHD indicates that Gemini and GPT-4 exhibit a superior overall capacity for discovering complete relations. An example of the output complete causal diagram is visualized in Figure <ref type="figure">7</ref>.</p><p>GPT-4 <ref type="bibr" target="#b32">[33]</ref> stands out as one of the most advanced LLM models, however, we found that even being provided with sufficient few-shot examples (detailed in Appendix Sec. C.2), its accuracy remains at  As illustrated in Tab. 6, we have assessed the inference speed of various models, with our VGCM achieving a swift 0.76 seconds per sample. The proposed method incurs an overhead of only 8.57% over the Videobert baseline. It is noteworthy that our inference speed is 3 to 6 times faster than that of all Video LLMs. The inference speed experiments were conducted on 1 NVIDIA A6000 GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Ablation Study</head><p>Video Granger Causality Model design We designed our causal discovery model based on the Granger Causality, three auxiliary losses are applied. The performances in Tab. 2 indicate that our VGCM benefits from the design of L V and L C , for they support our method of inferring causal relations by facilitating the model with event prediction ability. L S also benefits our model by supervising the causal feature similarity of e N with and without non-causal event e k masked.</p><p>Front-door adjustment with chain of thoughts candidate The method does improve reasoning ability in Tab. 2. We conduct an experiment in Tab. 4 for further proof. Since events closer to the result event are higher as the cause, the model likely learns these biased time-domain tendencies. So we compare the accuracy of VGCM without front-door adjustment with chain of thought candidate and VGCM in determining the first relation r 1 and the last relation r N -1 . The results demonstrate that temporal illusory causality is greatly mitigated, visualization can be found in Fig. <ref type="figure">8</ref> Example 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Counterfactual intervention with existence-only descriptions</head><p>The performance in Tab. 2 shows that counterfactual intervention with existence-only descriptions does facilitate the model with powerful reasoning ability. We dive into further analysis on the basis that when a non-causal event is masked, the causal feature F m k fed into the causal relation head should be similar to the unmasked feature F p , instead, a bigger gap appears when masking a causal event. For stronger proof, we measure the difference in feature similarity in Tab. 3 and Fig. <ref type="figure">6</ref>. We define the similarities division as the quotient of the similarity(F m k , F p ) with a non-causal e k masked over with a causal e k masked. In the experiment, we find that the similarity division is always above 1 without the counterfactual intervention, however, the existence illusory is solved with counterfactual intervention for the reason that the division is below 1 of VGCM, example visualization can be found in Fig. <ref type="figure">8</ref> Example 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Robustness Analysis</head><p>Model Robustness To prove our model's robust reasoning ability, we split the MECD dataset into five categories, and conduct an experiment similar to the open-set setting with cross-validation. VGCM </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Illusory temporal causality removed by causal inference Example 1</head><p>interviews riders race information wait to take off a man falls off interviews two men another falls off</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Illusory existence causality removed by existence description</head><p>Existence-only description: There is an interviewer and a racer.  2.12 VideoChat2 <ref type="bibr" target="#b46">[48]</ref> 2.96 MiniGPT4-video <ref type="bibr" target="#b43">[45]</ref> 3.98 MiniGPT-4 <ref type="bibr" target="#b44">[46]</ref> 4.72 reaches an average accuracy of 64.4%, outperforms VGCM without causal inference and VAR by 5.2% and 9.6%, details can be found in Appendix Sec. D.1.</p><p>Moreover, to further validate the generalization capabilities of our model, we evaluate the quality of output causal relations on a related and representative video reasoning task: Video Question Answering (VQA) as shown in Tab. 7. Specifically, during inference on the multi-event subset of ActivityNet-QA <ref type="bibr" target="#b57">[59]</ref> (The part that overlaps with the MECD test set), we prompted MiniGPT4video <ref type="bibr" target="#b43">[45]</ref> with additional causal relations outputs alongside the standard question inputs. This paradigm facilitates the VLLMs in considering the task from a causal perspective. As shown in the table below, when prompted with these additional causal relations, the answering accuracy of MiniGPT4-video <ref type="bibr" target="#b43">[45]</ref> improved by our VGCM surpasses other strong VLLMs like VideoChat2 <ref type="bibr" target="#b46">[48]</ref>. These findings confirm that our model can provide accurate causal perception for videos, significantly improving performance on related video reasoning tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset Robustness</head><p>We study the subjectivity and data volume of our proposed MECD dataset, which is shown in Tab. 5. In the experiments of increasing the ratio of randomly flipped annotated causal relations (flipping only one relation of the whole causal relations of video), the accuracy decreases slightly, demonstrating the small amount of subjectivity in labeling does not have a serious impact. Besides, we analyze the scale of data, the increment from 600 examples to 806 examples yields a very modest improvement, indicating the adequacy of our dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We proposed a novel task, multi-event video causal discovery (MECD), which focuses on eventlevel causal discovery in long-term videos. Besides, we built the MECD dataset with long-term daily life video datasets with causal relations to support this task and proposed the first video events causal discovery framework VGCM in the principles of Granger Causality. Additionally, our proposed VGCM was facilitated with deeper reasoning ability through causal inference with the chain of thoughts and existence-only descriptions. Our VGCM significantly outperforms GPT-4o and VideoLLaVA by 5.7% and 4.1%, respectively, demonstrating its robust reasoning ability.  does not directly lead to their victory, and the subsequent celebrations also lack any causal links with the outcome. Indeed, the false discovery by the GPT-4 API could stem from the illusion of causality, where the team's mere presence is perceived as a necessary condition for the outcome. Additionally, the illusion of temporal causality may also play a role, as statistics indicate that celebrations often occur before the announcement of the competition winner. These cognitive biases could contribute to the erroneous causal inference made by the GPT-4 API in this scenario.</p><p>When we request a detailed explanation from the GPT-4 API regarding the discovered causal relation between the result event and the initial appearance of the team, the response is "Setting up the motive for the last event." Obviously, the GPT-4 confuses causality with the illusion of existence causality. In contrast, our VGCM makes a correct inference in this scenario. Furthermore, when we seek detailed reasons from the GPT-4 API for the discovered causal relations between the result event and the celebrations, the answer is "Indicating their satisfaction and confidence in their performance, implying they believe they have a good chance to win." Here, the GPT-4 API misinterprets causality by associating it with the expression of subjective emotions unrelated to the events in question. It may mistake the display of subjective emotions for the presence of objectively implied causality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 Annotation pipeline of MECD dataset</head><p>To improve the accuracy and mitigate subjective biases in annotating causal relations, we employ a cross-annotation strategy <ref type="bibr" target="#b60">[62]</ref><ref type="bibr" target="#b61">[63]</ref><ref type="bibr" target="#b62">[64]</ref>. The interactive interface used by annotators during the labeling process of our MECD dataset is illustrated in Fig. <ref type="figure" target="#fig_10">11</ref>. Each video example is endowed with a without causality with causality</p><p>Team comes out Attempts flips Another girl follows Team dances </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Tips:</head><p>Your Annotation： (0 for without causality, 1 for with causality) "relation" attribute. First, GPT-4 <ref type="bibr" target="#b32">[33]</ref> provides an initial annotation of attribution, which is then further refined by five human annotators. Ground truth labels are determined based on the majority choices of the annotators regarding causal relations. This methodology ensures the creation of a more reliable and objective dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4 Annotation examples of MECD</head><p>Annotation examples of MECD are shown in Fig. <ref type="figure" target="#fig_12">12</ref>, our MECD dataset is carefully annotated to support the challenging task proposed with complete premise information.</p><p>"Existence" "Sentences"</p><p>1."man is sitting on a table eating a sandwich in a rstaurant talking to other man.", 2." man is watching the man while eats and talking to him.", 3." both men stands and play rock paper scissors and man hits the man sand this falls to floor.", 4." man is sitting on the chair and talks to the man on thefloor and throw him the sandwich.", 5." anoher man siting in front talk to the man in the desk.", 6." man stands and talk to the man on the floor and hits him again and the man spit blood."</p><p>"COT"</p><p>1."This event initially introduces the two key characters, one of which enjoying a sandwich in a public setting and communication with the other. This dynamic sets the ecosystem for an interactive relationship, which might include confrontation leading to the end scene.", 2."This event shows the observer-man's active interest and engagement with the eating man, illustrating a relationship that could potentially be conflicly, hence leading to a physical interaction in the end.", 3."This sudden aggressive action shifts the dynamic between the two men, escalating the situation from a previously peaceful meal to one with physical confrontation. This change in interaction sets the stage for the violent ending.", 4."This gesture of throwing the sandwich at the man on the floor further escalates the hostility in the interaction. Disregard for the other man's dignity is apparent, indicating potential for further violent interactions.", 5."This event depicts an interaction between a third man and the man on the desk, ignoring the man on the floor. The lack of intervention prolongs the conflict between the two main characters, pushing the situation towards the bloody ending."</p><p>1."There is a man sitting, a table, a restaurant, a sandwich and another man.", 2."There is a man watching and talking.", 3."There are two men, rock paper scissors game and a sand.", 4."There is a man sitting on a chair, another man on the floor and a sandwich.", 5."There is a man, a desk, a computer, a water, and another man laying on the floor.", 6."There is a man sitting in front and another man on the desk."</p><p>"Relations" 01111 without causality with causality "Existence" "Sentences" "COT"</p><p>1."A cricket match is ongoing with Sri Lanka playing against another team, the atmosphere is competitive, which naturally increases pressure and expectations, leading to intense crowd reactions.", 2."The competitive nature of the game implies that every action, every run scored, matters significantly, which in turn can cause intense reactions from the spectators." 3."The uniform identifies the teams and creates a sense of belonging, unity, and rivalry among the teams and the spectators, solidifying the sides spectators will cheer for.", 4."The act of scoring runs in a cricket match is a significant event -it directly contributes to the final outcome of the match and draws strong responses from the crowd; the more the runs, the louder the cheers.", 5."The video shows not just one, but several cricket matches featuring the Sri Lanka team. This series of events builds up the hype, intensity, and anticipation among the spectators, culminating in a stadium filled with cheering crowds."</p><p>1."There is a cricket team of Sri Lanka and another country.", 2."There are cricketers and a field.", 3."There is a Sri Lanka team in a blue uniform.", 4."There is a batsman, a bowler and an overhand ball.", 5."There are different cricket matches of Sri Lanka, and teams from different countries."</p><p>"Relations" 11010 without causality with causality </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Additional Experiments C.1 Modalities analysis of causality discovering</head><p>The MECD task employs both video input and corresponding captions to uncover causality. Our objective in this experiment is to evaluate the degree of reliance on these two modalities in causal discovery. Typically, each event in our MECD task consists of a textual input with an average of 13.5 words caption and a visual input of 50 frames.</p><p>To investigate the influence of the text modality, we employ a masking strategy for the input caption of the premise event, gradually increasing the masking ratio from 10% to 80%. The results presented  In contrast, the experimental results suggest a more obvious performance decrease towards less visual modality input in the causality discovery task, as shown in Tab. 9. However, even with 80% masking of either modality, the results consistently outperform our strong baseline model, VideoLLaVA, underscoring the robust causal discovery capability of VGCM. Furthermore, we conducted experiments involving simultaneous masking of both modalities of information. Interestingly, we observe a noticeable decrease in accuracy compared to when only one modality is masked. This observation highlights the importance of jointly considering both modalities in the causality discovery task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Adequacy of the prompts provided to GPT-4</head><p>To delve deeper into the limitations of the straightforward baseline approach of prompting GPT-4, we examined the correlation between its accuracy and the number of video examples provided in the few-shot prompts. The findings, illustrated in Fig. <ref type="figure" target="#fig_13">13</ref>, suggest that increasing the number of examples shown to GPT-4 does not effectively enhance its accuracy. This suggests that the limitation of the GPT-4 baseline is not strongly correlated with the number of presented examples but rather is more attributable to its intrinsic limitation in understanding complex causal relationships solely through text modality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Experiments details D.1 Details of causality discovery experiment</head><p>In the open-set experiment of exploring reasoning ability, the five categories mainly consist of the activities below, demonstrating the colorful daily activities included in our dataset. In this section, we introduce the detailed method of prompting GPT-4 <ref type="bibr" target="#b32">[33]</ref> to generate more premise information. Firstly, we prompt the GPT-4 with the following prompts to generate the description-only sentences.</p><p># Task: Each input consists of n sentences, and the text description of each sentence has been given correspondingly (separated by " ",). You need to offer the existence description of each sentence.</p><p>Besides the task description, we further append the few-shot paradigm (In-Context Learning) introduced in <ref type="bibr">[52-54, 65, 66]</ref>. Similarly, we prompt the GPT-4 <ref type="bibr" target="#b32">[33]</ref> with the following instructions to generate the chain-of-thoughts candidate sentences in the same few-shot paradigm.</p><p># Task: Each video consists of n events and the text description of each event has been given correspondingly separated by " ",). First n-1 events might be the cause of the last event. You need to offer the chain of thoughts you derive that causes the last event.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3 Chain of thoughts examples</head><p>In this section, we present an example of the chain of thoughts prompted, the corresponding premise event and result event descriptions are also shown below:</p><p>{"premise event sentence": "He continues sharpening the knife, turn it again to further sharpen the other side and wipe it with paper towel." "result event sentence": "Throws the old and dirty paper towel and reach the roll of paper towel and clean the knife."</p><p>"COT": "The repeated action of sharpening and wiping the knife underscores the importance of both the knife's sharpness and cleaners, leading directly to the final action of disposing of the used paper towel and getting a new one to ensure the knife is thoroughly clean"} The chain of thoughts shown above provides a logical causal chain between the event of the cleaning of the knife and the subsequent throwing of the dirty paper towel. The reasoning initiates by considering the heightened need for sharp and pristine knives achieved through sharpening. This causal chain is then expanded by suggesting that this demand could have led to the replacement of the paper towel. The chain of thoughts generated from GPT-4 serves as a candidate in the process of correct reasoning, contributing to the exploration of potential causal relations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Limitations and future works</head><p>1. The video we input for causal discovery needs to provide timestamps, we encourage future work to realize causal discovery with weakly annotated inputs.</p><p>2. VGCM might still require refinement in understanding causality within higher-level semantics, especially in the mining of some obscure mental or emotional influences according to the failure cases analysis in Appendix Sec. B.1.</p><p>3. VGCM is based on the supervised paradigm of causal discovery, subsequent works may be able to extend to the unsupervised paradigm.</p><p>4. The causal graphs proposed by the MECD may also enhance other video understanding tasks, such as video dense captioning and video event prediction, or could be introduced to other reasoning tasks, including text reasoning and mathematical reasoning tasks.</p><p>5. The evaluation results of VLLMs and LLMs on the MECD task also help researchers study language models' current issues and limitations in complex reasoning. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NeurIPS Paper Checklist</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Guidelines:</head><p>• The answer NA means that the abstract and introduction do not include the claims made in the paper. • The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. • The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. • It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Limitations</head><p>Question: Does the paper discuss the limitations of the work performed by the authors?</p><p>Answer: <ref type="bibr">[Yes]</ref> Justification: Limitations are discussed in Sec. E in the Appendix.</p><p>Guidelines:</p><p>• The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. • The authors are encouraged to create a separate "Limitations" section in their paper.</p><p>• The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. • The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. • The authors should reflect on the factors that influence the performance of the approach.</p><p>For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. • The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. • If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. • While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Theory Assumptions and Proofs</head><p>Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?</p><p>Answer: [Yes]</p><p>Answer: <ref type="bibr">[Yes]</ref> Justification: Codes and datasets are released on the GitHub Page.</p><p>Guidelines:</p><p>• The answer NA means that paper does not include experiments requiring code. • Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experimental Setting/Details</head><p>Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results?</p><p>Answer: [Yes]</p><p>Justification: Implementation details are provided in Sec. A in the Appendix.</p><p>• The answer NA means that the paper does not include experiments.</p><p>• The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. • The full details can be provided either with the code, in appendix, or as supplemental material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Experiment Statistical Significance</head><p>Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments?</p><p>Answer: <ref type="bibr">[No]</ref> Justification: We report the average results under three random seeds (2023, 2024, 2025).</p><p>Guidelines:</p><p>• The answer NA means that the paper does not include experiments.</p><p>• The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. • The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). • The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) • The assumptions made should be given (e.g., Normally distributed errors). • It should be clear whether the error bar is the standard deviation or the standard error of the mean.</p><p>• It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified. • For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). • If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Experiments Compute Resources</head><p>Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments?</p><p>Answer: [Yes]</p><p>Justification: Implementation details are provided in Sec. A in the Appendix.</p><p>Guidelines:</p><p>• The answer NA means that the paper does not include experiments.</p><p>• The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. • The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. • The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.">Code Of Ethics</head><p>Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics <ref type="url" target="https://neurips.cc/public/EthicsGuidelines">https://neurips.cc/public/EthicsGuidelines</ref>?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Answer: [Yes]</head><p>Justification: We conducted the research in the paper conform, in every respect, with the NeurIPS Code of Ethics.</p><p>Guidelines:</p><p>• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.</p><p>• If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. • The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.">Broader Impacts</head><p>Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Answer: [NA]</head><p>Justification: There is no societal impact of the work performed.</p><p>Guidelines:</p><p>• The answer NA means that there is no societal impact of the work performed.</p><p>• If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. • Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.</p><p>• The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. • The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. • If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11.">Safeguards</head><p>Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)?</p><p>Answer: [NA] Justification: The paper poses no such risks.</p><p>Guidelines:</p><p>• The answer NA means that the paper poses no such risks.</p><p>• Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. • Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. • We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.</p><p>12. Licenses for existing assets Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected?</p><p>Answer: <ref type="bibr">[Yes]</ref> Justification: ActivityNet Captions Dataset is with no license needed.</p><p>Guidelines:</p><p>• The answer NA means that the paper does not use existing assets.</p><p>• The authors should cite the original paper that produced the code package or dataset.</p><p>• The authors should state which version of the asset is used and, if possible, include a URL. • The name of the license (e.g., CC-BY 4.0) should be included for each asset.</p><p>• For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. • If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. • For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: (a): Illustration of Multi-Event Causal Discovery Task, where the 3rd and 5th premise events account for the occurrence of the final event. The objective of our task is to determine whether a causal relation exists between events and outputs a structured causal diagram. (c): Example of causality confounding. (d)&amp;(e): Illustration of illusory causality.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Constitute of MECD dataset. In (a1), we present 5 main video categories of the dataset.The word cloud is also summarized for video types. In (a2), the left chart indicates the impact of positions of events on their causality where we find the second last event tends to be more significant; while the right chart plots the number of events in videos.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Video Granger Causality Model. Two data streams V p and V m k serve as input, video and text embeddings are concatenated after being separately embedded. The VGCM incorporates two classifiers, the caption head takes the unmasked stream to accomplish the event-predicting task, while the relation head discovers the causal relations with two embedding streams.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Causal Effect of the Adjacent Events and Causality Diagram. (a1) shows the causality of the third event analyzed, the red causal effect needs to be compensated while the green needs to be mitigated. (a2) shows the causal inference methods corresponding to the two causal effects.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :Figure 6 :Figure 7 :</head><label>567</label><figDesc>Figure 5: Dataset robustness. Accuracy decreases slightly when increasing noise, and increases slowly when increasing the training data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>points to the knife another side</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Example 2 Figure 8 :</head><label>28</label><figDesc>Figure 8: Successful abduction examples of our VGCM. Results indicate that after utilizing causal inference methods, illusory causality is suppressed and robust abduction ability is facilitated.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>A②③①</head><label></label><figDesc>hits a ball B hits a ball C prepares to hit B misses hitting B hits again C hits back B&amp;C Continue hitting ① Creating an anticipation ② Expecting subsequent attempts ③ Creating the anticipation (Fail to discover) Collapse linked to the check ③ Attracting instructor's attention First arrow misses Loads another arrow A backdrop Backdrop collapses People laughing Instructor walks over without causality with causality A cricket play A competitive game Home team in blue Batsman scores four Different game Spectators cheering ① ② Game attracts spectators ② Competitive atmosphere intense crowds ③ Directly contributes to the final outcome Fail to discover</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: More successful abduction examples of our proposed VGCM. The relation which reveals our method of eliminating illusory causality is marked by a red five-pointed star ⋆. The failure case is annotated in a red dotted line .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 10 : 1 .</head><label>101</label><figDesc>Figure 10: Failure abduction examples of GPT-4. Many failure cases of GPT's causal reasoning are due to confusion with illusions and the conflation of subjective emotions with objective laws.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: Annotation pipeline of MECD dataset. Illustration of the interactive interface used by annotators during the labeling process of our MECD dataset. Key information is provided during annotation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>1 .</head><label>1</label><figDesc>"The cricket team of Sri Lanka is playing against another country.", 2." the cricketers are playing a competitive game in the field.", 3." The Sri Lanka team is represented by the blue uniform.", 4." The batsman scores four runs as the bowler throws an overhand ball.", 5." The video shows different cricket matches taking place where Sri Lanka is playing against teams from different countries.", 6." The stadium is filled with spectators cheering for the cricketers."</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 12 :</head><label>12</label><figDesc>Figure 12: Annotation examples of MECD. Annotation examples of MECD are shown. Newly annotated attributes "Relations", "COT", "Existence" and the existing caption attribute "Sentences" are shown along with the video frames.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 13 :</head><label>13</label><figDesc>Figure 13: The trend chart of inference accuracy as the number of examples changes under the In Context Learning paradigm. Accuracy increases slightly when increasing the number of few-shot examples, when the number of examples &gt; 3, the accuracy tends to remain constant.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Sports:</head><label></label><figDesc>Arm wrestling, BMX, Beach soccer, Blow-drying hair, Capoeira, Croquet, Futsal, Ice fishing, Kite flying, Playing beach volleyball Creating &amp; Making: Assembling bicycle, Baking cookies, Building sandcastles, Carving jacko-lanterns, Decorating the Christmas tree, Hanging wallpaper, Making a cake, Making an omelet, Painting fence, Putting in contact lenses Daily Activities: Changing car wheel, Cleaning sink, Drinking coffee, Eating ice cream, Gargling mouthwash, Hanging wallpaper, Kneeling, Peeling potatoes, Putting on shoes, Washing face Performing: Baton twirling, Bullfighting, Drum corps, Fun sliding down, Hula hoop, Playing congas, Playing drums, Playing rubik cube, Playing saxophone, Tumbling Socializing: Beer pong, Playing blackjack, Playing field hockey, Playing harmonica, Playing piano, Playing squash, Playing water polo, Rock climbing, Smoking hookah, Belly dance D.2 Prompts to generate auxiliary premise information</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>1 .</head><label>1</label><figDesc>ClaimsQuestion: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope?Answer:[Yes]    Justification: The main contributions and scope are summarized in Sec. 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Main results. Experiments validate the effectiveness of our VGCM framework in reasoning causal relations towards multi-event videos, outperforming GPT-4o and VideoLLaVA by 5.7% and 4.1%, respectively. ‡ indicates without causal inference. Random results and human performances are also provided.</figDesc><table><row><cell></cell><cell>Paradigm</cell><cell>Method</cell><cell cols="2">Ave SHD ↓ Accuracy ↑</cell></row><row><cell>-</cell><cell>Random Guess</cell><cell>Guess all causal. Guess all non-causal.</cell><cell>6.95 5.36</cell><cell>42.4 57.6</cell></row><row><cell></cell><cell>LLM Base</cell><cell>Gemini-1.5-Pro [44] GPT-4-0613 [33]</cell><cell>4.91 4.92</cell><cell>59.3 59.6</cell></row><row><cell></cell><cell></cell><cell>MiniGPT4-video [45]</cell><cell>5.16</cell><cell>56.8</cell></row><row><cell>Few-shot</cell><cell></cell><cell>MiniGPT-4 [46]</cell><cell>5.14</cell><cell>57.5</cell></row><row><cell></cell><cell>VLLM Base</cell><cell>Video-llama [47] VideoChat2 [48]</cell><cell>5.10 4.89</cell><cell>60.6 60.7</cell></row><row><cell></cell><cell></cell><cell>VideoLLaVA [49]</cell><cell>4.85</cell><cell>62.5</cell></row><row><cell></cell><cell></cell><cell>GPT-4o [33]</cell><cell>4.69</cell><cell>65.5</cell></row><row><cell></cell><cell></cell><cell>VAR [10]</cell><cell>4.96</cell><cell>57.3</cell></row><row><cell></cell><cell>Multi-modal</cell><cell>Videobert [50]</cell><cell>4.95</cell><cell>60.9</cell></row><row><cell></cell><cell></cell><cell>CLIP (ViT-L/14) [51]</cell><cell>4.77</cell><cell>62.9</cell></row><row><cell>Fine-tuned</cell><cell>VLLM Base</cell><cell>VideoChat2 [48] VideoLLaVA [49]</cell><cell>4.77 4.73</cell><cell>66.9 67.1</cell></row><row><cell></cell><cell>Ours</cell><cell>VGCM  ‡ VGCM</cell><cell>4.51 4.19</cell><cell>67.0 71.2</cell></row><row><cell>-</cell><cell>Humans</cell><cell>Deductive Reasoning</cell><cell>2.05</cell><cell>87.2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Ablation Study. Adj indicates the front-door adjustment, and inter indicates the counterfactual intervention.</figDesc><table><row><cell cols="3">Base designs L C L V L S</cell><cell cols="2">Causal methods Adj Inter</cell><cell>Acc</cell></row><row><cell></cell><cell>✓</cell><cell>✓</cell><cell></cell><cell></cell><cell>64.8</cell></row><row><cell>✓</cell><cell></cell><cell>✓</cell><cell></cell><cell></cell><cell>65.1</cell></row><row><cell>✓</cell><cell>✓</cell><cell></cell><cell></cell><cell></cell><cell>65.3</cell></row><row><cell>✓</cell><cell>✓</cell><cell>✓</cell><cell></cell><cell></cell><cell>67.0</cell></row><row><cell>✓</cell><cell>✓</cell><cell>✓</cell><cell>✓</cell><cell></cell><cell>68.7</cell></row><row><cell>✓</cell><cell>✓</cell><cell>✓</cell><cell></cell><cell>✓</cell><cell>69.3</cell></row><row><cell>✓</cell><cell>✓</cell><cell>✓</cell><cell>✓</cell><cell>✓</cell><cell>71.2</cell></row><row><cell cols="6">Table 3: Illusory existence causality exper-</cell></row><row><cell cols="6">iment. w/o C indicates without counterfac-</cell></row><row><cell cols="3">tual intervention.</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell></cell><cell cols="4">starting division Ending division</cell></row><row><cell cols="2">VGCM (w/o C)</cell><cell></cell><cell>1.12</cell><cell>1.04</cell><cell></cell></row><row><cell>VGCM</cell><cell></cell><cell></cell><cell>1.12</cell><cell>0.93</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Illusory temporal causality experiment. w/o F indicates without front-door adjustment and Ave indicates average.</figDesc><table><row><cell>Method</cell><cell>r 0 Acc</cell><cell cols="2">r N -1 Acc Ave r Acc</cell></row><row><cell>VAR [10]</cell><cell cols="2">53.8 (-3.5) 54.6 (-3.7)</cell><cell>57.3</cell></row><row><cell cols="3">VGCM (w/o F) 63.6 (-3.3) 63.7 (-3.2)</cell><cell>66.9</cell></row><row><cell>VGCM</cell><cell cols="2">68.0 (-0.7) 68.4 (-0.3)</cell><cell>68.7</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Open-set ability of VGCM. only 59.6%. Possible explanations may be due to task contamination<ref type="bibr" target="#b55">[57]</ref>, GPT-4 mainly performs well on datasets released before the training date, while our task is novel. Moreover, other reasons may include the causal hallucination problem of establishing a threshold for differentiating between scenarios with and without causality<ref type="bibr" target="#b56">[58]</ref>. For further insights into GPT-4's failure cases, refer to Appendix Sec. B.2.</figDesc><table><row><cell>Method</cell><cell>TOP-1 Accuracy</cell></row><row><cell>VAR [10]</cell><cell>54.8</cell></row><row><cell>VGCM  ‡</cell><cell>59.2</cell></row><row><cell>VGCM</cell><cell>64.4</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Infernce speed. Our VGCM is 3-6 times faster than all Video LLMs while slightly slower than the baseline.</figDesc><table><row><cell>Model</cell><cell>Inference Speed</cell></row><row><cell>Videobert [50]</cell><cell>0.70</cell></row><row><cell>Our VGCM</cell><cell>0.76</cell></row><row><cell>VideoLLaVA [49]</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 :</head><label>7</label><figDesc>Model's generalizability test. Higher VQA Acc and VQA Score are reached when prompted with causal relations from our VGCM.</figDesc><table><row><cell>Output Causal Relations</cell><cell cols="2">VQA Acc VQA Score</cell></row><row><cell>w/o (Standard QA setting for VLLMs)</cell><cell>43.17</cell><cell>2.82</cell></row><row><cell>w Gemini-Pro [44]</cell><cell>49.10</cell><cell>2.90</cell></row><row><cell>w GPT-4 [33]</cell><cell>49.36</cell><cell>2.89</cell></row><row><cell>w VideoChat2 [48]</cell><cell>51.01</cell><cell>2.95</cell></row><row><cell>w VideoLLaVA [49]</cell><cell>51.88</cell><cell>2.93</cell></row><row><cell>w Our VGCM</cell><cell>62.21</cell><cell>3.12</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8 :</head><label>8</label><figDesc>VGCM performance with masked premise event caption input. * indicates 30 frames masked at the same time.</figDesc><table><row><cell>Num of words masked</cell><cell>Accuracy</cell></row><row><cell>non-masked</cell><cell>71.2</cell></row><row><cell>2 per event</cell><cell>70.2</cell></row><row><cell>5 per event</cell><cell>69.7</cell></row><row><cell>8 per event</cell><cell>69.2</cell></row><row><cell>8 per event  *</cell><cell>67.4</cell></row><row><cell>11 per event</cell><cell>68.9</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 9 :</head><label>9</label><figDesc>VGCM performance with masked premise event visual input. * indicates 10 words at the same time.</figDesc><table><row><cell>Num of frames masked</cell><cell>Accuracy</cell></row><row><cell>non-masked</cell><cell>71.2</cell></row><row><cell>5 per event</cell><cell>70.3</cell></row><row><cell>15 per event</cell><cell>69.0</cell></row><row><cell>20 per event</cell><cell>68.3</cell></row><row><cell>20 per event  *</cell><cell>67.1</cell></row><row><cell>40 per event</cell><cell>67.9</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>• Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details. • While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). • The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details. • The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. • The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. • At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).</figDesc><table /></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Acknowledgement</head><p>The paper is supported in part by the National Natural Science Foundation of China (No. 62325109, U21B2013) and the Lenovo Academic Collaboration Project.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A Implementation details</head><p>Pretraining process For each video event, visual features are extracted using ActivityNet pretrained ResNet200 <ref type="bibr" target="#b58">[60]</ref>, following <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b36">[37]</ref><ref type="bibr" target="#b37">[38]</ref><ref type="bibr" target="#b38">[39]</ref>. Prior domain knowledge could benefit the Granger Causality Causal discovery method <ref type="bibr" target="#b59">[61]</ref>, so we fully pre-trained our model for the dense video captioning task on a 3.1k ActivityNet Captioning video dataset, each video sample contains more than 4 events. Training set All the experiments are conducted on 1 NVIDIA A40 GPU. We train our model for 20 epochs with a learning rate of 16e-5 about 6 hours. Our optimizer is consistent with BertAdam <ref type="bibr" target="#b48">[50]</ref> optimizer, with 3 epochs of warm-up. The open-set experiment set can be found in Appendix Sec. D.1. We report the average results during all experiments under three random seeds <ref type="bibr">(2023,</ref><ref type="bibr">2024,</ref><ref type="bibr">2025)</ref>. The ablation of two modalities can be found in Appendix Sec. C.1. Model details Our encoder Enc V , Enc C , and multi-modal video decoder Dec are built upon Videobert <ref type="bibr" target="#b48">[50]</ref>, a joint model for video and language representation learning. The details of the GPT-4 API prompt can be found in Sec. D.2 in the Appendix. Hyperparameters λ C , λ R , λ V , λ S are set to be 1.0, 4.0, 0.25, 0.05. Maximum input lengths of the caption, the chain of thoughts, and the existence-only descriptions are set to 50. Implementation of VAR * * We migrate the VAR to our task through an effective method: We mask any event e k , (k&lt;N), and then utilize the fully trained VAR to perform event prediction of e k . If the prediction results êk is obviously various from e k , it is considered that the event e k is non-causal. Then r k is labeled as 0; in the opposite case, r k is labeled as 1. We also report the average results of VAR under three random seeds <ref type="bibr">(2023,</ref><ref type="bibr">2024,</ref><ref type="bibr">2025)</ref>. Implementation of LLMs As for GPT-4 and Gemini-Pro, We report the average results of three calls. Implementation of VLLMs We report the average results of VLLMs under three random seeds <ref type="bibr">(2023,</ref><ref type="bibr">2024,</ref><ref type="bibr">2025)</ref>. When VLLMs do not output r in the required format, we order them to re-answer until the outputs match the format to measure their best performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Additional Visualization B.1 Successful abduction examples of our VGCM</head><p>In Fig. <ref type="figure">9</ref>, additional examples are presented to showcase the performance of our VGCM, particularly excelling in complex abduction scenarios. The first example successfully discovers that there is no causal relation between " We see the targets in front of a backdrop." and " The instructor walks over to the targets.", despite the backdrop being a necessary object of the result event. This abduction avoids the illusory existence causality.</p><p>The second example successfully discovers that there is no causal relation between " The video shows different cricket matches taking place where Sri Lanka is playing against teams from different countries." and " The stadium is filled with spectators cheering for the cricketers.", despite the spectators' cheering often happening after the game playing. This abduction avoids the illusory temporal causality. Both instances align with the foundational principles motivating our method design.</p><p>The third example shows the 83.3% accuracy of video causal relations abduction. Notably, it correctly discerns the most complex causal relations, however, it fails to realize that person B doesn't hit the tennis ball can contribute to the anticipation of the result event of continuous hitting. This indicates that VGCM might still require refinement in understanding causality within higher-level semantics, especially in the mining of some obscure mental or emotional influences. We will strive to explore further solutions in the follow-up work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Failure abduction examples of GPT-4</head><p>While examining the causal discovery results of GPT-4, we encountered some intriguing observations. In the example presented in Fig. <ref type="figure">10</ref>, the GPT-4 API incorrectly infers that all premise events have a causal relation with the result event of the team winning. However, the initial appearance of the team Justification: We have already provided the proof of theoretical results in Sec. 5.1, Sec. 5.2 and Sec. 5.3. Guidelines:</p><p>• The answer NA means that the paper does not include theoretical results.</p><p>• All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced. • All assumptions should be clearly stated or referenced in the statement of any theorems.</p><p>• The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. • Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. • Theorems and Lemmas that the proof relies upon should be properly referenced.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Result Reproducibility</head><p>Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: Implementation details are provided in Sec. A in the Appendix. Guidelines:</p><p>• The answer NA means that the paper does not include experiments.</p><p>• If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. • If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. • Depending on the contribution, reproducibility can be accomplished in various ways.</p><p>For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. • While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. , with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility.</p><p>In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Open access to data and code</head><p>Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?</p><p>• If this information is not available online, the authors are encouraged to reach out to the asset's creators. • We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. • For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Mist: Multimodal iterative spatial-temporal transformer for long-form video question answering</title>
		<author>
			<persName><forename type="first">Difei</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linchao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><forename type="middle">Zheng</forename><surname>Shou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="14773" to="14783" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Text-guided object detector for multimodal video question answering</title>
		<author>
			<persName><forename type="first">Ruoyue</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nakamasa</forename><surname>Inoue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koichi</forename><surname>Shinoda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="1032" to="1042" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Self-chained image-language model for video localization and question answering</title>
		<author>
			<persName><forename type="first">Shoubin</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaemin</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prateek</forename><surname>Yadav</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.06988</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Locate before answering: Answer guided question localization for video question answering</title>
		<author>
			<persName><forename type="first">Tianwen</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ran</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingjing</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pai</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaowei</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu-Gang</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">Kexin</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunzhu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pushmeet</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiajun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.01442</idno>
		<title level="m">Clevrer: Collision events for video representation and reasoning</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Causal discovery in physical systems from videos</title>
		<author>
			<persName><forename type="first">Yunzhu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anima</forename><surname>Anandkumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dieter</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Animesh</forename><surname>Garg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="9180" to="9192" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">Rohit</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.04744</idno>
		<title level="m">Cater: A diagnostic dataset for compositional actions and temporal reasoning</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Explainable video action reasoning via prior knowledge and state transitions</title>
		<author>
			<persName><forename type="first">Tao</forename><surname>Zhuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongkang</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohan</forename><surname>Kankanhalli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th acm international conference on multimedia</title>
		<meeting>the 27th acm international conference on multimedia</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="521" to="529" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Complex video action reasoning via learnable markov logic network</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linchao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yadong</forename><surname>Mu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="3242" to="3251" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Visual abductive reasoning</title>
		<author>
			<persName><forename type="first">Chen</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenguan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianfei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="15565" to="15575" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">Clement</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kiat</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheston</forename><surname>Yeo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Basura</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><surname>Fernando</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.13984</idno>
		<title level="m">Abductive action inference</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Anil Seth. Granger causality. Scholarpedia</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">1667</biblScope>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A review of the granger-causality fallacy</title>
		<author>
			<persName><forename type="first">Mariusz</forename><surname>Maziarz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The journal of philosophical economics: Reflections on economic and social issues</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="86" to="105" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Granger causality: A review and recent advances</title>
		<author>
			<persName><forename type="first">Ali</forename><surname>Shojaie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emily</forename><forename type="middle">B</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annual Review of Statistics and Its Application</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="289" to="319" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">Judea</forename><surname>Pearl</surname></persName>
		</author>
		<title level="m">Causal inference. Causality: objectives and assessment</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="39" to="58" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deconfounded image captioning: A causal retrospect</title>
		<author>
			<persName><forename type="first">Xu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfei</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="12996" to="13010" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Causal attention for vision-language tasks</title>
		<author>
			<persName><forename type="first">Xu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guojun</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfei</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="9847" to="9857" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Counterfactual samples synthesizing and training for robust visual question answering</title>
		<author>
			<persName><forename type="first">Long</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuhang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yulei</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Clicks can be cheating: Counterfactual recommendation for mitigating clickbait issue</title>
		<author>
			<persName><forename type="first">Wenjie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fuli</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1288" to="1297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Chain-of-thought prompting elicits reasoning in large language models</title>
		<author>
			<persName><forename type="first">Jason</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuezhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dale</forename><surname>Schuurmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ed</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denny</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="24824" to="24837" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Large language models are zero-shot reasoners</title>
		<author>
			<persName><forename type="first">Takeshi</forename><surname>Kojima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shane</forename><surname>Shixiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Machel</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yutaka</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yusuke</forename><surname>Matsuo</surname></persName>
		</author>
		<author>
			<persName><surname>Iwasawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="22199" to="22213" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingchang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qianglong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weijiang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haotian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2309.15402</idno>
		<title level="m">A survey of chain of thought reasoning: Advances, frontiers and future</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">High-recall causal discovery for autocorrelated time series with latent confounders</title>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Gerhardus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Runge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="12615" to="12625" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Discovery of extended summary graphs in time series</title>
		<author>
			<persName><forename type="first">Emilie</forename><surname>Charles K Assaad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Devijver</surname></persName>
		</author>
		<author>
			<persName><surname>Gaussier</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Uncertainty in Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="96" to="106" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Joint causal inference from multiple contexts</title>
		<author>
			<persName><forename type="first">Sara</forename><surname>Joris M Mooij</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Magliacane</surname></persName>
		</author>
		<author>
			<persName><surname>Claassen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3919" to="4026" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Nts-notears: Learning nonparametric dbns with prior knowledge</title>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oliver</forename><surname>Schulte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guiliang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Poupart</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.04286</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Dynotears: Structure learning from time-series data</title>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<editor>
			<persName><forename type="first">Roxana</forename><surname>Pamfil</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Nisara</forename><surname>Sriwattanaworachai</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Shaan</forename><surname>Desai</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Philip</forename><surname>Pilgerstorfer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Konstantinos</forename><surname>Georgatzis</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Paul</forename><surname>Beaumont</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Bryon</forename><surname>Aragam</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1595" to="1605" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Idyno: Learning nonparametric dags from interventional dynamic data</title>
		<author>
			<persName><forename type="first">Tian</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Debarun</forename><surname>Bhattacharjya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elliot</forename><surname>Nelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Yu</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="6988" to="7001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Thp: Topological hawkes processes for learning granger causality on event sequences</title>
		<author>
			<persName><forename type="first">Ruichu</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siyu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keli</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.10884</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning granger causality for non-stationary hawkes processes</title>
		<author>
			<persName><forename type="first">Wei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jibin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruichu</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuequn</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Hao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">468</biblScope>
			<biblScope unit="page" from="22" to="32" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Cardinality-regularized hawkesgranger model</title>
		<author>
			<persName><forename type="first">Tsuyoshi</forename><surname>Idé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georgios</forename><surname>Kollias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dzung</forename><surname>Phan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naoki</forename><surname>Abe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="2682" to="2694" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Densecaptioning events in videos</title>
		<author>
			<persName><forename type="first">Ranjay</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenji</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frederic</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="706" to="715" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<author>
			<persName><forename type="first">Josh</forename><surname>Achiam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Adler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lama</forename><surname>Ahmad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilge</forename><surname>Akkaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florencia</forename><surname>Leoni Aleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diogo</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Janko</forename><surname>Altenschmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Altman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.08774</idno>
		<title level="m">Shyamal Anadkat, et al. Gpt-4 technical report</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">The randomized causation coefficient</title>
		<author>
			<persName><forename type="first">David</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krikamol</forename><surname>Muandet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Recht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="2901" to="2907" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Meta learning for causal direction</title>
		<author>
			<persName><forename type="first">Jean-François</forename><surname>Ton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dino</forename><surname>Sejdinovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenji</forename><surname>Fukumizu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="9897" to="9905" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<author>
			<persName><forename type="first">Hebi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jin</forename><surname>Tian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.04697</idno>
		<title level="m">Supervised whole dag causal discovery</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Mart: Memoryaugmented recurrent transformer for coherent video paragraph captioning</title>
		<author>
			<persName><forename type="first">Jie</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yelong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tamara</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.05402</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">End-to-end dense video captioning with parallel decoding</title>
		<author>
			<persName><forename type="first">Teng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruimao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhichao</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ran</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="6847" to="6857" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Object relational graph with teacher-recommended learning for video captioning</title>
		<author>
			<persName><forename type="first">Ziqi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaya</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunfeng</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peijin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiming</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng-Jun</forename><surname>Zha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="13278" to="13288" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Nonlinear ica of temporally dependent stationary sources</title>
		<author>
			<persName><forename type="first">Aapo</forename><surname>Hyvarinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hiroshi</forename><surname>Morioka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="460" to="469" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Unsupervised feature extraction by time-contrastive learning and nonlinear ica</title>
		<author>
			<persName><forename type="first">Aapo</forename><surname>Hyvarinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hiroshi</forename><surname>Morioka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Cross-modal causal relational reasoning for event-level visual question answering</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guanbin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<author>
			<persName><forename type="first">Gemini</forename><surname>Team</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rohan</forename><surname>Anil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Borgeaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean-Baptiste</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiahui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johan</forename><surname>Schalkwyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anja</forename><surname>Hauth</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2312.11805</idno>
		<title level="m">a family of highly capable multimodal models</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Minigpt4-video: Advancing multimodal llms for video understanding with interleaved visual-textual tokens</title>
		<author>
			<persName><forename type="first">Kirolos</forename><surname>Ataallah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoqian</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eslam</forename><surname>Abdelrahman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Essam</forename><surname>Sleiman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deyao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohamed</forename><surname>Elhoseiny</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2404.03413</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Minigpt-4: Enhancing vision-language understanding with advanced large language models</title>
		<author>
			<persName><forename type="first">Deyao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoqian</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohamed</forename><surname>Elhoseiny</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.10592</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Video-llama: An instruction-tuned audio-visual language model for video understanding</title>
		<author>
			<persName><forename type="first">Hang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lidong</forename><surname>Bing</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2306.02858</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Mvbench: A comprehensive multi-modal video understanding benchmark</title>
		<author>
			<persName><forename type="first">Kunchang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yali</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhuo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jilan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="22195" to="22206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Video-llava: Learning united visual representation by alignment before projection</title>
		<author>
			<persName><forename type="first">Bin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Munan</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jin</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Yuan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2311.10122</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Videobert: A joint model for video and language representation learning</title>
		<author>
			<persName><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Austin</forename><surname>Myers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carl</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF international conference on computer vision</title>
		<meeting>the IEEE/CVF international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="7464" to="7473" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jong</forename><forename type="middle">Wook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="8748" to="8763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Large language models are latent variable models: Explaining and finding good demonstrations for in-context learning</title>
		<author>
			<persName><forename type="first">Xinyi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wanrong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Saxon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Steyvers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wang</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Open event causality extraction by the assistance of llm in task annotation, dataset, and method</title>
		<author>
			<persName><forename type="first">Kun</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yubo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop: Bridging Neurons and Symbols for Natural Language Processing and Knowledge Graphs Reasoning (NeusymBridge)@ LREC-COLING-2024</title>
		<meeting>the Workshop: Bridging Neurons and Symbols for Natural Language Processing and Knowledge Graphs Reasoning (NeusymBridge)@ LREC-COLING-2024</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="33" to="44" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<author>
			<persName><forename type="first">Aniket</forename><surname>Vashishtha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gowtham</forename><surname>Abbavaram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhinav</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saketh</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><surname>Bachu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amit</forename><surname>Vineeth N Balasubramanian</surname></persName>
		</author>
		<author>
			<persName><surname>Sharma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.15117</idno>
		<title level="m">Causal inference using llm-guided discovery</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Knowledge transfer for causal discovery</title>
		<author>
			<persName><forename type="first">Verónica</forename><surname>Rodríguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">-López</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Luis</forename><forename type="middle">Enrique</forename><surname>Sucar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Approximate Reasoning</title>
		<imprint>
			<biblScope unit="volume">143</biblScope>
			<biblScope unit="page" from="1" to="25" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Tuning causal discovery algorithms</title>
		<author>
			<persName><forename type="first">Konstantina</forename><surname>Biza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ioannis</forename><surname>Tsamardinos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sofia</forename><surname>Triantafillou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Probabilistic Graphical Models</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="17" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Task contamination: Language models may not be few-shot anymore</title>
		<author>
			<persName><forename type="first">Changmao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Flanigan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2312.16337</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<author>
			<persName><surname>Sm Tonmoy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vinija</forename><surname>Zaman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anku</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vipula</forename><surname>Rani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aman</forename><surname>Rawte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amitava</forename><surname>Chadha</surname></persName>
		</author>
		<author>
			<persName><surname>Das</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2401.01313</idno>
		<title level="m">A comprehensive survey of hallucination mitigation techniques in large language models</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Activitynet-qa: A dataset for understanding complex web videos via question answering</title>
		<author>
			<persName><forename type="first">Zhou</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dejing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhou</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yueting</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="9127" to="9134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Causal discovery from temporal data: An overview and new perspectives</title>
		<author>
			<persName><forename type="first">Chang</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Di</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuzhe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenbin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingping</forename><surname>Bi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.10112</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Bat: An open-source, web-based audio events annotation tool</title>
		<author>
			<persName><forename type="first">Emilio</forename><surname>Blai Meléndez Catalán</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emilia</forename><forename type="middle">Gómez</forename><surname>Molina</surname></persName>
		</author>
		<author>
			<persName><surname>Gutiérrez</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Hieve: A large-scale benchmark for human-centric video analysis in complex events</title>
		<author>
			<persName><forename type="first">Weiyao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huabin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shizhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongkai</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guojun</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicu</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">131</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2994" to="3018" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Machine learning of concepts hard even for humans: The case of online depression forums</title>
		<author>
			<persName><forename type="first">Renáta</forename><surname>Németh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Domonkos</forename><surname>Sik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fanni</forename><surname>Máté</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Qualitative Methods</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page">1609406920949338</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Large language models are few-shot testers: Exploring llm-based general bug reproduction</title>
		<author>
			<persName><forename type="first">Sungmin</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juyeon</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shin</forename><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2023 IEEE/ACM 45th International Conference on Software Engineering (ICSE)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="2312" to="2323" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Few-shot training llms for project-specific codesummarization</title>
		<author>
			<persName><forename type="first">Toufique</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Premkumar</forename><surname>Devanbu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th IEEE/ACM International Conference on Automated Software Engineering</title>
		<meeting>the 37th IEEE/ACM International Conference on Automated Software Engineering</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
