<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Personalized Binomial DAGs Learning with Network Structured Covariates</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2024-06-10">10 Jun 2024</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Boxin</forename><surname>Zhao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Booth School of Business</orgName>
								<orgName type="institution">University of Chicago</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Weishi</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Industrial and Operations Engineering</orgName>
								<orgName type="institution">University of Michigan</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Dingyuan</forename><surname>Zhu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ziqi</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Dong</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Zhiqiang</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jun</forename><surname>Zhou</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Mladen</forename><surname>Kolar</surname></persName>
							<affiliation key="aff2">
								<orgName type="department" key="dep1">Department of Data Sciences and Operations</orgName>
								<orgName type="department" key="dep2">Marshall School of Business</orgName>
								<orgName type="institution">University of Southern</orgName>
								<address>
									<country>California</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Personalized Binomial DAGs Learning with Network Structured Covariates</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2024-06-10">10 Jun 2024</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2406.06829v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-14T19:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Causal Discovery</term>
					<term>DAG Structure Learning</term>
					<term>Varying-Coefficient Model</term>
					<term>Graph Neural Network</term>
					<term>Binomial DAG</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The causal dependence in data is often characterized by Directed Acyclic Graphical (DAG) models, widely used in many areas. Causal discovery aims to recover the DAG structure using observational data. This paper focuses on causal discovery with multivariate count data. We are motivated by real-world web visit data, recording individual user visits to multiple websites. Building a causal diagram can help understand user behavior in transitioning between websites, inspiring operational strategy. A challenge in modeling is user heterogeneity, as users with different backgrounds exhibit varied behaviors. Additionally, social network connections can result in similar behaviors among friends. We introduce personalized Binomial DAG models to address heterogeneity and network dependency between observations, which are common in real-world applications.</p><p>To learn the proposed DAG model, we develop an algorithm that embeds the network structure into a dimension-reduced covariate, learns each node's neighborhood to reduce the DAG search space, and explores the variance-mean relation to determine the ordering. Simulations show our algorithm outperforms state-of-the-art competitors in heterogeneous data. We demonstrate its practical usefulness on a real-world web visit dataset.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Probabilistic directed acyclic graphical (DAG) models provide a powerful tool for modeling causal or directional dependence relationship among multiple variables, with applications in various domains <ref type="bibr" target="#b3">[Doya et al., 2007</ref><ref type="bibr" target="#b5">, Friedman et al., 2000</ref><ref type="bibr" target="#b8">, Jansen et al., 2003</ref><ref type="bibr" target="#b9">, Kephart and White, 1991]</ref>. However, the underlying DAG often needs to be learned from observational data. Causal discovery or causal structure learning research addresses this challenge.</p><p>We address causal discovery in multi-variate count data. Let X = (X1, . . . , X d X ) ⊤ be a random vector, where each Xj ∈ {0, 1, . . . , T } is a count on the j-th record, with T as the maximum. Our goal is to uncover deep learning methods, non-deep learning methods include matrix factorization <ref type="bibr" target="#b21">[Shen et al., 2018</ref><ref type="bibr" target="#b30">, Yang et al., 2018]</ref> and random walks <ref type="bibr">[Scott, 1981a]</ref>. Recently, <ref type="bibr" target="#b32">Zhao et al. [2022]</ref> proposed a linear node embedding method with good theoretical properties under linear assumptions. We recommend this method for linear node embedding.</p><p>Kernel smoothing is a popular nonparametric method to estimate real-valued functions, particularly probability density functions <ref type="bibr" target="#b25">[Wand and Jones, 1994]</ref>. <ref type="bibr" target="#b11">Kolar et al. [2010]</ref> proposes a penalized kernel smoothing estimator for non-zero elements of the precision matrix. We adopt a similar approach to allow personalized edge weights.</p><p>Overdispersion is a characteristic of random variables where the variance is directly proportional to the mean. It occurs when the observed variance is greater than the variance predicted by a theoretical model, such as the Generalized Linear Mixed Models (GLMM) <ref type="bibr" target="#b0">[Aitkin, 1996]</ref>. This phenomenon has been used in many applications <ref type="bibr" target="#b2">[Dean, 1992</ref><ref type="bibr" target="#b18">, Ravikumar et al., 2010]</ref>. <ref type="bibr">Park and</ref><ref type="bibr">Raskutti [2015, 2018]</ref> use overdispersion to address model identifiability. We also apply overdispersion to determine the ordering of our binomial DAG model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Preliminaries</head><p>Consider a data set D n = {X (i) , Z (i) } n i=1 , where</p><formula xml:id="formula_0">X (i) = (X (i) 1 , . . . , X<label>(i)</label></formula><p>d X ) ⊤ ∈ X , with X = {0, 1, . . . , T } d X ⊆ N d X , and</p><formula xml:id="formula_1">Z (i) = (Z (i) 1 , . . . , Z (i) d Z ) ⊤ ∈ Z ⊆ R d Z .</formula><p>Here, X (i) is a random vector whose distribution we investigate using a DAG, while Z (i) are covariates that aid in the inference of the DAG structure for X (i) . In our application, X (i) j denotes the number of visits to the website j by the user i over the last T records. The vector Z (i) includes features such as age, sex, and profession of the user i. Thus, the dataset integrates the observed count data with the corresponding covariates.</p><p>We assume that there is a known relationship network associated with observations. We represent this network with an undirected graph G = (V, E), where V = [n] is the node set and E ⊆ V × V is the edge set. Each node of V represents an observation, and an edge (i, j) ∈ E indicates a connection between observation i and j. In our motivating example, connections represent the contact or payment history between users. Connected observations often have similar distributions, which makes network information useful for learning the DAG structure.</p><p>Let D n Z = {Z (i) } n i=1 be deterministic. We explore the DAG structure of P(X (i) | D n Z , G) for i = 1, . . . , n. Let GX = (VX , EX ) be a directed graph without cycles (DAG) with vertex set VX = [dX ] and edge set EX ⊆ VX × VX . A directed edge from node j to k is denoted by (j, k) ∈ EX or j → k. Node j is the parent of node k, and node k is the child of node j. Let pa(j) be the set of all parents of node j, and ch(j) be the set of all children of node j. If there is a directed path j → . . . → k, then j is an ancestor of k and k is a descendant of j. Let de(j) be the set of descendants of j and an(j) be the set of ancestors of j. Define j ∈ de(j) and j ∈ an(j). Node j is a root node if pa(j) = ∅. By the definition of DAG, there is at least one root node.</p><p>Our first assumption states that a single DAG factorizes all P(X (i) | D n Z , G).</p><p>Assumption 1 (Common DAG Structure). There exists a DAG GX = (VX , EX ) such that for all i = 1, . . . , n, we have the following factorization:</p><formula xml:id="formula_2">P X (i) | D n Z , G = j∈V X fj X (i) j | D n Z , X (i) pa(j) , G ,</formula><p>where fj(X</p><formula xml:id="formula_3">(i) j | D n Z , X (i) pa(j) , G) represents the conditional distribution of X (i) j</formula><p>given D n Z , network G and its parent variables X (i) k , k ∈ pa(j). In addition, we assume that</p><formula xml:id="formula_4">X (i) ⊥ ⊥ X (l) | D n Z , G for all 1 ≤ i, l ≤ n and i ̸ = l.</formula><p>By Assumption 1, we need to learn one DAG structure for all observations' distributions. This is reasonable in our example as different users tend to visit websites in the same order. For GX in Assumption 1, the DAG property ensures a class of orderings of [dX ], denoted as Π ⋆ , such that for any</p><formula xml:id="formula_5">π ⋆ ∈ Π ⋆ , π ⋆ j &lt; π ⋆ k only if k / ∈ an(j). For 1 ≤ d ≤ dX , the incomplete ordering {π1, . . . , π d } is consistent with Π ⋆ if there exists π ⋆ ∈ Π ⋆ such that π ⋆ 1 = π1, . . . , π ⋆ d = π d .</formula><p>In other words, {π1, . . . , π d } is consistent with Π ⋆ if it can be completed to a consistent ordering.</p><p>To simplify the dependency structure in P(X (i) | D n Z , G), we assume the following about the node embedding function.</p><p>Assumption 2 (Node Embedding). We assume that there exists a node embedding function h ⋆ G , such that for all i = 1, . . . , n, we have</p><formula xml:id="formula_6">Z (i) ⋆ := h ⋆ G (Z (i) ; D n Z ) ⊆ R d 0 Z , 1 ≤ d 0 Z ≤ dZ , and P X (i) | D n Z , G = P X (i) | Z (i) ⋆ .</formula><p>We simplify the notation</p><formula xml:id="formula_7">h ⋆ G (Z (i) ; D n Z ) to h ⋆ G (Z (i) ).</formula><p>Assumption 2 states that a node embedding function h ⋆ G (•) exists, simplifying the conditional structure of X (i) on G and D n Z to depend only on h ⋆ G (Z (i) ), facilitating estimation. An alternative to simplify the dependency of X (i) on G and D n Z is to let</p><formula xml:id="formula_8">P X (i) | D n Z , G = P X (i) | Z (i) ,<label>(1)</label></formula><p>as adopted by <ref type="bibr" target="#b11">Kolar et al. [2010]</ref>. While this approach seems natural, our approach based on Assumption 2 has three advantages. First, while (1) assumes (X (i) , Z (i) ) is independent of (X (j) , Z (j) ) for i ̸ = j, our approach allows dependency, which is more practical. Second, the original feature vector Z (i) can be highdimensional, complicating the penalized kernel smoothing method in Section 4. Our approach relies on the embedded feature h ⋆ G (Z (i) ), which is typically smaller. Finally, Z (i) can be sparsely observed, as some users may lack certain feature records. In such cases, the approach based on (1) suffers from reduced sample size. Our approach leverages network information to infer missing features from neighbors, allowing use of the entire dataset for estimation.</p><p>Given Assumptions 1 and 2, we further specify fj(•) analytically.</p><p>Assumption 3 (Binomial SEM). Given G, D n Z , and GX , X (i) is generated according to the following structural equation model (SEM) for all i = 1, . . . , n. For all j ∈ VX , we have</p><formula xml:id="formula_9">X (i) j | D n Z , X (i) pa(j) , G ∼ Binomial T, pj(η (i) j ) ,</formula><p>where T is the number of trials in the Binomial distribution, pj(η) = 1/(1 + exp (-η)) with η &gt; 0 is the probability,</p><formula xml:id="formula_10">η (i) j = wjj h ⋆ G Z (i) + l∈pa(j) w lj h ⋆ G Z (i) X l ,<label>(2)</label></formula><p>and wjj(•), w lj (•) : R d 0 Z → R are smooth functions mapping embedded features to DAG weights.</p><p>Assumption 3 states that a node's value, given the embedding of its feature vector and parents, follows a Binomial distribution with a probability determined by these embeddings. The parents' influence on the node's mean is modeled via a generalized linear model. By (2) and the smoothness of</p><formula xml:id="formula_11">w jl (•), if h ⋆ G (Z (i) ) is close to h ⋆ G (Z (k)</formula><p>), then observations i and k will have similar distributions. Although Assumption 3 is specific to Binomial data, the concepts in Assumptions 1 and 2 can be generalized to other data types by adjusting Assumption 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Methodology</head><p>We introduce our DAG estimation algorithm in Algorithm 1. The algorithm consists of four steps: 1) estimate the node embedding function ĥG(•); 2) given the estimated node embedding function, estimate each node's neighbors via penalized kernel smoothing; 3) estimate the DAG ordering using overdispersion scores and the estimated neighborhood; 4) recover the DAG by repeating Step 2. Note that our Step 3 extends the overdispersion score from <ref type="bibr">Park and</ref><ref type="bibr">Raskutti [2015, 2018]</ref> to account for data heterogeneity.</p><p>Algorithm 1 DAG estimation with observation features 1: Input: {X (i) , Z (i) } n i=1 and the relationship network between observations G. 2: Output: Estimated DAG ordering π and DAG edge set</p><formula xml:id="formula_12">E X ∈ V X × V X . 3:</formula><p>Step 1: Encode Z (i) into a low-dimensional feature ĥG (Z (i) ) with the estimated node embedding function ĥG (•) for all i = 1, . . . , n.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4:</head><p>Step 2: Estimate the neighborhood set of node j as N (j) for all j ∈ V X . 5: Step 3: Estimate the ordering using re-weighted generalized overdispersion scores: 6: Calculate the overdispersion scores Ŝ(1, j), j = 1, . . . , d X using (7). 7: Let π1 = argmin j Ŝ(1, j).</p><formula xml:id="formula_13">8: for v = 2 . . . , d X -1 do 9: for j ∈ [d X ]\{π 1 , . . . , πv-1 } do 10:</formula><p>Calculate the overdispersion score Ŝ(v, j) using ( <ref type="formula" target="#formula_48">8</ref>). Let πv = argmin j Ŝ(v, j). 13: end for 14: The last element of the ordering πd</p><formula xml:id="formula_14">X = [d X ]\{π 1 , . . . , πd X -1 }. 15: Let π = (π 1 , . . . , πd X ). 16: Step 4: Get the directed graph E X = ∪ m={2,3,••• ,d X } D m</formula><p>, where D m is the estimated parent set of node πm . 17: Return: π and E X .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Estimation of Node Embedding Function</head><p>By Assumption 2, we assume a node embedding function that simplifies the dependency structure of observations on features and reduces the feature dimension. Thus, our first step of Algorithm 1 is to estimate this function. Given the relationship network G, our aim is to find a function ĥG(•) that encodes network information into the transformed feature vectors { ĥG(Z (i) )} n i=1 . Users i, j close to the graph G should have embedded features ĥG(Z (i) ), ĥG(Z (j) ) closer than raw feature vectors Z (i) , Z (j) . Next, we explain how to estimate linear and nonlinear embeddings. To simplify the notation, we use ĥ(i) to denote the embedding ĥG Z (i) .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Linear Embedding Function Estimation</head><p>Assuming h ⋆ G (•) is linear, it can be represented by a projection matrix:</p><formula xml:id="formula_15">h ⋆ G (•) = ⟨F ⋆ , •⟩.</formula><p>We follow the method of <ref type="bibr" target="#b32">Zhao et al. [2022]</ref> to estimate F ⋆ which has good theoretical properties under certain conditions. Given a graph G = (V, E), let W 0 ∈ R n×n be its adjacency matrix with w 0 ij = w 0 ji = 1 if (i, j) ∈ E and zero otherwise. <ref type="bibr" target="#b32">Zhao et al. [2022]</ref> </p><formula xml:id="formula_16">seeks F ∈ R d Z ×d 0 Z such that small ∥F ⊤ Z (i) -F ⊤ Z (j) ∥ corresponds to w 0 ij = 1. To ensure identifiability, F is restricted to Ω A := {F : F ⊤ AF = I} ⊆ R d Z ×d 0 Z where A ∈ R d Z ×d</formula><p>Z is a user-chosen positive definite matrix with bounded eigenvalues. The matrix F ⋆ is estimated as</p><formula xml:id="formula_17">F = arg max F ∈Ω A 1 n(n -1) i̸ =j (1 -w 0 ij ) F ⊤ Z (i) -Z (j) 2 ,</formula><p>which has an analytical solution as F = A -1/2 Ψ, where Ψ is the matrix of eigenvectors associated with the d 0 Z largest eigenvalues of A -1/2 CA and</p><formula xml:id="formula_18">C = 1 n(n -1) i̸ =j (1 -w 0 ij ) Z (i) -Z (j) Z (i) -Z (j) ⊤ If not specified, A = Cov(Z). The node embedding function is estimated as ĥG(•) = ⟨ F , •⟩.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Nonlinear Embedding Function Estimation</head><p>If h ⋆ G (•) is not required to be linear, we use Graph Auto-Encoders (GAEs) <ref type="bibr" target="#b10">[Kipf and Welling, 2016]</ref> to encode network information. GAEs are unsupervised frameworks that encode node features into a latent space via an encoder and reconstruct graph data via a decoder. The encoder uses graph convolutional layers to obtain low-dimensional node embeddings. The decoder computes pair-wise distances from these embeddings and reconstructs the adjacency matrix after a non-linear activation layer. The network is trained by minimizing the discrepancy between the real and reconstructed adjacency matrices. GAEs can output embedded features from node features or node embeddings encoding network topology without input features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Estimation of Neighborhood Set</head><p>The neighborhood set of node j ∈ VX , denoted by N (j), is the minimal subset of VX such that</p><formula xml:id="formula_19">X (i) j ⊥ ⊥ X (i) V X \N (j) | h ⋆ G Z (i) , X (i) N (j) for all i ∈ [n].</formula><p>Thus, pa(j) ⊆ N (j). If we knew the set N (j), this would allow us to reduce the search space for pa(j) to the nodes in N (j). Since estimating the set N (j) is easier than estimating pa(j), due to ignoring the directional information, the set N (j) is first estimated in Step 2 of Algorithm 1. Following <ref type="bibr" target="#b29">Yang et al. [2012]</ref> and <ref type="bibr" target="#b17">Park and Raskutti [2018]</ref>, we recast the estimation of the neighborhood set as a variable selection problem. Under Assumption 3, <ref type="bibr" target="#b17">Park and Raskutti [2018]</ref> proposed to estimate N (j) in an i.i.d. setting by estimating γ</p><formula xml:id="formula_20">(i) j = (γ (i) 1j , . . . , γ (i) dj ) ⊤ , defined as the minimizer of min γ∈Γ j : E X (i)   T -X (i) j   γj + l∈N (j) γ l X (i) l   + T log   1 + exp   -γj - l∈N (j) γ l X (i) l       ,<label>(3)</label></formula><p>where</p><formula xml:id="formula_21">Γj = {γ ∈ R d : γ l = 0 for all l / ∈ Nj ∪ {j}}. The estimator of γ (i) j</formula><p>is obtained by minimizing the following penalized objective:</p><formula xml:id="formula_22">γ(i) j = arg min γ∈R d X lj(γ) + λ l̸ =j |γ l |,</formula><p>where</p><formula xml:id="formula_23">lj(γ) = 1 n n i=1 T -X (i) j   γj + l∈N (j) γ l X (i) l   + T log   1 + exp   -γj - l∈N (j) γ l X (i) l     .</formula><p>The neighborhood set of node j is then obtained as the support of γ(i) j . Under mild conditions, Supp(γ <ref type="bibr" target="#b17">[Park and Raskutti, 2018]</ref>.</p><formula xml:id="formula_24">(i) j ) = N (j) with high probability</formula><p>In contrast to the setting in <ref type="bibr" target="#b17">Park and Raskutti [2018]</ref>, the observations {X (i) } n i=1 in our set-up depend on the covariates {Z (i) } n i=1 . As a result, the loss function lj(•) does not approximate E X (i) <ref type="bibr">[•]</ref> well. To overcome this challenge, we use the key observation that when h ⋆ G Z (i) ≈ h ⋆ G Z (j) , we would have γ</p><formula xml:id="formula_25">(i) j ≈ γ (j)</formula><p>j . This insight suggests employing a penalized kernel smoothing estimator to approximate the expected loss under E X (i) [•] by assigning higher weights to samples with node embeddings similar to h ⋆ G Z (i) , instead of distributing equal weights to all samples <ref type="bibr" target="#b11">[Kolar et al., 2010]</ref>.</p><p>We first construct an estimate of the expected loss in (3). Let</p><formula xml:id="formula_26">b (i) j = b (i) 1j , . . . , b (i) d X j ⊤ ∈ R d X , b (i) -j,j = b (i) 1j , . . . , b (i) j-1,j , b (i) j+1,j , . . . , b (i) d X j ⊤ , X (i) -j = X (i) 1 , . . . , X (i) j-1 , X (i) j+1 , . . . , X (i) d X ⊤ . For j ∈ [dX ] and i ∈ [n], a local estimate of (3) is L (i) j b (i) j := n k=1</formula><p>Kτ 1 ĥ(i) -ĥ(k)</p><formula xml:id="formula_27">n l=1 Kτ 1 ĥ(i) -ĥ(l) × T -X (k) j b (i) jj + b (i) -j,j , X (k) -j + T log 1 + exp -b (i) jj -b (i) -j,j , X (k) -j<label>(4)</label></formula><p>where Kτ (x) = K(∥x∥/τ ), K(•) is a symmetric positive real valued function that defines local weights, and τ1 &gt; 0 is the bandwidth. Throughout, we use K(u) = exp(-|u|). Minimizing the local loss in (4) gives us an estimate of γ (i) j . In (4), we assign a weight to observation k based on the similarity between h ⋆ G Z (i) and h ⋆ G Z (k) . Based on (4), we estimate γ</p><formula xml:id="formula_28">(i) lj , l, j ∈ [dX ] and i ∈ [n]</formula><p>, by minimizing the following penalized objective:</p><formula xml:id="formula_29">Bj := argmin B j ∈R d X ×n Lj (Bj ; D n ) := n i=1 L (i) j b (i) j + λ j l̸ =j ∥b lj ∥ 2 ,<label>(5)</label></formula><p>where Bj = b</p><p>(1)</p><formula xml:id="formula_30">j , . . . , b (n) j ∈ R d X ×n and b lj = b (1) lj , . . . , b (n) lj ⊤</formula><p>is the l-th row of Bj. The second term in ( <ref type="formula" target="#formula_29">5</ref>) is the penalty that encourages group-structured sparsity <ref type="bibr" target="#b31">[Yuan and Lin, 2005]</ref>. Since for l / ∈ N (j)∪{j}, {γ</p><formula xml:id="formula_31">(i) lj } n</formula><p>i=1 are all zero, the group lasso penalty encourages row sparsity in Bj. The penalty parameter λ j &gt; 0 controls the sparsity level of the neighborhood set of node j. Finally, we estimate the neighborhood of node j by N (j) := l : blj 2 &gt; 0 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Approximate Optimization Algorithm for Solving (5)</head><p>Although it is feasible to solve directly (5), in practice, a large number of observations n can present significant challenges for both storage and computation. We propose an approximate solution to the original problem (5) that is suitable for large-scale datasets. Our approximate solution is closely related to the concept of binning in the nonparametric statistics literature <ref type="bibr">[Scott, 1981b</ref><ref type="bibr" target="#b23">, Silverman, 2018</ref><ref type="bibr" target="#b4">, Fan and Marron, 1994</ref><ref type="bibr" target="#b26">, Wand, 1994]</ref>. However, while binning is effective for univariate or low-dimensional cases, our node embedding may have relatively high dimensions. To address this issue, we extend the binning concept to clustering. Instead of dividing the space into bins, we cluster samples into groups and fit one parameter for each group rather than for each sample. By doing so, we reduce the computational complexity from being proportional to the sample size to being proportional to the group size. For the clustering method, we use K-means clustering in this paper <ref type="bibr" target="#b7">[Hartigan and Wong, 1979]</ref>, but other clustering methods can also be applied here, such as spectral clustering <ref type="bibr" target="#b14">[Ng et al., 2001]</ref>.</p><p>Given embeddings { ĥ(i) } n i=1 , we apply k-means clustering to divide the observations into M groups {Am} M m=1 , with ∪ M m=1 Am = [n] and centers {cm} M m=1 . We replace ĥ(i) with cm for i ∈ Am, reducing the parameters needed to M instead of n. When M = n, we recover the original problem. We let</p><formula xml:id="formula_32">α (m) i = Kτ 1 ∥ ĥ(i) -cm∥ n l=1 Kτ 1 ∥ ĥ(l) -cm∥ for all 1 ≤ i ≤ n, 1 ≤ m ≤ M.</formula><p>Besides, for a given j ∈</p><formula xml:id="formula_33">[d], let b (m) j = (b (m) 1j , . . . , b (m) d X ,j ) ⊤ ∈ R d X , and Bj = [b (1) j , . . . , b (M ) j ] ∈ R d X ×M . We use b lj = b (1) lj , . . . , b (M ) lj ⊤</formula><p>to denote the l-th row of matrix Bj. We solve the following optimization problem as an approximation to (5):</p><formula xml:id="formula_34">min B F (B) := l(B) + λ d X j=2 ∥Bj•∥2,<label>(6)</label></formula><p>where</p><formula xml:id="formula_35">l(B) := 1 M M m=1 n i=1 α (m) i T -X (i) j b (m) jj + b (m) -j,j , X (i) -j +T log 1 + exp -b (m) jj -b (m) -j,j , X (i) -j</formula><p>.</p><p>Thus, the number of parameters to estimate is M dX , which is significantly smaller than ndX when n is large. Observe that (6) features a composite structure of smooth convex loss combined with a non-smooth convex penalty, we can then use a proximal-gradient method to solve (6) <ref type="bibr">[Parikh and Boyd, 2014]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Determine the Order</head><p>In Step 3 of Algorithm 1, we determine the ordering of the DAG by modifying the method of <ref type="bibr" target="#b17">Park and Raskutti [2018]</ref> to fit our context. The concept involves creating a series of overdispersion scores that depend on the conditional mean and conditional variance, and then determining the order by sequentially selecting the node with the lowest overdispersion score. Unlike <ref type="bibr" target="#b17">Park and Raskutti [2018]</ref>, in this paper, the conditional mean and variance differ across samples, adding complexity to the estimation. To address this, we employ a similar kernel smoothing technique as described in Section 4.2. We start by defining the overdispersion scores. By Assumption 3, we have the conditional expectation of a node j as E X</p><formula xml:id="formula_36">(i) j | h ⋆ G Z (i) , X (i) pa(j) = T pj(η (i) j</formula><p>) and the conditional variance as V X</p><formula xml:id="formula_37">(i) j | h ⋆ G Z (i) , X (i) pa(j) = T pj(η (i) j ) 1 -pj(η (i) j ) .</formula><p>In particular, there is a quadratic relation between conditional mean and variance as</p><formula xml:id="formula_38">V X (i) j | h ⋆ G Z (i) , X (i) pa(j) = E X (i) j | h ⋆ G Z (i) , X (i) pa(j) - 1 T E X (i) j | h ⋆ G Z (i) , X (i) pa(j)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2</head><p>.</p><p>By defining</p><formula xml:id="formula_39">ω i j := ωj h ⋆ G Z (i) , X (i) pa(j) := 1 -1 T E X (i) j | h ⋆ G Z (i) , X (i) pa(j)</formula><p>-1</p><p>, we have</p><formula xml:id="formula_40">V ω i j X (i) j | h ⋆ G Z (i) , X (i) pa(j) = E ω i j X (i) j | h ⋆ G Z (i) , X (i) pa(j) .</formula><p>Based on this, we define the following overdispersion scores that can help identify an ordering that is consistent with Π ⋆ . For j ∈ [dX ], we define</p><formula xml:id="formula_41">ω1j Z (i) := 1 - 1 T E X (i) j | Z (i) -1 , S i (1, j) := ω 2 1j Z (i) V X (i) j | Z (i) -ω1j Z (i) E X (i) j | Z (i) , S(1, j) := 1 n n i=1 S i (1, j).</formula><p>Following the proof of Theorem 5 in <ref type="bibr" target="#b17">Park and Raskutti [2018]</ref>, it can be demonstrated that if we set j ⋆ = argmin j S(1, j), then pa(j ⋆ ) = ∅. Thus, the root node is identified by checking S(1, j). Furthermore, for v ≥ 2 and an incomplete ordering π 1:(v-1) = {π1, . . . , πv-1} that is consistent with Π ⋆ , we define</p><formula xml:id="formula_42">ωvj Z (i) := 1 - 1 T E X (i) j | Z (i) , X (i) π 1:(v-1) -1 , S i (v, j) := ω 2 vj Z (i) V X (i) j | Z (i) , X (i) π 1:(v-1) -ωvj Z (i) E X (i) j | Z (i) , X (i) π 1:(v-1) , S(v, j) := 1 n n i=1 S i (v, j),</formula><p>for j ∈ [dX ]\{π1, . . . , πv-1}. Letting πv = argmin j S(v, j) ensures π1:v = {π1, . . . , πv, } is consistent with Π ⋆ . Thus, using these conclusions and induction, we can sequentially estimate overdispersion scores and the ordering.</p><p>We then discuss how to estimate overdispersion scores. We estimate overdispersion scores by first constructing conditional mean and variance estimates:</p><formula xml:id="formula_43">Ê X (i) j | Z (i) = n l=1 X (l) j θ i l , V X (i) j | Z (i) = n l=1 X (i) l -Ê X (i) j | Z (i) 2 θ i l ,</formula><p>where θ i l =</p><p>Kτ 2 ĥ(i) -ĥ(l) n k=1 Kτ 2 ĥ(i) -ĥ(k) ,</p><p>and Kτ (•) is defined in Section 4.2 with τ2 &gt; 0 being the bandwidth. We utilize the similar idea of kernel smoothing as in Section 4.2 to borrow information from neighboring samples when estimating the conditional mean and variance of a sample. We then define ω1j</p><formula xml:id="formula_44">Z (i) = 1 -1 T Ê X (i) j | Z (i) -1 and Ŝ(1, j) := 1 n n i=1 ω2 1j Z (i) V X (i) j | Z (i) -ω1j(Z (i) ) Ê X (i) j | Z (i) .<label>(7)</label></formula><p>The estimate Ŝ(1, j) is used to the root node as π1 = argmin j Ŝ(1, j). Suppose that we have obtained the set {π1, . . . , πv-1} for v ≥ 2. For any j ∈ [dX ] \ {π1, . . . , πv-1}, we define the candidate parent set of node j as Cvj = N (j) ∩ {π1, . . . , πv-1}. For any j ∈ VX and S ⊆ [dX ]\{j}, the estimate of the conditional expectation and variance given the set S is</p><formula xml:id="formula_45">μi j,x S = Ê X (i) j | Z (i) , X (i) S = xS = n l=1 X (i) j • θ i l,x S • 1{X (l) S = xS}, V X (i) j | Z (i) , X (i) S = xS = n l=1 X (i) l -μi j,x S 2 • θ i l,x S • 1{X (l) S = xS},</formula><p>where</p><formula xml:id="formula_46">X (i)</formula><p>S is a subvector of X (i) and</p><formula xml:id="formula_47">θ i l,x S = Kτ 2 ĥ(i) -ĥ(l) • 1{X (l) S = xS} n k=1 Kτ 2 ĥ(i) -ĥ(k) • 1{X (k) S = xS} . Let X (S) = {0, 1, . . . , T } |S| . For xS ∈ X (S), let n(xS) := n i=1 1(X (i) S = xS)</formula><p>denote the conditional sample size and nS = x S ∈X (S) n(xS)1(n(xS) ≥ n0) denote the truncated conditional sample size, where n0 is a tuning parameter with 1 ≤ n0 ≤ n. With this notation, for v = 2, . . . , dX -1 and the candidate parent set Ĉvj, we define ωvj</p><formula xml:id="formula_48">Z (i) , x = 1 -1 T Ê X (i) j | Z (i) , X (i) Ĉvj = x -1 and Ŝ(v, j) := 1 n n i=1 x∈X Ĉvj n(x) n Ĉvj ω2 vj (Z (i) , x) • V X (i) j | Z (i) , X (i) Ĉvj = x - ωvj(Z (i) , x) • Ê X (i) j | Z (i) , X (i) Ĉvj = x , (<label>8</label></formula><formula xml:id="formula_49">)</formula><p>where the summation over the set X Ĉvj := x Ĉvj ∈ X ( Ĉvj) : n(x Ĉvj ) ≥ n0 is to ensure that we have enough samples to estimate element of the overdispersion score. In this paper, we choose n0 = 2. Ultimately, we determine the subsequent element of the ordering estimate as πv = argmin j Ŝ(v, j).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Recover the DAG</head><p>With estimated ordering π, we can reconstruct the DAG using a penalized estimation procedure, similar to neighborhood selection in Section 4.2 and originally proposed in <ref type="bibr" target="#b22">Shojaie and Michailidis [2010]</ref>. To recover the DAG, we estimate the parent set of each node. Given π ⋆ ∈ Π ⋆ and v ≥ 2, to find the parent set of node π ⋆ v , we search among {π ⋆ 1 , . . . , π ⋆ v-1 }. We use π instead of π ⋆ for this search. For v = 2, . . . , dX , we define Âv ∈ R v×n as a minimizer of the following objective:</p><formula xml:id="formula_50">min Av ∈R v×n n i ′ =1 n i=1 Kτ 1 ĥ(i) -ĥ(i ′ ) n l=1 Kτ 1 ĥ(i) -ĥ(l) × T -X (i) πv a (i ′ ) vv + v-1 u=1 a (i ′ ) uv X (i) πu + T log 1 + exp -a (i ′ ) vv - v-1 u=1 a (i ′ ) uv X (i) πu + λ πv v-1 u=1 ∥auv∥ 2 ,<label>(9)</label></formula><p>where Av = a</p><formula xml:id="formula_51">(i) v n i=1 ∈ R v×n , a (i) v = (a (i) 1v , . . . , a<label>(i)</label></formula><p>vv ) ⊤ ∈ R v , and auv is the u-th row of Av. Finally, based on Âv we estimate the parent set of node πv as</p><formula xml:id="formula_52">Dv = pa(πv) := 1 ≤ u ≤ v -1 : ∥âuv∥ 2 &gt; 0 .</formula><p>The DAG estimate is EX = ∪ d X m=2 Dm. To solve (9), we use the approximation technique from Section 4.2.1.</p><p>Algorithm 2 Penalty Parameter Tuning for Heterogeneous Conditional Binomial DAG Learning</p><formula xml:id="formula_53">1: Input: {X (i) , Z (i) } n i=1 . 2: Output: Optimal parameter set {λ ⋆ j } d X j=1</formula><p>for loss (5) or loss (9). 3: Randomly partition {X (i) , Z (i) } n i=1 into q subsets with the same size.</p><formula xml:id="formula_54">4: for j = 1, 2, • • • , d X do 5:</formula><p>Calculate the candidate grid of λ j , denoted by λ j seq , by targeting λ j min and λ j max with (10).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>6:</head><p>for λ ∈ λ j seq do 7:</p><formula xml:id="formula_55">for l = 1, 2, • • • , q do 8:</formula><p>Choose the l-th partition of {X (i) , Z (i) } n i=1 , I l , as the validation set, and the remaining dataset, I -l , as the training set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>9:</head><p>Get Bj or Âj by optimizing loss (5) or loss (9), respectively, with λ on the training set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>10:</head><p>Calculate the mean squared error(MSE) ε j (λ, l) by ( <ref type="formula">11</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>11:</head><p>end for</p><formula xml:id="formula_56">12: Denote ε j (λ, •) = (ε j (λ, 1), ε j (λ, 2), • • • , ε j (λ, q)</formula><p>), calculate the standard error of MSE as se(λ) = V(ε j (λ, •))/q, where V(•) is the empirical variance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>13:</head><p>end for 14:</p><p>Let ε j,min = min λ∈λ j seq (1/q) q l=1 ε j (λ, l) and λ j,min = argmin λ∈λ j seq (1/q) q l=1 ε j (λ, l), then we have</p><formula xml:id="formula_57">λ ⋆ j = max λ ∈ λ j seq :</formula><p>1 q q l=1 ε j (λ, l) ≤ ε j,min + se(λ j,min ) .</p><p>15: end for 16: Return: {λ ⋆ j } d X j=1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Hyperparameter Tuning</head><p>In this section, we present our approach for selecting the hyperparameters in our algorithm. There are four sets of hyperparameters that need to be determined: the bandwidth τ1 and the group lasso penalty parameters {λj} d X j=1 in Step 2 and Step 4 of Algorithm 1, the bandwidth τ2 and the threshold sample size n0 in Step 3 of Algorithm 1. In our experiments, we set n0 = 2, and τ1 = τ2 = n -1/5 as recommended in <ref type="bibr" target="#b11">Kolar et al. [2010]</ref>. In the following, we elaborate on the selection of {λj} d X j=1 . In Section 4.2 and Section 4.4, the results of optimizing the l2-penalized loss depend on the selection of penalty parameters {λj} d X j=1 , which determine the sparsity level of the estimated graph. Given the training data configuration D n = {X (i) , Z (i) } n i=1 , we divide the dataset into q equally sized subsets {I l } q l=1 ⊆ [n], where ∪ q l=1 I l = [n]. We define I -l := ∪ q k=1,k̸ =l I k . For a chosen node j, inspired by the grid setup of <ref type="bibr" target="#b13">Meier et al. [2008]</ref>, we define the grid of penalty parameters λ j seq = {λ j min , . . . , λ j max }, where 0 ≤ λ j min &lt; . . . &lt; λ j max , by first letting</p><formula xml:id="formula_58">e j,l = n i=1</formula><p>Kτ 1 ĥ(i) -ĥ(l)</p><formula xml:id="formula_59">n k=1 Kτ 1 ĥ(k) -ĥ(l) × X (i) l X (i) j - 1 n n i ′ =1 X (i ′ ) j</formula><p>, and</p><formula xml:id="formula_60">λ j max = 1 n -1 max j∈[d X ] ∥(ej,1, ej,2, • • • , e j,d X )∥ 2 . (<label>10</label></formula><formula xml:id="formula_61">)</formula><p>We set λ j min = λ j max /1000 and distribute λ j seq evenly between them. The gap distance, which is userdefined, controls the refinement of λ j seq . We use mean squared error (MSE) for model evaluation. The model prediction is defined as  With heterogeneous DAGs, our method outperforms QVF in all four measurements. While both methods show similar moral recall, QVF has poor moral precision, especially with large node size, indicating low true positive rate when ignoring DAG heterogeneity. Our method achieves consistent ordering and DAG accuracy, approaching 1 with increasing sample size. In contrast, QVF's ordering accuracy is low, particularly when dX is large, indicating poor ordering recovery due to ignored heterogeneity.</p><formula xml:id="formula_62">X (i) j = T pj(η (i) j ) = T 1 + exp -η (i) j ,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Nonlinear Setup</head><p>The nonlinear data generation process is similar to the linear setup, except for Step 2 in generating the relationship network G. Specifically, we generate dZ -dimensional intermediate covariates C (i) ∼ N (µL i , Σ), where µL i and Σ are defined as in Section 6.1. We then obtain Z (i) by Z (i) = sin(C (i) ), applied element-wise.</p><p>We use GAEs from Section 4.1.2 for node embedding with input feature Z (i) and set d 0 Z = 4. The rest follows Section 6.1. Due to GAEs' high computational cost for large n and dX , we only experiment with dX = 10 and n ∈ {500, 1000, 2500, 5000, 7500}. Results in Figure <ref type="figure" target="#fig_3">2</ref> show our method outperforms QVF under nonlinear setup, especially in neighborhood selection and moral precision. Our method achieves consistency in order and DAG structure recovery, unlike QVF. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Real-World Application</head><p>In this section, we demonstrate our algorithm's practical usefulness using real-world web visit data from Alipay, which records Chinese users' behavior on multiple Alipay websites during the COVID-19 pandemic. Our goal is to understand user transitions between these websites. For instance, to use public transport, people need to show their green code, a digital code indicating COVID-19 exposure risk. Thus, the public transport payment website should cause visits to the green code website. Our aim is to reveal such relationships in the data, aiding the operational team in designing better strategies for customers.</p><p>We collect data {X (i) , Z (i) } n i=1 for n = 6,000 users. The vector X (i) ∈ {0, 1, . . . , 5000} 17 represents the visit counts of user i to 17 Alipay websites, where the j-th entry indicates the number of visits to page j in the past 5,000 records. The vector Z (i) ∈ R 21 comprises 21 features of user i, such as sex and age. The network G captures payment relationships, with an edge between users i and j indicating a money transfer between them on Alipay.  When implementing Algorithm 1, we use GAEs from Section 4.1.2 to estimate the node embedding function. The DAG estimation result of the 17 websites is shown in Figure <ref type="figure" target="#fig_4">3</ref>. Explanations of the abbreviations are summarized in Table <ref type="table" target="#tab_0">1</ref>. The estimated graph reveals interesting causal/directional relationships between different scenes. For instance, "City Service"→"Transport Pay"→"Green Code" demonstrates that users utilize the city service website to find public transportation routes such as subways and buses. Before boarding, they make payments via Alipay and display their green code to verify their health status, aligning with our earlier hypothesis about the link between transportation and green code. Additionally, "City Service"→"Catering Pay"→"Green Code" suggests that users first utilize the city service to find and book restaurants. They then make payments online via Alipay and are required to present their green code to confirm their health status before entering the restaurant. In the same way, "City Service"→"Retail Pay"→"Green Code" indicates that users first utilize the city service to find available markets and make reservations. They subsequently complete their payments online via Alipay, and to receive their orders, they must present their green code to the delivery personnel. From the perspective of merchants, "City Service"→"QRcode Contract"→"Green Code" indicates that small business owners use city service to apply for QR code payments, with Alipay checking the green code to mitigate health risks. Additionally, "City Service"→"Hospital"→"Yue Pay" shows that users search for nearby hospitals via city service and pay medical bills through Yu'e Bao. "City Service"→"Hospital"→"Green Code" demonstrates that users must present their green code before registering after finding nearby hospitals. Additionally, "Search"→"Retail Pay" indicates that users utilize a search engine to find items on the Tmall market and make payments via Alipay. Conversely, "Govern Service"→"Green Code" suggests that to access government services, such as obtaining social security funds, users must present their green code before receiving the service in person. Notably, "Epidemic Industry"→"Green Code"/"Health Code" indicates that users are required to show their green code before undergoing nucleic acid testing. Interestingly, "Green Code"→"Health Code" implies that if users check their green code and find it is not green, they will subsequently verify their health code, which might be yellow or red.</p><p>As illustrated earlier, the majority of the causal/directional relationships depicted in Figure <ref type="figure" target="#fig_4">3</ref> can be comprehensively and reasonably explained. Another significant finding from Figure <ref type="figure" target="#fig_4">3</ref> is that the green code is intricately linked to all facets of people's lives. Given that the green code was essential for nearly all activities during the COVID pandemic in China, this conclusion is intuitively accurate, further confirming the validity of our algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>In this paper, we introduce the heterogeneous DAGs model with network structured covariates, which are common in practice. We propose an algorithm to estimate the DAG structure. Our algorithm's correctness is demonstrated through simulations and real-world data. In simulations, our method outperforms stateof-the-art DAG discovery algorithms for count data that ignore heterogeneity, showing the importance of considering heterogeneity in causal discovery. We apply our algorithm to Alipay website visit data, providing intuitive results for operational strategies. Future research will explore theoretical guarantees and extend our model beyond binomial data.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Comparison of our algorithm with QVF [Park and Raskutti, 2018] on DAG learning accuracy under linear setup. Both algorithms were run on 10 independent realizations for each combination of d X and n. The solid dot shows the mean, and the error bar shows one standard deviation across 10 experiments.</figDesc><graphic coords="12,90.00,90.86,432.01,201.34" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>We use four metrics to evaluate algorithm performance: ordering accuracy, moral precision, moral recall, and DAG accuracy. Ordering accuracy, defined as the Hamming distance between π and π ⋆ , measures order recovery accuracy and Step 3 of Algorithm 1. Moral precision and recall are the precision and recall of moral graph estimation ∪j∈V X N (j) with the ground truth ∪j∈V X N (j), measuring performance on moral graph recovery and Step 2 of Algorithm 1. The DAG accuracy is the Hamming distance between ĜX and GX , measuring the ultimate goal and Algorithm 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Comparison of our algorithm with QVF [Park and Raskutti, 2018] on DAG learning accuracy under nonlinear setup. Both algorithms were run on 10 independent realizations for each combination of d X and n. The solid dot shows the mean, and the error bar shows one standard deviation across 10 experiments.</figDesc><graphic coords="13,90.00,90.86,431.99,113.71" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: DAG estimation result of the 17 websites of Alipay.</figDesc><graphic coords="14,90.00,90.86,432.01,346.07" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>The explanations of the abbreviations of the 17 websites.</figDesc><table><row><cell>Term</cell><cell>Explanation</cell></row><row><cell>3rd Business</cell><cell>Applications operated by the third party service providers</cell></row><row><cell>Hb Repay</cell><cell>Huabei repayment</cell></row><row><cell>Catering Pay</cell><cell>Payment on catering and restaurant</cell></row><row><cell>Retail Pay</cell><cell>Payment on retail and supermarket</cell></row><row><cell>Transport Pay</cell><cell>Payment on public transportation like bus and metro</cell></row><row><cell>Else Pay</cell><cell>Payment which can not be categorized into the former several types</cell></row><row><cell>Hospital</cell><cell>Service provided by the hospital</cell></row><row><cell cols="2">QRcode Contract QR payment code used by merchants to receive payment from customers</cell></row><row><cell>Bill Check</cell><cell>Bill and check owned by users</cell></row><row><cell>Search</cell><cell>Search engine to search for services</cell></row><row><cell>Govern Service</cell><cell>The service provided by the government</cell></row><row><cell>Message Check</cell><cell>Phone notification to remind users to check messages</cell></row><row><cell>Yue Buy</cell><cell>A type of currency fund</cell></row><row><cell cols="2">Epidemic Industry Industry related to the COVID-19, such as nucleic acid testing provider</cell></row><row><cell>Green Code</cell><cell>Digital code showing the risk of users to COVID-19 exposure</cell></row><row><cell>Health Code</cell><cell>Digital code containing the information of user's health condition</cell></row></table></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>where</p><p>for loss (5) and η(i)</p><p>πl for loss (9). Thus, the MSE for node j with penalty parameter λ and partition l is</p><p>(11)</p><p>After getting εj(λ, •) = (εj(λ, 1), εj(λ, 2), • • • , εj(λ, q)), we calculate the standard error of MSE as se(λ) = V(εj(λ, •))/q, where V(•) is the empirical variance. Then we choose λ ⋆ j as the largest value of λ j seq such that the error is within 1 standard error of the minimum MSE that can be achieved. More specifically, let εj,min = min λ∈λ j seq (1/q) q l=1 εj(λ, l) and λj,min = argmin λ∈λ j seq</p><p>(1/q) q l=1 εj(λ, l). Then we have</p><p>1 q q l=1 εj(λ, l) ≤ εj,min + se(λj,min) .</p><p>This rule selects λ &gt; λj,min to obtain a sparser graph, which is generally preferred.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Simulation Experiment</head><p>We validate our algorithm with simulations, demonstrating its superiority over previous QVF DAG models with covariates. To mimic real-world data, we generate the relationship network as in <ref type="bibr" target="#b27">Weng and Feng [2016]</ref>.</p><p>Experiments are divided based on whether the ground-truth embedding function is linear.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Linear Setup</head><p>We first generate the relationship network G by the following procedure:</p><p>1. Let Li ∈ {1, 2} be the user community label, generated from a Bernoulli distribution with P(L1) = P(L2) = 0.5. 2. We generate dZ -dimensional covariates Z (i) ∼ N (µL i , Σ) for i = 1, . . . , n, where Σ = (σt 1 t 2 ) with</p><p>3. Given <ref type="bibr">(Li, Lj, Zij)</ref> where Zij = Z (i) -Z (j) , we generate w 0 ji = w 0 ij ∈ {0, 1} with</p><p>where | • |+ denotes element-wise absolute value, and P(w <ref type="formula">12</ref>), the first part on the right is the label effect, and the second part is a logistic model for the nodal effect. We set</p><p>, and c coef = 3. This gives us the simulated network G and covariates {Z (i) } n i=1 . Next, we generate the DAG GX of X (i) . We set the true order of GX as π ⋆ = (1, 2, • • • , dX ), with (j, j + 1) ∈ EX for j = 0, . . . , dX -1. For each 3 ≤ j ≤ dX , we randomly choose l from {1, . . . , j -2} and set (l, j) ∈ EX . Thus, except for nodes j = 1, 2, each node has two parents. We then set w lj (h ⋆ G (Z (i) )) in ( <ref type="formula">2</ref>) by</p><p>for all (j, l) ∈ VX × VX and i ∈ [n]. Furthermore, we define the total number of trials in the Binomial distribution to be T = 4. We use the linear embedding function from Section 4.1.1 for node embedding. The estimation procedures follow Sections 4 and 5. The results are in Figure <ref type="figure">1</ref>. We test two DAGs with node sizes dX = 10 and dX = 100. For dX = 10, sample sizes are n ∈ {500, 1000, 2500, 5000, 7500}; for dX = 100, sample sizes are n ∈ {2000, 5000, 10000, 15000, 20000, 30000, 40000}. We compare our algorithm with the QVF algorithm in <ref type="bibr" target="#b17">Park and Raskutti [2018]</ref>, which ignores covariate information.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A general maximum likelihood analysis of overdispersion in generalized linear models</title>
		<author>
			<persName><forename type="first">Murray</forename><surname>Aitkin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistics and Computing</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="251" to="262" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deep neural networks for learning graph representations</title>
		<author>
			<persName><forename type="first">Shaosheng</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiongkai</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Testing for overdispersion in poisson and binomial regression models</title>
		<author>
			<persName><forename type="first">Charmaine B</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="issue">418</biblScope>
			<biblScope unit="page" from="451" to="457" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">Kenji</forename><surname>Doya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shin</forename><surname>Ishii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Pouget</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajesh Pn</forename><surname>Rao</surname></persName>
		</author>
		<title level="m">Bayesian brain: Probabilistic approaches to neural coding</title>
		<imprint>
			<publisher>MIT press</publisher>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Fast implementations of nonparametric curve estimators</title>
		<author>
			<persName><forename type="first">Jianqing</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><surname>James S Marron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computational and Graphical Statistics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="35" to="56" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Iftach Nachman, and Dana Pe&apos;er. Using bayesian networks to analyze expression data</title>
		<author>
			<persName><forename type="first">Nir</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michal</forename><surname>Linial</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computatonal Biology</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="601" to="620" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A novel circrna-mirna association prediction model based on structural deep neural network embedding</title>
		<author>
			<persName><forename type="first">Lu-Xiang</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhu-Hong</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changqing</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo-Wei</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhong-Hao</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Briefings Bioinform</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A k-means clustering algorithm</title>
		<author>
			<persName><forename type="first">A</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manchek</forename><forename type="middle">A</forename><surname>Hartigan</surname></persName>
		</author>
		<author>
			<persName><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society Series C: Applied Statistics</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="100" to="108" />
			<date type="published" when="1979">1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A bayesian networks approach for predicting protein-protein interactions from genomic data</title>
		<author>
			<persName><forename type="first">Ronald</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haiyuan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dov</forename><surname>Greenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuval</forename><surname>Kluger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Nevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sambath</forename><surname>Krogan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Emili</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><forename type="middle">F</forename><surname>Snyder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Greenblatt</surname></persName>
		</author>
		<author>
			<persName><surname>Gerstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">302</biblScope>
			<biblScope unit="issue">5644</biblScope>
			<biblScope unit="page" from="449" to="453" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Directed-graph epidemiological models of computer viruses</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">O</forename><surname>Kephart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>White</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Society Symposium on Research in Security and Privacy</title>
		<imprint>
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Variational graph auto-encoders</title>
		<author>
			<persName><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.07308</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">On sparse nonparametric conditional covariance selection</title>
		<author>
			<persName><forename type="first">Mladen</forename><surname>Kolar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ankur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Learning deep generative models of graphs</title>
		<author>
			<persName><forename type="first">Yujia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Battaglia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.03324</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The group lasso for logistic regression</title>
		<author>
			<persName><forename type="first">Lukas</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sara</forename><surname>Van De</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Geer</surname></persName>
		</author>
		<author>
			<persName><surname>Bühlmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society Series B: Statistical Methodology</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="53" to="71" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">On spectral clustering: Analysis and an algorithm</title>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yair</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Adversarially regularized graph autoencoder for graph embedding</title>
		<author>
			<persName><forename type="first">Ruiqi</forename><surname>Shirui Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guodong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lina</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengqi</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Artificial Intelligence</title>
		<editor>
			<persName><forename type="first">Neal</forename><surname>Parikh</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Stephen</forename><surname>Boyd</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2014">2018. 2014</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="127" to="239" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning large-scale poisson dag models based on overdispersion scoring</title>
		<author>
			<persName><forename type="first">Gunwoong</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Garvesh</forename><surname>Raskutti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning quadratic variance function (qvf) dag models via overdispersion scoring (ods)</title>
		<author>
			<persName><forename type="first">Gunwoong</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Garvesh</forename><surname>Raskutti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">224</biblScope>
			<biblScope unit="page" from="1" to="44" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">High-dimensional ising model selection using l1-regularized logistic regression</title>
		<author>
			<persName><forename type="first">Pradeep</forename><surname>Ravikumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><forename type="middle">J</forename><surname>Wainwright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">D</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1287" to="1319" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Using computer-binned data for density estimation</title>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">W</forename><surname>Scott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Science and Statistics: Proceedings of the 13th Symposium on the Interface</title>
		<imprint>
			<date type="published" when="1981">1981</date>
			<biblScope unit="page" from="292" to="294" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Using computer-binned data for density estimation</title>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">W</forename><surname>Scott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Science and Statistics: Proceedings of the 13th Symposium on the Interface</title>
		<imprint>
			<date type="published" when="1981">1981</date>
			<biblScope unit="page" from="292" to="294" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Yew-Soon Ong, and Quan-Sen Sun. Discrete network embedding</title>
		<author>
			<persName><forename type="first">Xiaobo</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shirui</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiwei</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Penalized likelihood methods for estimation of sparse high-dimensional directed acyclic graphs</title>
		<author>
			<persName><forename type="first">Ali</forename><surname>Shojaie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Michailidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="519" to="538" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Kernel density estimation using the fast fourier transform</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">W</forename><surname>Silverman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society Series C: Applied Statistics</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="93" to="99" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Graphvae: Towards generation of small graphs using variational autoencoders</title>
		<author>
			<persName><forename type="first">Martin</forename><surname>Simonovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial Neural Networks and Machine Learning</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Kernel smoothing</title>
		<author>
			<persName><forename type="first">P</forename><surname>Matt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Wand</surname></persName>
		</author>
		<author>
			<persName><surname>Jones</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994">1994</date>
			<publisher>CRC press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Fast computation of multivariate kernel estimators</title>
		<author>
			<persName><surname>Mp Wand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computational and Graphical Statistics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="433" to="445" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Community detection with nodal information</title>
		<author>
			<persName><forename type="first">Haolei</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Feng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.09735</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A comprehensive survey on graph neural networks</title>
		<author>
			<persName><forename type="first">Zonghan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shirui</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fengwen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guodong</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengqi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="4" to="24" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Graphical models via generalized linear models</title>
		<author>
			<persName><forename type="first">Eunho</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Genevera</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhandong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pradeep</forename><surname>Ravikumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="page">25</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Binarized attributed network embedding</title>
		<author>
			<persName><forename type="first">Hong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shirui</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ling</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Defu</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengqi</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Data Mining</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Model Selection and Estimation in Regression with Grouped Variables</title>
		<author>
			<persName><forename type="first">Ming</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society Series B: Statistical Methodology</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="49" to="67" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Dimension reduction for covariates in network data</title>
		<author>
			<persName><forename type="first">Junlong</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiumin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hansheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenlei</forename><surname>Leng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">109</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="85" to="102" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
