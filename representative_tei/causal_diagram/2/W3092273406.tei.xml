<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Visual Analysis of Discrimination in Machine Learning</title>
				<funder ref="#_M5Y6grz">
					<orgName type="full">MSRA</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Qianwen</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Hong Kong University of Science and Technology</orgName>
								<address>
									<addrLine>Under-7 10 23 66 Father 24 Above-7 10 23 20 66 Mum 18 Above-7 23 50 20 66 Father F 70 9 Above-7 23 50 20 66 Mum 12 Above-7 23 50 66 Mum 12 Above-7 50 20 66 Father S High S 70 42 Above-7 50 20 66 Mum 70 Mid F 14 Above-7 Mum 66 10 23 12 20 66 Above-7 Mum 23 50</addrLine>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="institution">Hong Kong University of Science and Technology</orgName>
								<address>
									<addrLine>Under-7 10 23 66 Father 24 Above-7 10 23 20 66 Mum 18 Above-7 23 50 20 66 Father F 70 9 Above-7 23 50 20 66 Mum 12 Above-7 23 50 66 Mum 12 Above-7 50 20 66 Father S High S 70 42 Above-7 50 20 66 Mum 70 Mid F 14 Above-7 Mum 66 10 23 12 20 66 Above-7 Mum 23 50</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhenhua</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Hong Kong University of Science and Technology</orgName>
								<address>
									<addrLine>Under-7 10 23 66 Father 24 Above-7 10 23 20 66 Mum 18 Above-7 23 50 20 66 Father F 70 9 Above-7 23 50 20 66 Mum 12 Above-7 23 50 66 Mum 12 Above-7 50 20 66 Father S High S 70 42 Above-7 50 20 66 Mum 70 Mid F 14 Above-7 Mum 66 10 23 12 20 66 Above-7 Mum 23 50</addrLine>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="institution">Hong Kong University of Science and Technology</orgName>
								<address>
									<addrLine>Under-7 10 23 66 Father 24 Above-7 10 23 20 66 Mum 18 Above-7 23 50 20 66 Father F 70 9 Above-7 23 50 20 66 Mum 12 Above-7 23 50 66 Mum 12 Above-7 50 20 66 Father S High S 70 42 Above-7 50 20 66 Mum 70 Mid F 14 Above-7 Mum 66 10 23 12 20 66 Above-7 Mum 23 50</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhutian</forename><surname>Chen</surname></persName>
							<email>zhutian.chen@connect.ust.hk</email>
							<affiliation key="aff0">
								<orgName type="institution">Hong Kong University of Science and Technology</orgName>
								<address>
									<addrLine>Under-7 10 23 66 Father 24 Above-7 10 23 20 66 Mum 18 Above-7 23 50 20 66 Father F 70 9 Above-7 23 50 20 66 Mum 12 Above-7 23 50 66 Mum 12 Above-7 50 20 66 Father S High S 70 42 Above-7 50 20 66 Mum 70 Mid F 14 Above-7 Mum 66 10 23 12 20 66 Above-7 Mum 23 50</addrLine>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="institution">Hong Kong University of Science and Technology</orgName>
								<address>
									<addrLine>Under-7 10 23 66 Father 24 Above-7 10 23 20 66 Mum 18 Above-7 23 50 20 66 Father F 70 9 Above-7 23 50 20 66 Mum 12 Above-7 23 50 66 Mum 12 Above-7 50 20 66 Father S High S 70 42 Above-7 50 20 66 Mum 70 Mid F 14 Above-7 Mum 66 10 23 12 20 66 Above-7 Mum 23 50</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yong</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Hong Kong University of Science and Technology</orgName>
								<address>
									<addrLine>Under-7 10 23 66 Father 24 Above-7 10 23 20 66 Mum 18 Above-7 23 50 20 66 Father F 70 9 Above-7 23 50 20 66 Mum 12 Above-7 23 50 66 Mum 12 Above-7 50 20 66 Father S High S 70 42 Above-7 50 20 66 Mum 70 Mid F 14 Above-7 Mum 66 10 23 12 20 66 Above-7 Mum 23 50</addrLine>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="institution">Hong Kong University of Science and Technology</orgName>
								<address>
									<addrLine>Under-7 10 23 66 Father 24 Above-7 10 23 20 66 Mum 18 Above-7 23 50 20 66 Father F 70 9 Above-7 23 50 20 66 Mum 12 Above-7 23 50 66 Mum 12 Above-7 50 20 66 Father S High S 70 42 Above-7 50 20 66 Mum 70 Mid F 14 Above-7 Mum 66 10 23 12 20 66 Above-7 Mum 23 50</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shixia</forename><forename type="middle">•</forename><surname>Liu</surname></persName>
							<email>shixia@tsinghua.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Hong Kong University of Science and Technology</orgName>
								<address>
									<addrLine>Under-7 10 23 66 Father 24 Above-7 10 23 20 66 Mum 18 Above-7 23 50 20 66 Father F 70 9 Above-7 23 50 20 66 Mum 12 Above-7 23 50 66 Mum 12 Above-7 50 20 66 Father S High S 70 42 Above-7 50 20 66 Mum 70 Mid F 14 Above-7 Mum 66 10 23 12 20 66 Above-7 Mum 23 50</addrLine>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="institution">Hong Kong University of Science and Technology</orgName>
								<address>
									<addrLine>Under-7 10 23 66 Father 24 Above-7 10 23 20 66 Mum 18 Above-7 23 50 20 66 Father F 70 9 Above-7 23 50 20 66 Mum 12 Above-7 23 50 66 Mum 12 Above-7 50 20 66 Father S High S 70 42 Above-7 50 20 66 Mum 70 Mid F 14 Above-7 Mum 66 10 23 12 20 66 Above-7 Mum 23 50</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Huamin</forename><surname>Qu</surname></persName>
							<email>huamin@ust.hk</email>
							<affiliation key="aff0">
								<orgName type="institution">Hong Kong University of Science and Technology</orgName>
								<address>
									<addrLine>Under-7 10 23 66 Father 24 Above-7 10 23 20 66 Mum 18 Above-7 23 50 20 66 Father F 70 9 Above-7 23 50 20 66 Mum 12 Above-7 23 50 66 Mum 12 Above-7 50 20 66 Father S High S 70 42 Above-7 50 20 66 Mum 70 Mid F 14 Above-7 Mum 66 10 23 12 20 66 Above-7 Mum 23 50</addrLine>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="institution">Hong Kong University of Science and Technology</orgName>
								<address>
									<addrLine>Under-7 10 23 66 Father 24 Above-7 10 23 20 66 Mum 18 Above-7 23 50 20 66 Father F 70 9 Above-7 23 50 20 66 Mum 12 Above-7 23 50 66 Mum 12 Above-7 50 20 66 Father S High S 70 42 Above-7 50 20 66 Mum 70 Mid F 14 Above-7 Mum 66 10 23 12 20 66 Above-7 Mum 23 50</addrLine>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Visual Analysis of Discrimination in Machine Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-14T19:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Machine Learning</term>
					<term>Discrimination</term>
					<term>Data Visualization</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>d -0.25 0.25 favor gender=F against gender=F 60 XGB RF Fig. 1. DiscriLens facilitates a better understanding and analysis of algorithmic discrimination: (a) scatter plots offer an overview of the discriminatory itemsets; (b) RippleSets reveal the intersections among these itemsets; (c) the attribute matrix represents the details of each discriminatory itemset; (d) the comparison mode enables users to compare two models side by side.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Machine learning (ML) has progressed dramatically in recent decades and become a useful technique in a variety of applications, including credit scoring <ref type="bibr" target="#b31">[33]</ref>, crime prediction <ref type="bibr" target="#b20">[22]</ref>, and college admission <ref type="bibr" target="#b50">[53]</ref>. Since decision-making in these areas may have ethical or legal issues <ref type="bibr">[15,</ref><ref type="bibr">49]</ref>, it is crucial for model users to go beyond model accuracy and consider the fairness of ML models.</p><p>Consider the following scenario. When reviewing loan applications, loan officers need to estimate the risk of default (i.e., the probability of failing to repay the loans), which is usually time-consuming and error-prone. A machine learning model trained on historical credit data can estimate the creditworthiness of applicants and thus facilitate the decision-making. However, this model can unintentionally make discriminatory predictions in the social sense, even though the training data describes objective facts and includes no human discrimination. For example, this model may treat two applicants unequally based on gender despite their same repayment capacity. To avoid making decisions based on protected attributes (attributes such as gender and race that are legally protected by laws <ref type="bibr">[49,</ref><ref type="bibr" target="#b57">61]</ref>), a straightforward method is to hide these attributes. But this method not only decreases the model accuracy but has also been proven ineffective since models are able to learn protected attributes from other non-protected attributes (e.g., predict gender based on address and occupation) <ref type="bibr" target="#b21">[23,</ref><ref type="bibr" target="#b49">52,</ref><ref type="bibr" target="#b69">73]</ref>.</p><p>To further promote the adoption of ML models and prevent potential negative social impacts, discrimination in ML is drawing increasing research attention. Many methods have been proposed to assess and mitigate discrimination from three main aspects: pre-process methods that investigate the discrimination in training data <ref type="bibr" target="#b26">[29,</ref><ref type="bibr" target="#b35">37,</ref><ref type="bibr" target="#b65">69]</ref>, in-process methods that adjust the model learning process <ref type="bibr" target="#b28">[31,</ref><ref type="bibr" target="#b42">44,</ref><ref type="bibr" target="#b64">68]</ref>, and post-process methods that modify the discriminatory model predictions <ref type="bibr" target="#b23">[25,</ref><ref type="bibr" target="#b67">71]</ref>. However, these studies usually formalize discrimination as summary statistics and may hinder a detailed assessment. Meanwhile, these studies simply assume that the representation of discrimination has been clearly defined, which usually does not hold in practice <ref type="bibr" target="#b23">[25,</ref><ref type="bibr" target="#b48">51]</ref>. Due to the complex nature of discrimination, it has no clear and uniform definition and its representation varies a lot in different domains. In this study, we develop a visual analysis tool that enables the involvement of domain knowledge and supports a systematical Fig. <ref type="figure">2</ref>. It is not easy to distinguish reasonable differences from discriminatory treatment. Here is a toy example of college admission. The overall low acceptance rate for females can be explainable by their tendency to apply to the more competitive major. assessment of discrimination, thus further benefiting the analysis and mitigation of discrimination <ref type="bibr" target="#b24">[26]</ref>.</p><p>To analyze discrimination in machine learning, it is important to access whether differential treatment is discriminatory (e.g., based on race) or reasonable (e.g., based on the qualification of the applicant). The term fairness used in this paper refers to the principle that any two individuals who are similar with respect to a particular task should be classified similarly <ref type="bibr" target="#b17">[19,</ref><ref type="bibr" target="#b32">34]</ref>. The first question is which individuals should be regarded as similar for the task at hand. The definition of similar people varies a lot among tasks and is important for the analysis of discrimination. Fig. <ref type="figure">2</ref> shows an example of college admission, where gender is a protected attribute. If all the applicants are treated as similar, there seems to be gender discrimination since the acceptance rate is 42% for females but 50% for males. Unequal treatment still exists if we group applicants based on the test score. But if we define similar people using the combination of {major, test score}, it then shows no unequal treatment in any of the sub-groups. Such a phenomenon is also mentioned in Simpson's paradox <ref type="bibr" target="#b25">[28]</ref>. The second question is how to present discrimination among these similar individuals effectively. We treat a group of similar people as an itemset defined by a series of attributes values (e.g., {test score=low, major=CS}). The interpretability of these itemsets is severely weakened when the definition is long and complex. Furthermore, the number of these itemsets can be large and these itemsets are often intricately intertwined together. Therefore, it is non-trivial to assist users in perceiving these itemsets and interpreting discrimination.</p><p>To tackle the challenges, we design and implement DiscriLens, an interactive visualization tool that facilitates an easy interpretation, evaluation, and comparison of the algorithmic discrimination. A demo is available at <ref type="url" target="https://qianwen.info/demos/discrilens/">https://qianwen.info/demos/discrilens/</ref> (recommend opening in Chrome). We develop a three-stage pipeline to identify a collection of potentially discriminatory itemsets based on causal modeling and classification rules mining. A set of user interactions are provided to incorporate human domain knowledge in discrimination analysis. A novel Euler-based visualization, RippleSet (Fig. <ref type="figure">1(b</ref>)), is proposed to provide an effective presentation of discrimination. Ripple-Set represents one set as several adjacent circles instead of one convex shape, thereby preventing the overlaps in traditional Euler diagram. We further combine the RippleSet with a matrix-based visualization (Fig. <ref type="figure">1(c</ref>)) to support users in examining the discriminatory itemsets from multiple aspects. We demonstrate the effectiveness of DiscriLens in analyzing discrimination through a user study and use cases.</p><p>The main contributions of this work are as follows:</p><p>• The design and development of DiscriLens, an interactive visual analysis tool with a set of novel visualization techniques for analyzing discrimination in machine learning. • A user study and a series of use cases that assess the utility and usability of DiscriLens.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Discrimination in Machine Learning</head><p>Existing studies mainly investigate algorithmic discrimination from two aspects: the data and the model training process <ref type="bibr" target="#b21">[23]</ref>.</p><p>Training data may include human discrimination and then influence the trained model <ref type="bibr" target="#b35">[37]</ref>. Various approaches have been proposed to discover discrimination existing in the training data <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b26">29,</ref><ref type="bibr" target="#b42">44,</ref><ref type="bibr" target="#b47">50,</ref><ref type="bibr" target="#b69">73]</ref>. A pragmatic solution is to examine different treatments among similar items <ref type="bibr" target="#b26">[29,</ref><ref type="bibr" target="#b47">50]</ref>. For example, Luong et al. <ref type="bibr" target="#b41">[43]</ref> used the k-nearest neighbor algorithm to group similar items and identify the discrimination among them. Pedreshi et al. <ref type="bibr" target="#b49">[52]</ref> employed classification rules to reveal itemsets that may lead to unequal treatment. Recently, to provide an interpretable notation of discrimination, several studies considered the relationships among item attributes and examined discrimination from a causal modeling perspective <ref type="bibr" target="#b35">[37,</ref><ref type="bibr" target="#b65">69]</ref>.</p><p>Even though human discrimination can be identified and eliminated from the training data, a model can still make discriminatory predictions due to the data distribution and the model learning mechanism (e.g., over-fitting) <ref type="bibr" target="#b40">[42,</ref><ref type="bibr" target="#b48">51,</ref><ref type="bibr" target="#b63">67,</ref><ref type="bibr" target="#b67">71,</ref><ref type="bibr" target="#b69">73]</ref>. To remove discrimination introduced in the training process, researchers developed regularizers that penalize discriminatory predictions <ref type="bibr" target="#b28">[31,</ref><ref type="bibr" target="#b64">68]</ref> and methods that directly modify the model predictions <ref type="bibr" target="#b23">[25,</ref><ref type="bibr" target="#b67">71]</ref>. Many studies have defined different criteria for model discrimination to provide meaningful <ref type="bibr" target="#b48">[51,</ref><ref type="bibr" target="#b67">71]</ref> and interpretable <ref type="bibr" target="#b23">[25]</ref> notations. Meanwhile, to support users in measuring and comparing discrimination in different models, initial efforts have been made to quantify model discrimination <ref type="bibr" target="#b55">[58]</ref>.</p><p>These studies shed light on discrimination analysis and form the foundation of this study. However, the discrimination mined by these methods can be complex and thus hard to interpret (e.g., a large number of rule lists), requiring the support of effective visual presentation. More importantly, the analysis of discrimination is, by nature, a subjective task and varies based on the application domains. This paper provides interactive analysis to help incorporate human domain knowledge in discrimination analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Visual Analysis for ML Discrimination</head><p>Visualization, an effective tool for information communication, has been employed to present and analyze discrimination in ML <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr">27,</ref><ref type="bibr" target="#b36">38,</ref><ref type="bibr" target="#b56">59]</ref>. For example, Google Big Picture developed an interactive visualization demo <ref type="bibr" target="#b7">[8]</ref> to illustrate the discrimination in ML decisions. Other tools, such as IBM AI Fairness 360 [27], Tensorflow Fairness Indicators <ref type="bibr" target="#b56">[59]</ref>, and FairLearn, enable the easy computation of fairness metrics and support visualizations of these fairness metrics. However, most tools only support segmenting groups based on one protective attribute and only provide basic visualizations (e.g., bar charts). Therefore, they cannot be directly applied to investigate and present the potentially complicated discrimination between different groups.</p><p>Recently, advanced visualization tools have been developed to enables more comprehensive analysis of discrimination, including Fair-Sight <ref type="bibr" target="#b0">[1]</ref> and FairVis <ref type="bibr" target="#b8">[9]</ref>. FairSight represents a workflow that supports the four fairness-aware actions (i.e., understand, measure, identify, and mitigate) required in decision making. FairSight shares the same highlevel goal as DiscriLens and also supports the analysis of individual fairness. However, FairSight either a) identifies discriminated instances based on an KNN algorithm or b) provides a global-level measure by aggregating over all instances. As a result, FariSight fails to uncover the discriminatory itemsets and to reveal "when and where will a model yield discriminatory predictions". FairVis is more related to DiscriLens due to its focus on the analysis of discriminatory itemsets. This tool helps users to audit the fairness among different itemsets. However, FairVis focuses on suggesting similar discriminatory itemsets during analysis. The possible complex definition of the itemsets are presented using tables, which is not an optimal representation method. No special visualizations are designed to reveal the intertwining relationships among the subgroups.</p><p>In DiscriLens, we propose novel visualizations for better understanding and analysing ML discrimination.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Set Visualization</head><p>The core of discrimination analysis is to understand how a set of items, grouped by attribute values, are treated unequally. Due to the ubiquity of set-based analysis, a large body of literature has been developed. Below we review some visualizations that are germane to our work. A comprehensive review of set visualizations can be found in the work of Alsallakh et al. <ref type="bibr" target="#b3">[4]</ref>.</p><p>The most common set visualizations are Euler and Venn diagrams <ref type="bibr" target="#b19">[21,</ref><ref type="bibr" target="#b37">39]</ref>, which represent sets as convex or concave shapes and show set intersections as overlapping shapes. Previous work has proposed many extensions of Euler diagram with varying design goals <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b15">17,</ref><ref type="bibr" target="#b52">55,</ref><ref type="bibr" target="#b54">57]</ref>. For example, Riche et al. <ref type="bibr" target="#b52">[55]</ref> simplified a sophisticated collection of intersecting sets into a strict hierarchy, thereby untangling Euler diagrams and improving its readability. Bubble Sets <ref type="bibr" target="#b12">[13]</ref> delineate set memberships and minimize cluster overlap while retaining the layout that reflects the semantic spatial organization. However, methods based on Euler and Venn diagrams often impose severe limitations on the number of sets and the complexity of set intersections <ref type="bibr" target="#b2">[3]</ref>.</p><p>Considering the inherent limits of Euler and Venn diagrams, researchers have developed many alternative presentations of set data, including curve-based presentation <ref type="bibr" target="#b1">[2]</ref>, treemap-based presentation <ref type="bibr" target="#b4">[5]</ref>, parallel coordinate-based presentation <ref type="bibr" target="#b34">[36]</ref>, matrix-based presentation <ref type="bibr" target="#b33">[35,</ref><ref type="bibr" target="#b37">39]</ref>, and chord diagram-based presentation <ref type="bibr" target="#b2">[3]</ref>. The most relevant ones to our work are Upset <ref type="bibr" target="#b4">[5]</ref> and Conset <ref type="bibr" target="#b33">[35]</ref>, which employed matrix to communicate the properties of the set aggregations and intersections. However, Upset and Conset are specified for visualizing binary set data and cannot be directly applied to the categorical set data in discrimination analysis. Meanwhile, in existing work, the comparison of two collections of set data has not been adequately explored and requires further investigation.</p><p>In this work, to facilitate the analysis and comparison of categorical set data, we propose a novel set visualization that combines an extended Euler diagram, RippleSet, with a matrix-based visualization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">DISCRIMINATION: A MATHEMATICAL NOTATION</head><p>This section provides background information and a mathematical notation of discrimination. In this paper, we focus on the analysis of a single protected attribute, denoted by A. We denote other non-protected attributes by X, and the outcome by Y . Note that the outcome Y can be the labels in the training data or the predictions of ML models. The beneficial class (e.g., a loan approval) is denoted by Y = 1, and the protected group (e.g., females) is denoted by A = 1. Unobserved attributes are not considered in this study.</p><p>A natural notation of fairness is demographic parity <ref type="bibr" target="#b23">[25,</ref><ref type="bibr" target="#b35">37]</ref>, which states that each segment of a protected class (e.g., blacks and nonblacks) should receive positive outcomes at equal rates (e.g., same admission rate), regardless of the value distribution of other attributes (e.g., score). In other words, this notation treats all individuals as similar and requires the decisions to be independent of the protected attribute, as shown in Fig. <ref type="figure" target="#fig_0">3</ref>(a)(b). Demographic parity is easy to examine and thus has been widely used in legal practice, including the US Equal Employment Opportunity Commission [60] and the UK Sex Discrimination Act <ref type="bibr" target="#b46">[48]</ref>. However, this notion can be flawed when the correlation between attributes is strong and needs to be considered, which is the situation we consider in this paper.</p><p>As shown in Fig. <ref type="figure" target="#fig_0">3(c</ref>), the protected attribute race are correlated with other attributes and can influence the admission from two paths: justified difference (race→major→admission) and indirect discrimination (race→postcode→admission). This first path is called justified difference since major offers objective information about the admission and can explain the outcome differences. E.g., the overall low admission rate for black students can be explained by their tendency to apply for more competitive majors. Such attributes that can justify decision differences are called resolving attributes <ref type="bibr" target="#b32">[34]</ref>. The second path is called indirect discrimination because the postcode is correlated with race (e.g., majority-black neighborhood) but should not affect the admission. These attributes that act as a proxy for protected attributes are called proxy attributes <ref type="bibr" target="#b32">[34]</ref>. As a result, resolving and proxy attributes should be both considered for the analysis of discrimination <ref type="bibr" target="#b32">[34,</ref><ref type="bibr" target="#b66">70]</ref>. In general, there is no ground truth for resolving or proxy attributes. Their definitions depend on the task (e.g., college admission) and require human domain knowledge. Therefore, interactive visual analysis of discrimination is needed.</p><p>In this study, we use risk difference (RD), a widely used metric in anti-discrimination literature <ref type="bibr" target="#b48">[51,</ref><ref type="bibr" target="#b53">56,</ref><ref type="bibr" target="#b65">69,</ref><ref type="bibr" target="#b66">70]</ref>, for the measurement of discrimination. where S includes all resolving attributes and no proxy attributes, ensuring that justified difference is separated from discrimination. Following the practice in <ref type="bibr" target="#b65">[69]</ref>, non-discrimination is claimed when RD &lt; τ holds for each itemset s i . Note that DiscriLens is not metric specific and can be easily extended to other metrics such as true positive rate difference, error rate ratio, and odds ratio <ref type="bibr">[27]</ref>. We would like to emphasize that the discrimination measurement used in this paper is just one of the many possible choices. The choice of measurement depends on the context of analysis.</p><formula xml:id="formula_0">RD = P(Y = 1|A = 1, S = s i ) -P(Y = 1|A = 0, S = s i ),</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">DESIGNING DISCRILENS 4.1 Design Goals</head><p>The goal of DiscriLens is to enable model users to flexibly interpret and analyze the discrimination in ML. The users have a certain level of expertise in ML and wish to apply ML models to a real-world application. They want to analyze discrimination to better avoid the potential ethical or legal issues. Based on the previous literature on algorithmic discrimination and discussions with two ML experts, we identify the following design goals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G1 Customize the definition of discrimination.</head><p>Discrimination is a complex concept and can have different representations at different domains <ref type="bibr" target="#b45">[47]</ref>. For example, a decision based on age is discrimination in certain domains but not in others. Also, the resolving attributes to a decision significantly vary from domain to domain. Therefore, the domain knowledge of the users is essential for the analysis of discrimination. Users should be allowed to customize the definition of discrimination (i.e., protected attribute, proxy attributes, resolving attributes, the threshold τ) based on their domain knowledge as well as the results of discrimination discovery algorithms.</p><p>G2 Measure the degree of discrimination.</p><p>For the analysis of discrimination, model users need to measure the degree of algorithmic discrimination <ref type="bibr" target="#b48">[51,</ref><ref type="bibr" target="#b55">58]</ref>, which can be evaluated by two indicators: the risk difference and size. The risk difference answers the question: "To which extent are the protected group (A = 1) and non-protected group (A = 0) treated unequally?" The size answers the question: "How many people are influenced by algorithmic discrimination?" High risk difference and large size indicate severe discrimination.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G3 Identify the condition of discrimination.</head><p>Apart from measuring the degree of discrimination, the condition of discrimination should be identified to reveal when a model will yield discriminatory predictions. This information can be conveyed by a set of attribute values that specify the discriminatory itemsets, i.e., a group of individuals who are similar to the task but are treated differently. The condition of discrimination is crucial since it can provide detailed guidance on applying models (e.g., identify the failure mode <ref type="bibr" target="#b49">[52]</ref>), improving the training data (e.g., modify inappropriate data labels <ref type="bibr" target="#b65">[69]</ref>, collect new data <ref type="bibr" target="#b68">[72]</ref>), and adjusting the training process (e.g., add regularization to penalize discriminatory predictions <ref type="bibr" target="#b42">[44,</ref><ref type="bibr" target="#b64">68]</ref>).</p><p>G4 Depict the distribution of discrimination. The degree and the condition of discrimination can vary a lot among itemsets <ref type="bibr" target="#b49">[52,</ref><ref type="bibr" target="#b69">73]</ref>. For example, in income prediction, the degree of discrimination towards blacks may vary with other attribute values, such as working hours, education level, and work class. As a result, algorithmic discrimination can hardly be comprehensively described by summary statistics or easily interpreted by humans <ref type="bibr" target="#b55">[58]</ref>. The difficulty is significantly increased when the intersections between itemsets are complex. To fully capture the discriminatory behavior of a machine learning model, the distribution of discrimination should be offered to help users understand how discrimination varies among and inside different itemsets.</p><p>G5 Compare discrimination. Summary statistics can be ineffective for the analysis of ML models <ref type="bibr" target="#b63">[67]</ref>. Models with the same accuracy can assign conflicting predictions for the same input data <ref type="bibr" target="#b43">[45]</ref>, leading to discriminatory predictions against different groups of data items. For example, one model may discriminate equally against all female students, while another model discriminates severely against females with low scores but mildly against other females. Therefore, to choose an appropriate model, it is important to compare models and analyze conflicting predictions.</p><p>Apart from model-level comparison, it is also important to compare discrimination among different itemsets. Several studies <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b48">51]</ref> have observed that model discrimination cannot be completely removed. Therefore, different itemsets should be compared to prioritize the most severe discrimination, whose identification is objective and requires human domain knowledge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">System Overview</head><p>DiscriLens consists of two main modules: a discovery module and a visualization module (Fig. <ref type="figure" target="#fig_1">4</ref>). The discovery module takes the training data, the model, and the user-defined protected group as input.</p><p>It then goes through a three-stage pipeline and produces a collection of potentially discriminatory itemsets. The visualization module serves as an interface that helps understand the discrimination, as well as a tool that provides guidance on applying and improving the model. Details of the discovery and the visualization module are introduced in the following two sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">DISCRIMINATION DISCOVERY</head><p>We view the task of discovering discrimination as a problem of mining discriminatory itemsets <ref type="bibr" target="#b49">[52,</ref><ref type="bibr" target="#b65">69]</ref>. Given the outcomes Ŷ = f (X) and a threshold τ, the goal is a collection of itemsets {s 1 , s 2 , ..., the discovery module contains three main components: mine resolving attributes, mine classification rules, and mine discriminatory itemsets.</p><formula xml:id="formula_1">s n } that |P( Ŷ = 1|s i , A = 0) -P( Ŷ = 1|s i , A = 1)| &gt; τ, i ∈ n,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Resolving Attributes</head><p>In this step, we mine resolving attributes following the causality-based method proposed in <ref type="bibr" target="#b65">[69]</ref>. Causal modeling is a complicated task and requires strong prior knowledge. Fortunately, when analyzing discrimination, we only need to mine local causality and identify the parents of Y (i.e., (Pa(Y )) without building the complete causal graph. We refer the readers to <ref type="bibr" target="#b32">[34,</ref><ref type="bibr" target="#b65">69]</ref> for more details and the complete mathematical proof. The resolving attributes can be then represented as Par(Y ) \ (A ∪ P), where P indicates the proxy attributes. The identification of proxy and resolving attributes requires users' domain knowledge. Therefore, we allow users to interactively modify resolving attributes, remove proxy attributes, and analyze the results in the visualization module. In this implementation, we use the Fast Greedy Equivalence Search <ref type="bibr" target="#b11">[12]</ref> for local causal discovery.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Classification Rules</head><p>In this stage, we can extract a list of classification rules to explain the outcome Y . Take a support vector machine trained on Adult dataset <ref type="bibr" target="#b16">[18]</ref> as an example. The task is to predict whether the annual income of a person is over 50k USD. One of the classification rules is shown in Fig. <ref type="figure" target="#fig_2">5(a)</ref>. The support of this rule supp(s → Ŷ = 1) = supp(s, Ŷ = 1) = 16 and the confidence of the rule con f (s → Ŷ = 1) = P( Ŷ = 1|s) = supp(s, Ŷ = 1)/supp(s) = 0.9. In this implementation, we use minimum description length <ref type="bibr" target="#b18">[20]</ref> to discretize continuous attributes and FP-Growth <ref type="bibr" target="#b22">[24]</ref> to mine the classification rules. Alternative methods can also be used, and we refer the reader to <ref type="bibr" target="#b62">[66]</ref> for a detailed discussion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Discriminatory Itemsets</head><p>Given two classification rules:</p><formula xml:id="formula_2">(s, A = 0) → Ŷ = 1, (s, A = 1) → Ŷ = 1, we can treat s as a discriminatory itemset if |P( Ŷ = 1|A = 0, s) -P( Ŷ = 1|A = 1, s)| &gt; τ</formula><p>and s includes all resolving attributes and no proxy attributes. We do not specify the sign due to reverse discrimination, i.e., discrimination against the non-protected group.</p><p>As shown in Fig. <ref type="figure" target="#fig_2">5</ref>, if [workclass, education, hours per week] include all resolving attributes and no proxy attributes, the confidence difference between rule (b) and (c) can be treated as discrimination towards females.</p><p>In other words, in the itemset s =[workclass=feder-gov, education&lt;8, hours per week&gt;65], males are less likely to be predicted to have low incomes compared with their female counterparts (0.8 vs. 1.0). Therefore, this itemset is called discriminatory itemset. To enable interactive modification, we do not specify resolving and proxy attributes when mining classification rules. Note that s may contain attributes which are neither proxy nor resolving attributes. We call these attributes as context attributes. In real-world, a discriminatory itemset can be defined by a long list of attribute values, and the itemsets are usually intertwined together. As a result, effective visualization is needed to assist users in the analysis of discrimination.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">VISUAL INTERFACE</head><p>The visualization module consists of three visualization components (Fig. <ref type="figure">1</ref>). The scatter plot (a) offers an overview of the discriminatory itemsets and allows users to filter itemsets. RippleSet (b) and attribute matrix (c) supplement each other to support a detailed analysis of discriminatory itemsets. RippleSet reveals the discrimination among items that belong to intricately intertwined sets, while the attribute matrix illustrates the details of each itemset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">RippleSet</head><p>When analyzing model discrimination, it is important to effectively represent discrimination among itemsets that have complex intersection and inclusion relationships (G4). Existing set visualization methods often impose severe limitations on the complexity of set intersections <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref>, and thus can hardly satisfy the design goals of DiscriLens. To better support the analysis in DiscriLens, we extend the widely-used Euler diagram and design RippleSet.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.1">Visual Encoding</head><p>As shown in Fig. <ref type="figure">7</ref>, the main idea of RippleSet is to represent one set using several adjacent shapes, instead of one shape, to avoid the complex overlapping shapes in the traditional Euler diagram. In Rip-pleSet, each shape represents one maximal inseparable subset, i.e., a subset that will not be further segmented based on the sets they belong to (Fig. <ref type="figure">7(b)</ref>). Therefore, more shapes in the RippleSet indicate more complex intersections among itemsets. During the analysis, a set can be highlighted using an outline that surrounds all the subsets of the set.</p><p>In RippleSet, we also offer instance-level visualization. Each data item is represented as a dot and placed inside the subset it belongs to, as shown in Fig. <ref type="figure">6(C</ref>). Inside each subset, we use the algorithm proposed by Wang et al. <ref type="bibr" target="#b60">[64]</ref> to layout the data items, which are inserted based on their original order in the dataset. Three visual channels, color, shape, and solid/hollow, are used to display the attributes of data items. Color, the most effective visual channel, is used to present the most important information, the direction and the degree of the discrimination. The color encoding is consistent with that used in the attribute matrix: color saturation indicates the absolute value of the risk difference while color hue indicates the discriminated group (protected group or non-protected group). A solid item represents a data item classified as the beneficial class ( Ŷ = 1), whereas a hollow item represents a data item classified as the non-beneficial class ( Ŷ = 0). The shapes of the items encode their groups: circles for the protected group (A = 1) due to their metaphor of softness and squares for the non-protected group (A = 0) due to their metaphor of hardness <ref type="bibr" target="#b39">[41]</ref>. Even though the shape is a less pop-out visual channel, previous studies <ref type="bibr" target="#b14">[16,</ref><ref type="bibr" target="#b38">40,</ref><ref type="bibr" target="#b61">65]</ref> have demonstrated that it can support users in analyzing different item groups. Meanwhile, we do admit that the shape encoding may reduce the analysis efficiency.</p><p>To address this issue, we plan to modify the layout algorithm and double encode the item groups using both the shape and item position in the future. For a large number of items, RippleSet supports the aggregation of items, as shown in Fig. <ref type="figure">6</ref>(D1). More details about the layout algorithm are provided in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.2">Design Alternatives</head><p>We tested several design alternatives (Fig. <ref type="figure" target="#fig_5">8</ref>) for the set visualization, during which we interviewed domain experts and discussed with visualization experts. We first tried a weighted Voronoi-based design, as shown in Fig. <ref type="figure" target="#fig_5">8</ref>(a). This design was unsuccessful since the irregular shapes posed challenges to users for comparing the size of subsets. We then implemented a Euler-based design, as shown in Fig. <ref type="figure" target="#fig_5">8(b)</ref>. From Fig. <ref type="figure" target="#fig_5">8</ref>(b) to (e), we clustered items belonging to the same subset to offer a clear organization, replaced irregular outlines with circles for aesthetics, and added detailed information of each item.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Attribute Matrix</head><p>The attribute matrix (Fig. <ref type="figure">6</ref>(A)) helps users investigate each discriminatory itemset in detail. The attribute matrix employs a similar matrix-based layout as RuleMatrix <ref type="bibr" target="#b44">[46]</ref>. This design has been proven effective in presenting classification rules, which is used to identify discriminatory itemsets. In the attribute matrix, each row stands for a group of individuals and each column stands for an attribute. The head (A1) represents the distribution of data items on different attributes, a root row (A2) represents a collection of discriminatory itemsets with the same resolving attribute values, and a row (A3) represents one discriminatory itemset. The head (Fig. <ref type="figure">6</ref>(A1)) of the attribute matrix consists of a row of charts. Histograms are used for visualizing the distribution on continuous attributes, and bar charts are used for discrete attributes. In each chart, the x-axis represents the attribute value, and the y-axis represents the number of items. We use dark blue to indicate the items classified as the beneficial class (e.g., a loan approval) and light blue for the nonbeneficial class (e.g., a loan decline). To distinguish resolving attributes, the names of resolving attributes are highlighted. Users can modify the resolving attributes based on their domain knowledge as well as their observation of the data distribution (G1). Discriminatory itemsets will dynamically update according to the modified resolving attributes.</p><p>To facilitate the interpretation and exploration, we group and aggregate discriminatory itemsets to offer a clear summary. We first group the discriminatory itemsets based on their resolving attribute values. All itemsets sharing the same resolving attribute values are put in one collection, which is represented as a bordered rectangle called root row (Fig. <ref type="figure">6(A2)</ref>). The number of items in a collection is labeled at the left end of the bordered rectangle. Then, in each collection, we organize itemsets, which are represented as rows (Fig. <ref type="figure" target="#fig_0">6(A3)</ref>), according to their inclusion relations. If an itemset A is included in itemset B (i.e., A ∈ B), A can be opened by clicking the expand on B.</p><p>For each discriminatory itemset, a simple glyph (Fig. <ref type="figure" target="#fig_2">6(A5)</ref>) is put at the left end to indicate the degree of discrimination (G2). The radius of the glyph encodes the size of the itemset. The angle of the outer arc is determined by the probability that the non-protected group is labeled as beneficial class (i.e., P( Ŷ = 1|A = 0, s)) and the angle of the inner arc is determined by the probability that the protected group is labeled as beneficial class (i.e., P( Ŷ = 1|A = 1, s)).</p><p>The condition of discrimination (i.e., a series of attribute values) is represented by an array of rectangles, whose solid parts represent the specified ranges/values of the aligned attributes (G3). Since we discretize continuous attributes for mining classification rules, all attributes here are actually categorical and are visualized using similar encodings with continuous attributes. The color saturation of these rectangles is determined by the absolute value of the risk difference and the color hue is determined by the sign of the risk difference (G2). To better compare the condition of discrimination (G5), itemsets are vertically arranged in an order that maximizes the Jaccard similarity of adjacent itemsets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Interactions</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Filter Itemsets &amp; Modify Resolving Attributes</head><p>The scatter plot (Fig. <ref type="figure">6(C</ref>)) allows the filtering of itemsets by offering a summary of all discriminatory itemsets (G4). In the scatter plot, one point represents one discriminatory itemset, with the x-axis indicating the risk difference (the extent of the unequal treatment) and the y-axis indicating the size of the itemset (G2). The scatter plots help users measure the discrimination from a coarse level and guide them in the filtering operation. By moving the sliders on the x-axis, users can change the threshold of risk difference and select itemsets for further exploration. The scatter plot also supports a coarse level comparison of models (G5) through two ways: juxtaposition (i.e., side by side) and superposition (i.e., overlay).</p><p>In the head of the attribute matrix (Fig. <ref type="figure">6</ref>(A1)), users can modify resolving attributes (G1) through drag and drop. Such modification changes the mined discriminatory itemsets. Other visual components, i.e., the scatter plot, the RippleSet, and the rows in the attribute matrix, will be updated dynamically.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Coordinate RippleSet with Attribute Matrix</head><p>The attribute matrix and the RippleSet are coordinated to support a comprehensive analysis. Each root row of the attribute matrix is linked to a RippleSet through a curve. Clicking on a RippleSet can expand the rows belonging to the root row. Users can hover over a row to highlight the corresponding itemset in RippleSet. By clicking on a row, users can draw an outline for this itemset (Fig. <ref type="figure">6(D)</ref>) to facilitate their understanding of the discrimination distribution (G4).</p><p>Three modes of analysis are supported: the compact mode, the parallel mode, and the comparison mode. In the compact mode (Fig. <ref type="figure">6(a)</ref>), rows in the attribute matrix are compact, and each RippleSet is placed to minimize the overall L1 distance between the RippleSets and the corresponding root rows. The compact mode provides space efficiency and can offer an overview of a relatively large number of itemsets. A "focus+context" technique is employed in the compact mode. The Table <ref type="table">1</ref>. Tasks in the user study.</p><p>Task Goals Questions T1 G1,G5 Compare the degrees of discrimination in the following settings of resolving attributes. T2 G2,G3 Which item will face the largest discrimination? T3 G2,G4 Which itemset has a varying degree of discrimination? T4 G3,G5 Which item will be discriminated against by both model m1 and model m2?</p><p>Table <ref type="table">2</ref>. Comparing different visualizations to choose a proper baseline.</p><p>G1 G2 G3 G4 G5 Euler implicit BubbleSets implicit ParallelSet <ref type="bibr" target="#b34">[36]</ref> only categorical attributes UpSet <ref type="bibr" target="#b37">[39]</ref> only binary attributes only between itemsets DiscriLens focused RippleSet will be horizontally aligned with the corresponding rows (Fig. <ref type="figure">6(D)</ref>). In the parallel mode (Fig. <ref type="figure">6(b</ref>)), RippleSets are placed vertically and each root row is horizontally aligned with the corresponding RippleSet. The parallel mode provides clear alignment and thus can guide users on the detailed exploration of a relatively small number of itemsets. The compact mode and the parallel mode help users smoothly switch between the overview and the details they are interested in. In the comparison mode (Fig. <ref type="figure">6(c</ref>)), users can conduct a side-by-side comparison of the discriminatory itemsets of two machine learning models (G5). As pointed out by Marx et al. <ref type="bibr" target="#b43">[45]</ref>, real-world datasets can admit ML models that have equal accuracy but assign conflicting predictions to the same input data. To help users identify these conflicting predictions and compare the fairness of two models, two RippleSets and their corresponding root rows are horizontally aligned if they have the same resolving attribute values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">LABORATORY STUDY</head><p>We conducted a laboratory study to assess participants' ability to use DiscriLens to analyze machine learning discrimination. Even though it is also important to separately evaluate RippleSet, we evaluate Dis-criLens as a whole due to our focus on discrimination analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Experimental Settings</head><p>We recruited 16 participants (12 males, 4 females, age 21-30) through university mailing lists. All participants are postgraduate students in the school of engineering and have experiences in machine learning.</p><p>A within-subject design was employed for the user study. We designed four tasks to assess participants' ability to conduct discrimination analysis (G1-G5), as shown in Table <ref type="table">1</ref>. Each task contains 2-4 different questions in the same format. We trained models and mined discriminatory itemsets for two datasets: an adult income dataset <ref type="bibr" target="#b16">[18]</ref> with a small number of discriminatory itemsets and a student performance dataset <ref type="bibr" target="#b5">[6]</ref> with a large number of discriminatory itemsets. For the user study baseline, we first considered existing fairness visualization tools, such as FairVis <ref type="bibr" target="#b8">[9]</ref>, FairSight <ref type="bibr" target="#b0">[1]</ref>, FairLearn <ref type="bibr" target="#b36">[38]</ref>, and TensorFlow Fairness Indicator <ref type="bibr" target="#b56">[59]</ref>. However, it is unfair to compare these tools with DiscriLens since they focus on different aspects and have different design goals, as discussed in Sect. 2.2. On the contrary,  we compare DiscriLens with other visualizations that can illustrate set intersections based on the survey results in <ref type="bibr" target="#b30">[32]</ref>. The result is shown in Table <ref type="table">2</ref>. DiscriLens, as a combination of Euler-based and Matrix-based set visualization, supports a more detailed analysis of discrimination. Since no existing set visualization can meet all the requirements of discrimination analysis, we used Euler diagram + table (with search, sort, and filter functions) as the baseline method (Fig. <ref type="figure" target="#fig_6">9</ref>).</p><p>For each participant, two datasets were randomly associated with the two conditions (DiscriLens and baseline) and were presented in a counter-balanced order. Before the formal study, each participant received a 20-minutes tutorial to learn the tool, complete the trial tasks, and freely ask questions. In each condition, participants completed the four tasks. Participants are randomly ordered in this user study. Finally, each participant completed a post-study questionnaire and received a short informal interview.</p><p>We set a threshold at 0.05 for p value and hypothesized that, compared with the baseline, H1) DiscriLens is harder to understand; H2) DiscriLens doesn't decrease the accuracy of tasks; H3) DiscriLens decreases the completion time of tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Results</head><p>We summarized the results of the performed tasks in Fig. <ref type="figure" target="#fig_7">10</ref> and the results of the post-study questionnaire in Fig. <ref type="figure" target="#fig_8">11</ref>.</p><p>Contrary to our expectation, participants thought DiscriLens was easier to understand than the baseline (Fig. <ref type="figure" target="#fig_8">11</ref>), rejecting H1. Although most (10/16) participants expressed that "the tool seems complicated at first glance", they stated that "the visual encodings are actually easy to understand and remember once you explained them" and "the tool offers better support to the analysis than the baseline". This highlighted the importance of designing visualizations suitable for the analysis tasks. Complicated visual designs are sometimes inevitable for the analysis of complicated data. A simple visualization can be hard to understand when it fails to support the analysis.</p><p>As shown in Fig. <ref type="figure" target="#fig_7">10</ref>(a), except T2 in small size (p = 0.09), participant gave significantly more accurate answers (p &lt; .05) when using DiscriLens. This result supports H2. Note that the overlap between accuracy confidence intervals decreased with the increase of data size. This phenomenon might indicate that DiscriLens has a more significant advantage in accuracy when analyzing a large volume of discrimination.</p><p>As shown in Fig. <ref type="figure" target="#fig_7">10(b</ref>), the time cost of DiscriLens is significantly less than that of the baseline (p &lt; .05), which supports H3. Unlike accuracy, the overlap between time confidence intervals increased with the increase of data size. This was because participants tended to give up some tasks when they found these tasks were difficult to complete using the baseline, which led to short completion time.</p><p>Meanwhile, we observed that participants employed different strategies with the two tools. When using the baseline to answer a question, most participants (12/16) switched back and forth between different question options and compared them carefully using the baseline tool. When using DiscriLens, most participants (10/16) first read the question, then examined the visualization to conclude an answer, and finally chose an option directly in the questionnaire.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">USE CASES</head><p>In addition to the laboratory study, we further demonstrated the effectiveness of DiscriLens in analyzing algorithmic discrimination through use cases. The cases were conducted in collaboration with two machine learning experts (E1 and E2) and one domain experts: a professor with more than ten years of teaching experience (E3).</p><p>We mainly used the xAPI dataset <ref type="bibr" target="#b5">[6]</ref> for demonstration and more use cases are available in the supplementary material. Each data point in the xAPI datasets has 9 attributes (e.g., raised hands, absence days) of a student and a binary label indicating whether the test score of this student is over 69. We set gender=female as the protected group and τ = 0.25. Six different types of ML models were trained: XGBoost, k-nearest neighbor (KNN), logistic regression (LR), support vector machine (SVM), random forest (RF), and decision tree (DT). The hyperparameters of all six models have been tuned to maximize the 5-fold cross-validation accuracy using AutoML <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b59">63]</ref>. More implementation details are available in the supplementary material. Identify Discrimination. E3 conducted analysis on the xAPI dataset and set the resolving attributes at first. Based on his domain knowledge and the observation of item distribution on different attributes (Fig. <ref type="figure">6</ref>(A1)), E3 was satisfied with the resolving attributes suggested by the discovery module set [announcements view,raised hands,absence days, relationship] and did not modify them. He then analyzed the XGBoost model, which had the highest 5-fold cross-validation accuracy.</p><p>In the training data, female and male students had a different proportion of high scores. However, E3 observed no discriminatory itemsets in the scatter plot. He deduced this phenomenon can be explained by the correlation between gender and resolving attributes. For example, female students might have lower values of absence days. He then verified his conjecture by dragging absence days and relationship from the resolving attributes. Discriminatory itemsets then appeared in the scatter plot, which confirmed his conjecture.</p><p>E3 then used the trained models to predict unforeseen samples. As shown in Fig. <ref type="figure">6</ref>, a number of discriminatory itemsets were identified and presented, indicating the model discrimination appeared in unforeseen samples. E3 also found that the majority of the discriminatory itemsets were orange (Fig. <ref type="figure" target="#fig_10">12(c</ref>)), which represented discrimination towards the non-protected group (i.e., male students).</p><p>Moreover, E2 conducted the same analysis on the Adult dataset <ref type="bibr" target="#b16">[18]</ref>. The Adult dataset contains 45,222 data items. The task of this dataset is to predict whether the annual income of a person is over 50k USD. Interestingly, for models trained on the Adult dataset, discrimination did not increase in test data. One possible explanation of this phenomenon is that models trained on large datasets are more stable and make more consistent predictions. Another possible explanation is that the test data and training data had a very similar distribution of the Adult dataset, while the opposite goes for xAPI dataset. Understand Discrimination among Itemsets. E2 was interested in the XGBoost result, which had the highest 5-fold cross validation accuracy. He examined the discrimination in different itemsets at XGBoost. In the attribute matrix, the solid parts in rectangles represented the condition of discrimination. E2 found that all discriminatory items had the condition absence days:&gt;7 (Fig. <ref type="figure" target="#fig_10">12A</ref>). In other words, if a student was absent less than 7 days, this student was less likely to be treated   differently based on gender. This observation can be easily explained by the histogram of absence day, which indicated that almost all students whose absence day&lt;7 were predicted as the high-score class. E2 then compared the RippleSets of different itemsets. As shown in Fig. <ref type="figure" target="#fig_10">12B</ref>, among all RippleSets, the RippleSet of [absence days:&gt;7, announcements view:20-66, raised hands:23-50, relationship=father] had the highest color saturation, indicating the largest value of RD. Meanwhile, this RippleSet consisted of three clusters of dots with different saturation, indicating that the RD value varies. For a detailed examination, E2 clicked on this RippleSet and expanded the corresponding root row. As shown in Fig. <ref type="figure" target="#fig_10">12</ref>(C), the row with higher saturation indicated that, inside the itemset [absence days:&gt;7, announcements view:20-66,raised hands: 23-50, relationship:father], male students whose discussion&gt;70 were more likely to be treated unequally.</p><p>These observations reveal when the model predictions are potentially discriminatory and should be checked and modified by human experts or discrimination-removal methods. Compare Discrimination among Models. Instead of directly applying XGBoost, the model with the highest accuracy, E1 compared this model with other models from a fairness perspective. According to the scatter plots (Fig. <ref type="figure" target="#fig_10">12(a)</ref>), DT and KNN had itemsets with higher values of risk difference (i.e., points with larger y-position) than XGBoost. Therefore, these two models were first excluded.</p><p>E1 then compared XGBoost with RF, the model with the second highest accuracy. Overall, RF had more discriminatory itemsets than XGBoost. An interesting finding was that four of the five discriminatory itemsets in XGBoost also existed in RF: coincident points in the scatter plot corresponded to itemsets with the same size and the same degree of discrimination (Fig. <ref type="figure" target="#fig_10">12(b</ref>)); aligned rows in attribute matrix indicated the same condition of discrimination (Fig. <ref type="figure" target="#fig_10">12(c</ref>)). For these four discriminatory itemsets, the only difference was in the itemset [absence days:&gt;7, announcements view:20-66, raised hands:10-23, relationship:mom]], where the RippleSet of RF has more hollow items than XGBoost (Fig. <ref type="figure" target="#fig_10">12(D)</ref>). In other words, XGBoost was more likely to predict students in this itemset as the lowscore class. Considering that most discriminatory itemsets in XGBoost also existed in RF and that RF had more discriminatory itemsets, E1 concluded that XGBoost was a better choice than RF even from the fairness perspective.</p><p>E1 also compared XGBoost with LR and SVM. He found these models had discrimination towards different itemsets, as most of their RippleSets were not aligned. Therefore, comparing the discrimination of these models depended on which groups of students were more important for the model user. E1 commented, "Without visualization, all these differences might just be obscured in a summary statistic." Remove Discrimination. E1 adopted Reject Option <ref type="bibr" target="#b27">[30]</ref>, a popular discrimination-removal algorithm 1 , to remove discrimination in RF, a 1 The implementation by IBM Fairness 360 [27] is used here.  model with relatively more complex discriminatory itemsets. One solution is to apply Reject Option to all discriminatory itemsets whose RD &gt; τ = 0.25. However, like most discrimination-removal methods <ref type="bibr" target="#b35">[37,</ref><ref type="bibr" target="#b55">58]</ref>, this operation led to unexpected reverse discrimination (i.e., towards female students) and a decrease in accuracy. One possible solution to this problem is to increase τ and apply Reject Option to discrimination with higher RD values.</p><p>However, in practice, severe discrimination is not only those with high RD values but also those towards critical groups. For example, E3 was concerned with the discrimination happened to hard-working students (e.g., low absenteeism, active discussion). Therefore, E1 employed DiscriLens to identify severe discrimination. Discriminatory itemsets with high RD can be easily identified through their colors (Fig. <ref type="figure" target="#fig_11">13(A)</ref>). E1 then checked the discrimination towards critical groups. He first examined the students with low absenteeism and selected the itemset whose absent days&lt;7 (Fig. <ref type="figure" target="#fig_11">13(B)</ref>). He did not open the row for further examination since the corresponding RippeleSet, with only one cluster of items, indicated there was no complex intersection. E1 then checked the students with active class participation by examining the values of raised hands. Among the three RippleSets with high values (&gt; 50) of raised hands, two had high values of RD and only one (Fig. <ref type="figure" target="#fig_11">13(C</ref>)) was left to be examined. E1 expanded the rows of this RippleSet and selected an itemset (Fig. <ref type="figure">6</ref>(F)) that also had a high value (&gt; 70) of discussion.</p><p>Reject Option was then applied to the user-defined discriminatory region. For convenience, we denoted RF using default discriminatory region by RF + , and RF using the user-defined discriminatory region by RF +U . As shown in Fig. <ref type="figure" target="#fig_11">13</ref>(b), compared with RF + , RF +U reduced both the accuracy loss and the reverse discrimination . By providing more detailed information, DiscriLens enables users a precise removal of discrimination. Expert Interview All the experts expressed great enthusiasm for Dis-criLens and offered useful suggestions for its improvement. They commented that it helped them "understand ML models from a novel perspective" (E3) and could "contribute to the adoption of ML to realworld applications" (E1). E1 suggested combining the analysis of model accuracy with the analysis of discrimination. E2 was curious about the comparison between algorithmic discrimination and human discrimination, "Even though a model makes discriminatory predictions, it can still be less biased than humans." E3 suggested adding a "what-if" function to check whether and how a model would discriminate against a given data item.</p><p>The issue of trust was mentioned and discussed by the experts. E3 suspected that the training data might omit some attributes related to the students performance. He also commented that the results of discrimination analysis were influenced by the choices of the analysts (e.g., the setting of key attributes). "Two analysts might draw opposite conclusions." E1, the machine learning expert, suggested the comparison between algorithmic discrimination and human discrimination to increase user confidence. "Even though a model makes discriminatory predictions, it can still be less biased than humans and thus be helpful."</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">DISCUSSION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.1">Scalability</head><p>For the discrimination discovery, it takes about six minutes to run the four-stage pipeline on 4,000 samples with 14 features on a PC (2.3GHz dual-core, Intel Core i5 processor). The major bottlenecks lie in the FEGS algorithm (two minutes), the FP-Growth algorithm (one minute), and the discriminatory rules mining (three minutes).</p><p>The scalability of Rippleset is mainly limited by the number of sets and the number of items. According to our user study, Rippleset effectively reduces the visual clutter in traditional Euler diagrams, which has difficulties to handle more than three sets. However, the readability of RippleSet hasn't been validated with more than seven intertwining sets. For a large number of items, RippleSet supports the aggregation of items, as shown in Fig. <ref type="figure">6</ref>(D1). In the future, we plan to support more grouping strategies, such as those discussed in Squares <ref type="bibr" target="#b51">[54]</ref>, and provide more statistical summaries in the aggregation view of RippleSet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.2">Learning Curve</head><p>We acknowledge that the novel and complicated visualization designs in DiscriLens can pose challenges to users, especially those with no prior knowledge in visual analytics. Even though we have provided interactive tutorials and legends to alleviate this issue, the steep learning curve still exists and can lead to low accessibility to novices. For example, RippleSet and the attribute matrix can be overwhelming when there are too many itemsets. Users may stop at the scatter plot, blindly trying to optimize RD without conducting detailed visual analysis of discrimination. On the other hand, complicated visual designs are sometimes inevitable for the analysis of complicated data <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b58">62]</ref>. Even with easy-to-understand encodings, a simple visualization can be hard to use when it fails to support the required analysis. How to strike a good balance between designing intuitive visualizations and accomplishing complex analysis tasks is still an open question in the field of visual analytics and requires further investigation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.3">Evaluation of RippleSet</head><p>In this paper, we proposed a novel set visualization, RippleSet, to assist the analysis of discrimination. RippleSet supplements the attribute matrix by illustrating the discrimination distribution among data items, especially these belong to intricately intertwined sets. While the current evaluation shows the utility of DiscriLens as a whole, it fails to demonstrate the contribution of RippleSet to the effectiveness of DiscriLens. Apart from discrimination analysis, RippleSet also shows the potential to stand alone and be applied for more general tasks. Unfortunately, due to page limits and our focus on discrimination analysis, we are unable to provide a stand-alone evaluation of RippleSet in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.4">Subjectivity in Analysis</head><p>In DiscriLens, we allow users to customize the definition of discrimination and support the integration of human domain knowledge. While this feature is regarded as a strength by the interviewed experts, we also admit that user customization can act as a double-edged sword in discrimination analysis.</p><p>On the one hand, user customization enables the integration of domain knowledge. Since the definition of discrimination differs across domains, the involvement of domain knowledge enables a more comprehensive analysis. On the other hand, additional bias can be potentially imposed by the subjective choices of the analysts. Different customization may lead to different analysis results. One possible solution is to support what-if analysis and cross-validate the analysis results of different customization. Analysts can test and compare different customization before drawing the final conclusion. Similar methods are commonly used in verifying subjective analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.5">More Intelligent Discrimination Mining</head><p>In this work, we do not consider the hidden attributes in the causal graph(i.e., attributes that cannot be explicitly observed). We assume the key attributes can capture the main model decisions and set a threshold of the risk difference to allow oscillations caused by hidden attributes. However, the protected or resolving attributes may not be included in the dataset, and this assumption sometimes can fail. In future work, we plan to take the hidden attributes into consideration and offer more comprehensive explanations of the model predictions.</p><p>Meanwhile, the current version of DiscriLens only supports the analysis of one protected attribute and requires users to define the protected group as input. When there are many groups (e.g., females, blacks, LGBTs) qualified for special protection by a law, users cannot analyze multiple protected attributes at the same time and need to examine different attributes separately one by one.</p><p>Compared with other explainable models (e.g., decision tree), our study provides limited support in explaining the identified discrimination. Therefore, an interesting future direction is to integrate discrimination analysis with model interpretation and diagnosis techniques, which can help us track the origin of algorithmic discrimination and better understand model behaviors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10">CONCLUSION</head><p>In this work, we designed and developed DiscriLens, an interactive visualization tool that facilitated a better understanding and analysis of algorithmic discrimination. A four-stage pipeline was developed for the discovery of discriminatory predictions. For an effective presentation, a novel set visualization was designed by combining an extended Euler diagram with a matrix-based set visualization. Two case studies demonstrated the usability and utility of DiscriLens in understanding and removing algorithmic discrimination. Context-aware Reject Option, a post-processing method, was proposed to better remove discrimination while reducing the accuracy loss. We also reported insights into algorithmic discrimination that were obtained during the development and evaluation of DiscriLens.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. In a toy example of college admission, different types of relationship exist between the protected attribute race and the result admission. (a) race doesn't influence admission. (b) race directly influences admission. (c) race indirectly influences admission through other attributes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. DiscriLens consists of two main modules: a discovery module (a) and a visualization module (b).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. (a) is an example of classification rules. The confidence difference between rule (b) and (c) might indicate a discrimination towards females and requires further examination.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 .Fig. 7 .</head><label>67</label><figDesc>Fig. 6. The visual interface consists of three key components: the attribute matrix (A), the RippleSet (B), and the scatter plot(C). Three modes of analysis are supported: the compact mode (a), the parallel mode (b), and the comparison mode (c).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Alternative designs of the set visualization: the weighted Voronoibased design (a) contains irregular shapes and makes it difficult to compare the size of subsets; the Euler-based design (b) introduces significant visual clutter. We improved the design in (b) through an iterative design process((c)-(e)).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. The baseline system used in the user study: Euler diagram + table (with search, sort, and filter functions).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. Accuracy and completion time under different conditions. Error bars indicate 95% confidence intervals.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 11 .</head><label>11</label><figDesc>Fig. 11. Comparison of DiscriLens and the baseline based on the poststudy questionnaires.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 12 .</head><label>12</label><figDesc>Fig. 12. Use DiscriLens to analyze discrimination: (a) analyze the discriminatory itemsets of different models using the scatter plots; (b) compare RF with XGBoost in a scatter plot; (c) compare RF with XGBoost in the comparison mode.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 13 .</head><label>13</label><figDesc>Fig. 13. (a) Users can identify discrimination with high values of RD (A) and discrimination towards critical groups (BC). (b) Compare the discriminatory itemsets of the original RF model, the RF model using Reject Option (RF + ), and the RF model using the user-defined Reject Option (RF +U ).</figDesc></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGMENTS</head><p>This research was supported in part by <rs type="programName">Hong Kong Theme-based Research Scheme</rs> grant <rs type="grantNumber">T41-709/17N</rs> and also a grant from <rs type="funder">MSRA</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_M5Y6grz">
					<idno type="grant-number">T41-709/17N</idno>
					<orgName type="program" subtype="full">Hong Kong Theme-based Research Scheme</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Fairsight: Visual analytics for fairness in decision making</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-R</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.00176</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Design study of linesets, a novel set visualization technique</title>
		<author>
			<persName><forename type="first">B</forename><surname>Alper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Riche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Czerwinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2259" to="2267" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Radial sets: Interactive visual analysis of large overlapping sets</title>
		<author>
			<persName><forename type="first">B</forename><surname>Alsallakh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Aigner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Miksch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hauser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2496" to="2505" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Visualizing sets and set-typed data: State-of-the-art and future challenges</title>
		<author>
			<persName><forename type="first">B</forename><surname>Alsallakh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Micallef</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Aigner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hauser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Miksch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Rodgers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eurographics Conference on Visualization, EuroVis</title>
		<meeting><address><addrLine>Swansea, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Eurographics Association</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Powerset: A comprehensive visualization of set intersections</title>
		<author>
			<persName><forename type="first">B</forename><surname>Alsallakh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="361" to="370" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Mining educational data to predict student&apos;s academic performance using ensemble methods</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">A</forename><surname>Amrieh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hamtini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Aljarah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Database Theory and Application</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="119" to="136" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Random search for hyper-parameter optimization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="281" to="305" />
			<date type="published" when="2012-02">Feb. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Attacking discrimination in ML</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">Big</forename><surname>Picture</surname></persName>
		</author>
		<ptr target="http://research.google.com/bigpicture/attacking-discrimination-in-ml/" />
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2019" to="2025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">Á</forename><forename type="middle">A</forename><surname>Cabrera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Epperson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hohman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kahng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Morgenstern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Chau</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.05419</idno>
		<title level="m">Fairvis: Visual analytics for discovering intersectional bias in machine learning</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Oodanalyzer: Interactive analysis of out-of-distribution samples</title>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2020.2973258</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Handbook of data visualization</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">K</forename><surname>C.-H. Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Härdle</surname></persName>
		</author>
		<author>
			<persName><surname>Unwin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Optimal structure identification with greedy search</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Chickering</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="507" to="554" />
			<date type="published" when="2002-11">Nov. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Bubble sets: Revealing set relations with isocontours over existing visualizations</title>
		<author>
			<persName><forename type="first">C</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Penn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Carpendale</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1009" to="1016" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Algorithmic decision making and the cost of fairness</title>
		<author>
			<persName><forename type="first">S</forename><surname>Corbett-Davies</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Pierson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Feller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Huq</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining<address><addrLine>Halifax, NS, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="797" to="806" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning perceptual kernels for visualization design</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">¸</forename><surname>Demiralp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Heer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1933" to="1942" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Kelp diagrams: Point set membership visualization</title>
		<author>
			<persName><forename type="first">K</forename><surname>Dinkla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Van Kreveld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Speckmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Westenberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="875" to="884" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note>pt1</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">UCI machine learning repository</title>
		<author>
			<persName><forename type="first">D</forename><surname>Dua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Graff</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Fairness through awareness</title>
		<author>
			<persName><forename type="first">C</forename><surname>Dwork</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pitassi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Reingold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd Innovations in Theoretical Computer Science Conference, ITCS &apos;12</title>
		<meeting>the 3rd Innovations in Theoretical Computer Science Conference, ITCS &apos;12<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="214" to="226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Multi-interval discretization of continuous-valued attributes for classification learning</title>
		<author>
			<persName><forename type="first">U</forename><surname>Fayyad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">13th International Joint Conference on Artificial Intelligence</title>
		<meeting><address><addrLine>Chambéry, France</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1993">1993</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1022" to="1027" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Interactive visual analysis of settyped data</title>
		<author>
			<persName><forename type="first">W</forename><surname>Freiler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Matkovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hauser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1340" to="1347" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Predicting crime using twitter and kernel density estimation</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Gerber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Decision Support Systems</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="115" to="125" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Algorithmic bias: From discrimination discovery to fairness-aware data mining</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hajian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bonchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Castillo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining<address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2125" to="2126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Mining frequent patterns by pattern-growth: methodology and implications</title>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGKDD Explorations Newsletter</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="14" to="20" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Equality of opportunity in supervised learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Srebro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3315" to="3323" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Improving fairness in machine learning systems: What do industry practitioners need?</title>
		<author>
			<persName><forename type="first">K</forename><surname>Holstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Wortman</forename><surname>Vaughan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iii</forename></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dudik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wallach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems</title>
		<meeting>the 2019 CHI Conference on Human Factors in Computing Systems<address><addrLine>Glasgow, UK</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page">600</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H G</forename><surname>Ivan Koswara</surname></persName>
		</author>
		<ptr target="http://https://brilliant.org/wiki/simpsons-paradox/" />
		<title level="m">Ananya Aaniya. Simpson&apos;s paradox</title>
		<imprint>
			<biblScope unit="page" from="2019" to="2031" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Data preprocessing techniques for classification without discrimination</title>
		<author>
			<persName><forename type="first">F</forename><surname>Kamiran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Calders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowledge and Information Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="33" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Decision theory for discriminationaware classification</title>
		<author>
			<persName><forename type="first">F</forename><surname>Kamiran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Karim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 12th International Conference on Data Mining</title>
		<meeting><address><addrLine>Brussels</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="924" to="929" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Fairness-aware classifier with prejudice remover regularizer</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kamishima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Akaho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Asoh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sakuma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint European Conference on Machine Learning and Knowledge Discovery in Databases</title>
		<imprint>
			<biblScope unit="page" from="35" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title/>
		<author>
			<persName><surname>Springer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<pubPlace>Bristol, UK</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Visual techniques for analyzing set-typed data</title>
		<author>
			<persName><surname>Keshif</surname></persName>
		</author>
		<ptr target="https://gallery.keshif.me/setvis" />
		<imprint>
			<date type="published" when="2010">2010. 2019-07-30</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Consumer credit-risk models via machine-learning algorithms</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Khandani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Lo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Banking and Finance</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2767" to="2787" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Avoiding discrimination through causal reasoning</title>
		<author>
			<persName><forename type="first">N</forename><surname>Kilbertus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rojas-Carulla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Parascandolo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Janzing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Neural Information Processing Systems</title>
		<meeting>the 31st International Conference on Neural Information Processing Systems<address><addrLine>Long Beach, California, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="656" to="666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Visualizing set concordance with permutation matrices and fan diagrams</title>
		<author>
			<persName><forename type="first">B</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Seo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Interacting with Computers</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">5-6</biblScope>
			<biblScope unit="page" from="630" to="643" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Parallel sets: Interactive exploration and visual analysis of categorical data</title>
		<author>
			<persName><forename type="first">R</forename><surname>Kosara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bendix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hauser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="558" to="568" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Counterfactual fairness</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Kusner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Loftus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Silva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="page" from="4066" to="4076" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">F</forename><surname>Learn</surname></persName>
		</author>
		<ptr target="https://github.com/fairlearn/fairlearn" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Upset: visualization of intersecting sets</title>
		<author>
			<persName><forename type="first">A</forename><surname>Lex</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Gehlenborg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Strobelt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Vuillemot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Pfister</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1983" to="1992" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Evaluation of symbol contrast in scatterplots</title>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Van Wijk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-B</forename><surname>Martens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE Pacific Visualization Symposium</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="97" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Form symbolism, analogy, and metaphor</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychonomic Bulletin &amp; Review</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="546" to="551" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Towards better analysis of deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="91" to="100" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">k-nn as an implementation of situation testing for discrimination discovery and prevention</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ruggieri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Turini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 17th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining<address><addrLine>San Diego, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="502" to="510" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Combating discrimination using bayesian networks</title>
		<author>
			<persName><forename type="first">K</forename><surname>Mancuhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Clifton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial intelligence and law</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="211" to="238" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">T</forename><surname>Marx</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">D P</forename><surname>Calmon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ustun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.06677</idno>
		<title level="m">Predictive multiplicity in classification</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Rulematrix: visualizing and understanding classifiers with rules</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ming</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Bertini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on visualization and computer graphics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="342" to="352" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Subjective vs. objective discrimination in government: Adding to the picture of barriers to the advancement of women</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">C</forename><surname>Naff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Political Research Quarterly</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="535" to="557" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<ptr target="https://www.legislation.gov.uk/ukpga/1975/65" />
		<title level="m">Sex Discrimination Act</title>
		<imprint>
			<date type="published" when="1975">1975. 1975</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Measuring discrimination in socially-sensitive decision records</title>
		<author>
			<persName><forename type="first">D</forename><surname>Pedreschi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ruggieri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Turini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2009 SIAM International Conference on Data Mining</title>
		<meeting>the 2009 SIAM International Conference on Data Mining<address><addrLine>SIAM, Nevada, US</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="581" to="592" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">A study of top-k measures for discrimination discovery</title>
		<author>
			<persName><forename type="first">D</forename><surname>Pedreschi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ruggieri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Turini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th Annual ACM Symposium on Applied Computing</title>
		<meeting>the 27th Annual ACM Symposium on Applied Computing<address><addrLine>Riva, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="126" to="131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Discrimination-aware data mining</title>
		<author>
			<persName><forename type="first">D</forename><surname>Pedreshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ruggieri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Turini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 14th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining<address><addrLine>Las Vegas, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="560" to="568" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">A comparative analysis of classification algorithms for students college enrollment approval using data mining</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H M</forename><surname>Ragab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Noaman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Al-Ghamdi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">I</forename><surname>Madbouly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Workshop on Interaction Design in Educational Environments</title>
		<meeting>the 2014 Workshop on Interaction Design in Educational Environments<address><addrLine>Albacete, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page">106</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Squares: Supporting interactive performance analysis for multiclass classifiers</title>
		<author>
			<persName><forename type="first">D</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Amershi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Suh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="61" to="70" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Untangling euler diagrams</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">H</forename><surname>Riche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Dwyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1090" to="1099" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">A multidisciplinary survey on discrimination analysis</title>
		<author>
			<persName><forename type="first">A</forename><surname>Romei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ruggieri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Knowledge Engineering Review</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="582" to="638" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Fully automatic visualisation of overlapping sets</title>
		<author>
			<persName><forename type="first">P</forename><surname>Simonetto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Auber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Archambault</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="967" to="974" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">A unified approach to quantifying algorithmic unfairness: Measuring individual and group unfairness via inequality indices</title>
		<author>
			<persName><forename type="first">T</forename><surname>Speicher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Heidari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Grgic-Hlaca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">P</forename><surname>Gummadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Singla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Weller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">B</forename><surname>Zafar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining<address><addrLine>London, UK</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2239" to="2248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title/>
		<ptr target="https://github.com/tensorflow/fairness-indicators" />
	</analytic>
	<monogr>
		<title level="j">TensorFlow. Fairness indicators</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Civil Rights Act of</title>
		<ptr target="https://www.eeoc.gov/eeoc/history/35th/1990s/civilrights.html" />
		<imprint>
			<date type="published" when="1991">1991. 1991</date>
			<publisher>U.S. Federal Legislation</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Narvis: Authoring narrative slideshows for introducing data visualization designs</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Qu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on visualization and computer graphics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="779" to="788" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Atmseer: Increasing transparency and controllability in automated machine learning</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ming</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Veeramacha-Neni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Qu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Human Factors in Computing Systems</title>
		<meeting>the 2019 Conference on Human Factors in Computing Systems<address><addrLine>Glasgow, Scotland, UK</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page">681</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Visualization of large hierarchical data by circle packing</title>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SIGCHI Conference on Human Factors in Computing Systems</title>
		<meeting>the SIGCHI Conference on Human Factors in Computing Systems<address><addrLine>Montréal, Québec, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="517" to="520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Opinionseer: interactive visualization of hotel customer feedback</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Au</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Qu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1109" to="1118" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Cpar: Classification based on predictive association rules</title>
		<author>
			<persName><forename type="first">X</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2003 SIAM International Conference on Data Mining</title>
		<meeting>the 2003 SIAM International Conference on Data Mining<address><addrLine>SIAM, San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="331" to="335" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">A survey of visual analytics techniques for machine learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Visual Media</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="31" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">B</forename><surname>Zafar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Valera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">P</forename><surname>Gummadi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1507.05259</idno>
		<title level="m">Fairness constraints: Mechanisms for fair classification</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Achieving non-discrimination in data release</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining<address><addrLine>Halifax, NS, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1335" to="1344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">A causal framework for discovering and removing direct and indirect discrimination</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Joint Conference on Artificial Intelligence</title>
		<meeting>the 26th International Joint Conference on Artificial Intelligence<address><addrLine>San Francisco, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3929" to="3935" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Achieving non-discrimination in prediction</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence, IJCAI</title>
		<meeting>the Twenty-Seventh International Joint Conference on Artificial Intelligence, IJCAI<address><addrLine>Stockholm,Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3097" to="3103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Examining CNN representations with respect to dataset bias</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-Second Conference on Artificial Intelligence</title>
		<meeting>the Thirty-Second Conference on Artificial Intelligence<address><addrLine>New Orleans, Louisiana, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI press</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4464" to="4473" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Handling conditional discrimination</title>
		<author>
			<persName><forename type="first">I</forename><surname>Žliobaite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Kamiran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Calders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">11th International Conference on Data Mining (ICDM)</title>
		<meeting><address><addrLine>Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="992" to="1001" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
