<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Facial EMG Responses to Emotional Expressions Are Related to Emotion Perception Ability</title>
				<funder ref="#_68MFKNX">
					<orgName type="full">German Research Foundation</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2014-01-28">January 28, 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Janina</forename><surname>Ku ¨necke</surname></persName>
							<email>janina.kuenecke@hu-berlin.de</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Psychology</orgName>
								<orgName type="institution">Humboldt Universita ¨t zu Berlin</orgName>
								<address>
									<settlement>Berlin</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Andrea</forename><surname>Hildebrandt</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Psychology</orgName>
								<orgName type="institution">Humboldt Universita ¨t zu Berlin</orgName>
								<address>
									<settlement>Berlin</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Guillermo</forename><surname>Recio</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Psychology</orgName>
								<orgName type="institution">Humboldt Universita ¨t zu Berlin</orgName>
								<address>
									<settlement>Berlin</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Psychology</orgName>
								<orgName type="institution">University Ulm</orgName>
								<address>
									<settlement>Ulm</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Werner</forename><surname>Sommer</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Psychology</orgName>
								<orgName type="institution">Humboldt Universita ¨t zu Berlin</orgName>
								<address>
									<settlement>Berlin</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Oliver</forename><surname>Wilhelm</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Psychology</orgName>
								<orgName type="institution">University Ulm</orgName>
								<address>
									<settlement>Ulm</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">The University of Queensland</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Facial EMG Responses to Emotional Expressions Are Related to Emotion Perception Ability</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2014-01-28">January 28, 2014</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1371/journal.pone.0084053</idno>
					<note type="submission">Received August 20, 2013; Accepted November 11, 2013;</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-21T19:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Although most people can identify facial expressions of emotions well, they still differ in this ability. According to embodied simulation theories understanding emotions of others is fostered by involuntarily mimicking the perceived expressions, causing a ''reactivation'' of the corresponding mental state. Some studies suggest automatic facial mimicry during expression viewing; however, findings on the relationship between mimicry and emotion perception abilities are equivocal. The present study investigated individual differences in emotion perception and its relationship to facial muscle responsesrecorded with electromyogram (EMG) -in response to emotional facial expressions. Nu = u269 participants completed multiple tasks measuring face and emotion perception. EMG recordings were taken from a subsample (Nu = u110) in an independent emotion classification task of short videos displaying six emotions. Confirmatory factor analyses of the m. corrugator supercilii in response to angry, happy, sad, and neutral expressions showed that individual differences in corrugator activity can be separated into a general response to all faces and an emotion-related response. Structural equation modeling revealed a substantial relationship between the emotion-related response and emotion perception ability, providing evidence for the role of facial muscle activation in emotion perception from an individual differences perspective.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>The identification and production of facial expressions of emotion are important interpersonal abilities. Although people normally perform well in classifying facial expressions of basic emotions, there are considerable individual differences in the accuracy of their judgments <ref type="bibr" target="#b0">[1]</ref>. Hildebrandt, Sommer, Schacht, and Wilhelm <ref type="bibr" target="#b1">[2]</ref> found that individual differences in the ability of perceiving facial expressions of emotion are partially independent of abilities in face perception and general cognition. Here we use the term ability of ''perceiving'' facial expressions of emotions (or short: emotion perception) instead of emotion recognition which is often used in the literature. The tasks we use to measure this ability include identification (or classification) and visual search. So the ability is captured in a rather broad sense.</p><p>It is well documented that perceiving emotional facial expressions of others elicits corresponding facial responses in the observer, which are incidental and covert -often invisible -but measurable by electromyography (EMG; <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref>). These responsesoften termed facial mimicry -are considered to be automatic as they occur already within 300 ms after stimulus presentation <ref type="bibr" target="#b4">[5]</ref>. Previous studies have shown EMG activity in the corresponding muscles during the perception of facial expressions <ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref>. For example, the m. corrugator supercilii (corr) is important for frowning in expressions like anger or sadness, and m. zygomaticus major (zyg) produces smiling expressions (e.g. <ref type="bibr" target="#b6">[7]</ref>). The occurrence and intensity of mimicry are modulated by stimulus type, task, and context <ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref>. Facial mimicry is more pronounced in response to dynamic than static facial expressions (e.g., <ref type="bibr" target="#b11">[12]</ref>).</p><p>According to embodied simulation theories, involuntarily mimicking emotional expressions of others fosters the understanding of these emotions by simulating of the corresponding mental states in the perceiver <ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref>. Simulation in this sense implies the reactivation of basal motoric, somatosensory, affective and rewardrelated systems representing the meanings of the perceiver's expressions <ref type="bibr" target="#b16">[17]</ref>. Hence, perceiving an emotional facial expression supposedly triggers simulation, that is, the perceiver uses his bodily and neural states elicited by the perceived expression <ref type="bibr" target="#b15">[16]</ref> in order to access the corresponding emotional concept. Covert facial mimicry is attributed to sub-threshold muscular simulation of emotions <ref type="bibr" target="#b13">[14]</ref>.</p><p>If simulation is indeed a fundamental aspect of emotion processing, facial mimicry should be a functional element of emotion-related abilities. Thus, the intensity of facial mimicry should be related to the accuracy of emotion perception (EP). In line with this view, experimental studies have shown that obstructing incidental mimicry leads to performance impairment in EP <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19]</ref>.</p><p>In contrast to these reports Hess and colleagues did not observe a correlation between incidental mimicry and EP <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b19">20]</ref>. A relationship between anger perception and anger mimicry was only present for elderly but not for younger participants <ref type="bibr" target="#b20">[21]</ref>.</p><p>In a recent review Hess and Fischer <ref type="bibr" target="#b21">[22]</ref> concluded that facial mimicry is not necessary for the classification of prototypical emotional displays. In their Emotion Mimicry in Context view they argue that emotional mimicry requires a specific context in which signals are interpreted as emotional intentions. Moreover, empirical evidence of covert facial mimicry in response to simple facial expression stimuli speaks for valence-related facial responses rather than for emotion-specific mimicry <ref type="bibr" target="#b21">[22]</ref>. This is in line with the view that facial responses of corr and zyg reflect affective states in the perceiver. Larsen, Norris, and Cacioppo <ref type="bibr" target="#b22">[23]</ref> reported a substantial relation between facial muscle activity and self-reported valence ratings in response to affective pictures, words, and sounds. These affect-related facial responses seem to be involuntary or incidental <ref type="bibr" target="#b23">[24]</ref>. If incidental facial responses to emotional stimuli reflect an affective state in the perceiver, perception of emotional expressions could lead to a simulation of an affective state which facilitates the access to emotional concepts in the same way as emotion-specific mimicry could.</p><p>In sum, evidence regarding the relation between incidental facial responses to facial expressions of emotions -in terms of emotion-specific or valence-related simulation -and EP is inconclusive. Testing the relationship between individual differences in the amount of facial responses to emotional expressions and EP ability within an individual differences approach is therefore of pivotal interest for embodied simulation theories and may provide new insights on the role of mimicry in EP.</p><p>It is important to separate construct-related variance from method specificity of the behavioral indicators and their measurement error (e.g., <ref type="bibr" target="#b24">[25]</ref>.). This can be accomplished by using a broader variety of tasks to measure a construct of interest. By establishing a latent variable that captures the communality of the behavioral indicators collected with those tasks it is possible to abstract from method specificity and measurement error. Modeling the relationships between latent variables and behavioral indicators also allows deriving estimates of construct reliability and the reliability of its indicators <ref type="bibr" target="#b25">[26]</ref>.In the present study, we aim to provide more conclusive evidence about the nature and strength of the relationship between facial responses to emotional expressions and EP from an individual differences perspective. To this end we used multivariate assessment and modeling of EP, face perception (FP) and -for the first time in the literature -of facial responses to emotional expressions.</p><p>We assessed individual differences in EP and FP with the extensive task battery developed by Herzmann, Danthiir, Schacht, Sommer, and Wilhelm <ref type="bibr" target="#b26">[27]</ref> and Wilhelm, Hildebrandt, Manske, Schacht, and Sommer <ref type="bibr" target="#b27">[28]</ref>. Incidental facial responses were measured with EMG during a separate emotion classification task. Stimuli were video clips of facial expressions of different intensities, approximating the appearance of expressions in social communication (e.g., <ref type="bibr" target="#b28">[29]</ref>). The videos showed six emotions (anger, disgust, fear, happiness, sadness, surprise), and two neutral facial movements <ref type="bibr" target="#b29">[30]</ref>, allowing to compare different emotions and to separate emotion-related from general responses to face stimuli. Importantly, we heeded experimental independence of all measurements, a crucial aspect when applying correlation techniques.</p><p>As a first main aim, we establish a measurement model of incidental covert facial responses to emotional expressions controlling for measurement error in the physiological indicators. The second aim addressed the relationship between EP and facial responses to emotional expressions Regarding our first aim, we expected stronger EMG activation in corr in response to anger and sadness and decreased activation in response to expressions of happiness. Conversely, activation in zyg should be stronger for smiles relative to all other expressions (e.g., <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b30">31]</ref>). Since facial muscles are responsive to all stimuli, effort or cognitive load <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b32">33]</ref>, we also modeled a general face response factor, accounting for the common variance of EMG indicators for both emotional and neutral facial expressions. We predicted that the average EMG activity in emotion-relevant muscles should reliably indicate a latent emotional facial response factor. Such a general emotion-related factor is theoretically plausible; however, personality traits and exposure effects might lead to emotionspecific rank orders of individuals in facial responses to emotional expression. For example, neuroticism or depressiveness might specifically affect facial responses to anger and sadness <ref type="bibr" target="#b33">[34]</ref>. Thus, we postulated emotion-specific factors ordered under the general face response factor.</p><p>Regarding our second aim, and in line with embodied simulation theories we expected a substantial correlation between emotion-related facial responses and EP. The ability to correctly perceive and recognize unfamiliar faces is highly correlated with EP and therefore a highly relevant covariate <ref type="bibr" target="#b1">[2]</ref>. By controlling for FP in EP on the behavioral side and general muscular responses to face stimuli in facial responses to emotional expressions on the physiological level we aimed to rule out alternative, more general explanations for a possible relationship, for instance attention or reactivity to face stimuli per se. Such a multivariate individual differences approach constitutes a new perspective on testing the relationship of facial responses to emotional expressions with EP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Materials and Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ethics Statement</head><p>This study received institutional ethics approval (provided from the committee of ethics of the Department of Psychology, Humboldt-University Berlin). Written informed consent was obtained from all participants included in the study reported here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Design and Sample</head><p>The study consisted of two parts, a psychometric and a psychophysiological one. In the psychometric part 269 participants (52.4% female) completed three FP and three EP tasks. Additionally, they completed measures of object cognition and general cognitive abilities, which are not in the scope of this article and will be reported elsewhere. Mean age was 25.90 years (SDu = u4.41), educational background was heterogeneous (21.1% without, 48.0% with high school degrees, and 30.8% with academic degrees), and all participants reported normal or corrected-to-normal visual acuity; 175 of them volunteered for psychophysiological part as well. Of those volunteers a subsample of 110 participants (45.5% female) was randomly selected. This subsample was representative of the total sample regarding age (Mu = u26.55, SD = 4.82) and educational background (47.3% and 27.3% with high school and academic degrees, respectively; 25.4% without degree). The psychophysiological part consisted of three tasks (a face familiarity test, an emotion classification task, and an explicit emotion expression task) in which we co-registered EEG and EMG. Here, we only report the EMG data recorded during the emotion classification task. The EEG data deal with emotionrelated ERP components while recognizing dynamic facial expressions of emotions and will be reported elsewhere <ref type="bibr" target="#b29">[30]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Stimuli and Procedure</head><p>Psychometric part. FP and EP tasks will be shortly described in the following. All stimuli were gray-scaled, adjusted in skin texture and fitted with standardized head-size into a vertical elliptical frame of 2006300 pixel. We refer to Herzmann, Danthiir, Schacht, Sommer, &amp; Wilhelm <ref type="bibr" target="#b26">[27]</ref> for more details on the task procedures and examples of stimuli for the face cognition tasks. Stimulus examples and schematic representations of the EP tasks are provided in the supporting information (Figures <ref type="figure" target="#fig_1">S1</ref> and<ref type="figure" target="#fig_2">S2</ref>). The composition of the emotion was constructed according to empirical findings that some emotional expressions are better recognized from the upper part of the face, and others from the lower <ref type="bibr" target="#b34">[35]</ref>. There were nine different expression composites: Anger/disgust, anger/happiness, anger/surprise, fear/disgust, fear/happiness, fear/surprise, sadness/disgust, sadness/happiness and sadness/surprise. Composites were presented with a prompt word (''TOP'' or ''BOTTOM'').</p><p>Participants indicated the emotion expressed in the face half indicated by the prompt word by pressing one of six emotionlabeled buttons. Unbiased hit rates <ref type="bibr" target="#b35">[36]</ref> were defined as indicators.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5) Identification of Emotion Expressions of Different Intensity from Upright</head><p>and Inverted Dynamic Face Stimuli. Stimuli were video clips of six dynamic emotional expressions developing within 500 ms. Emotional expressions were morphed to reach three different intensity levels (i.e., mixing neutral and emotional expression to different degrees: 20% neutral and 80% emotional, 40% neutral and 60% emotional, 80% neutral and 20% emotional); faces were presented either upright or inverted (50% of the trials each). The frame displaying the full emotional expression remained on the screen until participants made a classification response. We used unbiased hit rates <ref type="bibr" target="#b35">[36]</ref>  The order of these six tasks during the psychometric part was the same for all participants (1, 3, 4, 5, 2, and 6). The FP and EP tasks were interspersed between other tasks measuring object cognition and general cognitive abilities.</p><p>Psychophysiological part. Stimuli were color videos of six facial expressions (anger, disgust, fear, happiness, sadness, surprise) displayed at two intensity levels (100%, 80%), and two types of neutral movements (blinking, chewing). All stimuli were taken from the Radboud Faces Database <ref type="bibr" target="#b36">[37]</ref> and were morphed with FantaMorphß software <ref type="bibr" target="#b37">[38]</ref>. The video frames (30 fps) increased linearly in expression intensity from neutral to either 100% or 80% expression within 200 ms, hence, in a sequence of six frames. The last frame with the most intense emotional expression was displayed for another 400 ms. All faces were 8612 cm in size and framed by a gray oval (see Fig. <ref type="figure" target="#fig_1">1</ref>).</p><p>Participants sat in an electrically shielded cabin with dimmed lighting. Viewing distance to a 17 inch LCD computer-monitor (75 Hz refresh rate, 128061024 pixels) was 80 cm. Trials started with a fixation cross presented for 700 ms, followed by a (video) face stimulus displayed for 600 ms, on a background of the same light gray color as the oval frame. In each trial participants categorized the expression of the face stimulus by mouse-clicking one of seven boxes labeled with the German words for the six basic emotions and ''neutral''. The spatial arrangement of the labels was kept constant across trials and participants. The scale was present until a response occurred. After the response a blank screen of 500 ms was shown, followed by the next trial. The reader may have noticed that the presentation time of 600 ms and the interstimulus-interval employed here were shorter than in other EMG studies. This was done in order to optimize the study design for coregistration with EEG which requires more repetitions trials. Recio, Schacht, and Sommer <ref type="bibr" target="#b38">[39]</ref> showed that ISI variation while co-registration of SCR (a comparable slow peripheral signal) and EEG did not affect the effects in general.</p><p>The set of trials included portraits of 38 models displaying all seven expressions at both intensity levels; all models were shown once in both intensity levels; a third presentation occurred in the low intensity condition for 50% of the models and for the others in the high intensity condition. This resulted in a total of 798 trials, 57 for each of the 14 conditions; order of presentation was randomized, with short, self-administered breaks after every 200 trials. The psychophysiological experiment reported here, followed a face familiarity task of 60 min and a 10-min break. It took 45 min and was followed after another break by an explicit expression task (see below). During the whole session participants were monitored by means of a non-recording video camera.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>EMG Recordings</head><p>Facial EMG was measured with two pairs of Ag/AgCl electrodes, 4 mm in diameter, one each for the corr, and zyg muscle, placed on the left side of the face, as recommended by Fridlund and Cacioppo <ref type="bibr" target="#b39">[40]</ref>; ground was placed on the upper half of the right forehead <ref type="bibr" target="#b40">[41]</ref>. Impedances were kept below 10 kV using a conductive EMG-gel (Neurgel). The raw EMG signal was amplified and filtered online at a band pass of 8-10000 Hz, using a Coulbourn V75-04 bio-amplifier and rectified and integrated with a Coulbourn V76-24 contour following integrator (TCu = u10 ms). The signal was digitized with BrainVision Recorder Software (Brain Products GmbH, 2010) at a sampling rate of 1000 Hz and notch-filter at 50 Hz.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data Analyses</head><p>Psychometric data. Performance indicators from the psychometric part were the average number of correctly solved trials. For two of the EP tasks with categorical judgments unbiased hit rates were calculated <ref type="bibr" target="#b35">[36]</ref> and used as indicators for the measurement model. These scores control for response bias, which often occurs in emotion classification tasks since some emotions are typically confused with each other. In order to detect multivariate outliers, we calculated Mahalanobis Distances in Mplus 5.21 <ref type="bibr" target="#b41">[42]</ref>. There were no severe outliers detected by visual inspection.</p><p>EMG data. Five participants were excluded due to error rates exceeding 25%. Further participants were excluded due to EMG data quality as follows. After the emotion classification task, participants completed an explicit emotion expression task. In this task subjects were asked to produce angry and happy facial expressions. The EMG signals recorded during this task served as sensitivity check for the EMG. Thus, exclusion of participants for a given EMG data set in the emotion classification task was based on the EMG activity during the explicit emotion expression task. EMG data sets for a given muscle and participant were discarded if there was no discernible EMG activity of that muscle during the explicit task. This led to a further exclusion of 10 participants based on corr data and of 36 on zyg data. The high exclusion rate for zyg data might be due to the high variability of the morphology of this muscle <ref type="bibr" target="#b42">[43]</ref>. This is no exception; for example, Aguado, Roma ´n, Rodrı ´guez, Die ´guez-Risco, Romero-Ferreiro et al. <ref type="bibr" target="#b43">[44]</ref> reported that only in 58% of their participants zyg activity differentiated between emotions.</p><p>The EMG signal was segmented offline into 1200-ms epochs, including a 200-ms pre-stimulus baseline. This time-window appears to be sufficient to detect peaks of corr and zyg activity in response to dynamic facial expressions <ref type="bibr" target="#b11">[12]</ref>. For each muscle and participant we performed an automatic artifact rejection implemented in MATLAB R2010a based on the SD of EMG over all trials of a given participant. If the range within a 50-ms segment in a given trial exceeded 3 SDs, the trial was rejected. This automatic method was compared to manual artifact rejection and delivered highly similar results. Even blink artifacts were identified with this algorithm.</p><p>We also rejected all trials with incorrect responses. Remaining trials were z-standardized within each participant (e.g., <ref type="bibr" target="#b5">[6]</ref>). On average we rejected 8.50% of trials for corr and 2.60% for zyg. In order to extract reliable averaged EMG responses, we required at least 18 averaged trials for each indicator. One additional participant of the corr sample was excluded due to this criterion. Final analyses of corr and zyg data were performed for N = 94 and 69 participants, respectively. There were no multivariate outliers for EMG data according to the same type of inspection as for the psychometric data. Finally, we calculated average EMG activities for the 200-ms pre-stimulus baseline and 10 consecutive 100-ms segments starting at stimulus onset.</p><p>Statistical analyses. Experimental effects on the average EMG activity were tested with repeated-measures analysis of variance (rmANOVA) with factors expression (7 levels) and intensity (2 levels). Given the different nature of the manipulation in intensity for emotional and neutral expressions, the assignment of the blinking and chewing conditions to high and low intensity levels were defined according the muscle. For corr blinking was defined as high intensity condition assuming that motion is more salient in the eye. Conversely for zyg, chewing was defined as high intensity. If the sphericity assumption was violated, p-values corresponding to Greenhouse-Geisser adjusted degrees of freedom are reported. All post-hoc comparisons were Bonferroni corrected. Given the large sample size, we interpret effect sizes partial g 2 (g p 2 )u.u0.15 rather than p-values. For confirmatory factor analyses (CFA) and structural equation modeling (SEM) <ref type="bibr" target="#b44">[45]</ref> the trials of each condition (7 expressions62 intensities) were split into odd and even, generating a total of 28 conditions. The averaged mean EMG activity in the segments from 300 to 800 ms in the emotional and neutral conditions were used as indicators in CFA and SEM. Latent variable analyses were conducted using the lavaan package <ref type="bibr" target="#b45">[46]</ref> implemented in the R Environment for Statistical Computing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experimental Effects</head><p>For the corr activity, the rmANOVA in consecutive 100-ms segments showed substantial (pu,u.05, g p 2 u.u0.15) effects of emotion starting at 300 ms, F(2.90, 269.90)u = u44.31, pu,u.001, g p 2 u = u.323, continuing with a similar effect size (psu,u.05, g p 2 su.u.214) until 900 ms. Post-hoc analyses indicated that the corr activity in response to happy faces was diminished (psu,u.05) relative to all other emotions and to the neutral conditions in all segments between 300-900 ms. Significantly higher corr activity for angry and sad expressions started in the 500-600 ms segment and lasted until 900-1000 ms (psu,u.05). Figure <ref type="figure" target="#fig_2">2</ref> shows the mean amplitude of corr activity for all emotions over time. There were significant effects for intensity from 0-300 ms, but effect sizes were small, Fs(1, 93)u,u11.84, psu$u.001 and the g p 2 values wereu,u.113. The emotion x intensity interaction reached significance at 500-600 ms but the effect size was very small, F(6, 558)u = u2.33, pu = u.031, g p 2 u = u.024. The zyg data did not reveal any significant experimental effects. Table <ref type="table">1</ref> shows p-values and effects sizes for main and interaction effects for all time-windows for both muscles. Detailed tables with descriptive statistics for accuracy of emotion classification and means, standard deviations and standard errors for all time windows for corr and zyg are provided in the supporting information (see Tables <ref type="table">S1</ref> and<ref type="table">S2</ref>).</p><p>Since zyg data showed no emotion-related effects, only data from corr were considered for further analyses. We analyzed the correlation between the mean corr activity and accuracy rates for the conditions where experimental effects were substantial (mean corr activity in response to angry, happy, and sad facial expressions in the 300-800 ms time-window). This analysis of corr activity and EP accuracy within the EMG emotion classification task revealed significant correlations between the average emotion classification accuracy and the corr activity in response to angry (ru = u.322, pu,u.05) and sad (ru = u.310, pu,u.05) faces (see Fig. <ref type="figure" target="#fig_5">3</ref>). The correlation with corr activity in response to happy faces was not significant (r = .096, p = .356). However, these relationships may be diminished by measurement error. Alternatively, they could be driven by the experimental dependency of the measures as these were assessed within the same trials. Using structural equation modeling these issues will be addressed with the analyses reported further below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Measurement Model</head><p>For the measurement models for corr we only used data from expressions with significant experimental effects on corr activity, namely, anger, happiness, sadness, and neutral. Due to the lack of experimental emotion effects in zyg recordings, we did not model data from zyg EMG. Structural Equation Models were estimated with the maximum-likelihood (ML) algorithm. We report the chisquare test (x2) that allows estimation of the exact model fit. In addition to x2, which is sensitive for large sample sizes common for CFA, we report further fit indices: (1) The Comparative Fit Index (CFI) with values above .95 denoting acceptable fit (2) the Root Mean Square Error of Approximation (RMSEA), which should be below .08, and (3) the Standardized Root Mean Square Residual (SRMR), which should not exceed .08 <ref type="bibr" target="#b46">[47]</ref>. For comparisons between nested models we used the chi-square difference test (Dx2). This test is significant (pu,u.05) if the restrictions in the nested model lead to substantial decrement of fit compared to the less restricted model. The reliability of latent factors was calculated with the omega (v) index, which represents the common variance of all indicators of a given factor <ref type="bibr" target="#b47">[48]</ref>.</p><p>In Model 1 both emotional and neutral indicators loaded on a general factor of corr responses to faces (Fcorr) and all emotional indicators loaded on an emotion-related factor of corr response (Ecorr). The fit of Model 1 was acceptable (x2 [92]u = u145.63, pu,u.001; CFIu = u.959; RMSEAu = u.079; SRMRu = u.036). Model 2 estimated three correlated emotion category-related corr factors: anger (ANcorr), happiness (HAcorr), and sadness (SAcorr) and Fcorr. Emotion specific corr factors accounted for the specific variance in corresponding emotion indicators only. Descriptively, Model 2 fitted the data better than Model 1: x2 [89]u = u117.98, pu = u.022; CFIu = u.978; RMSEAu = u.059; SRMRu = u.031. The correlations between the emotion-specific corr factors were substantial (r an/ha u = u2.543; pu,u.001; r an/sa u = u.741; pu,u.001; r ha/sa u = u2.656, pu,u.001). Given these sufficiently high correlations, we included an emotion-related second-order factor (EScorr), accounting for the common variance of ANcorr, HAcorr, and SAcorr (Model 3a). The model fit was the same as in Model 2 because both models are equivalent in terms of the model implied covariance matrix. All factor loadings on Fcorr were similarly high -independent of the emotion category. Thus, it appeared reasonable to restrict the loadings on Fcorr to be equal in a Model 3b. The model fit was still acceptable under this restriction (x2 [104]u = u137.16, pu = u.016; CFIu = u.975; RMSEAu = u.058; SRMRu = u.090). The Dx2 with 19.175 to 15 Ddf was not significant (pu = u.206), indicating that the fit was not substantially affected by introducing equality constraints on those loadings. Thus, in all indicators the same amount of variance was accounted for by Fcorr. Moreover, these restrictions increased the robustness of the model, given the relatively small sample size and number of parameters estimated.</p><p>The reliabilities of the latent factors were acceptable (vu = u.936 [Fcorr], vu = u.627 [ANcorr], vu = u.799 [HAcorr], vu = u.338 [SAcorr], and vu = u.626 [EScorr]). We used Model 3b for the following SEM analyses testing the relationship of facial mimicry with FP and EP (see Fig. <ref type="figure" target="#fig_4">4</ref>). Standardized factor loadings estimated in Model 3b can be found in the supporting information (see Table <ref type="table">S3</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Structural Models</head><p>Next we tested the relationships among Fcorr, EScorr, FP, and EP. There were five FP indicators (face1 to face5) derived from three FP tasks and three EP indicators derived from three EP tasks (emo1, emo2, and emo3). EP was nested under FP; thus FP accounted for the FP-specific variance in all psychometric indicators, while EP captured the specific variance of EP indicators (for factor loadings see supplementary Table <ref type="table">S4</ref>) after FP was accounted for. In Structural Model 1a all correlations among latent factors were freely estimated (Fig. <ref type="figure" target="#fig_6">5</ref>). This model fitted the data well (x2 [245]u = u321.01, pu = u.001; CFIu = u.949; RMSEAu = u.057; SRMRu = u.087). Correlations among FP and EScorr (ru = u.075, pu = u.585) and EP and Fcorr (ru = u.069, pu = u.690) were not significant. Significant correlations were observed between FP and Fcorr (ru = u.233, pu = u.047) and between EP and EScorr (ru = u.485, pu = u.015). To test if the numerically higher correlation between EP and EScorr compared to FP and EScorr was statistically significant, we estimated a further Structural Model 1b, in which these correlations were constrained to equality. This model showed acceptable fit (x2 [246]u = u326.31, pu = u.001; CFIu = u.946; RMSEAu = u.059; SRMRu = u.090), but the chi-square difference of Dx2u = u5.3 with Ddfu = u1 was significant (pu = u.021). Thus, Structural Model 1b with correlations forced to equality fitted worse than the Table <ref type="table">1</ref>. p-values (two-tailed) and partial g 2 for main effect of emotion (E) and intensity (I) and interaction effects (E6I) for all time windows for corr and zyg.  Structural Model 1a with freely estimated correlations, demonstrating that the relationship between EP and EScorr was substantially higher than the correlation between FP and EScorr.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion</head><p>The first aim of the present study was measuring facial responses to emotional expressions as a latent construct by applying measurement models to EMG activity recorded in two facial muscles. Second, we assessed whether individual differences in facial reactions to emotional expressions are substantially related to emotion perception.</p><p>In the corr muscle facial expressions were mimicked as expected: Corr activity increased in response to angry and sad faces and decreased to happy faces. Hypotheses related to the zyg muscles could not be confirmed. Accuracy rates in emotion classification correlated with the amount of mimicry in corr in response to angry and sad faces within the same experiment. Further, we could establish a measurement model of corr responses to emotional expressions; the model involved a general face response factor and  an emotion-related second-order corr factor that was sensitive for emotions, but integrated across all emotion categories -hence, it was unspecific for any of the three particular emotions. In structural equation modeling including a latent face perception factor and a nested emotion perception factor we found a substantial correlation between emotion-related corr response and emotion perception.</p><p>In contrast to other studies that examined the correlation between facial mimicry and emotion <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21]</ref>, we found a substantial relationship, at least for the corr response to angry, happy and sad faces with EP. This discrepancy might be due to methodological differences of the present study and more traditional single task approaches. In the present study a test battery captured task-general abilities of face and emotion perception, latent variables took into account measurement error, and relationships among construct variances were assessed with structural equation models. In addition, in our structural models we took into account the emotion-unrelated factors of face perception ability and EMG responses to faces in general. These methodological advances may have revealed relationships that remain hidden in single task studies.</p><p>According to embodied simulation theories the perception of an emotional expression leads to the simulation of a corresponding affective state in the perceiver which in turn facilitates the access to the emotional concept. Our results suggest that humans show individual differences in facial responses to emotional facial expressions -as indicated by emotion-related corr response. This simulation may facilitate the classification of facial expressions through the activation of corresponding emotional concepts. As suggested by Hess and Fischer <ref type="bibr" target="#b21">[22]</ref>) this may especially be the case when emotion recognition is difficult -as in the psychometric tasks of the present study.</p><p>The observed positive correlation between emotion-elicited corr responses and psychometric emotion perception by itself cannot demonstrate causality. However, considering that experimental studies have shown impairments of emotion recognition when facial mimicry is blocked <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19]</ref>, our results provide convergent evidence that the amount of facial responses to emotional expressions is related to emotion perception. Hence, individual differences in emotion-related corr responses may-at least to some extent -account for differences in emotion perception ability.</p><p>Alternatively one might argue that the differences in corr activity between emotion conditions reflect the relatively increased mental effort in more difficult conditions <ref type="bibr" target="#b48">[49]</ref>. Since happy faces are easy to identify (accuracy rates are .99) less mental effort is required relative to all other emotional expressions. Thus, the observed relaxation in the corr activity in response to happy faces might be due to reduced deployment of processing effort. The relation between emotion-related corr response and emotion perception could then be explained by individual differences in mental effort or resource mobilization <ref type="bibr" target="#b49">[50]</ref>. However, the mental effort account does not seem to be a plausible explanation of the present corr results. Accuracy rates for neutral faces were also relatively high (.96/.97) whereas corr activity for neutral faces was as high as for disgust expressions, which showed clearly lower accuracy rates (.85/.84). Moreover, more recent studies did not find a relation between corr activity and required mental effort induced through manipulating processing difficulty <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b51">52]</ref>. Other authors who found a decline in corr activity in response to happy faces <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b52">53]</ref> explained this in terms of enhanced corr activity in the baseline interval due to mental preparation for the upcoming stimulus. This may at least partly obscure the subsequent activity in response to angry faces but would enhance the difference to the corr response to happy faces.</p><p>A further explanation of the present results might interpret the larger corr activation observed for anger and sadness, as a linear effect of valence rather than facial mimicry or effort. Thus Larsen and colleagues <ref type="bibr" target="#b22">[23]</ref> reported that valence ratings of affective pictures, words and sounds were negatively correlated with corr activity. That is, decreasing corr activity was accompanied by more positive valence ratings. Hence, if the corr activity reflects affective states, we may assume that its response to facial emotional stimuli is also affective in nature and not effort-related. Moreover, Kappas, Lu ¨ckman, Pleser and Ku ¨ster <ref type="bibr" target="#b53">[54]</ref> showed that valencerelated corr responses depend on the task, being more pronounced during affective versus complexity ratings.</p><p>The distinction between effort-related corr activity, emotionspecific facial mimicry, and valence sensitive corr-responses needs to be investigated more carefully. In addition, the amount of facial mimicry is influenced by the social context and characteristics of the sender and the perceiver and their interaction intentions <ref type="bibr" target="#b21">[22]</ref>.</p><p>It is an open and intriguing question whether the rank order of individuals concerning facial responses to emotional expressions would change when relevant social information were incorporated into the tasks. Thus, future research might include different context information and subjectively experienced emotional states, for example, by requiring affective valence ratings.</p><p>An important limitation in the present study is that expected emotion effects in the zyg could not be confirmed. One reason could be lower degree of voluntary control over the corr than the zyg muscle <ref type="bibr" target="#b54">[55]</ref>, which could lead to more pronounced incidental facial reactions to emotional expressions in corr as compared to zyg. Although effects of facial mimicry in zyg have been reported during emotional intensity ratings <ref type="bibr" target="#b11">[12]</ref> or even unconscious (subliminal) processing <ref type="bibr" target="#b30">[31]</ref> others did not find such clear effects <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b56">57]</ref>. One could speculate that providing contextual information might reveal the expected effects in response to happy expressions (e.g. <ref type="bibr" target="#b57">[58,</ref><ref type="bibr" target="#b21">22]</ref>. Mimicry effects in the zyg might also specifically be suppressed by the experimental context like movement restrictions, monotony, and artificiality. More natural settings with real interaction partners might lead to stronger mimicry reactions, especially in response to happy facial expressions <ref type="bibr" target="#b58">[59]</ref>.</p><p>Our results show, for the first time, that reliable measurement of individual differences in incidental facial responses to emotional expressions is feasible. In perspective this provides a tool for testing hypotheses concerning emotion processing deficits in special populations like patients with autism <ref type="bibr" target="#b59">[60]</ref> or disruptive behavior disorder <ref type="bibr" target="#b60">[61]</ref>. Moreover, research trying to elicit meaningful relations facial responses to emotional expressions and other emotion-related outcomes can benefit from the measurement model established here. For example, automatic mimicking of emotional states as a component of empathy <ref type="bibr" target="#b61">[62]</ref> could be examined in a more elaborate way. As a first step, we showed here that under well-controlled conditions, incidental emotion-related corr responses are measurable and related to emotion perception. This might be a first piece of evidence from an individual differences perspective on the role of emotion-related facial responses in emotion perception within the scope of embodied simulation theory.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>1 ) 3 ) 4 )</head><label>134</label><figDesc>Sequential Matching of Part-Whole Faces-with condition Part and condition Whole. First, a face stimulus was presented for 1000 ms; following a blank screen of 200 ms either two faces (whole condition) or two parts of a face (two pairs of eyes, two noses, two mouths; part condition) were presented. One of these faces or parts of faces was the face or part seen before (target) and the other was a new face or face part (distracter). Participants were expected to identify the target. Part and whole conditions served as different indicators. 2) Simultaneous Matching of Spatially Manipulated Faces-with Conditions of Upright and Inverted. Participants had to decide if two faces presented simultaneously were the same or different. Faces were generated from identical pictures by changing spatial relations between features (e.g., distance between eyes and nose, between the eyes, or between mouth and nose). In 50% of the trials faces were different. Moreover, in 50% of the trials both faces were presented upside down (inverted). Upright and inverted conditions were defined as separate indicators. Facial Resemblance. In each trial three faces were presented simultaneously. Participants had to indicate, which of two morphed faces in frontal view most resembled a third face shown in three-quarter view (target face). Both morphs consisted of a mixture of the target face and another unfamiliar face (e.g., 20% face A and 80% face B, for morph 1; 40% face A and 60% face B, for morph 2; face B being the target face). Identification of Emotion Expressions from Composite Faces. Participants saw composite faces of the same person displaying different emotions in the upper and lower halves of the face.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Time course of development of facial expression display within the first 6 frames (0-200 ms) of a trial; the last 12 frames (200-600 ms) showed full emotional expressions. doi:10.1371/journal.pone.0084053.g001</figDesc><graphic coords="4,58.05,60.78,319.24,263.85" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Time course of corr z-standardized EMG response for all emotion conditions. doi:10.1371/journal.pone.0084053.g002</figDesc><graphic coords="5,58.05,470.21,318.95,233.63" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Schematic representation of the measurement model of emotion-related corr responses -Model 3b. EScorr = second order emotion-related corr factor; ANcorr, HAcorr, SAcorr = corr factors as responses to anger, happiness, and sadness, respectively; Fcorr = general face response factor; ne1-n4 = neutral indicators, an1-an4 = anger indicators, ha1-ha4 = happiness indicators, sa1-sa4 = sadness indicators. doi:10.1371/journal.pone.0084053.g004</figDesc><graphic coords="7,58.05,479.23,462.33,208.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Correlations between mean accuracy rates and mean corr actvitiy (300-800 ms) to anger, happiness and sadness within the EMG emotion classification task. doi:10.1371/journal.pone.0084053.g003</figDesc><graphic coords="7,58.05,60.77,318.84,239.70" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Schematic representation of the structural model of face reaction, emotion-related corr response, face perception and emotion perception -Model 1a. EScorr = second order emotion-related corr factor; ANcorr, HAcorr, SAcorr = corr factors as responses to anger, happiness, and sadness, respectively; Fcorr = general face response factor, ne1 -n4 = neutral indicators, an1-an4 = anger indicators, ha1-ha4 = happiness indicators, sa1-sa4 = sadness indicators, FP = face perception, EP = emotion perception, face1-face5u = uface perception indicators, emo1-emo3 = emotion perception indicators. *pu,u.05, two-tailed. doi:10.1371/journal.pone.0084053.g005</figDesc><graphic coords="8,58.05,60.78,382.62,340.72" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>In this paradigm the emotions of anger, disgust, fear, sadness, and surprise were used. Participants had to identify and indicate the expressions that diverged from the dominating emotion by setting check marks via mouse click below the corresponding stimuli. The number of targets to be detected was indicated by a number within a box on top of the screen.</figDesc><table /><note><p>as performance indicators. 6) Visual Search for Faces with Corresponding Emotion Expressions of Different Intensity. Nine face stimuli of the same person were presented simultaneously in a 363 grid. The majority of pictures displayed the same emotion, but a varying number of targets (1, 2, 3 or 4) showed a neutral or a different emotional expression.</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>PLOS ONE | www.plosone.org</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1"><p>January 2014 | Volume 9 | Issue 1 | e84053</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>We give thanks to <rs type="person">Volha Shmuilovich</rs>, <rs type="person">Ulrike Bunzenthal</rs>, <rs type="person">Karsten Manske</rs>, <rs type="person">Astrid Kiy</rs>, <rs type="person">Jo ¨rg Paschke</rs> and <rs type="person">Linda Gerresheim</rs> for data acquisition and <rs type="person">Thomas Pinkpank</rs> and <rs type="person">Rainer Kniesche</rs> for technical support.</p></div>
			</div>
			<div type="funding">
<div><p>This research was supported by the <rs type="funder">German Research Foundation</rs>, Grant No. <rs type="grantNumber">WI2667/2-4</rs> to <rs type="person">Oliver Wilhelm</rs> and <rs type="person">Werner Sommer</rs>. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_68MFKNX">
					<idno type="grant-number">WI2667/2-4</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supporting Information</head><p>Table <ref type="table">S1</ref> Means and standard deviations of accuracy rates for experimental conditions in all subsamples. (PDF)</p><p>Table <ref type="table">S2</ref> Means, standard deviations and standard errors for all experimental conditions and time windows for corr and zyg.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(PDF)</head><p>Table <ref type="table">S3</ref> Standardized factor loadings of measurement model 3b.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(PDF)</head><p>Table <ref type="table">S4</ref> Standardized factor loadings of measurement model of face perception and nested emotion perception. (PDF)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Author Contributions</head><p>Conceived and designed the experiments: JK AH GR OW WS. Performed the experiments: GR JK. Analyzed the data: JK. Contributed reagents/ materials/analysis tools: AH GR WS OW. Wrote the paper: JK.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Facial expressions of emotions</title>
		<author>
			<persName><forename type="first">P</forename><surname>Ekman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Oster</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annual Review of Psychology</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="527" to="554" />
			<date type="published" when="1979">1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">On the Specificity of Individual Differences in Emotion Perception and Emotion Recognition Ability</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hildebrandt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Sommer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Schacht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Wilhelm</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note>Manuscript in preparation</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Facial Reactions to Facial Expressions</title>
		<author>
			<persName><forename type="first">U</forename><surname>Dimberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychophysiology</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="643" to="647" />
			<date type="published" when="1982">1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Facial EMG reactions to facial expressions: A case of facial emotional contagion?</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">O</forename><surname>Lundqvist</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scandinavian Journal of Psychology</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="130" to="141" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Facial reactions to emotional stimuli: Automatically controlled emotional responses</title>
		<author>
			<persName><forename type="first">U</forename><surname>Dimberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Thunberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Grunedal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognition &amp; Emotion</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="449" to="471" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Facial mimicry and emotional contagion to dynamic emotional facial expressions and their influence on decoding accuracy</title>
		<author>
			<persName><forename type="first">U</forename><surname>Hess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Blairy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Psychophysiology</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="129" to="141" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Facial Action Coding System</title>
		<author>
			<persName><forename type="first">P</forename><surname>Ekman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">V</forename><surname>Friesen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1978">1978</date>
			<publisher>Consulting Psychologists Press</publisher>
			<pubPlace>Palo Alto, CA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The impact of social context on mimicry</title>
		<author>
			<persName><forename type="first">P</forename><surname>Bourgeois</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Hess</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biological Psychology</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="page" from="343" to="352" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Expectations of cooperation and competition and their effects on observers&apos; vicarious emotional responses</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Lanzetta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">G</forename><surname>Englis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Personality and Social Psychology</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="page" from="543" to="554" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Spontaneous facial mimicry, liking and emotional contagion</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">N</forename><surname>Mcintosh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Polish Psychological Bulletin</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="31" to="42" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Reactions to facial expressions: effects of social context and speech anxiety on responses to neutral, anger, and joy expressions</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Vrana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gross</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biological Psychology</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="page" from="63" to="78" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">EMG activity in response to static and dynamic facial expressions</title>
		<author>
			<persName><forename type="first">K</forename><surname>Rymarczyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Biele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Grabowska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Majczynski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Psychophysiology</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="page" from="330" to="333" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Embodied simulation: From neurons to phenomenal experience</title>
		<author>
			<persName><forename type="first">V</forename><surname>Gallese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phenomenology and the Cognitive Sciences</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="23" to="48" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Simulationist models of face-based emotion recognition</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">I</forename><surname>Goldman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">S</forename><surname>Sripada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognition</title>
		<imprint>
			<biblScope unit="volume">94</biblScope>
			<biblScope unit="page" from="193" to="213" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Embodying emotion</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M</forename><surname>Niedenthal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">316</biblScope>
			<biblScope unit="page" from="1002" to="1005" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Understanding Simulation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hurley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Philosophy and Phenomenological Research</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="page" from="755" to="774" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The Simulation of smiles (SIMS) model: Embodied simulation and the meaning of facial expression</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M</forename><surname>Niedenthal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mermillod</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Maringer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Hess</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behavioral and Brain Sciences</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="433" to="480" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Embodied emotion perception: Amplifying and dampening facial feedback modulates emotion perception accuracy</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">T</forename><surname>Neal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Chartrand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Social Psychological and Personality Science</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="673" to="678" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Face to face: Blocking facial mimicry can selectively impair recognition of emotional expressions</title>
		<author>
			<persName><forename type="first">L</forename><surname>Oberman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Winkielman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ramachandran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Social Neuroscience</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="167" to="178" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Mimicry and the Judgment of Emotional Facial Expressions</title>
		<author>
			<persName><forename type="first">S</forename><surname>Blairy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Herrera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Hess</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Nonverbal Behavior</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="5" to="41" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Electromyographic evidence for agerelated differences in the mimicry of anger</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">E</forename><surname>Bailey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Henry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Nangle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychology and Aging</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="224" to="229" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Emotional Mimicry as Emotion Regulation</title>
		<author>
			<persName><forename type="first">U</forename><surname>Hess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fischer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Personality and Social Psychology Review</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="142" to="157" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Effects of positive and negative affect on electromyographic activity over zygomaticus major and corrugator supercilii</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Larsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Norris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Cacioppo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychophysiology</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="776" to="785" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">The ´riault M (2000) Inhibiting Facial Expressions: Limitations to the voluntary control of facial expressions of emotions</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kappas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bherer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Motivation and Emotion</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="259" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Individual differences in perceiving and recognizing faces-One element of social cognition</title>
		<author>
			<persName><forename type="first">O</forename><surname>Wilhelm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Herzmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Kunina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Danthiir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Schacht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of personality and social psychology</title>
		<imprint>
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="page" from="530" to="548" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<author>
			<persName><forename type="first">W</forename><surname>Revelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Zinbarg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Coefficients alpha, beta, omega and the glb: Comments on Sijtsma</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="page" from="145" to="154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Toward a comprehensive test battery for face cognition: Assessment of the tasks</title>
		<author>
			<persName><forename type="first">G</forename><surname>Herzmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Danthiir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Schacht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Sommer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Wilhelm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behavior Research Methods</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="840" to="857" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">A Task Battery for Measuring the Perception and Recognition of Facial Expressions of Emotion: Assessment of the Tasks</title>
		<author>
			<persName><forename type="first">O</forename><surname>Wilhelm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hildebrandt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Manske</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Schacht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Sommer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note>Manuscript submitted for publication</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Facial expressions in Hollywood&apos;s portrayal of emotion</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Carroll</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Russell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Personality and Social Psychology</title>
		<imprint>
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="page" from="164" to="176" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Recognizing dynamic facial expressions of emotion: specificity and effects of intensity in event-related brain potentials</title>
		<author>
			<persName><forename type="first">G</forename><surname>Recio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Schacht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Sommer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note>Manuscript submitted for publication</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Unconscious facial reactions to emotional facial expressions of emotions</title>
		<author>
			<persName><forename type="first">U</forename><surname>Dimberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Thunberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Elmehed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Science</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="86" to="89" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Rapid Emotional Contagion and Expressive Congruence Under Strong Test Conditions</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Lishner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Cooter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Zald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Nonverbal Behavior</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="225" to="239" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Anticipatory EMG responses of pericranial muscles in relation to heart rate during a warned simple reaction time task</title>
		<author>
			<persName><forename type="first">A</forename><surname>Van Boxtel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Damen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Brunia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychophysiology</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="576" to="583" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Emotional experience and expression in schizophrenia and depression</title>
		<author>
			<persName><forename type="first">H</forename><surname>Berenbaum</surname></persName>
		</author>
		<author>
			<persName><surname>Oltmannstf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Abnormal Psychology</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="page" from="37" to="44" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Configural Information in Facial Expression Perception</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Calder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Keane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="527" to="551" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">On measuring performance in category judgment studies of nonverbal behavior</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wagner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Nonverbal Behavior</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="3" to="28" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Presentation and validation of the Radboud Faces Database</title>
		<author>
			<persName><forename type="first">O</forename><surname>Langner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Dotsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Bijlstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dhj</forename><surname>Wigboldus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">T</forename><surname>Hawk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognition &amp; Emotion</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="1377" to="1388" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m">FantaMorph (Version 5.0)</title>
		<imprint>
			<publisher>Abrosoft</publisher>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
	<note>Computer software</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Effects of inter-stimulus interval on skin conductance responses and event-related potentials in a Go/NoGo task</title>
		<author>
			<persName><forename type="first">G</forename><surname>Recio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Schacht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Sommer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biological Psychology</title>
		<imprint>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="246" to="250" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Guidelines for human electromyographic research</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Fridlund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Cacioppo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychophysiology</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="567" to="589" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Evidence for Unintentional Emotional Contagion Beyond Dyads</title>
		<author>
			<persName><forename type="first">G</forename><surname>Dezecache</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Conty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chadwick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Soussignan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS One</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">K</forename><surname>Muthe ´n</surname></persName>
		</author>
		<author>
			<persName><surname>Muthe ´n</surname></persName>
		</author>
		<author>
			<persName><surname>Bo</surname></persName>
		</author>
		<title level="m">Mplus user&apos;s guide</title>
		<meeting><address><addrLine>Los Angeles, CA</addrLine></address></meeting>
		<imprint>
			<publisher>Muthe ´n and Muthe ´n</publisher>
			<date type="published" when="1998">1998-2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Double or bifid zygomaticus major muscle: Anatomy, incidence, and clinical correlation</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Pessa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">P</forename><surname>Zadoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Garza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrian</forename><forename type="middle">Ek</forename><surname>Dewitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">I</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Clinical Anatomy</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="310" to="313" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Learning of Facial Responses to Faces Associated with Positive or Negative Emotional Expressions</title>
		<author>
			<persName><forename type="first">L</forename><surname>Aguado</surname></persName>
		</author>
		<author>
			<persName><surname>Roma ´n Fj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rodrı ´guez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Die</forename><surname>´guez-Risco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Romero-Ferreiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Spanish Journal of Psychology</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Latent variables in psychology and the social sciences</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Bollen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annual Review of Psychology</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="page" from="605" to="634" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">lavaan: an R package for structural equation modeling and more Version 0.4-13 (BETA)</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Rosseel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Statistical Software</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="1" to="36" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Cutoff criteria for fit indexes in covariance structure analysis: Conventional criteria versus new alternatives</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">T</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M</forename><surname>Bentler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Structural Equation Modeling: A Multidisciplinary Journal</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="1" to="55" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Test theory: A unified treatment</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Mcdonald</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<publisher>Erlbaum</publisher>
			<pubPlace>Mahwah NJ</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Amplitude and bilateral coherency of facial and jaw-elevator EMG activity as an index of effort during a two-choice reaction task</title>
		<author>
			<persName><forename type="first">A</forename><surname>Van Boxtel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jessurun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychophysiology</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="589" to="604" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Facial and jaw-elevator EMG activity in relation to changes in performance level during a sustained informtaion processing task</title>
		<author>
			<persName><forename type="first">W</forename><surname>Waterink</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Van Boxtel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biological Psychology</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="183" to="198" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Mood-regulative hedonic incentive interacts with mood and task difficulty to determine effort-related cardiovascular response and facial EMG</title>
		<author>
			<persName><forename type="first">N</forename><surname>Silvestrini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">H</forename><surname>Gendolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biological Psychology</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="page" from="54" to="63" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Mind at ease puts a smile on the face: psychophysiological evidence that processing facilitation elecits positive affect</title>
		<author>
			<persName><forename type="first">P</forename><surname>Winkielman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Cacioppo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Personality and Social psychology</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="page" from="989" to="1000" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Simultaneous recording of EEG and facial muscle reactions during spontaneous emotional mimicry</title>
		<author>
			<persName><forename type="first">A</forename><surname>Achaibou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Pourtois</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vuilleumier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuropsychologi</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="1104" to="1113" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Ku ¨ster D (2013,08) Corrugator Supercilii and Zygomaticus Major EMG responses to IAPS pictures as a function of affective vs. descriptive judgments</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kappas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Lu ¨ckmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pleser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Society for Research on Emotion (ISRE) post-conference on Facial Expression</title>
		<meeting><address><addrLine>Berkeley</addrLine></address></meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">The neuropsychology of facial expression: A review of the neurological and psychological mechanisms for producing facial expressions</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">E</forename><surname>Rinn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Bulletin</title>
		<imprint>
			<biblScope unit="volume">95</biblScope>
			<biblScope unit="page" from="52" to="77" />
			<date type="published" when="1984">1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Facial and emotional reactions to Duchenne and non-Duchenne smiles</title>
		<author>
			<persName><forename type="first">Hietanen</forename><surname>Surakkav</surname></persName>
		</author>
		<author>
			<persName><surname>Jk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Psychophysiology</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="23" to="33" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Facial expression arousal level modulates facial mimicry</title>
		<author>
			<persName><forename type="first">T</forename><surname>Fujimura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Sato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Suzuki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Psychophysiology</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="page" from="88" to="92" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Enhanced embodied response following ambiguous emotional processing</title>
		<author>
			<persName><forename type="first">B</forename><surname>Beffara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Oullet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Vermeulen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Basu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Morisseau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mermillod</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Process</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="103" to="106" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">You smile -I smile: Emotion expression in social interaction</title>
		<author>
			<persName><forename type="first">U</forename><surname>Hess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Bourgeois</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biological Psychology</title>
		<imprint>
			<biblScope unit="volume">84</biblScope>
			<biblScope unit="page" from="514" to="520" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">The simulating social mind: the role of the mirror neuron system and simulation in the social and communicative deficits of autism spectrum disorders</title>
		<author>
			<persName><forename type="first">L</forename><surname>Oberman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ramachandran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Bulletin</title>
		<imprint>
			<biblScope unit="volume">133</biblScope>
			<biblScope unit="page" from="310" to="327" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Facial EMG responses to dynamic emotional facial expressions in boys with disruptive behavior disorders</title>
		<author>
			<persName><forename type="first">M</forename><surname>De Wied</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Van Boxtel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zaalberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">P</forename><surname>Goudena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Matthys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Psychiatry Research</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="112" to="121" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Automatic mimicry reactions as related to differences in emotional empathy</title>
		<author>
			<persName><forename type="first">M</forename><surname>Sonnby-Borgstro ¨m</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scandinavian Journal of Psychology</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="433" to="443" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
