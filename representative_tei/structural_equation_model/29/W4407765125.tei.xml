<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Causal discovery in heavy-tailed linear structural equation models via scalings</title>
				<funder>
					<orgName type="full">EPFL Doctoral School of Mathematics</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2025-02-20">February 20, 2025</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Mario</forename><surname>Krali</surname></persName>
							<email>mario.krali@epfl.ch</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Mathematics École Polytechnique Fédérale de Lausanne (EPFL)</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Causal discovery in heavy-tailed linear structural equation models via scalings</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2025-02-20">February 20, 2025</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2502.13762v1[stat.ME]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-21T19:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>AMS 2020 Subject Classifications: primary: 60G70</term>
					<term>62D20</term>
					<term>62G32</term>
					<term>62H22 causal discovery</term>
					<term>extreme value theory</term>
					<term>graphical model</term>
					<term>heavy tails</term>
					<term>linear structural equation model</term>
					<term>multivariate regular variation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Causal dependence modelling of multivariate extremes is intended to improve our understanding of the relationships amongst variables associated with rare events. Regular variation provides a standard framework in the study of extremes. This paper concerns the linear structural equation model with regularly varying noise variables. We focus on extreme observations generated from such a model and propose a causal discovery method based on the scaling parameters of its extremal angular measure. We implement the method as an algorithm, establish its consistency and evaluate it by simulation and by application to river discharge datasets. Comparison with the only alternative extremal method for such model reveals its competitive performance.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction and motivation</head><p>Extreme events can result in catastrophic consequences, of which floods, droughts and heatwaves are obvious manifestations. Extreme value theory (EVT) has become the standard tool in studying such phenomena and in helping to better prepare to mitigate their effects. Often, these events are subject to cause-and-effect types of behaviour, which raises questions on the underlying extremal causal mechanisms.</p><p>Causal discovery and structure learning assist in identifying causal structures and serve as a stepping-stone towards causal inference, and are studied using graphical models <ref type="bibr" target="#b29">[Pearl, 2009</ref><ref type="bibr" target="#b24">, Lauritzen, 1996]</ref>. The process of uncovering cause-and-effect relationships involves a large number of variables, thus giving rise to high-dimensional settings. Observations in high dimensions are notorious for posing challenges in statistics, and this is further exacerbated in the study of extremes, which manifest only rarely <ref type="bibr">[de Haan and Ferreira, 2006, Ch. 7]</ref>.</p><p>The focus of this paper is on graphical modelling, and in particular on the linear structural equation model (LSEM) <ref type="bibr" target="#b29">[Pearl, 2009]</ref>. An LSEM is supported on a directed acyclic graph (DAG) D = (V, E) with nodes V = {1, . . . , d} and edge set E ⊂ V ×V , and is recursively defined as</p><formula xml:id="formula_0">X i = j∈pa(i) c ij X j + s ii Z i , i ∈ V,<label>(1.1)</label></formula><p>for innovation variables Z i . We shall restrict ourselves to non-negative edge weights, so c ij ≥ 0 for i ̸ = j, and innovation weights s ii &gt; 0 in (1.1), and refer to the resulting model as LSEM+. This restriction arises because of the proposed methodology, but one may nevertheless find it plausible given our focus on multivariate extremes, which are often associated with the behaviour of maxima. In EVT, dependence between extreme observations of the variables is characterised by the limiting angular measure.</p><p>The LSEM+ is closely related to the so-called recursive max-linear model (RMLM) studied in <ref type="bibr" target="#b14">Gissibl and Klüppelberg [2018]</ref>, which is known to possess a discrete limiting angular measure <ref type="bibr" target="#b12">[Fougères et al., 2013]</ref>. Unlike the RMLM, which only allows the extreme shocks to travel through the network, the LSEM+ preserves smaller shocks. However, under the framework of regularly varying innovations, the LSEM+ also has a discrete limiting angular measure. The RMLM manifests multivariate extremes in a ray-like fashion starting from a penultimate or pre-asymptotic level, but by contrast, the LSEM+ only shows such behaviour in the limit. The finite number of data points in real datasets naturally corresponds to a penultimate setup, which makes the LSEM+ more realistic in applications.</p><p>Recent research has focused on combining extremes and methods from graphical modelling <ref type="bibr" target="#b14">[Gissibl and Klüppelberg, 2018</ref><ref type="bibr" target="#b9">, Engelke and Hitz, 2020</ref><ref type="bibr">, Engelke et al., 2022]</ref>, causal discovery <ref type="bibr" target="#b15">[Gnecco et al., 2021</ref><ref type="bibr">, Klüppelberg and Krali, 2021</ref><ref type="bibr" target="#b22">, Krali et al., 2023</ref><ref type="bibr" target="#b27">, Mhalla et al., 2020</ref><ref type="bibr" target="#b36">, Tran et al., 2024]</ref>, causality for extremes of time-series <ref type="bibr" target="#b3">[Bodik et al., 2024]</ref>, sparsity modelling <ref type="bibr">[Engelke and Volgushev, 2022</ref><ref type="bibr" target="#b5">, Cooley and Thibaud, 2019</ref><ref type="bibr" target="#b26">, Meyer and Wintenberger, 2023</ref><ref type="bibr" target="#b17">, Goix et al., 2017</ref><ref type="bibr" target="#b25">, Lee and Cooley, 2022]</ref>), extreme quantile prediction <ref type="bibr" target="#b16">[Gnecco et al., 2023]</ref> and clustering <ref type="bibr">[Chautru, 2015, Janßen and</ref><ref type="bibr" target="#b20">Wan, 2020]</ref>. Another prominent facet of extreme statistics concerns conditional modelling of extremes <ref type="bibr" target="#b18">[Heffernan and Tawn, 2004]</ref>. Although each of these methods enriches a specific area of statistical modelling, questions remain on how to bridge the resulting conclusions to other aspects of statistical analysis. For instance, conditional extremes, seemingly a well-established sub-domain, is intended to capture the phenomenon of asymptotic independence between extremes, but does not offer a viable way to incorporate the resulting conditional marginal distributions into an overall joint dependence structure. Another example is the undirected graphical structure in <ref type="bibr" target="#b9">Engelke and Hitz [2020]</ref>, which makes it impossible to infer causal relationships amongst the node variables.</p><p>The choice of the LSEM+ is partly motivated by the fact that discrete limiting angular measures form a dense subset among all angular measures <ref type="bibr" target="#b12">[Fougères et al., 2013]</ref>. The idea is thus to work with a model that is able to encode extreme causal dependencies and that may potentially approximate the extremal dependence structure. Secondly, the equations of the LSEM+ are instances of linear regression, which find wide applications in many domains. Apart from applications in causal discovery, extremal causal dependencies may also complement the conditional approach of <ref type="bibr" target="#b18">Heffernan and Tawn [2004]</ref>, providing possibly useful information in the selection of a conditioning variable based on causal dependencies.</p><p>Throughout our analysis we work with the standardised representation of the LSEM+ (1.1) in order to facilitate the study of its extremal causal dependence structure; however, because of our focus on extremes, the proposed methodology can cover a wider class of models whose extremal behaviour is asymptotically equivalent to the LSEM+. One can think here of a model Y = X + V , with the components of V having lighter tail indices relative to the components of the LSEM+ X. As the vectors X and V need not have the same dependence structure, the extreme causal relations of Y may differ from those in the bulk of its distribution.</p><p>In this paper we propose a methodology to uncover causal dependencies between node variables using only extreme observations. Under the standard assumption of regular variation, we exploit certain asymmetries reflected in the scaling parameters of the extreme angular measure. In particular, we refine the methodology of <ref type="bibr">Klüppelberg and Krali [2021]</ref>, which relies on the angular measure of the d-dimensional vector X to compute the scaling parameters. In contrast, our approach starts with the bivariate angular measure, and then augments its dimension by accounting for the node variables identified. <ref type="bibr" target="#b15">Gnecco et al. [2021]</ref> develop an algorithm for the causal ordering of heavy-tailed LSEMs using the causal tail coefficient. We use their approach as a benchmark in a simulation study and data application. Building on <ref type="bibr" target="#b15">Gnecco et al. [2021]</ref>, and considering settings with confounders, <ref type="bibr" target="#b28">Pasche et al. [2023]</ref> propose testing for direct causal links between two variables by including confounders in a regression setting. Several methods have been proposed to tackle structure learning for the LSEM in the non-extremal setting, using all n observations. Amongst these are LiNGAM <ref type="bibr" target="#b33">[Shimizu et al., 2006]</ref>, the pairwise likelihood ratio method <ref type="bibr" target="#b19">[Hyvärinen and Smith, 2013]</ref>, Di-rectLiNGAM <ref type="bibr" target="#b34">[Shimizu et al., 2011]</ref>, and modified DirectLiNGAM <ref type="bibr" target="#b38">[Wang and Drton, 2020]</ref>. <ref type="bibr" target="#b7">Drton and Maathuis [2017]</ref> survey these methods.</p><p>This paper is organised as follows. Section 2 introduces the necessary basics from graphical modelling and gives some properties of the LSEM+. Section 3 outlines the required notions from multivariate regular variation and scalings. In Section 4 we introduce the scaling methodology used for causal discovery and show its consistency. Section 5 describes a simulation study and outlines applications to river discharges from stations along the Upper Danube basin and from the Rhine basin in Switzerland. Appendix A provides the necessary regular variation background; Appendix B contains all proofs; Appendix C outlines the estimation procedure and gives the consistency of the estimated causal orderings; and Appendix D contains further numerical results.</p><p>2 Graphs and linear structural equation models</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Vocabulary and notation</head><p>We first introduce terminology for directed graphs <ref type="bibr" target="#b24">[Lauritzen, 1996]</ref>. Let D = (V, E) denote a directed acyclic graph (DAG) with node set V = {1, . . . , d} and edge set E ⊂ V × V . For each node i ∈ V the sets pa(i) = {j ∈ V : (j, i) ∈ E}, an(i) and de(i) respectively denote its parents, ancestors and descendants, and we write Pa(i) = pa(i) ∪ {i}, An(i) = an(i) ∪ {i} and De(i) = de(i) ∪ {i}. Finally, for I ⊂ V , an(I) denotes the ancestors of all nodes in I, and we write An(I) = an(I) ∪ I. A node i ∈ V is called a source node if pa(i) = ∅, and we let V 0 denote the set of all source nodes. We use j → i to denote an edge from node j to node i. A path</p><formula xml:id="formula_1">p ji := [ℓ 0 = j → ℓ 1 → • • • → ℓ m = i] from j to i has length |p ji | = m,</formula><p>and the set of all such paths is denoted by P ji . We also write j ⇝ i for p ji .</p><p>Structural equation models (SEMs) supported on graphs provide a convenient way to capture causal dependencies between observed random variables. The underlying graph associates each node j with a random variable X j , and we say that X j causes X i (or j causes i) whenever there is a path j ⇝ i. Our definition of causal dependence is thus path-induced and founded on ancestral relations, rather than solely based on the presence of edges.</p><p>A DAG D = (V, E) is well-ordered if i &lt; j for all j ∈ pa(i), which we call a causal order. Given nodes i, j, k ∈ V , we say that X i is a confounder of X j and X k (or i is a confounder of j and k) if there exist distinct paths i ⇝ j and i ⇝ k which do not pass through k and j, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Solution of linear structural equation models via innovations</head><p>The LSEM X in (1.1) has a solution in terms of the innovation vector Z. Given a well-ordered DAG D = (V, E), we take the edge coefficient matrix C to be strictly upper-triangular. Let the diagonal matrix S have entries s ii serving as weights for the components of Z. The matrix C is nilpotent, so matrix inversion gives</p><formula xml:id="formula_2">X = (I -C) -1 SZ = AZ, (2.1)</formula><p>where we call A the (innovation) coefficient matrix. This implies that</p><formula xml:id="formula_3">X i = j∈An(i) a ij Z j , i ∈ V, (2.2)</formula><p>since a ij = 0 for j / ∈ An(i). In comparison to the LSEM in (1.1), the max-linear analogue is defined via</p><formula xml:id="formula_4">X i = j∈pa(i) c ij X j ∨ s ii Z i , i ∈ V, (2.3)</formula><p>with non-negative coefficients c ij and with ∨ denoting the maximum operator. A path analysis approach analogous to max-linear operations <ref type="bibr" target="#b14">[Gissibl and Klüppelberg, 2018]</ref> shows that the coefficients a ij of the LSEM equal</p><formula xml:id="formula_5">a ij = p ji ∈P ji d(p ji ) for j ∈ an(i), a ij = 0 for j ∈ V \ An(i), a ii = s ii , (2.4)</formula><p>with path weights d(p ji ) := s jj c k 1 j • • • c ik l-1 corresponding to the product of the edge weights along the path p ji . The coefficient a ij therefore equals the sum of all path weights from node j to node i, unlike the RMLM, where a ij is computed only for the max-weighted path.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Linear structural equation models with positive edge weights</head><p>While non-negative edge-weights constrain the model, positive dependence is of particular interest when studying extremes, which manifest in the behaviour of maxima and, in general, are supported on the positive orthant R d + . As seen in (2.4), non-negativity of the edge weights translates into non-negative path weights. By contrast, a negative edge coefficient c ij might lead to a negative coefficient a ij , which moves the support of X outside R d + . The study of maxima and peaks over thresholds in applications typically involves a preprocessing step to recenter and truncate the observations to the non-negative orthant or uses transformations of linear models <ref type="bibr">[Cooley and</ref><ref type="bibr">Thibaud, 2019, Lee and</ref><ref type="bibr" target="#b25">Cooley, 2022]</ref>. These papers focus only on extremal dependence on the positive orthant, corresponding to setting all negative entries of a general matrix A ∈ R d×d in (2.1) to zero. Proposition 2 shows that the matrix A provides information on the support of the angular measure, which fully characterises the dependence structure of multivariate extremes. The non-negativity constraint on the matrix C therefore serves as a natural remedy that ensures non-negativity of A in (2.1).</p><p>We now study some properties of the innovation coefficient matrix A, which resemble certain characteristics of the coefficient matrix in the RMLM. The next proposition characterises whether paths j ⇝ i between nodes i and j are of the form j ⇝ k ⇝ i for some k ∈ de(j) ∩ an(i), which proves to be important in deriving properties of the diagonal coefficients a ii of the coefficient matrix.</p><p>Proposition 1. Consider an LSEM+ supported on the DAG D = (V, E), with edge weight matrix C ∈ R d×d + and innovation coefficient matrix A ∈ R d×d + . For i ∈ V and j ∈ an(i), all paths j ⇝ i are of the form j ⇝ k ⇝ i, where k ∈ de(j) ∩ an(i), if and only if</p><formula xml:id="formula_6">a ij = a ik a kj a kk ; (2.5) otherwise a ij &gt; a ik a kj /a kk .</formula><p>The next corollary is a consequence of Proposition 1 and is analogous to Corollary 3.12 of <ref type="bibr" target="#b14">Gissibl and Klüppelberg [2018]</ref>.</p><p>Corollary 1. In the setting of Proposition 1, a ij ≥ a ik a kj /a kk for any nodes i, j, k of V .</p><p>We now consider the standardised innovation coefficient matrix with entries defined as ( Ā) ij = a α ij / d k=1 a α ik 1/α for α &gt; 0, which is used in deriving the extremal scalings in Lemma 2. The next lemma shows that the diagonal entries of Ā are the largest in their respective columns, a property that also holds for the coefficient matrix of an RMLM and is important in establishing asymmetries in the scaling methodology in Theorems 1 and 2.</p><p>Lemma 1. Let Ā ∈ R d×d + be the standardised innovation coefficient matrix of an LSEM+. Then ājj &gt; āij whenever i ̸ = j.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Multivariate regular variation</head><p>As our interest lies in the extremal behaviour of the LSEM+, we let the innovation vector Z ∈ R d + be regularly varying with index α &gt; 0, i.e., Z ∈ RV d + (α), and with independent and standardised components, i.e., for all i ∈ {1, . . . , d}, nP(n -1/α Z i &gt; z) → z -α for z &gt; 0 as n → ∞ . The LSEM+ has properties similar to max-linear models with independent regularly varying innovations <ref type="bibr" target="#b37">[Wang and Stoev, 2011</ref><ref type="bibr" target="#b14">, Gissibl and Klüppelberg, 2018</ref><ref type="bibr">, Klüppelberg and Krali, 2021]</ref>. Both are multivariate regularly varying and possess a discrete angular measure H X on the positive unit simplex Θ d-1</p><formula xml:id="formula_7">+ = {ω ∈ R d + : ∥ω∥ = 1}.</formula><p>The measure H X is not necessarily a probability measure but can be normalised into one, HX , following Remark 3. Details on multivariate regular variation, the angular representation and the angular measure are given in Appendix A; see also <ref type="bibr" target="#b31">Resnick [1987</ref><ref type="bibr" target="#b32">Resnick [ , 2007]]</ref>.</p><p>Our methodology is based on a particular extremal dependence measure introduced in Propositions 3 and 4 of <ref type="bibr" target="#b23">Larsson and Resnick [2012]</ref>; see also <ref type="bibr">Cooley and Thibaud [2019, § 4]</ref> and <ref type="bibr">Klüppelberg and Krali [2021, § 2.2]</ref>.</p><formula xml:id="formula_8">Definition 1. Let X ∈ RV d + (2) have angular measure H X on Θ d-1</formula><p>+ and angular representation (R, ω) = (∥X∥, X/R). For i, j ∈ {1, . . . , d} define the extremal dependence measure</p><formula xml:id="formula_9">σ 2 ij = σ 2 X ij := Θ d-1 + ω i ω j dH X (ω), ω = (ω 1 , . . . , ω d ) ∈ Θ d-1 + .</formula><p>We call σ i := σ X ii the scaling parameter, or the scaling, of X i .</p><p>The next proposition is similar to results that characterise the angular measure H X from <ref type="bibr" target="#b12">Fougères et al. [2013]</ref> and <ref type="bibr" target="#b5">Cooley and Thibaud [2019]</ref>, adapted to account for linear operations. In this paper we work with the regular variation index α = 2 and the Euclidean norm ∥ • ∥. In case of the LSEM+ (1.1), this allows a convenient representation of both the angular measure and the scalings from the entries of the coefficient matrix A.</p><p>Proposition 2. Consider the LSEM+ vector X = AZ, where Z ∈ RV d + (2) has standardised margins and A ∈ R d×d + . Then (i) the angular measure H X is supported on</p><formula xml:id="formula_10">(a k /∥a k ∥)= (a k /( d i=1 a 2 ik ) 1/2 ) for k ∈ V ; (ii) σ 2 ij = (AA ⊤ ) ij = d k=1 a ik a jk for i, j ∈ V ; and</formula><p>(iii) each margin X i has squared scaling</p><formula xml:id="formula_11">σ 2 i = d k=1 a 2 ik for i ∈ V .</formula><p>As is common in extremes, we only work with the standardised LSEM+ vector X, obtained from (1.1) by standardizing the innovation coefficient matrix A. The discussion preceding Lemma 1 then implies that we are working directly with Ā, i.e., X = ĀZ, whereby Proposition 2 entails unit scalings for the margins.</p><p>We summarise the assumptions used throughout the rest of the paper.</p><p>Assumptions A:</p><p>(A1) The innovations vector Z ∈ RV d + (2) has independent and standardised components.</p><p>(A2) The choice of norm is the Euclidean norm, denoted by ∥ • ∥.</p><p>(A3) The components of X are standardised.</p><p>In preparation for Theorems 1 and 2 and the examples of the next section, we briefly summarise some important properties of the scalings. In particular, we make use of maxima over the rescaled components of the vector X under Assumptions A. Lemma 2, which is a consequence of the discussion in Appendix A.3, adapts Lemma 6 of <ref type="bibr">Klüppelberg and Krali [2021]</ref> to the LSEM+ and employs Lemma 1 to characterise the scalings of the maxima M I = max(X i : i ∈ I), defined as</p><formula xml:id="formula_12">σ 2 M I = Θ d-1 + ∨ i∈I ω 2 i dH X (ω),</formula><p>in terms of the coefficient matrix A.</p><p>Lemma 2 <ref type="bibr">(Klüppelberg and Krali [2021]</ref>). Let X be an LSEM+ satisfying Assumptions A.</p><p>Then the random variable M I := max(X i , i ∈ I) lies in RV + (2) and its squared scaling equals</p><formula xml:id="formula_13">(i) σ 2 M I = d k=1 i∈I a 2 ik if I ⊊ V , then; and (ii) σ 2 M I = d k=1 a 2 kk if I = V .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Structure learning</head><p>Structure learning, or causal discovery, often relies on specific assumptions on the graphical structure supporting the random variables. These typically amount to assuming causal sufficiency <ref type="bibr">[Spirtes et al., 2000, p. 45]</ref>, which is closely related to Markovianity of the model <ref type="bibr">[Pearl, 2009, p. 30]</ref>, or to no 'hidden confounding', which ensures that there are no unmeasured sources of error <ref type="bibr">[Pearl, 2009, p. 62</ref>]. While Markovianity may not hold in practice, causing it to attract much criticism <ref type="bibr">[Pearl, 2009, p. 252]</ref>, it is nevertheless exploited by many methods, including the graphical lasso <ref type="bibr" target="#b13">[Friedman et al., 2008]</ref>, which relies on conditional independence properties of the multivariate Gaussian distribution. If there are no hidden confounders in an LSEM+, we can uncover its causal structure recursively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Identification of source nodes</head><p>We first identify the source nodes by exploiting scaling asymmetries in pairwise maxima of the node variables. Our approach differs from that of <ref type="bibr">Klüppelberg and Krali [2021]</ref>, who consider maxima over the d node variables of the DAG. Instead, we focus on maxima of lower-dimensional subvectors, which are important in determining the angular measure: the approach of <ref type="bibr">Klüppelberg and Krali [2021]</ref> uses the d-dimensional angular measure of X, whereas the approach we propose relies on the angular measure of the corresponding subvector. We address this difference in the following remark, which discusses some disadvantages of high-dimensional angular measures.</p><p>Remark 1. Consider the LSEM+ vector X ∈ RV d + (2) and its subvector X ij = (X i , X j ) with normalised angular measures HX and HX ij , respectively supported on Θ d-1 + and Θ + . Proposition 2 (i) shows that the columns of the coefficient matrix A provide the support for HX , and that in combination with representation (2.2) we can index these columns according to the nodes. Similarly, we can combine the i-th and j-th rows of A into the matrix</p><formula xml:id="formula_14">A (ij) ∈ R 2×d</formula><p>+ , whose columns a (ij) ∈ R 2 + provide the support for HX ij . Suppose that a j has entries such that a jj &gt; 0 and a ij = 0 for i ̸ = j, which implies that j / ∈ an(i). Typical examples of such nodes j are the descendants of a lower order on a DAG. Consider the two measures HX and HX ij :</p><p>(i) under HX , we invoke Proposition 5 and find that the probability of observing realisations supported on the atom a j /∥a j ∥ equals ∥a j ∥/d = a jj /d;</p><p>(ii) under HX ij , the probability of observing realisations supported on the atom a</p><formula xml:id="formula_15">(ij) j /∥a (ij) j ∥ equals ∥a (ij) j ∥/2 = a jj /2.</formula><p>Remark 1 illustrates that the angular measure HX ij may be preferable to HX for a high dimension d, as the former assigns higher probability to the atoms a (ij) j , whose entries are important in the computation of the scalings. Similar reasoning applies when there are very small, but positive, entries a ij . We show in Theorem 1 that deriving the scalings from the measures HX ij over all pairs i ̸ = j carries necessary and sufficient information to identify the source nodes, which implies that there is no loss of information from not using HX as in Theorem 2 of <ref type="bibr">Klüppelberg and Krali [2021]</ref>. However, the main drawback of using HX ij is the cost of computing scalings over the available d(d -1) pairs in the DAG, whereas the approach of <ref type="bibr">Klüppelberg and Krali [2021]</ref> only requires computations of scalings over d maxima.</p><p>We use the following notation for partially rescaled maxima: for a &gt; 1 we write</p><formula xml:id="formula_16">M i,aj := max(X i , aX j ), i, j ∈ V. (4.1)</formula><p>The next theorem exploits the asymmetry between the scalings of M i,j and M i,aj in providing a criterion that helps to identify the source nodes in an LSEM+.</p><p>Theorem 1. Let X ∈ RV d + (2) be an LSEM+ with innovation coefficient matrix A. Then j is a source node on a well ordered DAG if and only if there exists a scalar a &gt; 1 such that</p><formula xml:id="formula_17">σ 2 M i,aj -σ 2 M ij = a 2 -1, for all i ̸ = j. (4.2) If j is not a source node, then σ 2 M i,aj -σ 2 M ij ≤ a 2 -1, with strict inequality if i ∈ an(j).</formula><p>In order to illustrate the difference between this result and Theorem 2 in Klüppelberg and <ref type="bibr">Krali [2021]</ref> we re-consider their Example 3 with d = 4.</p><p>Example 1. Let X satisfy the setting in Theorem 1 with innovation coefficient matrix A and corresponding DAG given in Figure <ref type="figure">1</ref>.</p><formula xml:id="formula_18">A =        a 11 a 12 a 13 a 14 0 a 22 a 23 a 24 0 0 a 33 0 0 0 0 a 44        4 3 2 1</formula><p>Figure <ref type="figure">1</ref>: Innovation coefficient matrix A and corresponding DAG.</p><p>We first investigate whether node 2 is a source node for the DAG in Figure <ref type="figure">1</ref>. Set a &gt; 1, i = 4 and j = 2, and compute</p><formula xml:id="formula_19">σ 2 M 4,a2 -σ 2 M 4,2 = a 2 (a 2 22 + a 2 23 ) + (a 2 a 2 24 ) ∨ a 2 44 -(a 2 22 + a 2 23 + a 2 44 ) &lt; (a 2 -1). (4.3)</formula><p>The last inequality follows on noting that if (a 2 a 2 24 )∨a 2 44 = a 2 44 , then (a 2 a 2 24 )∨a 2 44 -a 2 44 = 0; if not, then (a 2 a 2 24 )∨a 2 44 = a 2 a 2 24 , in which case (a 2 a 2 24 )∨a 2 44 -a 2 44 &lt; (a 2 a 2 24 )-a 2 24 = (a 2 -1)a 2 24 , by Lemma 1. The last inequality, together with Assumption (A3), whereby the normalised rownorms give a 2 22 + a 2 23 + a 2 24 = 1, produces the upper bound (a 2 -1)(a 2 22 + a 2 23 + a 2 24 ) = a 2 -1 in (4.3). Thus node 2 does not satisfy Theorem 1, so it is not a source node.</p><p>If j = 4, then for i ∈ {1, 2, 3} Lemma 1 implies that</p><formula xml:id="formula_20">σ 2 M i,a4 -σ 2 M i,4 = j̸ =4 a 2 ij + a 2 a 44 - j̸ =4 a 2 ij -a 44 = a 2 -1.</formula><p>Hence Theorem 1 entails that 4 is a source node. The same equalities hold for j = 3, so both 3 and 4 are source nodes. Theorem 2 of <ref type="bibr">Klüppelberg and Krali [2021]</ref>, by contrast, only requires the computation of the difference</p><formula xml:id="formula_21">σ 2 M 1,2,3,a4 -σ 2 M 1,2,3,4</formula><p>, which is derived from a four-dimensional angular measure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Identification of descendants</head><p>In this section we assume that we have identified and reordered p nodes of the DAG and grouped them into an ordered set</p><formula xml:id="formula_22">I := (i d-p+1 , . . . , i d ), so that i k 1 &gt; i k 2 if i k 1 ∈ pa(i k 2 )</formula><p>; we write I c = V \ I for the set of remaining unidentified (or unordered) nodes. We assume that there are no ancestors of I amongst the unidentified nodes, i.e., An(I) = I. A typical example for I is the set of source nodes, which clearly satisfies these properties and can be identified by employing Theorem 1, as demonstrated in Example 1.</p><p>Similarly to (4.1), but accounting for the non-empty ordered set I, we define for a &gt; 1, M i,aj,aI := max(X i , aX j , aX I ), i, j ∈ I c . (4.4)</p><p>The following theorem exploits asymmetries in the scalings of M i,aj,aI and provides a criterion for node j to have no ancestors other than the identified ones in I.</p><p>Theorem 2. Let X ∈ RV d + (2) be an LSEM+ with innovation coefficient matrix A. Let I denote the set of identified and ordered p nodes having no ancestors outside I. Then j ∈ I c has no ancestors outside I if and only if there exists a scalar a &gt; 1 such that</p><formula xml:id="formula_23">σ 2 M i,aj,aI -σ 2 M i,j,I = (a 2 -1)σ 2 M j,I , for all i / ∈ I ∪ {j}. (4.5) Otherwise, σ 2 M i,aj,aI -σ 2 M ijI ≤ (a 2 -1)σ 2 M j,I , with strict inequality for i ∈ I c ∩ an(j).</formula><p>When two nodes, say j 1 and j 2 , satisfy eq. (4.5), then there is no causal link between them, as summarised in the next corollary.</p><p>Corollary 2. In the setting of Theorem 2, suppose that the nodes j 1 , j 2 / ∈ I satisfy (4.5). Then j 1 / ∈ an(j 2 ) and j 2 / ∈ an(j 1 ).</p><p>We illustrate Theorem 2 by reconsidering the DAG in Example 1.</p><p>Example 2. Suppose that in Example 1 we have identified the two source nodes 3 and 4: hence I = (3, 4). Let j = 1, and suppose we want to check whether 1 ∈ de(2). Then,</p><formula xml:id="formula_24">σ 2 M 2,a1,aI -σ 2 M 2,1,I = a 2 a 2 11 + a 2 a 2 12 ∨ a 2 22 + a 2 a 2 33 + a 2 a 2 44 - 4 k=1 a 2 kk = (a 2 -1) k̸ =2 a 2 kk + a 2 a 2 12 ∨ a 2 22 -a 2 22 &lt; (a 2 -1) k̸ =2 a 2 kk + (a 2 -1)a 2 12 = (a 2 -1)σ 2 M I,j ,</formula><p>where the first equality applies Lemma 2, and the inequality follows from Lemma 1 and arguments similar to those in (4.3). Since the inequality is strict, Theorem 2 implies that 1 ∈ de(2). Instead, if we take i = 1 and j = 2 and follow the same procedure, we reach equality (4.5), indicating that 2 / ∈ de(1). This gives the causal ordering I = (1, 2, 3, 4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">A consistent algorithm to estimate a causal ordering</head><p>We now combine the results of Sections 4.1 and 4.2 into an algorithm to identify a causal ordering. We use Theorem 1 to initialise Algorithm 1 (with I = ∅), and then apply Theorem 2 until the d nodes have been ordered. We defer details on the estimation of the scalings to Appendix C, where we also show that their consistency translates into consistency of our algorithm. We first outline the elements of Algorithm 1:</p><p>-the matrix ∆ Î ∈ R d×d , with entries</p><formula xml:id="formula_25">(∆ Î ) ij = σ2 M i,aj,aI -σ2 M i,j,I -(a 2 -1)σ 2 M j,I , i, j ∈ V \ I; (4.6) (∆ Î ) ij = ∞, i or j ∈ I;</formula><p>Algorithm 1 Estimation of a causal order of the LSEM+ X Input:</p><formula xml:id="formula_26">(X i : i ∈ {1, . . . , n}) from X ∈ RV d + (2) with standard margins, a &gt; 1, ε &gt; 0, I = ∅, ∆ Î = (0) d×d , δ Î = (0) 1×d ; Output: The ordered set I 1: procedure 2: while |I| &lt; d do 3: Update ∆ Î using (4.6) 4: Set δ Î = colMins V (∆ Î ) -max(colMins I c (∆ Î )) 5: π = arg sort { p ∈ I c : |δ Î,p | ≤ ε Î } δ Î 6:</formula><p>Update I by adding π: I ← (π, I)</p><formula xml:id="formula_27">7:</formula><p>end while 8:</p><p>return I</p><p>-the term ε Î := ε max colMins I c ∆ Î , where ε &gt; 0 and the operator colMins takes the entrywise minimum of each column not indexed in I of the matrix ∆I ; and</p><formula xml:id="formula_28">-the vector δ Î = (δ Î,1 , . . . , δ Î,d ).</formula><p>At each iteration of the while loop in Algorithm 1, we update ∆ Î and ε Î by accounting for the set I of identified nodes.</p><p>We briefly illustrate the motivation behind Algorithm 1, in particular for the ε term. To do so, we reconsider Example 1 and go through the first iteration when I = ∅.</p><p>Remark 2. Consider the LSEM+ X with DAG in Figure <ref type="figure">1</ref> and suppose that I = ∅. We look at the theoretical variants of the estimates used in Algorithm 1, i.e., the d × d matrix ∆ I , computed from the theoretical scalings.</p><p>For j ∈ {3, 4} and i ̸ = j, it follows from Example 1 that the vector colMins V (∆ I ) equals (c 1 , c 2 , c 3 , c 4 ), with c 3 = c 4 = 0, c 1 and c 2 strictly negative, and ε I = 0. Line 4 of the algorithm gives δ I = (c 1 , c 2 , 0, 0). For finitely many observations, however, the estimated vector δ Î = (ĉ 1 , ĉ2 , ĉ3 , ĉ4 ) is almost surely different from δ I , and, if ε &gt; 0, then ε Î &gt; 0. If ε = 0 we may then only select one of the source nodes between 3 and 4, but if we let ϵ &gt; 0, we allow for a difference between ĉ3 and ĉ4 , whereby we may select nodes 3 and 4 as source nodes in line 5 of the algorithm. The error term preserves consistency in selecting the correct nodes: to see why, note that ĉ3 , ĉ4 and ε Î approach zero in probability, and ĉ1 and ĉ2 respectively converge to c 1 and c 2 . The introduction of the ε term therefore enables the selection of more than one source node at a time. When I ̸ = ∅, a similar reasoning applies to the nodes with ancestors in I.</p><p>The methodology proposed in <ref type="bibr">Klüppelberg and Krali [2021, § 7]</ref> relies on fixed error terms ε 1 and ε 2 which, if too small, may prevent their algorithm from re-ordering the nodes. In contrast, Algorithm 1 updates ε Î for each iteration, ensuring that a causal ordering is returned.</p><p>The next proposition, proved in Appendix 3, establishes consistency of Algorithm 1.</p><p>Proposition 3. Let X ∈ RV d + (2) be an LSEM+ with coefficient matrix A. Let I = (i 1 , . . . , i d ) denote the output of Algorithm 1. Then I is a consistent estimator of a causal ordering of the DAG supporting X.</p><p>5 Simulation study and data application<ref type="foot" target="#foot_0">foot_0</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Methods</head><p>In this section we evaluate the empirical performance of Algorithm 1 by simulation and an application to two river discharge datasets. We compare our method to EASE <ref type="bibr" target="#b15">[Gnecco et al., 2021]</ref>, the only alternative method that can infer causal orderings from extreme observations for the LSEM. EASE is based on the causal tail coefficient,</p><formula xml:id="formula_29">Γ ij = lim u→1 - E[F j (X j )|F i (X i ) &gt; u],</formula><p>where F i and F j denote the distribution functions of X i and X j . The Γ ij are computed for all available pairs to estimate a causal ordering. To implement EASE we use the R package causalXtremes <ref type="bibr" target="#b15">[Gnecco et al., 2021]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Simulation setup</head><p>The simulations are carried over random DAGs which are used to support the LSEM+'s. To give a variety of such models, we consider several configurations of the dimension d and the sparsity p of the DAGs, and of the parameters of the LSEM+, namely the index of regular variation α and the sample size n. For every combination of d ∈ {20, 40, 50}, p ∈ {0.05, 0.1}, α ∈ {2, 3} and n ∈ {1000, 5000}, we generate 50 random DAGs and compare the two methods.</p><p>We construct each DAG via its upper-triangular adjacency matrix C ∈ R d×d + , with edges drawn independently from a Bernoulli distribution with success probability p. The edge weights in C and the innovation weights s ii of S in (1.1) are drawn as independent U[0.1, 1.5] random variables.</p><p>The innovation vector Z has independent |t α |-distributed components, where t α denotes a Student-t distribution with α degrees of freedom.</p><p>As choice of input parameters for Algorithm 1 we consider ϵ ∈ {0.1, 0.4} and fix the scalar parameter a = 1.3. Appendix D.1 investigates its stability for other choices of a.</p><p>To estimate the scaling parameters for Algorithm 1, we employ the empirical probability integral transform to first standardise the original observations to standard Fréchet(2) margins, and then use the k largest thresholded observations. This procedure is described in Appendix C. We apply the one-tailed version of EASE with the prespecified choice k = n ⌊0.4⌋ , which was found to yield the best performance in <ref type="bibr" target="#b15">Gnecco et al. [2021]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Evaluation metrics</head><p>To evaluate the performance of Algorithm 1 we use the Structural Intervention Distance (SID) metric <ref type="bibr" target="#b30">[Peters and Bühlmann, 2015]</ref> which was used to assess the performance of EASE in <ref type="bibr" target="#b15">Gnecco et al. [2021]</ref>. This metric focuses on causal relations in a DAG and employs intervention distributions via parental adjustments between pairs of nodes. This renders SID the natural choice in evaluating differences between causal orderings. The metric can be used to quantify the distance between two DAGs, or a DAG and a completed partially directed acyclic graph (CPDAG). We use the implementation of the metric in the package causalXtremes <ref type="bibr" target="#b15">[Gnecco et al., 2021]</ref>, where SID is normalised to take values in [0, 1], with lower values indicating a smaller distance and thus a better performance. A SID equal to zero indicates valid causal orderings on both DAGs. In the simulation study we compute the SID between the true DAG and the fully connected DAG corresponding to the estimated causal ordering. The latter is obtained by adding the edges j → i if j &gt; i.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Simulation results</head><p>Figures 2 and 3 summarise our findings. The performances of both algorithms are affected by the choice of the parameters. In particular, both Algorithm 1 and EASE perform worse in higher-dimensional and denser regimes, which give rise to more complex dependence structures with more edges and parental adjustments. The difference in performance can be seen by comparing SID between the settings with p = 0.05 and p = 0.1 in Figure <ref type="figure" target="#fig_0">2</ref>.</p><p>Figure <ref type="figure">3</ref> shows that Algorithm 1 performs better in the sparser regime for a suitable choice of threshold k, whereas EASE is better in the denser, higher-dimensional setup and for the heavier tail, i.e., (d, p, α) = (50, 0.1, 2).</p><p>The performances of the algorithms are affected by the regular variation index α, which influences the rate of convergence of the respective componentwise maxima to their limiting Fréchet distributions <ref type="bibr">[Resnick, 1987, Prop. 2.12]</ref>. In particular, a lower value of α corresponds to a quicker convergence rate and leads to better performance of both methods. However, a comparison between the configurations (d, p, α) = (50, 0.1, 2) and (d, p, α) = (50, 0.1, 3) shows a larger difference in SID for EASE, which is less robust to lighter tails.</p><p>Finally, the results for n = 5000 are provided in Appendix D and show better performance in both methods, with Algorithm 1 still outperforming EASE in the sparser configuration. We also see greater stability of the former for different thresholds k. EASE retains a mild advantage over Algorithm 1 in the denser setup and for α = 2, perhaps because of the faster convergence of the tail dependence with higher n, and the fact that EASE accounts for hidden confounders in pairwise dependencies. Algorithm 1 performs better for α = 3, even in the denser regime.</p><p>As our method estimates the scalings from the angular measure of a vector of dimension up to d, as described in Appendix C, it also requires more thresholded observations k to attain a better performance. This can be seen for the larger values of k when n = 1000.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Data application</head><p>We now attempt to infer causal orderings for datasets of river discharges from the upper Danube and the Rhine basin in Switzerland. Given the results in the simulation study and the sparse regime of the networks, we fix a = 1.3 and ε = 0.4 in Algorithm 1. The procedure for estimating the scalings is identical to that in the simulation study.</p><p>The DAG adjacency matrices of the networks are constructed based on the physical flow connections between the gauging stations.</p><p>To evaluate the robustness of the algorithms when the assumption of independent and identically distributed observations fails, we study the stability of the SID metric by analysing observations from both unprocessed and declustered datasets, following a multivariate declustering procedure introduced by Asadi et al. <ref type="bibr">[2015]</ref> and described in Section 5.6.2. This leaves only those observations that are approximately independent in time. To assess uncertainty in the causal orderings, we evaluate the SID metric over 100 bootstrap replications. EASE (d=50, p=0.1,</p><formula xml:id="formula_30">α = 3)</formula><p>Figure <ref type="figure">3</ref>: Scatterplots of the SID of the causal orderings of Algorithm 1 and of EASE evaluated for 50 random DAGs for each configuration of (d, p, α) and for n = 1000.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Upper Danube basin</head><p>The Danube dataset, available from <ref type="url" target="http://gdk.bayern.de">http://gdk.bayern.de</ref>, or as a supplemental file of <ref type="bibr" target="#b0">Asadi et al. [2015]</ref>, has become a benchmark for testing methodologies in extremal graphical modelling <ref type="bibr" target="#b9">[Engelke and Hitz, 2020</ref><ref type="bibr" target="#b15">, Gnecco et al., 2021</ref><ref type="bibr" target="#b27">, Mhalla et al., 2020</ref><ref type="bibr" target="#b36">, Tran et al., 2024]</ref>.</p><p>The data span the months June-August from 1960 to 2010, thus eliminating any seasonality, and consists of n = 4692 daily discharges at d = 31 gauging stations, depicted in Figure <ref type="figure" target="#fig_1">4</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6.1">Unprocessed Danube data</head><p>The analysis of <ref type="bibr" target="#b0">Asadi et al. [2015]</ref> indicates large variations in discharges across the stations at different altitudes. Since our methodology requires RV + (2) random variables, we employ the empirical probability integral transform to obtain standard Fréchet(2) margins (see Appendix C). Figure <ref type="figure">5</ref> summarises the performances of the methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6.2">Declustered Danube data</head><p>Both Algorithm 1 and EASE yield consistent causal orderings for independent observations from an LSEM+. Independence, however, is questionable, as high discharges typically persist for a number of days, and thus cluster in time. To account for this we apply the multivariate declustering procedure of <ref type="bibr" target="#b0">Asadi et al. [2015]</ref>, which sequentially selects non-overlapping windows of l days of observations for each of the 50 summer periods. These windows are initially taken around the highest observations in each series and across all stations. Each window retains only the largest observation; we repeat this procedure until there are no time-windows with l consecutive observations. This processing step with time windows of width l = 9 yields n = 428 approximately independent observations. Figure <ref type="figure">5</ref> provides the SID for the declustered Danube data. We assess its uncertainty for 100 bootstrap replicates, which show that, despite the dependence in time, Algorithm 1 performs better than EASE for both the unprocessed and the declustered data. Figure 5: Boxplots of the SID of the causal orderings of Algorithm 1 and of EASE for 100 bootstrap replicates of the unprocessed (left) and declustered (right) Danube datasets. The orange diamonds show the SID evaluated for the observed data for each method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.7">Rhine basin</head><p>This dataset consists of daily river discharge amounts from d = 68 gauging stations along the Rhine basin in Switzerland and was studied in <ref type="bibr" target="#b1">Asadi et al. [2018]</ref>. The data range from 1913 to 2014, but we only select those n = 2024 observations when measurements are available across all 68 stations. To construct the adjacency matrix of the DAG underlying the graphical model of the river network, we start from the physical flow connections in <ref type="bibr" target="#b1">Asadi et al. [2018]</ref>, and put edges only between neighbouring stations, with direction corresponding to downstream flow. Station 67 has the highest in-degree due to incoming flows from most of the stations; see Figure <ref type="figure" target="#fig_3">6</ref>.  <ref type="bibr" target="#b1">[Asadi et al., 2018]</ref>.</p><p>Preliminary assessment of the pairwise dependencies amongst the stations reveals correlations above 0.99 for three pairs of stations in close proximity, namely <ref type="bibr">(48, 49), (63, 64)</ref> and <ref type="bibr">(67,</ref><ref type="bibr">68)</ref>. As our methodology exploits asymmetries in scalings, such strong dependence complicates the identification of a causal direction, especially when coupled with high dimensionality and time dependence.</p><p>To see how robust Algorithm 1 is to the presence of strong dependence amongst pairs in the river network, we conduct a parallel analysis by removing stations 48, 63 and 67. The new DAG is obtained by putting edges only between the 65 neighbouring stations. For instance, for k ∈ pa(j) and j ∈ pa(i), we add edges k → i after removing node j ∈ {48, 63, 67}. Since none of the removed nodes is a confounder, the reduced DAG has no hidden confounding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.7.1">Unprocessed Rhine data</head><p>In this section we work with the unprocessed discharges. We evaluate the performance of Algorithm 1 and EASE for both the full river network consisting of d = 68 stations and the reduced one of d = 65 stations.</p><p>The left-hand panel in Figure <ref type="figure" target="#fig_4">7</ref> indicates superior performance for EASE when analysing the full river network. The uncertainty boxplots for Algorithm 1 show a pattern similar to the Danube data but with higher variation of SID, possibly due to the higher dimensionality.</p><p>The effect of time dependence of high discharges is likely to be more pronounced in Algorithm 1, which relies on a larger number k of thresholded observations to approximate higher-dimensional angular measures.</p><p>The results with d = 65 stations in Figure <ref type="figure" target="#fig_4">7</ref> show a similar performance from Algorithm 1, yet reveal a drastic worsening for EASE. This is surprising given that the latter also works in settings with hidden nodes. Nevertheless, EASE relies on comparisons between pairwise causal tail coefficients, and leaving out nodes seems to make their algorithm prone to high variability, thus pointing to high sensitivity to the coefficients Γ ij .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.7.2">Declustered Rhine data</head><p>In this analysis we decluster the observations using time windows of width l = 9. The bottom panel in Figure <ref type="figure" target="#fig_4">7</ref> contains the results for the full (d = 68) and the reduced (d = 65) networks, and show an improved performance of Algorithm 1 relative to unprocessed data, reflected in a reduction of the SID and of its uncertainty. For d = 68 and k = 60, Algorithm 1 performs like EASE, and its performance remains robust across both networks. Interestingly, we see the same pattern as in the top panel: the performance of EASE is adversely affected in the reduced network. Although declustering seemingly leads to an improvement of SID for the latter, it also increases its variability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this work we propose a scaling methodology for causal discovery in heavy-tailed linear structural equation models, based on scaling parameters derived from the angular measure that characterises multivariate extremal dependence. Following consistency of the estimators of the scalings we show that our causal discovery algorithm yields a consistent causal ordering. We employ the SID metric <ref type="bibr" target="#b30">[Peters and Bühlmann, 2015]</ref>, derived from structural interventions, to evaluate the performance of our algorithm in a simulation study and a data application to river discharges. Comparison with EASE <ref type="bibr" target="#b15">[Gnecco et al., 2021]</ref> shows that our methodology is competitive with the latter and can be more robust.</p><p>Causal dependence modelling in extremes is in its infancy, and high-dimensional multivariate extremal analysis remains challenging. The data applications pinpoint vulnerabilities of both methods, with EASE being highly sensitive to pairwise dependencies of the node variables and to lighter tails, and Algorithm 1 being sensitive to the increasing dimension of the angular measure in denser graphs. Interesting directions for future work include investigating the possibilities for further reduction in the dimension of the angular measure, which may aid in constructing better estimators for the scalings.</p><p>Lemma 3. Let Z ∈ RV + (α) be a standardised random variable, and let a ∈ R d + . Then aZ has exponent measure</p><formula xml:id="formula_31">ν aZ ([0, x] c ) = max i∈{1,...,d} a α i x α i , x ∈ R d + \ {0}.</formula><p>Proof. From Lemma 6.1 in <ref type="bibr" target="#b32">Resnick [2007]</ref>, convergence on the rectangles [0, x] c for x ∈ R d + which are continuity points of ν aZ is equivalent to convergence in M + ([0, ∞] d \{0}). Therefore it suffices to consider only such rectangular sets, for which</p><formula xml:id="formula_32">nP (aZ/b n ∈ [0, x] c ) = nP d ∪ i=1 {a i Z &gt; b n x i } = nP d ∪ i=1 Z &gt; b n x i a i = nP Z &gt; b n min i∈{1,...,d} x i a i = min i∈{1,...,d} x i a i -α = max i∈{1,...,d} a α i x α i ,</formula><p>proving the claim.</p><p>We turn our attention to the following result from <ref type="bibr">Resnick [2007, Proposition 7.4</ref>].</p><p>Proposition 4. Let the independent vectors X, Y ∈ RV d + (α) be defined on the same probability space and with the same normalising sequence b n . Then</p><formula xml:id="formula_33">nP X + Y b n ∈ • v → ν X (•) + ν Y (•) (A.4) in M + ([0, ∞] d \ {0}).</formula><p>Using Lemma 3 and Proposition 4, we obtain the following.</p><formula xml:id="formula_34">Proposition 5. Consider A ∈ R d×d + and Z ∈ RV d + (α).</formula><p>Then AZ is multivariate regularly varying and its angular measure on Θ d-1</p><formula xml:id="formula_35">+ equals H AZ (•) = d j=1 ∥a j ∥ α δ a j ∥a j ∥ (•),</formula><p>where a j is the j-th column of the matrix A.</p><p>Proof. We may write</p><formula xml:id="formula_36">AZ = d j=1 (a j Z j ) ⊤ .</formula><p>As in Lemma 3, we need only prove convergence on the rectangles [0, x] c for x ∈ R d + \ {0}. Proposition 4 applied to the vectors a j Z j gives</p><formula xml:id="formula_37">ν AZ ([0, x] c ) = d j=1 max i∈{1,...,d} a α ij x α i .</formula><p>The angular decomposition from Definition 2(b) gives</p><formula xml:id="formula_38">ν AZ ([0, x] c ) = Θ d-1 + i∈{1,...,d} x i ω i -α H AZ (dω),</formula><p>and it follows that H AZ (•) = d j=1 ∥a j ∥ α δ {a j /∥a j ∥} (•).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Scalings</head><p>In this section we work under Assumptions A and consider the LSEM+ vector X = AZ with Z ∈ RV d + (2) and with exponent and angular measures ν X and H X . We use Definition 2 (b) and recall that the scalings for the i-th margin of X can be expressed as</p><formula xml:id="formula_39">lim n→∞ nP(X i /b n &gt; x) = ν X x ∈ R d + : x ∥x∥ ∈ Θ d-1 + , x i &gt; x = {ω∈Θ d-1 + } {r&gt;x/ω i } 2r -3 drdH X (ω) = x -2 {ω∈Θ d-1 + } ω 2 i dH X (ω) = x -2 σ 2 i ,</formula><p>with σ 2 i obtained by setting x = 1. Proposition 5 gives</p><formula xml:id="formula_40">σ 2 i = Θ d-1 + ω 2 i dH X (ω) = d k=1 ∥a k ∥ 2 a 2 ik ∥a k ∥ 2 = d k=1 a 2 ik , i ∈ {1, . . . , d}.</formula><p>Following the discussion on page 330 of <ref type="bibr" target="#b2">Beirlant et al. [2004]</ref>, who assume standardisation with α = 1, a particularly useful choice of the norm is ∥•∥ α (without loss of generality for α ≥ 1). Taking α = 2 and the Euclidean norm ∥•∥ gives</p><formula xml:id="formula_41">H X (Θ d-1 + ) = Θ d-1 + d i=1 ω 2 i dH X (ω) = d i=1 σ 2 i = d i=1 d j=1 a 2 ij = d, (A.5)</formula><p>which follows from Assumption (A3) on the standardised rows of A by the ∥•∥ norm.</p><p>Finally, for M I = max(X i : i ∈ I), Proposition 5 gives</p><formula xml:id="formula_42">σ 2 M I = Θ d-1 + ∨ i∈I ω 2 i dH X (ω) = d k=1 ∥a k ∥ 2 ∨ i∈I a 2 ik ∥a k ∥ 2 = d k=1 ∨ i∈I a 2 ik .</formula><p>An application of Lemma 1 to the computed scalings gives Lemma 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Proofs for Sections 2 and 4</head><p>Proof of Proposition 1. We mimic the steps used in the proof of Theorem 3.10 of <ref type="bibr" target="#b14">Gissibl and Klüppelberg [2018]</ref>, but adjust for linear operations. By (2.4) and the definition of path weights we have that d(p jk ) = s jj c jl 1 • • • c l v-1 i for some path length v. Likewise we obtain d(p ki ) for the path k ⇝ i. Then, for the path j ⇝ k ⇝ i, composed of p jk and p ki and denoted by [p jk , p ki ], we compute the path weight</p><formula xml:id="formula_43">d([p jk , p ki ]) = d(p jk )d(p ki )/s kk = d(p jk )d(p ki )/a kk .</formula><p>For ease of notation we write K ji for the set of paths from i to j that pass through k, and K c ji for those which do not. We note that we can also set the node indices i or j to k. These two sets form a partition of the paths from j to i, so we may write</p><formula xml:id="formula_44">a ij = p ji ∈K ji d(p ji ) + p ji ∈K c ji d(p ji ) = {p jk ∈K jk , p ki ∈K ki } d([p jk , p ki ]) + p ji ∈K c ji d(p ji ) = {p jk ∈K jk , p ki ∈K ki } d(p jk )d(p ki )/a kk + p ji ∈K c ji d(p ji ) = a -1 kk p jk ∈K jk d(p jk ) p ki ∈K ki d(p ki ) + p ji ∈K c ji d(p ji ) = a kj a ik /a kk + p ji ∈K c ji d(p ji ).</formula><p>Now suppose that a ij &gt; a ik a kj /a kk . Because C and A are non-negative, this can happen if and only if K c ji is non-empty. Otherwise, a ij = a ik a kj /a kk , thus proving the proposition.</p><p>Proof of Lemma 1. First note that the set An(i) \ An(j) contains i and thus is non-empty.</p><p>We then obtain a α jj k∈An(i)\An(j) a α ik &gt; 0, which is equivalent to</p><formula xml:id="formula_45">a α jj l∈An(j) a α ij a α jl a α jj + a α jj k∈An(i)\An(j) a α ik &gt; a α ij l∈An(j) a α jl ,</formula><p>and re-arranging the terms gives</p><formula xml:id="formula_46">a α jj l∈An(j) a α jl &gt; a α ij l∈An(j) a α ij a α jl a α jj + k∈An(i)\An(j) a α ik .</formula><p>We apply Corollary 1 to the inequality in the last display and find</p><formula xml:id="formula_47">a α jj l∈An(j) a α jl &gt; a α ij l∈An(j) a α il + k∈An(i)\An(j) a α ik ,</formula><p>which corresponds to āα jj &gt; āα ij or equivalently ājj &gt; āij , and thus proves the claim of the lemma.</p><p>Proof of Theorem 1. (⇒) Suppose that j is a source node. Then an(j) = ∅ by definition, so a jk = 0 for all k ̸ = j, and a jj = 1.</p><p>Consider M ij for some i ̸ = j. By Lemma 2, its squared scaling is</p><formula xml:id="formula_48">σ 2 M ij = k∈An(i)∪an(j)\{j} a 2 ik + a 2 ij ∨ a 2 jj = k∈An(i)∪an(j)\{j} a 2 ik + a 2 jj .</formula><p>Choose a &gt; 1, and consider M i,aj . A second application of Lemma 2 gives</p><formula xml:id="formula_49">σ 2 M i,aj = k∈An(i)∪an(j)\{j} a 2 ik + a 2 a 2 jj .</formula><p>Finally, we compute the difference</p><formula xml:id="formula_50">σ 2 M i,aj -σ 2 M ij = (a 2 -1)a 2 jj = a 2 -1. (⇐)</formula><p>For the other direction we argue by contradiction. Suppose that j is not a source node, so there exists a node i such that i ∈ an(j) and a ji &gt; 0, and that equality (4.2) holds. Lemma 1 implies that a ji &lt; a ii . We proceed similar to the first part of the proof and compute</p><formula xml:id="formula_51">σ 2 M ij = k∈An(j)∪an(i)\{i} a 2 jk ∨ a 2 ik + a 2 ji ∨ a 2 ii = k∈An(j)∪an(i)\{i} a 2 jk ∨ a 2 ik + a 2 ii .</formula><p>Likewise, the squared scaling for M i,aj equals</p><formula xml:id="formula_52">σ 2 M i,aj = k∈An(i)∪an(i)\{i} a 2 a 2 jk ∨ a 2 ik + a 2 a 2 ji ∨ a 2 ii .</formula><p>We first define the sets S 1 (i, j) = {k ∈ An(i) ∪ An(j) : a jk ≥ a ik } and S 2 (i, j) = {k ∈ An(i) ∪ An(j) : a jk &lt; a ik and a a jk ≥ a ik }. Then</p><formula xml:id="formula_53">σ 2 M i,aj -σ 2 M ij = k∈S 1 (i,j) (a 2 -1)a 2 jk + k∈S 2 (i,j) (a 2 a 2 jk -a 2 ik ) &lt; k∈S 1 (i,j) (a 2 -1)a 2 jk + k∈S 2 (i,j) (a 2 -1) a 2 jk ≤ k∈An(i)∪An(j) (a 2 -1) a 2 jk = (a 2 -1)σ 2 M j = (a 2 -1),</formula><p>where the inequality from the first to the second lines follows by an argument similar to (4.3) in Example 1, because the index k = i lies in the set S 2 (i, j) in the second summand. We have a contradiction of (4.2), so j cannot be a source node.</p><p>Proof of Theorem 2. (⇒) Suppose that node j is such that an(j) ∩ I c = ∅. Choose an arbitrary node i / ∈ I ∪ {j} and consider the squared scalings of M i,j,I , and M i,aj,aI . Note that since I has no ancestors outside I, a rk = 0 for all nodes r ∈ I and k ∈ I c .</p><p>Using the properties of I, Lemmas 1 and 2, and following the steps in the proof of Theorem 1, we compute</p><formula xml:id="formula_54">σ 2 M i,aj,aI = a 2 k∈I a 2 kk + k∈I c ∩An(j) a 2 a 2 jk + k∈I c ∩An(j) c ∩An(i) a 2 ik , σ 2 M i,j,I = k∈I a 2 kk + k∈I c ∩An(j) a 2 jk + k∈I c ∩An(j) c ∩An(i) a 2 ik , σ 2 M i,aj,aI -σ 2 M i,j,I = (a 2 -1)   k∈I a 2 kk + k∈I c ∩An(j) a 2 jk   = (a 2 -1)σ 2 M j,I ,</formula><p>where the last line corresponds to (4.5).</p><p>(⇐) Suppose now that (4.5) holds and that there exists i ∈ I c ∩ an(j). For notational simplicity, we define the sets S 1 (i, j, I) = {k ∈ (An(j) ∪ An(i)) ∩ I c : a jk ≥ a ik } and S 2 (i, j, I) = {k ∈ (An(j) ∪ An(i)) ∩ I c : a jk &lt; a ik and a a jk ≥ a ik }. We then compute</p><formula xml:id="formula_55">σ 2 M i,aj,aI -σ 2 M i,j,I -(a 2 -1) k∈I a 2 kk = k∈S 1 (i,j,I) (a 2 -1)a 2 jk + k∈S 2 (i,j,I) (a 2 a 2 jk -a 2 ik ) &lt; k∈S 1 (i,j,I) (a 2 -1)a 2 jk + k∈S 2 (i,j,I) (a 2 -1) a 2 jk ≤ k∈An(j)∩I c (a 2 -1) a 2 jk ,</formula><p>where the first equality follows because of Lemmas 1 and 2, and properties of the set I. The inequality in the seond line follows by Lemmas 1 and 2, and the inequality a ii &gt; a ji for i ∈ S 2 (i, j, I). Rearranging the terms in the first and last line of the last display we obtain</p><formula xml:id="formula_56">σ 2 M i,aj,aI -σ 2 M i,j,I &lt; (a 2 -1) k∈I a 2 kk + k∈An(j)∩I c (a 2 -1) a 2 jk = (a 2 -1)σ 2 M j,I ,</formula><p>which contradicts the initial equality (4.5). Hence j cannot be such that an(j) ∩ I c = ∅.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C The empirical angular measure</head><p>In this section we outline the statistical theory used in estimating the scaling parameters in Sections 4 and 5. The material starts from Section 9.2 in <ref type="bibr" target="#b32">Resnick [2007]</ref>, and is similar to Section 6 of <ref type="bibr">Klüppelberg and Krali [2021]</ref>. We write D → to denote convergece in distribution, P → for convergence in probability and w → for weak convergence. Given n independent replicates X 1 , . . . , X n of X ∈ RV d + (2), we compute</p><formula xml:id="formula_57">R ℓ := ∥X ℓ ∥ 2 , ω ℓ = (ω ℓ1 , . . . , ω ℓd ) := X ℓ R ℓ , ℓ ∈ {1, . . . , n}, (C.1)</formula><p>which gives the angular representations. Based on the limit relation (A.3), HX,n/k in equation (9.32) in <ref type="bibr" target="#b32">Resnick [2007]</ref> serves as a consistent estimator for the standardised angular measure HX , where <ref type="bibr">Resnick, 2007, discussion below (9.32)</ref>], with R (k) the k-th largest radius among R 1 , . . . , R n , replacing b n/k with R</p><formula xml:id="formula_58">HX, n k (•) = n ℓ=1 1{(R ℓ /b n k , ω ℓ ) ∈ [1, ∞] × •} n ℓ=1 1{R ℓ /b n k ≥ 1} w → HX (•), k, n → ∞ and k/n → 0. Since R (k) /b n k P → 1 [</formula><formula xml:id="formula_59">(k) implies n ℓ=1 1{R ℓ ≥ R (k) } = k, so HX,n/k (•) = 1 k n ℓ=1 1{R ℓ ≥ R (k) , ω ℓ ∈ •}.</formula><p>We then construct for E HX [f (ω)] the estimator</p><formula xml:id="formula_60">Ê HX [f (ω)] = 1 k n ℓ=1 f (ω ℓ )1{R ℓ ≥ R (k) }. (C.2)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 Estimation of the scaling</head><p>To estimate the squared scalings σ 2 M S of M S for S ⊆ {1, . . . , d}, Klüppelberg and Krali [2021] apply (C.2) with f : Θ d-1 + → R + defined via the continuous function f (ω) = d r∈S ω 2 r . In contrast, we only employ the angular measure of the components of X in S, which is of cardinality smaller than d.</p><p>Given the independent replicates X 1 , . . . , X n of the vector X ∈ RV d + (2) and an ordered vector of indices I = (I 1 , . . . , I |I| ) ⊊ V such that i, j / ∈ I, we define</p><formula xml:id="formula_61">-X = (X i , X j , X I ) ∈ RV |I|+2 +</formula><p>(2), and the independent replicates X1 , . . . , Xn with angular representations ( Rℓ , ωℓ ) for ℓ ∈ {1, . . . , n}; -Xa = (X i , aX j , aX I ) ∈ RV |I|+2 +</p><p>(2), and the independent replicates Xa,1 , . . . , Xa,n with angular representations ( Ra,ℓ , ωa,ℓ ) for ℓ ∈ {1, . . . , n};</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>-σ2</head><p>M i,aj,aI : for a threshold 1 ≤ k ≤ n, we employ the non-parametric scaling estimators from the angular measure of Xa to estimate σ 2 M i,aj,aI as σ2 M i,j,I : For σ2 M i,j,I we use the angular measure of the vector Xa , but with the modification</p><formula xml:id="formula_62">σ2 M i,j,I = 1 + a 2 (|I| + 1) k n ℓ=1 r∈{1,...,|I|+2} ω 2 ℓr 1 Ra,ℓ ≥ R(k) a , (C.4)</formula><p>where ω ℓr = ωa,ℓr /a, for r ∈ {2, . . . , |I| + 2}, and ω ℓ1 = ωa,ℓ1 .</p><p>Concerning the last point, the angular measure H Xa has atoms ãk with entries ã1k = a ik , ã2k = a a jk and ãr+2,k = a a Irk for r ≥ 1. As H Xa (Θ |I|+1 ) = 1 + a 2 (|I| + 1) by (A.5), the estimator (C.4) estimates the theoretical quantity</p><formula xml:id="formula_63">(1 + a 2 (|I| + 1))E H Xa   r∈{1,...,|I|+2} ω 2 ℓr   = d k=1 ∥ã k ∥ 2 r∈{1,...,|I|+2} a 2 rk ∥ã k ∥ 2 = d k=1 r∈{1,...,|I|+2} a 2 rk = σ 2 M i,j,I ,</formula><p>which can be used to establish consistency of σ2 M i,j,I , as shown in Section C.2. Alternatively, one can estimate σ 2 M i,j,I directly from the angular measure of X, using Let the radial component R of X have distribution function F . We restate a version of Theorem 4 from <ref type="bibr">Klüppelberg and Krali [2021]</ref>.</p><p>Theorem 3 (Central Limit Theorem). Let X ∈ RV d + (2) with angular representation (R, ω) and let X 1 , . . . , X n be independent replicates of X. Choose k such that k</p><formula xml:id="formula_64">= o(n) and k → ∞ as n → ∞. Let f : Θ d-1 + → R + be a continuous function. Assume that lim n→∞ √ k n k E f (ω 1 )1 R 1 ≥ b n k t -1/α -E HX [f (ω 1 )] n k F b n k t -1/α = 0 (C.6)</formula><p>holds locally uniformly for t ∈ [0, ∞), and σ</p><formula xml:id="formula_65">2 := Var HX (f (ω)) &gt; 0. Then √ k Ê HX [f (ω)] -E HX [f (ω)] D → N (0, σ 2 ), n → ∞.</formula><p>We apply the theorem to the vector Xa = (X i , aX j , aX I ) for I = (I 1 , . . . , I |I| ) ⊊ V with i, j / ∈ I and for the functions f used in the estimators (C.3), (C.4) and (C.5). This gives consistency of the estimators, similar to <ref type="bibr">Klüppelberg and Krali [2021, Theorem 5</ref>], which we adapt below.</p><p>Theorem 4. Let X = AZ be an LSEM+ with innovation coefficient matrix A and let X 1 , . . . , X n be independent replicates of X. Take the subvector Xa for I ⊊ {1, . . . , d} with i, j / ∈ I and consider the independent replicates Xa,1 , . . . , Xa,n with angular representations ( Ra,ℓ , ωa,ℓ ) for ℓ ∈ {1, . . . , n}. Choose k such that k = o(n) and k → ∞ as n → ∞. Furthermore, assume that (C.6) holds for Xa and f (ω a ) = 1 + a 2 (|I| + 1) ∨ r∈{1,...,|I|+2} ω2 a,r and that Var H Xa (f ( ωa )) &gt; 0. Then</p><formula xml:id="formula_66">√ k Ê H Xa [f ( ωa )] -E H Xa [( ωa )] D → N (0, v 2 ), n → ∞, with variance v 2 = 1 + a 2 (|I| + 1)   d p=1 r∈{1,...,|I|+2} ã4 rp ∥ã p ∥ 2 2   -   d p=1 r∈{1,...,|I|+2} ã2 rp   2 ,</formula><p>where the vector ãp ∈ R 2+|I| +</p><p>has entries ã1k = a ik , ã2k = a a jk and ãr+2,k = a a Irk for r ≥ 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3 Proof of consistency of Algorithm 1</head><p>Proof of Proposition 3. Let S be the total number of iterations within the while loop of Algorithm 1, and note that S ≤ d, because I is augmented by at least one element at each iteration. For each iteration s within the while loop, we let π s denote the output of the vector π in line 6 of Algorithm 1, and write I s = (π s , π s-1 , . . . , π 1 ) for the ordered vector of nodes estimated up to iteration s. Likewise, we use ∆s to denote the matrix ∆ Îs in line 3. We want to show that the probability of an incorrect causal ordering converges to zero, namely, P(∃i, j ∈ V, j ∈ an(i) :</p><formula xml:id="formula_67">I j &lt; I i ) → 0, n → ∞.</formula><p>Clearly, I = I S is an incorrect causal ordering if only if there is an incorrect ordering, or error, in the s-th step of the algorithm, for some s ≤ S, that is, if π s j &lt; π s i for i, j ∈ V \ I s such that j ∈ an(i).</p><p>We proceed via induction over the number of steps, s, and show that the probability of obtaining an error at a particular step converges to zero.</p><p>Consider the probability of an error at step s = 1. Due to the existence of a causal ordering of the nodes supported on a DAG, Algorithm 1 should select only amongst the source nodes in this step. Theorem 1 establishes that ∆ s ij = 0 for all nodes j ∈ V 0 and j ∈ an(i), so ∆s By the existence of source nodes, it follows that the error term ε Îs converges in probability to zero, i.e., ε Îs P → 0. Consider the event E s := {select node i ∈ V \ I s , such that ∃j ∈ an(i) ∩ V \ I s , and π s i &gt; π s j }.</p><p>Then, the probability of an error at step s = 1 equals</p><formula xml:id="formula_68">P(E s ) ≤ P ∪ {i,j∈V :j∈an(i)} min j∈V \{i} ∆s ji &gt; min i∈V \{j} ∆s ij ∩ |δ Îs ,i | ≤ ε Îs ≤ {i,j∈V :j∈an(i)} P |δ Îs ,i | ≤ ε Îs P → 0, n → ∞, (C.7) because δ Îs ,i P → c i &lt; 0.</formula><p>Hence, the probability of selecting a node i with an(i) ̸ = ∅ at step s = 1 converges to zero.</p><p>Suppose, by the inductive hypothesis, that we can consistently recover a causal ordering up to step r -1 ≤ S -1, that is,</p><formula xml:id="formula_69">P ∪ s≤r-1 E s P → 0, n → ∞. (C.8)</formula><p>Let s = r and consider the event E r , whose probability, given the recursive nature of Algorithm 1, we write as</p><formula xml:id="formula_70">P(E r ) = P E r ∩ ∪ s 1 ≤r-1 E s 1 + P E r ∩ ∪ s 1 ≤r-1 E s 1 c , = P E r ∩ ∪ s 1 ≤r-1 E s 1 + P E r | ∪ s 1 ≤r-1 E s 1 c P ∪ s 1 ≤r-1 E s 1 c . (C.9)</formula><p>The first term in the last line of (C.9) vanishes as n → ∞ because of the inductive hypothesis (C.8), i.e., P(∪ s 1 ≤r-1 E s 1 ) P → 0. The conditioning event {∪ s 1 ≤r-1 E s 1 } c in the second term on the right-hand side of (C.9) satisfies the setting in Theorem 2, that there are no ancestors of I s outside the set I s , and, by (C.8), occurs with probability approaching one. Due to the existence of a causal ordering of the nodes on a DAG, at every step s we can identify a node j s with no ancestors in V \ I s . Theorem 2 and consistency of the estimators imply that for such a node min i∈V \I s ∪{js} ∆s ijs P → 0, and therefore ε Îs P → 0 for s = r. Now consider the two nodes i, j ∈ V \ I r , with j ∈ an(i). By Theorem 2, ∆r ji P → c ji &lt; 0, which implies that min j∈V \{i} ∆r ji = δ Îr ,i P → c i &lt; 0. We consider P(E r |{∪ s 1 ≤r-1 E s 1 } c ) and, similar to (C.7), we apply the upper bound</p><formula xml:id="formula_71">P E r | ∪ s 1 ≤r-1 E s 1 c ≤ { i,j ∈ V \I r : j ∈ an(i)} P |δ Îr ,i | ≤ ε Îs 1 | ∪ s 1 ≤r-1 E s 1 c P → 0, n → ∞.</formula><p>This proves the inductive claim, that we consistently recover a correct ordering at every step of Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.4 Standardisation of the margins</head><p>The methodology of this paper requires that the vector X ∈ RV d + (2) has unit scalings. In practice, one may not know the index of regular variation α needed to standardise the margins.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Simulation results</head><p>This section contains the results of the simulation study for the sample size n = 5000, commented but not shown in Section 5.4. EASE (d=50, p=0.1, α = 3)</p><p>Figure <ref type="figure">9</ref>: Scatterplots of the SID of the causal orderings of Algorithm 1 and of EASE for 50 random DAGs for each configuration of (d, p, α) and for n = 5000.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1 Choice of the scalar a</head><p>In this section we investigate the stability of Algorithm 1 for the scalar a for the smaller sample size n = 1000. Following the recipe outlined in Section 5.2, we perform 20 simulations over random DAGs from configurations <ref type="bibr">(d, p, α)</ref>, where each parameter is randomly sampled from d ∈ {20, 40, 50}, p ∈ {0.05, 0.1} and α ∈ {2, 3}. To each DAG we apply Algorithm 1 for each scalar a ∈ {1.0001, 1.15, 1.3, 1.5, 2} and estimate a causal ordering. In this simulation we fix ε = 0.4 and k = 250. The resulting plots in Figure <ref type="figure" target="#fig_8">10</ref> indicate better performance for values of a larger than 1.0001, with a = 1.3 giving the lowest SID when averaged over all DAGs. For the denser graphs with p = 0.1 the same choice of a yields a better and less variable performance. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure2: Boxplots of the SID of the causal orderings of Algorithm 1 and of EASE evaluated for 50 random DAGs for each configuration of (d, p, α) and for n = 1000.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Upper Danube basin network induced by physical flow connections<ref type="bibr" target="#b0">[Asadi et al., 2015]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Rhine basin network induced by physical flow connections<ref type="bibr" target="#b1">[Asadi et al., 2018]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Boxplots of the SID of the causal orderings of Algorithm 1 and of EASEfor 100 bootstrap replicates for the unprocessed (top panel) and declustered (bottom panel) Rhine datasets with d = 68 (left-hand panel) and d = 65 (right-hand panel) stations. The orange diamonds show the SID evaluated for the observed data for each method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>C.5) and (C.4) perform similarly in the inital step of the algorithm, when I = ∅. However, for I ̸ = ∅, due to the increasing dimension of the angular measure, we use only (C.3) and (C.4) to estimate the scalings involved in the matrix ∆ Î in Algorithm 1.C.2 Consistency of the scaling estimatesResults fromLarsson and Resnick [2012, Theorem 1]  were used inKlüppelberg and Krali  [2021, Theorem 4]  to show consistency and asymptotic normality of the estimators in (C.3), (C.4), and (C.5), under a mild condition. We use the angular representation (C.1), and focus on the general form of the estimator Ê HX [f (ω)] in (C.2).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>consistency of the estimates, whereas ∆ s ji = c ji &lt; 0, which implies that ∆s ji P → c ji . The continuous mapping theorem applied to the minimum function gives that min i∈V \{j} ∆s ij P → 0.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :</head><label>8</label><figDesc>Figure8: Boxplots of the SID of the causal orderings of Algorithm 1 and of EASE for 50 random DAGs for each configuration of (d, p, α) and for n = 5000.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Boxplots (top) and mean plots (bottom) of the SID of the causal orderings of Algorithm 1 for 20 random DAGs for different choices of a. The left-hand panels show the results over all DAG configurations, and the right-hand panels show the results only for the denser DAGs (p = 0.1).</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>A statistical package containing code for the proposed methodology is under development and will soon be available on github.com/mariokrali .</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>I thank <rs type="person">Anthony Davison</rs> for comments and suggestions that have improved the manuscript. I am grateful to the <rs type="funder">EPFL Doctoral School of Mathematics</rs> for the financial support.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A Multivariate regular variation</head><p>A.1 Definitions and results for regularly varying vectors</p><p>We use two equivalent definitions of multivariate regular variation from Theorem 6.1 of <ref type="bibr" target="#b32">Resnick [2007]</ref>.</p><p>), the set of non-negative Radon measures on [0, ∞] d \ {0}, and ν X is called the exponent measure of X. (ii) A random vector X ∈ R d + is multivariate regularly varying if for any norm ∥ • ∥ there exists a finite measure H X on the positive unit sphere Θ d-1</p><p>dx for some α &gt; 0, and for Borel subsets</p><p>The measure H X is called the angular measure of X, we write X ∈ RV d + (α), and α is called the index of regular variation.</p><p>The angular measure H X may not be a probability measure because it carries information on the scalings of the components of X. This motivates its normalisation.</p><p>Remark 3. (i) As H X is a finite measure, it can be normalised to a probability measure by defining</p><p>+ ), by vague convergence we have</p><p>(A.3)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Regularly varying linear structural equation models</head><p>In this section we mostly follow <ref type="bibr" target="#b32">Resnick [2007]</ref>, but the results are similar to those in <ref type="bibr">Einmahl et al. [2012, Section 6]</ref> or <ref type="bibr" target="#b5">Cooley and Thibaud [2019]</ref>. The latter consider linear operations between extremes, but apply an additional transformation to map observations to the positive orthant.</p><p>Initially, for a given component Z i of Z, for simplicity denoted by Z, we consider the vector aZ = (a 1 Z, . . . , a d Z), where a i ≥ 0 for i ∈ {1, . . . , d}.</p><p>In both, the simulation study, where α ∈ {2, 3}, and the data analysis, we transform the data to Fréchet margins using the empirical probability integral transform <ref type="bibr">[Beirlant et al., 2004, p. 338]</ref>. Given original observations X * 1 , . . . , X * n , we obtain the standardised margins via</p><p>, ℓ ∈ {1, . . . , n}.</p><p>(C.10) EASE also applies the empirical integral transform to uniform margins, so this standardisation step does not affect its performance.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Extremes on river networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Asadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Engelke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Applied Statistics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="2023" to="2050" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Optimal regionalization of extreme value distributions for flood estimation</title>
		<author>
			<persName><forename type="first">P</forename><surname>Asadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Engelke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Davison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Hydrology</title>
		<imprint>
			<biblScope unit="volume">556</biblScope>
			<biblScope unit="page" from="182" to="193" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Statistics of Extremes: Theory and Applications</title>
		<author>
			<persName><forename type="first">J</forename><surname>Beirlant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Goegebeur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Segers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Teugels</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
			<publisher>Wiley</publisher>
			<pubPlace>Chichester</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Causality in extremes of time series</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bodik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Paluš</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Pawlas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Extremes</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="67" to="121" />
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Dimension reduction in multivariate extreme value analysis</title>
		<author>
			<persName><forename type="first">E</forename><surname>Chautru</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Electronic Journal of Statistics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="383" to="418" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Decompositions of dependence for high-dimensional extremes</title>
		<author>
			<persName><forename type="first">D</forename><surname>Cooley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Thibaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="587" to="604" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Extreme Value Theory: An Introduction</title>
		<author>
			<persName><forename type="first">L</forename><surname>De Haan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ferreira</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>Springer</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Structure learning in graphical modeling</title>
		<author>
			<persName><forename type="first">M</forename><surname>Drton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Maathuis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annual Review of Statistics and Its Application</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="365" to="393" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">An M-estimator for tail dependence in arbitrary dimensions</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H J</forename><surname>Einmahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krajina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Segers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1764" to="1793" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Graphical models for extremes (with discussion)</title>
		<author>
			<persName><forename type="first">S</forename><surname>Engelke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Hitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society, Series B</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="871" to="932" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Structure learning for extremal tree models</title>
		<author>
			<persName><forename type="first">S</forename><surname>Engelke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Volgushev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society, Series B</title>
		<imprint>
			<biblScope unit="volume">84</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2055" to="2087" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Learning extremal graphical structures in high dimensions</title>
		<author>
			<persName><forename type="first">S</forename><surname>Engelke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lalancette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Volgushev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.00840</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Dense classes of multivariate extreme value distributions</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Fougères</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Mercadier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Nolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Multivariate Analysis</title>
		<imprint>
			<biblScope unit="volume">116</biblScope>
			<biblScope unit="page" from="109" to="129" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Sparse inverse covariance estimation with the graphical lasso</title>
		<author>
			<persName><forename type="first">J</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biostatistics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="432" to="441" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Max-linear models on directed acyclic graphs</title>
		<author>
			<persName><forename type="first">N</forename><surname>Gissibl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Klüppelberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bernoulli</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">4A</biblScope>
			<biblScope unit="page" from="2693" to="2720" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Causal discovery in heavy-tailed models</title>
		<author>
			<persName><forename type="first">N</forename><surname>Gnecco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Meinshausen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Engelke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1755" to="1778" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Extremal random forests</title>
		<author>
			<persName><forename type="first">N</forename><surname>Gnecco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Terefe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Engelke</surname></persName>
		</author>
		<idno type="DOI">10.1080/01621459.2023.2300522</idno>
		<ptr target="https://doi.org/10.1080/01621459.2023.2300522" />
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Sparse representation of multivariate extremes with applications to anomaly detection</title>
		<author>
			<persName><forename type="first">N</forename><surname>Goix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sabourin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Clémençon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Multivariate Analysis</title>
		<imprint>
			<biblScope unit="volume">161</biblScope>
			<biblScope unit="page" from="12" to="31" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A conditional approach for multivariate extreme values (with discussion)</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Heffernan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Tawn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society, Series B</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="497" to="546" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Pairwise likelihood ratios for estimation of non-gaussian structural equation models</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hyvärinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="111" to="152" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">k-means clustering of extremes</title>
		<author>
			<persName><forename type="first">A</forename><surname>Janßen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Electronic Journal of Statistics</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1211" to="1233" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Estimating an extreme Bayesian network via scalings</title>
		<author>
			<persName><forename type="first">C</forename><surname>Klüppelberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Krali</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.jmva.2020.104672</idno>
		<ptr target="https://doi.org/10.1016/j.jmva.2020.104672" />
	</analytic>
	<monogr>
		<title level="j">Journal of Multivariate Analysis</title>
		<imprint>
			<biblScope unit="volume">181</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Heavy-tailed max-linear structural equation models in networks with hidden nodes</title>
		<author>
			<persName><forename type="first">M</forename><surname>Krali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Klüppelberg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2306.15356</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Extremal dependence measure and extremogram: the regularly varying case</title>
		<author>
			<persName><forename type="first">M</forename><surname>Larsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">I</forename><surname>Resnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Extremes</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="231" to="256" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Lauritzen</surname></persName>
		</author>
		<title level="m">Graphical Models</title>
		<meeting><address><addrLine>Oxford</addrLine></address></meeting>
		<imprint>
			<publisher>Clarendon Press</publisher>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Partial tail correlation for extremes</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cooley</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.02048</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Multivariate sparse clustering for extremes</title>
		<author>
			<persName><forename type="first">N</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Wintenberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="issue">547</biblScope>
			<biblScope unit="page" from="1911" to="1922" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Causal mechanism of extreme river discharges in the upper Danube basin network</title>
		<author>
			<persName><forename type="first">L</forename><surname>Mhalla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Chavez-Demoulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Dupuis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society, Series C</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="741" to="764" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Causal modelling of heavy-tailed variables and confounders with application to river flow</title>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">C</forename><surname>Pasche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Chavez-Demoulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Davison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Extremes</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="573" to="594" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Causality: Models, Reasoning, and Inference</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>Cambridge University Press</publisher>
			<pubPlace>Cambridge</pubPlace>
		</imprint>
	</monogr>
	<note>second edition</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Structural intervention distance for evaluating causal graphs</title>
		<author>
			<persName><forename type="first">J</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bühlmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="771" to="799" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Extreme Values, Regular Variation, and Point Processes</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">I</forename><surname>Resnick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1987">1987</date>
			<publisher>Springer</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Heavy-Tail Phenomena: Probabilistic and Statistical Modeling</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">I</forename><surname>Resnick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
			<publisher>Springer</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A linear non-Gaussian acyclic model for causal discovery</title>
		<author>
			<persName><forename type="first">S</forename><surname>Shimizu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">O</forename><surname>Hoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hyvärinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kerminen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<date type="published" when="2003">2003-2030, 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">DirectLiNGAM: A direct method for learning a linear non-Gaussian structural equation model</title>
		<author>
			<persName><forename type="first">S</forename><surname>Shimizu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Inazumi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sogawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hyvarinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kawahara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Washio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">O</forename><surname>Hoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Bollen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="1225" to="1248" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Causation, Prediction, and Search</title>
		<author>
			<persName><forename type="first">P</forename><surname>Spirtes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Glymour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Scheines</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
	<note>second edition</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Estimating a directed tree for extremes</title>
		<author>
			<persName><forename type="first">N</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Buck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Klüppelberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society, Series B</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="771" to="792" />
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Conditional sampling for spectrally discrete max-stable random fields</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Stoev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Applied Probability</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="461" to="483" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">High-dimensional causal discovery under non-gaussianity</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Drton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">107</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="41" to="59" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
