<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DirectLiNGAM: A Direct Method for Learning a Linear Non-Gaussian Structural Equation Model</title>
				<funder ref="#_TyRvM9d #_eE3aPQd">
					<orgName type="full">JSPS</orgName>
				</funder>
				<funder ref="#_D6dEeZk">
					<orgName type="full">Ministry of Education, Culture, Sports, Science and Technology</orgName>
					<orgName type="abbreviated">MEXT</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Shohei</forename><surname>Shimizu</surname></persName>
							<email>sshimizu@ar.sanken.osaka-u.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">The Institute of Scientific and Industrial Research Osaka University</orgName>
								<address>
									<addrLine>Mihogaoka 8-1</addrLine>
									<postCode>567-0047</postCode>
									<settlement>Ibaraki</settlement>
									<region>Osaka</region>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="institution">Hamilton Hall University of North</orgName>
								<address>
									<addrLine>Carolina Chapel Hill</addrLine>
									<postCode>27599-3210</postCode>
									<region>NC</region>
									<country key="US">U.S.A</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Aapo</forename><surname>Hyvärinen</surname></persName>
							<email>aapo.hyvarinen@helsinki.fi</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Mathematics and Statistics</orgName>
								<orgName type="institution">University of Helsinki Helsinki Institute for Information Technology</orgName>
								<address>
									<addrLine>FIN-00014</addrLine>
									<country>Finland Yoshinobu Kawahara</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="institution">Hamilton Hall University of North</orgName>
								<address>
									<addrLine>Carolina Chapel Hill</addrLine>
									<postCode>27599-3210</postCode>
									<region>NC</region>
									<country key="US">U.S.A</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Takashi</forename><surname>Washio</surname></persName>
							<email>washio@ar.sanken.osaka-u.ac.jp</email>
							<affiliation key="aff2">
								<orgName type="department">Helsinki Institute for Information Technology</orgName>
								<orgName type="institution">The Institute of Scientific and Industrial Research Osaka University</orgName>
								<address>
									<addrLine>Mihogaoka 8-1</addrLine>
									<postCode>567-0047</postCode>
									<settlement>Ibaraki</settlement>
									<region>Osaka</region>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="institution">Hamilton Hall University of North</orgName>
								<address>
									<addrLine>Carolina Chapel Hill</addrLine>
									<postCode>27599-3210</postCode>
									<region>NC</region>
									<country key="US">U.S.A</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Patrik</forename><forename type="middle">O</forename><surname>Hoyer</surname></persName>
							<email>patrik.hoyer@helsinki.fi</email>
							<affiliation key="aff3">
								<orgName type="department">Department of Sociology</orgName>
								<orgName type="institution">University of Helsinki</orgName>
								<address>
									<postCode>FIN-00014 3210</postCode>
									<region>CB</region>
									<country>Finland Kenneth Bollen</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="institution">Hamilton Hall University of North</orgName>
								<address>
									<addrLine>Carolina Chapel Hill</addrLine>
									<postCode>27599-3210</postCode>
									<region>NC</region>
									<country key="US">U.S.A</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Takanori</forename><surname>Inazumi</surname></persName>
							<email>inazumi@ar.sanken.osaka-u.ac.jp</email>
							<affiliation key="aff4">
								<orgName type="institution">Hamilton Hall University of North</orgName>
								<address>
									<addrLine>Carolina Chapel Hill</addrLine>
									<postCode>27599-3210</postCode>
									<region>NC</region>
									<country key="US">U.S.A</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yasuhiro</forename><surname>Sogawa</surname></persName>
							<email>sogawa@ar.sanken.osaka-u.ac.jp</email>
							<affiliation key="aff4">
								<orgName type="institution">Hamilton Hall University of North</orgName>
								<address>
									<addrLine>Carolina Chapel Hill</addrLine>
									<postCode>27599-3210</postCode>
									<region>NC</region>
									<country key="US">U.S.A</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yoshinobu</forename><surname>Kawahara</surname></persName>
							<email>kawahara@ar.sanken.osaka-u.ac.jp</email>
							<affiliation key="aff4">
								<orgName type="institution">Hamilton Hall University of North</orgName>
								<address>
									<addrLine>Carolina Chapel Hill</addrLine>
									<postCode>27599-3210</postCode>
									<region>NC</region>
									<country key="US">U.S.A</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kenneth</forename><surname>Bollen</surname></persName>
							<email>bollen@unc.edu</email>
							<affiliation key="aff4">
								<orgName type="institution">Hamilton Hall University of North</orgName>
								<address>
									<addrLine>Carolina Chapel Hill</addrLine>
									<postCode>27599-3210</postCode>
									<region>NC</region>
									<country key="US">U.S.A</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">DirectLiNGAM: A Direct Method for Learning a Linear Non-Gaussian Structural Equation Model</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note type="submission">Submitted 1/11;</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-21T19:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>structural equation models</term>
					<term>Bayesian networks</term>
					<term>independent component analysis</term>
					<term>non-Gaussianity</term>
					<term>causal discovery</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Structural equation models and Bayesian networks have been widely used to analyze causal relations between continuous variables. In such frameworks, linear acyclic models are typically used to model the data-generating process of variables. Recently, it was shown that use of non-Gaussianity identifies the full structure of a linear acyclic model, that is, a causal ordering of variables and their connection strengths, without using any prior knowledge on the network structure, which is not the case with conventional methods. However, existing estimation methods are based on iterative search algorithms and may not converge to a correct solution in a finite number of steps. In this paper, we propose a new direct method to estimate a causal ordering and connection strengths based on non-Gaussianity. In contrast to the previous methods, our algorithm requires no algorithmic parameters and is guaranteed to converge to the right solution within a small fixed number of steps if the data strictly follows the model, that is, if all the model assumptions are met and the sample size is infinite.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Many empirical sciences aim to discover and understand causal mechanisms underlying various natural phenomena and human social behavior. An effective way to study causal relationships is to conduct a controlled experiment. However, performing controlled experiments is often ethically impossible or too expensive in many fields including social sciences <ref type="bibr" target="#b2">(Bollen, 1989)</ref>, bioinformatics <ref type="bibr" target="#b28">(Rhein and Strimmer, 2007)</ref> and neuroinformatics <ref type="bibr" target="#b24">(Londei et al., 2006)</ref>. Thus, it is necessary and important to develop methods for causal inference based on the data that do not come from such controlled experiments.</p><p>Structural equation models (SEM) <ref type="bibr" target="#b2">(Bollen, 1989)</ref> and Bayesian networks (BN) <ref type="bibr" target="#b27">(Pearl, 2000;</ref><ref type="bibr" target="#b36">Spirtes et al., 1993)</ref> are widely applied to analyze causal relationships in many empirical studies. A linear acyclic model that is a special case of SEM and BN is typically used to analyze causal effects between continuous variables. Estimation of the model commonly uses only the covariance structure of the data and in most cases cannot identify the full structure, that is, a causal ordering and connection strengths, of the model with no prior knowledge on the structure <ref type="bibr" target="#b27">(Pearl, 2000;</ref><ref type="bibr" target="#b36">Spirtes et al., 1993)</ref>.</p><p>In <ref type="bibr" target="#b29">Shimizu et al. (2006)</ref>, a non-Gaussian variant of SEM and BN called a linear non-Gaussian acyclic model (LiNGAM) was proposed, and its full structure was shown to be identifiable without pre-specifying a causal order of the variables. This feature is a significant advantage over the conventional methods <ref type="bibr" target="#b36">(Spirtes et al., 1993;</ref><ref type="bibr" target="#b27">Pearl, 2000)</ref>. A non-Gaussian method to estimate the new model was also developed in <ref type="bibr" target="#b29">Shimizu et al. (2006)</ref> and is closely related to independent component analysis (ICA) <ref type="bibr" target="#b16">(Hyvärinen et al., 2001)</ref>. In the subsequent studies, the non-Gaussian framework has been extended in various directions for learning a wider variety of SEM and BN <ref type="bibr" target="#b13">(Hoyer et al., 2009;</ref><ref type="bibr" target="#b17">Hyvärinen et al., 2010;</ref><ref type="bibr" target="#b23">Lacerda et al., 2008)</ref>. In what follows, we refer to the non-Gaussian model as LiNGAM and the estimation method as ICA-LiNGAM algorithm.</p><p>Most of major ICA algorithms including <ref type="bibr" target="#b0">Amari (1998)</ref> and <ref type="bibr" target="#b15">Hyvärinen (1999)</ref> are iterative search methods <ref type="bibr" target="#b16">(Hyvärinen et al., 2001)</ref>. Therefore, the ICA-LiNGAM algorithms based on the ICA algorithms need some additional information including initial guess and convergence criteria. Gradientbased methods <ref type="bibr" target="#b0">(Amari, 1998)</ref> further need step sizes. However, such algorithmic parameters are hard to optimize in a systematic way. Thus, the ICA-based algorithms may get stuck in local optima and may not converge to a reasonable solution if the initial guess is badly chosen <ref type="bibr" target="#b11">(Himberg et al., 2004)</ref>.</p><p>In this paper, we propose a new direct method to estimate a causal ordering of variables in the LiNGAM with no prior knowledge on the structure. The new method estimates a causal order of variables by successively subtracting the effect of each independent component from given data in the model, and this process is completed in steps equal to the number of the variables in the model. It is not based on iterative search in the parameter space and needs no initial guess or similar algorithmic parameters. It is guaranteed to converge to the right solution within a small fixed number of steps if the data strictly follows the model, that is, if all the model assumptions are met and the sample size is infinite. These features of the new method enable more accurate estimation of a causal order of the variables in a disambiguated and direct procedure. Once the causal orders of variables is identified, the connection strengths between the variables are easily estimated using some conventional covariance-based methods such as least squares and maximum likelihood approaches <ref type="bibr" target="#b2">(Bollen, 1989)</ref>. We also show how prior knowledge on the structure can be incorporated in the new method.</p><p>The paper is structured as follows. First, in Section 2, we briefly review LiNGAM and the ICAbased LiNGAM algorithm. We then in Section 3 introduce a new direct method. The performance of the new method is examined by experiments on artificial data in Section 4, and experiments on real-world data in Section 5. Conclusions are given in Section 6. Preliminary results were presented in <ref type="bibr" target="#b30">Shimizu et al. (2009)</ref>, <ref type="bibr" target="#b18">Inazumi et al. (2010)</ref> and <ref type="bibr" target="#b34">Sogawa et al. (2010)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Background</head><p>In this section, we first review LiNGAM and the ICA-LiNGAM algorithm <ref type="bibr" target="#b29">(Shimizu et al., 2006)</ref> in Sections 2.1-2.3 and next mention potential problems of the ICA-based algorithm in Section 2.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">A Linear Non-Gaussian Acyclic Model: LiNGAM</head><p>In <ref type="bibr" target="#b29">Shimizu et al. (2006)</ref>, a non-Gaussian variant of SEM and BN, which is called LiNGAM, was proposed. Assume that observed data are generated from a process represented graphically by a directed acyclic graph, that is, DAG. Let us represent this DAG by a m×m adjacency matrix B={b i j } where every b i j represents the connection strength from a variable x j to another x i in the DAG. Moreover, let us denote by k(i) a causal order of variables x i in the DAG so that no later variable determines or has a directed path on any earlier variable. (A directed path from x i to x j is a sequence of directed edges such that x j is reachable from x i .) We further assume that the relations between variables are linear. Without loss of generality, each observed variable x i is assumed to have zero mean. Then we have</p><formula xml:id="formula_0">x i = ∑ k( j)&lt;k(i) b i j x j + e i ,<label>(1)</label></formula><p>where e i is an external influence. All external influences e i are continuous random variables having non-Gaussian distributions with zero means and non-zero variances, and e i are independent of each other so that there are no latent confounding variables <ref type="bibr" target="#b36">(Spirtes et al., 1993)</ref>. We rewrite the model (1) in a matrix form as follows:</p><formula xml:id="formula_1">x = Bx + e,<label>(2)</label></formula><p>where x is a p-dimensional random vector, and B could be permuted by simultaneous equal row and column permutations to be strictly lower triangular due to the acyclicity assumption <ref type="bibr" target="#b2">(Bollen, 1989)</ref>. Strict lower triangularity is here defined as a lower triangular structure with all zeros on the diagonal. Our goal is to estimate the adjacency matrix B by observing data x only. Note that we do not assume that the distribution of x is faithful <ref type="bibr" target="#b36">(Spirtes et al., 1993)</ref> to the generating graph. We note that each b i j represents the direct causal effect of x j on x i and each a i j , the (i, j)-th element of the matrix A=(I -B) -1 , the total causal effect of x j on x i <ref type="bibr" target="#b12">(Hoyer et al., 2008)</ref>.</p><p>We emphasize that x i is equal to e i if no other observed variable x j ( j =i) inside the model has a directed edge to x i , that is, all the b i j ( j =i) are zeros. In such a case, an external influence e i is observed as x i . Such an x i is called an exogenous observed variable. Otherwise, e i is called an error. For example, consider the model defined by</p><formula xml:id="formula_2">x 2 = e 2 , x 1 = 1.5x 2 + e 1 , x 3 = 0.8x 1 -1.5x 2 + e 3 ,</formula><p>where x 2 is equal to e 2 since it is not determined by either x 1 or x 3 . Thus, x 2 is an exogenous observed variable, and e 1 and e 3 are errors. Note that there exists at least one exogenous observed variable x i (=e i ) due to the acyclicity and the assumption of no latent confounders.</p><p>An exogenous observed variable is usually defined as an observed variable that is determined outside of the model <ref type="bibr" target="#b2">(Bollen, 1989)</ref>. In other words, an exogenous observed variable is a variable that any other observed variable inside the model does not have a directed edge to. The definition does not require that it is equal to an independent external influence, and the external influences of exogenous observed variables may be dependent. However, in the LiNGAM (2), an exogenous observed variable is always equal to an independent external influence due to the assumption of no latent confounders.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Identifiability of the Model</head><p>We next explain how the connection strengths of the LiNGAM (2) can be identified as shown in <ref type="bibr" target="#b29">Shimizu et al. (2006)</ref>. Let us first solve Equation (2) for x. Then we obtain</p><formula xml:id="formula_3">x = Ae,<label>(3)</label></formula><p>where A = (I -B) -1 is a mixing matrix whose elements are called mixing coefficients and can be permuted to be lower triangular as well due to the aforementioned feature of B and the nature of matrix inversion. Since the components of e are independent and non-Gaussian, Equation (3) defines the independent component analysis (ICA) model <ref type="bibr" target="#b16">(Hyvärinen et al., 2001)</ref>, which is known to be identifiable <ref type="bibr" target="#b4">(Comon, 1994;</ref><ref type="bibr" target="#b8">Eriksson and Koivunen, 2004)</ref>. ICA essentially can estimate A (and W = A -1 = I -B), but has permutation, scaling and sign indeterminacies. ICA actually gives W ICA =PDW, where P is an unknown permutation matrix, and D is an unknown diagonal matrix. But in LiNGAM, the correct permutation matrix P can be found <ref type="bibr" target="#b29">(Shimizu et al., 2006)</ref>: the correct P is the only one that gives no zeros in the diagonal of DW since B should be a matrix that can be permuted to be strictly lower triangular and W = I -B. Further, one can find the correct scaling and signs of the independent components by using the unity on the diagonal of W=I-B. One only has to divide the rows of DW by its corresponding diagonal elements to obtain W. Finally, one can compute the connection strength matrix B = I -W.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">ICA-LiNGAM Algorithm</head><p>The ICA-LiNGAM algorithm presented in <ref type="bibr" target="#b29">Shimizu et al. (2006)</ref> is described as follows:</p><p>ICA-LiNGAM algorithm 1. Given a p-dimensional random vector x and its p × n observed data matrix X, apply an ICA algorithm (FastICA of Hyv ärinen 1999 using hyperbolic tangent function) to obtain an estimate of A.</p><p>2. Find the unique permutation of rows of W=A -1 which yields a matrix W without any zeros on the main diagonal. The permutation is sought by minimizing ∑ i 1/| W ii |.</p><p>3. Divide each row of W by its corresponding diagonal element, to yield a new matrix W ′ with all ones on the diagonal.</p><p>4. Compute an estimate B of B using B = I -W ′ .</p><p>5. Finally, to estimate a causal order k(i), find the permutation matrix P of B yielding a matrix B = P B P T which is as close as possible to a strictly lower triangular structure. The lowertriangularity of B can be measured using the sum of squared b i j in its upper triangular part ∑ i≤ j b 2 i j for small number of variables, say less than 8. For higher-dimensional data, the following approximate algorithm is used, which sets small absolute valued elements in B to zero and tests if the resulting matrix is possible to be permuted to be strictly lower triangular:</p><p>(a) </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Potential Problems of ICA-LiNGAM</head><p>The original ICA-LiNGAM algorithm has several potential problems: i) Most ICA algorithms including FastICA <ref type="bibr" target="#b15">(Hyvärinen, 1999)</ref> and gradient-based algorithms <ref type="bibr" target="#b0">(Amari, 1998)</ref> may not converge to a correct solution in a finite number of steps if the initially guessed state is badly chosen <ref type="bibr" target="#b11">(Himberg et al., 2004)</ref> or if the step size is not suitably selected for those gradient-based methods. The appropriate selection of such algorithmic parameters is not easy. In contrast, our algorithm proposed in the next section is guaranteed to converge to the right solution in a fixed number of steps equal to the number of variables if the data strictly follows the model. ii) The permutation algorithms in Steps 2 and 5 are not scale-invariant. Hence they could give a different or even wrong ordering of variables depending on scales or standard deviations of variables especially when they have a wide range of scales. However, scales are essentially not relevant to the ordering of variables. Though such bias would vanish for large enough sample sizes, for practical sample sizes, an estimated ordering could be affected when variables are normalized to make unit variance for example, and hence the estimation of a causal ordering becomes quite difficult.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">A Direct Method: DirectLiNGAM</head><p>In this section, we present a new direct estimation algorithm named DirectLiNGAM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Identification of an Exogenous Variable Based on Non-Gaussianity and Independence</head><p>In this subsection, we present two lemmas and a corollary 1 that ensure the validity of our algorithm proposed in the next subsection 3.2. The basic idea of our method is as follows. We first find an exogenous variable based on its independence of the residuals of a number of pairwise regressions (Lemma 1). Next, we remove the effect of the exogenous variable from the other variables using least squares regression. Then, we show that a LiNGAM also holds for the residuals (Lemma 2) and that the same ordering of the residuals is a causal ordering for the original observed variables as well (Corollary 1). Therefore, we can find the second variable in the causal ordering of the original observed variables by analyzing the residuals and their LiNGAM, that is, by applying Lemma 1 to the residuals and finding an "exogenous" residual. The iteration of these effect removal and causal ordering estimates the causal order of the original variables.</p><p>We first quote Darmois-Skitovitch theorem <ref type="bibr" target="#b5">(Darmois, 1953;</ref><ref type="bibr" target="#b33">Skitovitch, 1953)</ref> since it is used to prove Lemma 1:</p><p>Theorem 1 (Darmois-Skitovitch theorem) Define two random variables y 1 and y 2 as linear combinations of independent random variables s i (i=1, • • • , q):</p><formula xml:id="formula_4">y 1 = q ∑ i=1 α i s i , y 2 = q ∑ i=1 β i s i .</formula><p>Then, if y 1 and y 2 are independent, all variables s j for which α j β j = 0 are Gaussian.</p><p>In other words, this theorem means that if there exists a non-Gaussian s j for which α j β j =0, y 1 and y 2 are dependent.</p><p>Lemma 1 Assume that the input data x strictly follows the LiNGAM (2), that is, all the model assumptions are met and the sample size is infinite. Denote by r ( j) i the residual when x i is regressed on x j : r</p><formula xml:id="formula_5">( j) i = x i - cov(x i ,x j )</formula><p>var(x j ) x j (i = j). Then a variable x j is exogenous if and only if x j is independent of its residuals r ( j) i for all i = j. Proof (i) Assume that x j is exogenous, that is, x j =e j . Due to the model assumption and Equation (3), one can write x i =a i j x j + ē( j) i (i = j), where ē( j) i =∑ h = j a ih e h and x j are independent, and a i j is a mixing coefficient from x j to x i in Equation (3). The mixing coefficient a i j is equal to the regression coefficient when x i is regressed on x j since cov(x i , x j )=a i j var(x j ). Thus, the residual r ( j) i is equal to the corresponding error term, that is, r</p><formula xml:id="formula_6">( j) i = ē( j)</formula><p>i . This implies that x j and r</p><formula xml:id="formula_7">( j) i (= ē( j) i ) are independent.</formula><p>(ii) Assume that x j is not exogenous, that is, x j has at least one parent. Let P j denote the (nonempty) set of the variable subscripts of parent variables of x j . Then one can write x j = ∑ h∈P j b jh x h + e j , where x h and e j are independent and each b jh is non-zero. Let a vector x P j and a column vector b P j collect all the variables in P j and the corresponding connection strengths, respectively. Then, the covariances between x P j and x j are</p><formula xml:id="formula_8">E(x P j x j ) = E{x P j (b T P j x P j + e j )} = E(x P j b T P j x P j ) + E(x P j e j ) = E(x P j x T P j )b P j .<label>(4)</label></formula><p>The covariance matrix E(x P j x T P j ) is positive definite since the external influences e h that correspond to those parent variables x h in P j are mutually independent and have positive variances. Thus, the covariance vector E(x P j x j ) = E(x P j x T P j )b P j in Equation ( <ref type="formula" target="#formula_8">4</ref>) cannot equal the zero vector, and there must be at least one variable x i (i ∈ P j ) with which x j covaries, that is, cov(x i , x j ) =0. Then, for such a variable x i (i ∈ P j ) that cov(x i , x j ) =0, we have</p><formula xml:id="formula_9">r ( j) i = x i - cov(x i , x j ) var(x j ) x j = x i - cov(x i , x j ) var(x j ) ∑ h∈P j b jh x h + e j = 1 - b ji cov(x i , x j ) var(x j ) x i - cov(x i , x j ) var(x j ) ∑ h∈P j ,h =i b jh x h - cov(x i , x j ) var(x j ) e j .</formula><p>Each of those parent variables x h (including x i ) in P j is a linear combination of external influences other than e j due to the relation of x h to e j that x j = ∑ h∈P j b jh x h + e j = ∑ h∈P j b jh ∑ k(t)≤k(h) a ht e t + e j , where e t and e j are independent. Thus, the r ( j) i and x j can be rewritten as linear combinations of independent external influences as follows:</p><formula xml:id="formula_10">r ( j) i = 1 - b ji cov(x i , x j ) var(x j ) ∑ l = j a il e l - cov(x i , x j ) var(x j ) ∑ h∈P j ,h =i b jh ∑ t = j a ht e t - cov(x i , x j ) var(x j ) e j ,<label>(5)</label></formula><formula xml:id="formula_11">x j = ∑ h∈P j b jh ∑ t = j a ht e t + e j .<label>(6)</label></formula><p>The first two terms of Equation ( <ref type="formula" target="#formula_10">5</ref>) and the first term of Equation ( <ref type="formula" target="#formula_11">6</ref>) are linear combinations of external influences other than e j , and the third term of Equation ( <ref type="formula" target="#formula_10">5</ref>) and the second term of Equation (6) depend only on e j and do not depend on the other external influences. Further, all the external influences including e j are mutually independent, and the coefficient of non-Gaussian e j on r ( j) i and that on x j are non-zero. These imply that r ( j) i and x j are dependent since r ( j) i , x j and e j correspond to y 1 , y 2 , s j in Darmois-Skitovitch theorem, respectively.</p><p>From (i) and (ii), the lemma is proven.</p><p>Lemma 2 Assume that the input data x strictly follows the LiNGAM (2). Further, assume that a variable x j is exogenous. Denote by r ( j) a (p-1)-dimensional vector that collects the residuals r</p><formula xml:id="formula_12">( j) i</formula><p>when all x i of x are regressed on x j (i = j). Then a LiNGAM holds for the residual vector r ( j) : r ( j) = B ( j) r ( j) + e ( j) , where B ( j) is a matrix that can be permuted to be strictly lower-triangular by a simultaneous row and column permutation, and elements of e ( j) are non-Gaussian and mutually independent.</p><p>Proof Without loss of generality, assume that B in the LiNGAM (2) is already permuted to be strictly lower triangular and that x j =x 1 . Note that A in Equation ( <ref type="formula" target="#formula_3">3</ref>) is also lower triangular (although its diagonal elements are all ones). Since x 1 is exogenous, a i1 are equal to the regression coefficients when x i are regressed on x 1 (i = 1). Therefore, after removing the effects of x 1 from x i by least squares estimation, one gets the first column of A to be a zero vector, and x 1 does not affect the residuals r</p><p>(1)</p><p>i . Thus, we again obtain a lower triangular mixing matrix A (1) with all ones in the diagonal for the residual vector r (1) and hence have a LiNGAM for the vector r (1) .</p><p>Corollary 1 Assume that the input data x strictly follows the LiNGAM (2). Further, assume that a variable x j is exogenous. Denote by k r ( j) (i) a causal order of r ( j) i . Recall that k(i) denotes a causal order of x i . Then, the same ordering of the residuals is a causal ordering for the original observed variables as well: k r</p><formula xml:id="formula_13">( j) (l)&lt;k r ( j) (m) ⇔ k(l)&lt;k(m).</formula><p>Proof As shown in the proof of Lemma 2, when the effect of an exogenous variable x 1 is removed from the other observed variables, the second to p-th columns of A remain the same, and the submatrix of A formed by deleting the first row and the first column is still lower triangular. This shows that the ordering of the other variables is not changed and proves the corollary.</p><p>Lemma 2 indicates that the LiNGAM for the (p-1)-dimensional residual vector r ( j) can be handled as a new input model, and Lemma 1 can be further applied to the model to estimate the next exogenous variable (the next exogenous residual in fact). This process can be repeated until all variables are ordered, and the resulting order of the variable subscripts shows the causal order of the original observed variables according to Corollary 1.</p><p>To apply Lemma 1 in practice, we need to use a measure of independence which is not restricted to uncorrelatedness since least squares regression gives residuals always uncorrelated with but not necessarily independent of explanatory variables. A common independence measure between two variables y 1 and y 2 is their mutual information MI(y 1 , y 2 ) <ref type="bibr" target="#b16">(Hyvärinen et al., 2001)</ref>. In <ref type="bibr" target="#b1">Bach and Jordan (2002)</ref>, a nonparametric estimator of mutual information was developed using kernel methods. 2  Let K 1 and K 2 represent the Gram matrices whose elements are Gaussian kernel values of the sets of n observations of y 1 and y 2 , respectively. The Gaussian kernel values K 1 (y</p><formula xml:id="formula_14">(i) 1 , y ( j) 1 ) and K 2 (y (i) 2 , y ( j) 2 ) (i, j = 1, • • • , n) are computed by K 1 (y (i) 1 , y ( j) 1 ) = exp - 1 2σ 2 y (i) 1 -y ( j) 1 2 , K 2 (y (i) 2 , y ( j) 2 ) = exp - 1 2σ 2 y (i) 2 -y ( j) 2 2 ,</formula><p>where σ&gt;0 is the bandwidth of Gaussian kernel. Further let κ denote a small positive constant. Then, in <ref type="bibr" target="#b1">Bach and Jordan (2002)</ref>, the kernel-based estimator of mutual information is defined as:</p><formula xml:id="formula_15">MI kernel (y 1 , y 2 ) = - 1 2 log det K κ det D κ ,</formula><p>where</p><formula xml:id="formula_16">K κ = K 1 + nκ 2 I 2 K 1 K 2 K 2 K 1 K 2 + nκ 2 I 2 , D κ = K 1 + nκ 2 I 2 0 0 K 2 + nκ 2 I 2 .</formula><p>2. Matlab codes can be downloaded at <ref type="url" target="http://www.di.ens.fr/">http://www.di.ens.fr/</ref> ˜fbach/kernel-ica/index.htm.</p><p>As the bandwidth σ of Gaussian kernel tends to zero, the population counterpart of the estimator converges to the mutual information up to second order when it is expanded around distributions with two variables y 1 and y 2 being independent <ref type="bibr" target="#b1">(Bach and Jordan, 2002)</ref>. The determinants of the Gram matrices K 1 and K 2 can be efficiently computed by using the incomplete Cholesky decomposition to find their low-rank approximations of rank M (≪ n). In <ref type="bibr" target="#b1">Bach and Jordan (2002)</ref>, it was suggested that the positive constant κ and the width of the Gaussian kernel σ are set to κ = 2 × 10 -3 , σ = 1/2 for n &gt; 1000 and κ = 2 × 10 -2 , σ = 1 for n ≤ 1000 due to some theoretical and computational considerations.</p><p>In this paper, we use the kernel-based independence measure. We first evaluate pairwise independence between a variable and each of the residuals and next take the sum of the pairwise measures over the residuals. Let us denote by U the set of the subscripts of variables x i , that is, U={1, • • • , p}. We use the following statistic to evaluate independence between a variable x j and its residuals r</p><formula xml:id="formula_17">( j) i = x i - cov(x i ,x j )</formula><p>var(x j ) x j when x i is regressed on x j :</p><formula xml:id="formula_18">T kernel (x j ;U) = ∑ i∈U,i = j MI kernel (x j , r ( j) i ).<label>(7)</label></formula><p>Many other nonparametric independence measures <ref type="bibr" target="#b10">(Gretton et al., 2005;</ref><ref type="bibr" target="#b22">Kraskov et al., 2004</ref>) and more computationally simple measures that use a single nonlinear correlation <ref type="bibr" target="#b14">(Hyvärinen, 1998)</ref> have also been proposed. Any such proposed method of independence could potentially be used instead of the kernel-based measure in Equation ( <ref type="formula" target="#formula_18">7</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">DirectLiNGAM Algorithm</head><p>We now propose a new direct algorithm called DirectLiNGAM to estimate a causal ordering and the connection strengths in the LiNGAM (2):</p><p>DirectLiNGAM algorithm 1. Given a p-dimensional random vector x, a set of its variable subscripts U and a p × n data matrix of the random vector as X, initialize an ordered list of variables K := / 0 and m := 1.</p><p>2. Repeat until p-1 subscripts are appended to K:</p><p>(a) Perform least squares regressions of x i on x j for all i ∈ U\K (i = j) and compute the residual vectors r ( j) and the residual data matrix R ( j) from the data matrix X for all j ∈ U\K. Find a variable x m that is most independent of its residuals:</p><formula xml:id="formula_19">x m = arg min j∈U\K T kernel (x j ;U\K),</formula><p>where T kernel is the independence measure defined in Equation ( <ref type="formula" target="#formula_18">7</ref>).</p><p>(b) Append m to the end of K.</p><p>(c) Let x := r (m) , X := R (m) .</p><p>3. Append the remaining variable to the end of K.</p><p>4. Construct a strictly lower triangular matrix B by following the order in K, and estimate the connection strengths b i j by using some conventional covariance-based regression such as least squares and maximum likelihood approaches on the original random vector x and the original data matrix X. We use least squares regression in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Computational Complexity</head><p>Here, we consider the computational complexity of DirectLiNGAM compared with the ICA-LiNGAM with respect to sample size n and number of variables p. A dominant part of Di-rectLiNGAM is to compute Equation ( <ref type="formula" target="#formula_18">7</ref>) for each x j in Step 2(a). Since it requires O(np 2 M 2 + p 3 M 3 ) operations <ref type="bibr" target="#b1">(Bach and Jordan, 2002)</ref>  ). Though general evaluation of the required iteration number C is difficult, it can be conjectured to grow linearly with regards to p. Hence the complexity of the ICA-LiNGAM is presumed to be O(np 3 + p 4 ). Thus, the computational cost of DirectLiNGAM would be larger than that of ICA-LiNGAM especially when the low-rank approximation of the Gram matrices is not so efficient, that is, M is large. However, we note the fact that DirectLiNGAM has guaranteed convergence in a fixed number of steps and is of known complexity, whereas for typical ICA algorithms including FastICA, the run-time complexity and the very convergence are not guaranteed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Use of Prior Knowledge</head><p>Although DirectLiNGAM requires no prior knowledge on the structure, more efficient learning can be achieved if some prior knowledge on a part of the structure is available because then the number of causal orders and connection strengths to be estimated gets smaller.</p><p>We present three lemmas to use prior knowledge in DirectLiNGAM. Let us first define a matrix A knw =[a knw ji ] that collects prior knowledge under the LiNGAM (2) as follows:</p><formula xml:id="formula_20">a knw ji :=        0 if x i</formula><p>does not have a directed path to x j 1 if x i has a directed path to x j -1 if no prior knowledge is available to know if either of the two cases above (0 or 1) is true.</p><p>Due to the definition of exogenous variables and that of prior knowledge matrix A knw , we readily obtain the following three lemmas.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lemma 3 Assume that the input data x strictly follows the LiNGAM (2). An observed variable x j is exogenous if a knw</head><p>ji is zero for all i = j.</p><p>Lemma 4 Assume that the input data x strictly follows the LiNGAM ( <ref type="formula" target="#formula_1">2</ref>). An observed variable x j is endogenous, that is, not exogenous, if there exist such i = j that a knw ji is unity.</p><p>Lemma 5 Assume that the input data x strictly follows the LiNGAM ( <ref type="formula" target="#formula_1">2</ref>). An observed variable x j does not receive the effect of x i if a knw ji is zero.</p><p>The principle of making DirectLiNGAM algorithm more accurate and faster based on prior knowledge is as follows. We first find an exogenous variable by applying Lemma 3 instead of Lemma 1 if an exogenous variable is identified based on prior knowledge. Then we do not have to evaluate independence between any observed variable and its residuals. If no exogenous variable is identified based on prior knowledge, we next find endogenous (non-exogenous) variables by applying Lemma 4. Since endogenous variables are never exogenous we can narrow down the search space to find an exogenous variable based on Lemma 1. We can further skip to compute the residual of an observed variable and take the variable itself as the residual if its regressor does not receive the effect of the variable due to Lemma 5. Thus, we can decrease the number of causal orders and connection strengths to be estimated, and it improves the accuracy and computational time. The principle can also be used to further analyze the residuals and find the next exogenous residual because of Corollary 1. To implement these ideas, we only have to replace Step 2a in DirectLiNGAM algorithm by the following steps:</p><p>2a-1 Find such a variable(s) x j ( j ∈ U\K) that the j-th row of A knw has zero in the i-th column for all i ∈ U\K (i = j) and denote the set of such variables by U exo . If U exo is not empty, set U c := U exo . If U exo is empty, find such a variable(s) x j ( j ∈ U\K) that the j-th row of A knw has unity in the i-th column for at least one of i ∈ U\K (i = j), denote the set of such variables by U end and set U c := U\K\U end .</p><p>2a-2 Denote by V ( j) a set of such a variable subscript i ∈ U\K (i = j) that a knw i j = 0 for all j ∈ U c . First set r ( j) i := x i for all i ∈ V ( j) , next perform least squares regressions of x i on x j for all i ∈ U\K\V ( j) (i = j) and estimate the residual vectors r ( j) and the residual data matrix R ( j)  from the data matrix X for all j ∈ U c . If U c has a single variable, set the variable to be x m . Otherwise, find a variable x m in U c that is most independent of the residuals:</p><formula xml:id="formula_21">x m = arg min j∈U c T kernel (x j ;U\K),</formula><p>where T kernel is the independence measure defined in Equation ( <ref type="formula" target="#formula_18">7</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Simulations</head><p>We first randomly generated 5 data sets based on sparse networks under each combination of number of variables p and sample size n <ref type="bibr">(p=10, 20, 50, 100; n=500, 1000, 2000)</ref>:</p><p>1. We constructed the p × p adjacency matrix with all zeros and replaced every element in the lower-triangular part by independent realizations of Bernoulli random variables with success probability s similarly to <ref type="bibr" target="#b19">Kalisch and Bühlmann (2007)</ref>. The probability s determines the sparseness of the model. The expected number of adjacent variables of each variable is given by s(p -1). We randomly set the sparseness s so that the number of adjacent variables was 2 or 5 <ref type="bibr" target="#b19">(Kalisch and Bühlmann, 2007)</ref>.</p><p>2. We replaced each non-zero (unity) entry in the adjacency matrix by a value randomly chosen from the interval [-1.5, -0.5] ∪ [0.5, 1.5] and selected variances of the external influences e i from the interval [1, 3] as in <ref type="bibr" target="#b31">Silva et al. (2006)</ref>. We used the resulting matrix as the datagenerating adjacency matrix B.</p><p>3. We generated data with sample size n by independently drawing the external influence variables e i from various 18 non-Gaussian distributions used in <ref type="bibr" target="#b1">Bach and Jordan (2002)</ref>   <ref type="bibr" target="#b1">Bach and Jordan (2002)</ref> for the shapes of the probability density functions.</p><p>4. The values of the observed variables x i were generated according to the LiNGAM (2). Finally, we randomly permuted the order of x i .</p><p>Further we similarly generated 5 data sets based on dense (full) networks, that is, full DAGs with every pair of variables is connected by a directed edge, under each combination of number of variables p and sample size n. Then we tested DirectLiNGAM and ICA-LiNGAM on the data sets generated by sparse networks or dense (full) networks. For ICA-LiNGAM, the maximum number of iterations was taken as 1000 <ref type="bibr" target="#b29">(Shimizu et al., 2006)</ref>. The experiments were conducted on a standard PC using Matlab 7.9. Matlab implementations of the two methods are available on the web: DirectLiNGAM: <ref type="url" target="http://www.ar.sanken.osaka-u.ac.jp/">http://www.ar.sanken.osaka-u.ac.jp/</ref> ˜inazumi/dlingam.html, ICA-LiNGAM: <ref type="url" target="http://www.cs.helsinki.fi/group/neuroinf/lingam/">http://www.cs.helsinki.fi/group/neuroinf/lingam/</ref>.</p><p>We computed the distance between the true B and ones estimated by DirectLiNGAM and ICA-LiNGAM using the Frobenius norm defined as trace{(B true -B) T (B true -B)}. Tables <ref type="table" target="#tab_2">1</ref> and<ref type="table" target="#tab_4">2</ref> show the median distances (Frobenius norms) and median computational times (CPU times), respectively. In Table <ref type="table" target="#tab_2">1</ref>, DirectLiNGAM was better in distances of B and gave more accurate estimates of B than ICA-LiNGAM for all of the conditions. In Table <ref type="table" target="#tab_4">2</ref>, the computation amount of DirectLiNGAM was rather larger than ICA-LiNGAM when the sample size was increased. A main bottleneck of computation was the kernel-based independence measure. However, its computation amount can be considered to be tractable. In fact, the actual elapsed times were approximately one-quarter of their CPU times respectively probably because the CPU had four cores. Interestingly, the CPU time of ICA-LiNGAM actually decreased with increased sample size in some cases. This is presumably due to better convergence properties.</p><p>To visualize the estimation results, Figures <ref type="figure" target="#fig_0">1</ref> and<ref type="figure" target="#fig_2">2</ref> give combined scatterplots of the estimated elements of B of DirectLiNGAM and ICA-LiNGAM versus the true ones for sparse networks and  dense (full) networks, respectively. The different plots correspond to different numbers of variables and different sample sizes, where each plot combines the data for different adjacency matrices B and 18 different distributions of the external influences p(e i ). We can see that DirectLiNGAM worked well and better than ICA-LiNGAM, as evidenced by the grouping of the data points onto the main diagonal.</p><p>Finally, we generated data sets in the same manner as above and gave some prior knowledge to DirectLiNGAM by creating prior knowledge matrices A knw as follows. We first replaced every non-zero element by unity and every diagonal element by zero in A=(I -B) -1 and subsequently hid each of the off-diagonal elements, that is, replaced it by -1, with probability 0.5. The bottoms of Tables <ref type="table" target="#tab_2">1</ref> and<ref type="table" target="#tab_4">2</ref> show the median distances and median computational times. It was empirically confirmed that use of prior knowledge gave more accurate estimates and less computational times in most cases especially for dense (full) networks. The reason would probably be that for dense (full) networks more prior knowledge about where directed paths exist were likely to be given and it narrowed down the search space more efficiently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Applications to Real-world Data</head><p>We here apply DirectLiNGAM and ICA-LiNGAM on real-world physics and sociology data. Both DirectLiNGAM and ICA-LiNGAM estimate a causal ordering of variables and provide a full DAG.</p><p>Then we have two options to do further analysis <ref type="bibr" target="#b17">(Hyvärinen et al., 2010)</ref>: i) Find significant directed edges or direct causal effects b i j and significant total causal effects a i j with A=(I -B) -1 ; ii) Estimate redundant directed edges to find the underlying DAG. We demonstrate an example of the former in Section 5.1 and that of the latter in Section 5.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Application to Physical Data</head><p>We applied DirectLiNGAM and ICA-LiNGAM on a data set created from a physical system called a double-pendulum, a pendulum with another pendulum attached to its end <ref type="bibr" target="#b25">(Meirovitch, 1986)</ref> as in Figure <ref type="figure">3</ref>. The data set was first used in <ref type="bibr" target="#b21">Kawahara et al. (2011)</ref>. The raw data consisted of four time series provided by Ibaraki University (Japan) filming the pendulum system with a high-speed video camera at every 0.01 second for 20.3 seconds and then reading out the position using an image analysis software. The four variables were θ 1 : the angle between the top limb and the vertical, θ 2 : the angle between the bottom limb and the vertical, ω 1 : the angular speed of θ 1 or θ1 and ω 2 : the angular speed of θ 2 or θ2 . The number of time points was 2035. The data set is available on the web: <ref type="url" target="http://www.ar.sanken.osaka-u.ac.jp/">http://www.ar.sanken.osaka-u.ac.jp/</ref> ˜inazumi/data/furiko.html.</p><p>In <ref type="bibr" target="#b21">Kawahara et al. (2011)</ref>, some theoretical considerations based on the domain knowledge implied that the angle speeds ω 1 and ω 2 are mainly determined by the angles θ 1 and θ 2 in both cases where the swing of the pendulum is sufficiently small (θ 1 , θ 2 ≈ 0) and where the swing is not very small. Further, in practice, it was reasonable to assume that there were no latent confounders <ref type="bibr" target="#b21">(Kawahara et al., 2011)</ref>.</p><p>As a preprocessing, we first removed the time dependency from the raw data using the ARMA (AutoRegressive Moving Average) model with 2 autoregressive terms and 5 moving average terms following <ref type="bibr" target="#b21">Kawahara et al. (2011)</ref>. Then we applied DirectLiNGAM and ICA-LiNGAM on the preprocessed data. The estimated adjacency matrices B of θ 1 , θ 2 , ω 1 and ω 2 were as follows: The estimated orderings by DirectLiNGAM and ICA-LiNGAM were identical, but the estimated connection strengths were very different. We further computed their 95% confidence intervals by using bootstrapping <ref type="bibr" target="#b7">(Efron and Tibshirani, 1993)</ref> with the number of bootstrap replicates 10000. The estimated networks by DirectLiNGAM and ICA-LiNGAM are graphically shown in Figure <ref type="figure">4</ref>, where only significant directed edges (direct causal effects) b i j are shown with 5% significance level.<ref type="foot" target="#foot_1">foot_1</ref> DirectLiNGAM found that the angle speeds ω 1 and ω 2 were determined by the angles θ 1 or θ 2 , which was consistent with the domain knowledge. Though the directed edge from θ 1 to θ 2 might be a bit difficult to interpret, the effect of θ 1 on θ 2 was estimated to be negligible since the coefficient of determination <ref type="bibr" target="#b2">(Bollen, 1989)</ref> of θ 2 , that is, 1-var( ê2 )/var( θ2 ), was very small and was 0.01. (The coefficient of determination of ω 1 and that of ω 2 were 0.46 and 0.49, respectively.) On the other hand, ICA-LiNGAM could not find any significant directed edges since it gave very different estimates for different bootstrap samples.</p><formula xml:id="formula_22">DirectLiNGAM :     θ 1 θ 2 ω 1 ω 2 θ 1 0 0 0 0 θ 2 -0.</formula><p>For further comparison, we also tested two conventional methods <ref type="bibr" target="#b35">(Spirtes and Glymour, 1991;</ref><ref type="bibr" target="#b3">Chickering, 2002)</ref> based on conditional independences. Figure <ref type="figure">5</ref> shows the estimated networks by PC algorithm <ref type="bibr" target="#b35">(Spirtes and Glymour, 1991)</ref> with 5% significance level and GES <ref type="bibr" target="#b3">(Chickering, 2002)</ref> with the Gaussianity assumption. We used the Tetrad IV<ref type="foot" target="#foot_2">foot_2</ref> to run the two methods. PC algorithm found the same directed edge from θ 1 on ω 1 as DirectLiNGAM did, but did not found the directed edge from θ 2 on ω 2 . GES found the same directed edge from θ 1 on θ 2 as DirectLiNGAM did, but did not find that the angle speeds ω 1 and ω 2 were determined by the angles θ 1 or θ 2 .</p><p>We also computed the 95% confidence intervals of the total causal effects a i j using bootstrap. DirectLiNGAM found significant total causal effects from θ 1 on θ 2 , from θ 1 on ω 1 , from θ 1 on ω 2 , from θ 2 on ω 1 , and from θ 2 on ω 2 . These significant total effects would also be reasonable based on similar arguments. ICA-LiNGAM only found a significant total causal effect from θ 2 on ω 2 .</p><p>Overall, although the four variables θ 1 , θ 2 , ω 1 and ω 2 are likely to be nonlinearly related according to the domain knowledge <ref type="bibr" target="#b25">(Meirovitch, 1986;</ref><ref type="bibr" target="#b21">Kawahara et al., 2011)</ref>, DirectLiNGAM gave interesting results in this example.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Application to Sociology Data</head><p>We analyzed a data set taken from a sociological data repository on the Internet called General Social Survey (<ref type="url" target="http://www.norc.org/GSS+Website/">http://www.norc.org/GSS+Website/</ref>). The data consisted of six observed vari-  <ref type="bibr" target="#b6">(Duncan et al., 1972)</ref>. A directed edge between two variables in the figure means that there could be a directed edge between the two. A bi-directed edge between two variables means that the relation is not modeled. For instance, there could be latent confounders between the two, there could be a directed edge between the two, or the two could be independent.</p><p>ables, x 1 : father's occupation level, x 2 : son's income, x 3 : father's education, x 4 : son's occupation level, x 5 : son's education, x 6 : number of siblings. (x 6 is discrete but is relatively close to be continuous since it is an ordinal scale with many points.) The sample selection was conducted based on the following criteria: i) non-farm background; ii) ages 35 to 44; iii) white; iv) male; v) in the labor force at the time of the survey; vi) not missing data for any of the covariates; vii) years 1972-2006. The sample size was 1380. Figure <ref type="figure" target="#fig_4">6</ref> shows domain knowledge about their causal relations <ref type="bibr" target="#b6">(Duncan et al., 1972)</ref>. As shown in the figure, there could be some latent confounders between x 1 and x 3 , x 1 and x 6 , or x 3 and x 6 . An objective of this example was to see how our method behaves when such a model assumption of LiNGAM could be violated that there is no latent confounder.</p><p>The estimated adjacency matrices B by DirectLiNGAM and ICA-LiNGAM were as follows:</p><p>DirectLiNGAM : We subsequently pruned redundant directed edges b i j in the full DAGs by repeatedly applying a sparse method called Adaptive Lasso <ref type="bibr" target="#b38">(Zou, 2006)</ref> on each variable and its potential parents. See Appendix A for some more details of Adaptive Lasso. We used a matlab implementation in <ref type="bibr" target="#b32">Sjöstrand (2005)</ref> to run the Lasso. Then we obtained the following pruned adjacency matrices B:</p><formula xml:id="formula_23">        x 1 x 2 x 3 x 4 x 5 x 6 x 1 0 0 3.</formula><formula xml:id="formula_24">DirectLiNGAM :         x 1 x 2</formula><p>x 3 x 4 x 5 x 6 x 1 0 0 3.19 0 0 0 x 2 0 0 0 422.87 0 0 x 3 0 0 0 0 0.55 0 x 4 0 0 0 0 4.61 0 x 5 0 0 0 0 0 -0.12 x 6 0 0 0 0 0 0</p><formula xml:id="formula_25">        , ICA -LiNGAM :        </formula><p>x 1 x 2 x 3 x 4 x 5 x 6 x 1 0 0 0.93 0 0 0 x 2 0 0 0 200.84 0 0 x 3 0 0 0 0 0.24 0 x 4 0 0 0 0 -0.14 0 x 5 0 0 0 0 0 0 x 6 0 0 0 0 -0.08 0</p><formula xml:id="formula_26">       </formula><p>.</p><p>The estimated networks by DirectLiNGAM and ICA-LiNGAM are graphically shown in Figure <ref type="figure">7</ref> and Figure <ref type="figure">8</ref>, respectively. All the directed edges estimated by DirectLiNGAM were reasonable to the domain knowledge other than the directed edge from x 5 : son's education to x 3 : father's education. Since the sample size was large and yet the estimated model was not fully correct, the mistake on the directed edge between x 5 and x 3 might imply that some model assumptions might be more or less violated in the data. ICA-LiNGAM gave a similar estimated network but did one more mistake that x 6 : number of siblings is determined by x 5 : son's education.</p><p>Further, Figure <ref type="figure">9</ref> and Figure <ref type="figure" target="#fig_0">10</ref> show the estimated networks by PC algorithm with 5% significance level and GES with the Gaussianity assumption. Both of the conventional methods did not find the directions of many edges. The two conventional methods found a reasonable direction of the edge between x 1 : father's occupation and x 3 : father's education, but they gave a wrong direction of the edge between x 1 : father's occupation and x 4 : son's occupation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We presented a new estimation algorithm for the LiNGAM that has guaranteed convergence to the right solution in a fixed number of steps if the data strictly follows the model, that is, if all the model assumptions are met and the sample size is infinite. Further, the new algorithm has known computational complexity. This is the first algorithm specialized to estimate the LiNGAM. Simulations implied that the new method often provides better statistical performance than a state of the art method based on ICA. In real-world applications to physics and sociology, interesting results were obtained. Future works would include i) assessment of practical performance of statistical tests to detect violations of the model assumptions including tests of independence <ref type="bibr" target="#b9">(Gretton and Györfi, 2010)</ref>; ii) implementation issues of our algorithm to improve the practical computational efficiency; iii) extensions of our algorithm to more general cases including the cases with latent confounders A big difference is that the adaptive Lasso assumes that the set of such potential parent variables x j that k( j)&lt;k(i) is known and LiNGAM estimates the set of such variables. The adaptive Lasso penalizes connection strengths b i j in L 1 penalty by minimizing the objective function defined as:</p><formula xml:id="formula_27">x i -∑ k( j)&lt;k(i) b i j x j 2 + λ ∑ k( j)&lt;k(i) |b i j | | bi j | γ ,</formula><p>where λ and γ are tuning parameters and bi j is a consistent estimate of b i j . In <ref type="bibr" target="#b38">Zou (2006)</ref>, it was suggested to select the tuning parameters by five-fold cross validation and to obtain bi j by ordinary least squares regression. The adaptive Lasso has a very attractive property that it asymptotically selects the right set of such variables x j that b i j is not zero, where k( j)&lt;k(i).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Left: Scatterplots of the estimated b i j by DirectLiNGAM versus the true values for sparse networks. Right: Scatterplots of the estimated b i j by ICA-LiNGAM versus the true values for sparse networks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>including (a) Student with 3 degrees of freedom; (b) double exponential; (c) uniform; (d) Student with 5 degrees of freedom; (e) exponential; (f) mixture of two double exponentials; (g)-(h)-(i) symmetric mixtures of two Gaussians: multimodal, transitional and unimodal; (j)-(k)-(l) nonsymmetric mixtures of two Gaussians, multimodal, transitional and unimodal; (m)-(n)-(o) symmetric mixtures of four Gaussians: multimodal, transitional and unimodal; (p)-(q)-(r) nonsymmetric mixtures of four Gaussians: multimodal, transitional and unimodal. See Figure 5 of</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Left: Scatterplots of the estimated b i j by DirectLiNGAM versus the true values for dense (full) networks. Right: Scatterplots of the estimated b i j by ICA-LiNGAM versus the true values for dense (full) networks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :Figure 4 :Figure 5 :</head><label>345</label><figDesc>Figure 3: Abstract model of the double-pendulum used in Kawahara et al. (2011). AE AE Ö Ö</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Status attainment model based on domain knowledge<ref type="bibr" target="#b6">(Duncan et al., 1972)</ref>. A directed edge between two variables in the figure means that there could be a directed edge between the two. A bi-directed edge between two variables means that the relation is not modeled. For instance, there could be latent confounders between the two, there could be a directed edge between the two, or the two could be independent.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>in p-1 iterations, complexity of the step is O(np 3 M 2 + p 4 M 3 ), where M (≪ n) is the maximal rank found by the low-rank decomposition used in the kernel-based independence measure. Another dominant part is the regression to estimate the matrix B in Step 4. The complexity of many representative regressions including the least square algorithm is O(np 3 ). Hence, we have a total budget of O(np 3 M 2 + p 4 M 3 ). Meanwhile, the ICA-LiNGAM requires O(p 4 ) time to find a causal order in Step 5. Complexity of an iteration in FastICA procedure at Step 1 is known to be O(np 2 ). Assuming a constant number C of the iterations in FastICA steps, the complexity of the ICA-LiNGAM is considered to be O(Cnp 2 + p 4</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Median distances (Frobenius norms) between true B and estimated B of DirectLiNGAM and ICA-LiNGAM with five replications.</figDesc><table><row><cell>Sparse networks</cell><cell></cell><cell cols="2">Sample size</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="3">500 1000 2000</cell></row><row><cell>DirectLiNGAM</cell><cell>dim. = 10</cell><cell>0.48</cell><cell>0.31</cell><cell>0.21</cell></row><row><cell></cell><cell>dim. = 20</cell><cell>1.19</cell><cell>0.70</cell><cell>0.50</cell></row><row><cell></cell><cell>dim. = 50</cell><cell>2.57</cell><cell>1.82</cell><cell>1.40</cell></row><row><cell></cell><cell>dim. = 100</cell><cell>5.75</cell><cell>4.61</cell><cell>2.35</cell></row><row><cell>ICA-LiNGAM</cell><cell>dim. = 10</cell><cell>3.01</cell><cell>0.74</cell><cell>0.65</cell></row><row><cell></cell><cell>dim. = 20</cell><cell>9.68</cell><cell>3.00</cell><cell>2.06</cell></row><row><cell></cell><cell cols="4">dim. = 50 20.61 20.23 12.91</cell></row><row><cell></cell><cell cols="4">dim. = 100 40.77 43.74 36.52</cell></row><row><cell>DirectLiNGAM with</cell><cell>dim. = 10</cell><cell>0.48</cell><cell>0.30</cell><cell>0.24</cell></row><row><cell cols="2">prior knowledge (50%) dim. = 20</cell><cell>1.00</cell><cell>0.71</cell><cell>0.49</cell></row><row><cell></cell><cell>dim. = 50</cell><cell>2.47</cell><cell>1.75</cell><cell>1.19</cell></row><row><cell></cell><cell>dim. = 100</cell><cell>4.94</cell><cell>3.89</cell><cell>2.27</cell></row><row><cell>Dense (full) networks</cell><cell></cell><cell cols="2">Sample size</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="3">500 1000 2000</cell></row><row><cell>DirectLiNGAM</cell><cell>dim. = 10</cell><cell>0.45</cell><cell>0.46</cell><cell>0.20</cell></row><row><cell></cell><cell>dim. = 20</cell><cell>1.46</cell><cell>1.53</cell><cell>1.12</cell></row><row><cell></cell><cell>dim. = 50</cell><cell>4.40</cell><cell>4.57</cell><cell>3.86</cell></row><row><cell></cell><cell>dim. = 100</cell><cell>7.38</cell><cell>6.81</cell><cell>6.19</cell></row><row><cell>ICA-LiNGAM</cell><cell>dim. = 10</cell><cell>1.71</cell><cell>2.08</cell><cell>0.39</cell></row><row><cell></cell><cell>dim. = 20</cell><cell>6.70</cell><cell>3.38</cell><cell>1.88</cell></row><row><cell></cell><cell cols="4">dim. = 50 17.28 16.66 12.05</cell></row><row><cell></cell><cell cols="4">dim. = 100 34.95 34.02 32.02</cell></row><row><cell>DirectLiNGAM with</cell><cell>dim. = 10</cell><cell>0.45</cell><cell>0.31</cell><cell>0.19</cell></row><row><cell cols="2">prior knowledge (50%) dim. = 20</cell><cell>0.84</cell><cell>0.90</cell><cell>0.41</cell></row><row><cell></cell><cell>dim. = 50</cell><cell>2.48</cell><cell>1.86</cell><cell>1.56</cell></row><row><cell></cell><cell>dim. = 100</cell><cell>4.67</cell><cell>3.60</cell><cell>2.61</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Median computational times (CPU times) of DirectLiNGAM and ICA-LiNGAM with five replications.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>We prove the lemmas and corollary without assuming the faithfulness<ref type="bibr" target="#b36">(Spirtes et al., 1993)</ref> unlike our previous work<ref type="bibr" target="#b30">(Shimizu et al., 2009)</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1"><p>The issue of multiple comparisons arises in this context, which we would like to study in future work.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2"><p>Tetrad IV is available at http://www.phil.cmu.edu/projects/tetrad/.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>We are very grateful to <rs type="person">Hiroshi Hasegawa</rs> (<rs type="affiliation">College of Science, Ibaraki University, Japan</rs>) for providing the physics data and <rs type="person">Satoshi Hara</rs> and <rs type="person">Ayumu Yamaoka</rs> for interesting discussion. We thank the three anonymous reviewers whose comments helped to improve the exposition of the paper. This work was partially carried out at <rs type="institution">Department of Mathematical and Computing Sciences and Department of Computer Science, Tokyo Institute of Technology, Japan</rs>. S.S., Y.K. and T.W. were partially supported by <rs type="funder">MEXT</rs> <rs type="grantName">Grant-in-Aid for Young Scientists #</rs><rs type="grantNumber">21700302</rs>, by <rs type="funder">JSPS</rs> <rs type="grantName">Grant-in-Aid for Young Scientists #</rs><rs type="grantNumber">20800019</rs> and by <rs type="grantName">Grant-in-Aid for Scientific Research</rs> (A) #<rs type="grantNumber">19200013</rs>, respectively. S.S. and Y.K. were partially supported by <rs type="funder">JSPS</rs> <rs type="programName">Global COE program</rs> 'Computationism</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_D6dEeZk">
					<idno type="grant-number">21700302</idno>
					<orgName type="grant-name">Grant-in-Aid for Young Scientists #</orgName>
				</org>
				<org type="funding" xml:id="_TyRvM9d">
					<idno type="grant-number">20800019</idno>
					<orgName type="grant-name">Grant-in-Aid for Young Scientists #</orgName>
				</org>
				<org type="funding" xml:id="_eE3aPQd">
					<idno type="grant-number">19200013</idno>
					<orgName type="grant-name">Grant-in-Aid for Scientific Research</orgName>
					<orgName type="program" subtype="full">Global COE program</orgName>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Figure <ref type="figure">8</ref>: The estimated network by ICA-LiNGAM and Adaptive Lasso. A red solid directed edge is reasonable to the domain knowledge. <ref type="bibr" target="#b12">(Hoyer et al., 2008;</ref><ref type="bibr" target="#b20">Kawahara et al., 2010)</ref> or nonlinear relations <ref type="bibr" target="#b13">(Hoyer et al., 2009;</ref><ref type="bibr" target="#b26">Mooij et al., 2009)</ref> and iv) comparison of our method and related algorithms on many other real-world data sets. Figure <ref type="figure">9</ref>: The estimated network by PC algorithm with 5% significance level. An undirected edge between two variables means that there is a directed edge from a variable to the other or the reverse. A red solid directed edge is reasonable to the domain knowledge. Figure <ref type="figure">10</ref>: The estimated network by GES. An undirected edge between two variables means that there is a directed edge from a variable to the other or the reverse. A red solid directed edge is reasonable to the domain knowledge.</p><p>as a Foundation for the Sciences'. A.H. was partially supported by the Academy of Finland Centre of Excellence for Algorithmic Data Analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A. Adaptive Lasso</head><p>We very briefly review the adaptive Lasso <ref type="bibr" target="#b38">(Zou, 2006)</ref>, which is a variant of the Lasso <ref type="bibr" target="#b37">(Tibshirani, 1996)</ref>. See <ref type="bibr" target="#b38">Zou (2006)</ref> for more details. The adaptive Lasso is a regularization technique for variable selection and assumes the same data generating process as LiNGAM:</p><p>x i = ∑ k( j)&lt;k(i) b i j x j + e i .</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Natural gradient learning works efficiently in learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Amari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="251" to="276" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Kernel independent component analysis</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">R</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1" to="48" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Structural Equations with Latent Variables</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Bollen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989">1989</date>
			<publisher>John Wiley &amp; Sons</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Optimal structure identification with greedy search</title>
		<author>
			<persName><forename type="first">D</forename><surname>Chickering</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="507" to="554" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Independent component analysis, a new concept</title>
		<author>
			<persName><forename type="first">P</forename><surname>Comon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Processing</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="62" to="83" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Analyse générale des liaisons stochastiques</title>
		<author>
			<persName><forename type="first">G</forename><surname>Darmois</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Review of the International Statistical Institute</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="2" to="8" />
			<date type="published" when="1953">1953</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Socioeconomic Background and Achievement</title>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">D</forename><surname>Duncan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Featherman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Duncan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1972">1972</date>
			<publisher>Seminar Press</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">An Introduction to the Bootstrap</title>
		<author>
			<persName><forename type="first">B</forename><surname>Efron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993">1993</date>
			<publisher>Chapman &amp; Hall</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Identifiability, separability, and uniqueness of linear ICA models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Eriksson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Koivunen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="601" to="604" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Consistent nonparametric tests of independence</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Györfi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="1391" to="1423" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Measuring statistical dependence with Hilbert-Schmidt norms</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Bousquet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Algorithmic Learning Theory: 16th International Conference (ALT2005)</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="63" to="77" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Validating the independent components of neuroimaging time-series via clustering and visualization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Himberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hyvärinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Esposito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeuroImage</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="1214" to="1222" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Estimation of causal effects using linear non-gaussian causal models with hidden variables</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">O</forename><surname>Hoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shimizu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kerminen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Palviainen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Approximate Reasoning</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="362" to="378" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Nonlinear causal discovery with additive noise models</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">O</forename><surname>Hoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Janzing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mooij</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Koller</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Schuurmans</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="689" to="696" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">New approximations of differential entropy for independent component analysis and projection pursuit</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hyvärinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="273" to="279" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Fast and robust fixed-point algorithms for independent component analysis</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hyvärinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="626" to="634" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Hyvärinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Karhunen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Oja</surname></persName>
		</author>
		<title level="m">Independent Component Analysis</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Wiley</publisher>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Estimation of a structural vector autoregressive model using non-Gaussianity</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hyvärinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shimizu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">O</forename><surname>Hoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="1709" to="1731" />
			<date type="published" when="2010-05">May 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Use of prior knowledge in a non-Gaussian method for learning linear structural equation models</title>
		<author>
			<persName><forename type="first">T</forename><surname>Inazumi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shimizu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Washio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 9th International Conference on Latent Variable Analysis and Signal Separation (LVA/ICA2010)</title>
		<meeting>9th International Conference on Latent Variable Analysis and Signal Separation (LVA/ICA2010)</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="221" to="228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Estimating high-dimensional directed acyclic graphs with the PCalgorithm</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kalisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bühlmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="613" to="636" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Kawahara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Bollen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shimizu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Washio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1006.5041</idno>
		<title level="m">GroupLiNGAM: Linear non-Gaussian acyclic models for sets of variables</title>
		<imprint>
			<date type="published" when="2010-06">June 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Analyzing relationships among ARMA processes based on non-Gaussianity of external influences</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Kawahara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shimizu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Washio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Estimating mutual information</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kraskov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Stögbauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Grassberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical Review E</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">66138</biblScope>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Discovering cyclic causal models by independent components analysis</title>
		<author>
			<persName><forename type="first">G</forename><surname>Lacerda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Spirtes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ramsey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">O</forename><surname>Hoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th Conference on Uncertainty in Artificial Intelligence (UAI2008)</title>
		<meeting>the 24th Conference on Uncertainty in Artificial Intelligence (UAI2008)</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="366" to="374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A new method for detecting causality in fMRI data of cognitive processing</title>
		<author>
			<persName><forename type="first">A</forename><surname>Londei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>D'ausilio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Basso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">O</forename><surname>Belardinelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive processing</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="42" to="52" />
			<date type="published" when="2006-03">March 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>Meirovitch</surname></persName>
		</author>
		<title level="m">Elements of Vibration Analysis</title>
		<imprint>
			<publisher>McGraw-Hill</publisher>
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
	<note>nd ed.</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Regression by dependence minimization and its application to causal inference in additive noise models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Mooij</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Janzing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Conference on Machine Learning (ICML2009)</title>
		<meeting>the 26th International Conference on Machine Learning (ICML2009)</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="745" to="752" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Causality: Models, Reasoning, and Inference</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000. 2009</date>
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">From correlation to causation networks: a simple approximate learning algorithm and its application to high-dimensional plant gene expression data</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">O</forename><surname>Rhein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Strimmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Systems Biology</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="37" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A linear non-gaussian acyclic model for causal discovery</title>
		<author>
			<persName><forename type="first">S</forename><surname>Shimizu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">O</forename><surname>Hoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hyvärinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kerminen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<date type="published" when="2003">2003-2030, 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A direct method for estimating a causal ordering in a linear non-gaussian acyclic model</title>
		<author>
			<persName><forename type="first">S</forename><surname>Shimizu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hyvärinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kawahara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Washio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th Conference on Uncertainty in Artificial Intelligence (UAI2009)</title>
		<meeting>the 25th Conference on Uncertainty in Artificial Intelligence (UAI2009)<address><addrLine>Montreal, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>AUAI Press</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="506" to="513" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning the structure of linear latent variable models</title>
		<author>
			<persName><forename type="first">R</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Scheines</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Glymour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Spirtes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="191" to="246" />
			<date type="published" when="2006-02">Feb 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Matlab implementation of LASSO, LARS, the elastic net and SPCA</title>
		<author>
			<persName><forename type="first">K</forename><surname>Sjöstrand</surname></persName>
		</author>
		<ptr target="http://www2.imm.dtu.dk/pubdb/p.php?3897.Version2.0" />
		<imprint>
			<date type="published" when="2005-06">June 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">On a property of the normal distribution</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">P</forename><surname>Skitovitch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Doklady Akademii Nauk SSSR</title>
		<imprint>
			<biblScope unit="volume">89</biblScope>
			<biblScope unit="page" from="217" to="219" />
			<date type="published" when="1953">1953</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">An experimental comparison of linear non-Gaussian causal discovery methods and their variants</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Sogawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shimizu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kawahara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Washio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 2010 International Joint Conference on Neural Networks (IJCNN2010)</title>
		<meeting>2010 International Joint Conference on Neural Networks (IJCNN2010)</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="768" to="775" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">An algorithm for fast recovery of sparse causal graphs</title>
		<author>
			<persName><forename type="first">P</forename><surname>Spirtes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Glymour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Social Science Computer Review</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="67" to="72" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Causation, Prediction, and Search</title>
		<author>
			<persName><forename type="first">P</forename><surname>Spirtes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Glymour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Scheines</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993">1993. 2000</date>
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
	<note>nd ed.</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Regression shrinkage and selection via the lasso</title>
		<author>
			<persName><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Royal Statistical Society: Series B</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="267" to="288" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">The adaptive Lasso and its oracle properties</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="page" from="1418" to="1429" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
