<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Convex Formulation for Regularized Estimation of Structural Equation Models</title>
				<funder>
					<orgName type="full">Chula Engineering</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2019-05-02">2 May 2019</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Anupon</forename><surname>Pruttiakaravanich</surname></persName>
							<email>anupon106@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Electrical Engineering</orgName>
								<orgName type="department" key="dep2">Faculty of Engineering Chulalongkorn University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jitkomut</forename><surname>Songsiri</surname></persName>
							<email>jitkomut.s@chula.ac.th</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Electrical Engineering</orgName>
								<orgName type="department" key="dep2">Faculty of Engineering Chulalongkorn University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Convex Formulation for Regularized Estimation of Structural Equation Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2019-05-02">2 May 2019</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:1809.06156v2[math.OC]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-14T17:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Path analysis is a model class of structural equation modeling (SEM), which it describes causal relations among measured variables in the form of a multiple linear regression. This paper presents two estimation formulations, one each for confirmatory and exploratory SEM, where a zero pattern of the estimated path coefficient matrix can explain a causality structure of the variables. The original nonlinear equality constraints of the model parameters were relaxed to an inequality, allowing the transformation of the original problem into a convex framework. A regularized estimation formulation was then proposed for exploratory SEM using an l1-type penalty of the path coefficient matrix. Under a condition on problem parameters, our optimal solution is low rank and provides a useful solution to the original problem. Proximal algorithms were applied to solve our convex programs in a large-scale setting. The performance of this approach was demonstrated in both simulated and real data sets, and in comparison with an existing method. When applied to two real application results (learning causality among climate variables in Thailand and examining connectivity differences in autism patients using fMRI time series from ABIDE data sets) the findings could explain known relationships among environmental variables and discern known and new brain connectivity differences, respectively.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Structural equation modelling (SEM) is a class of multivariate models used for learning a causal relationship among variables (exploratory modelling) or for testing whether the model is best fit by given data (confirmatory modelling). A general SEM includes the observed and latent variables, while their relationships are explained by a linear model whose parameters explain the cause or influence from one variable to another. As such, SEM has been widely used in behavioural research, such as in psychology, sociology, business and medical research <ref type="bibr" target="#b37">[MGL94,</ref><ref type="bibr" target="#b45">PLFI09]</ref>. The details of the model and its historical background in SEM are given in <ref type="bibr" target="#b7">[Bol89,</ref><ref type="bibr">§1]</ref>. Path analysis is a special problem in SEM, where it provides a model for explaining the relationships among measured variables only (no latent variables). This can be better associated with scientific research where the observed variables are often of primary interest. For example, one aims to explore causal relationships among brain regions from brain signals (such as fMRI data) [MGL94, BF97, BHH + 00, KZC + 07, JKC + 09, CGS + 11] where the entries of the path coefficient matrix in the model explain how much change in activities of one region influences another region.</p><p>In path analysis, one applies a prior knowledge about the relationship structure of variables of interest to construct a model and encode a structure, such as the zero pattern of the path matrix. The first problem type in path analysis, the confirmatory SEM, is to estimate the value of nonzero entries in the path matrix and the covariance matrix of the model residual errors so that the model-reproduced covariance matrix fits well with the sample covariance matrix in an optimal sense, as evaluated by various types of criterion functions, such as maximum likelihood (ML) and ordinary or weighted least-squares (LS) <ref type="bibr" target="#b7">[Bol89,</ref><ref type="bibr">§4]</ref>. The second type of problem, exploratory SEM, is to learn the causal structure of variables from a zero structure of the estimated path matrix. The existing approach for exploratory SEM is to begin with a base model, where a certain set of paths are affirmative but the existence of some other paths is in question. This results in a set of a few candidate models associated with different zero structures of the path matrix and then the significance of the difference between these models can be determined from the χ 2 statistic <ref type="bibr" target="#b7">[Bol89,</ref><ref type="bibr">§7]</ref>. Examples of this approach can be seen in brain network studies <ref type="bibr" target="#b37">[MGL94,</ref><ref type="bibr" target="#b4">BF97]</ref>, where only a few variables (in the order of up to 10 brain regions) are selected. One can locally search for a path structure by starting from a null model and sequentially allowing the coefficient corresponding to the largest Lagrangian multiplier to be nonzero [BHH + 00]. The most optimal, but not tractable, method is to perform an exhaustive search that enumerates all possible path patterns and chooses the model corresponding to the lowest minimized ML function [CGS + 11]. However, as the number of all possible models grows exponentially to the number of variables, it is not a feasible approach.</p><p>Both confirmatory and exploratory SEM problems are nonlinear optimization problems in matrix variables with a quadratic equality and positive definite cone constraints. Common techniques based on Newton-Raphson or gradient descent are implemented to estimate the model parameters [Mul09, §7], [Bol89, §4] and there are many existing SEM commercial softwares, such as LISREL, EQS and Mplus [RM06, JSTT00, Bol89], so in an estimation process, a starting value for the update iteration is required. Although these numerical methods work well under normal conditions, some initial values may not lead to the convergence of the optimal solution or may stick into local minima, and so several strategies for selecting initial values have been proposed <ref type="bibr" target="#b7">[Bol89,</ref><ref type="bibr">§4]</ref>. These include choosing an instrumental variable estimate or selecting the strength of the path coefficient magnitude. When the iterative method in these software does not converge, the user is suggested not to interpret the result.</p><p>In this work, we present two alternative estimation formulations, one each for confirmatory and exploratory SEM problems. The original nonlinear equality constraints of the model parameters are relaxed to an inequality, leading the problems to transform into convex formulations that can be solved efficiently by many existing convex program solvers, where the solution is guaranteed to be the global minima. For exploratory SEM, we propose an objective function that is added with an 1 -type regularization of the path coefficient matrix, called sparse SEM. Such formulations are regarded as a lasso formulation <ref type="bibr" target="#b26">[HTF09b]</ref> and encourage many zeros in the path matrix, allowing us to read off the zero pattern and interpret it as a causal structure of the variables. Solving exploratory SEM using a regularization approach has previously been discussed <ref type="bibr" target="#b28">[JGM16]</ref>, where the reticular action model (RAM) was the focus of the model class, or where the model explains a relationship between the dependent variables and latent variables <ref type="bibr" target="#b60">[ZTT16,</ref><ref type="bibr" target="#b23">HCW17]</ref>.</p><p>Difficulties in estimating sparse coefficients in latent equations arise <ref type="bibr" target="#b60">[ZTT16,</ref><ref type="bibr" target="#b23">HCW17]</ref>, and so the expected conditional maximization (ECM) algorithm was applied to manage the unobserved latent variables. To our knowledge, the algorithms in <ref type="bibr" target="#b60">[ZTT16,</ref><ref type="bibr" target="#b23">HCW17]</ref> may potentially suffer from local minimum issues if one attempts to solve highdimensional settings, such as in brain applications. Despite the fact that these model classes are more general than a path analysis problem, our approach could serve as a convex framework targeted to solve a special class of RAM, and was shown to perform better than <ref type="bibr" target="#b28">[JGM16]</ref> in some cases. We applied alternating direction method of multipliers (ADMM), and proximal parallel algorithm (PPXA), which requires a feasible amount of memory storage suitable for large-scale implementation. More importantly, we showed that, under a condition on problem parameters, the optimal covariance error is diagonal, meaning that the errors are uncorrelated. When this assumption holds, the optimal solution has a low rank, providing an estimate of the path matrix for the original problem.</p><p>Despite the difference in our estimation formulation and the original one, we believe that our proposed formulations serve two folds. Firstly, unlike previous SEM applications where only a few variables are of interest [MGL94, BHH + 00, KZC + 07, JKC + 09], many applications tend to consider a much larger number of variables, such as fMRI studies where the variables are neuronal activities and number up to thousands. Existing approaches of learning causal structures in the exploratory SEM may experience a computational difficulty in terms of the memory storage or convergence. Secondly, our solution for confirmatory SEM is obtained under an assumption of homoskedasticity of residual errors, so if this assumption holds, our and the original solution coincide. Even if it does not hold, and our solution is then not optimal for the original problem, our solution can serve as a starting value for the iterative algorithm used in the original one when a convergence was not obtained.</p><p>Related formulations of using SEMs in order to reveal causal structures in multivariate variables chave been reported [BMG14, SBG17, TSG17, CBG11], where the models described a relationship between the output and exogenous variables. These researches applied a sum-of-norm regularization (regarded as an 1 -type penalty) to promote a sparsity in the path matrix that further inferred a sparse network topology. In <ref type="bibr" target="#b5">[BMG14]</ref>, the authors proposed a formulation that includes a forgetting factor in the cost objective to track the change in the dynamic network over time, while the contribution of <ref type="bibr" target="#b50">[SBG17]</ref> was to propose a kernel-based nonlinear SEM model to capture nonlinear features in some applications. One common underlying characteristic in these works is the use of the LS objective as a goodness of fit and an 1 norm as a regularization term, so that the resulting formulation was convex and where the variables were the coefficients from exogenous variables to the output. However, it is known that the noise covariance in a LS framework is not a variable that can be estimated and LS estimators are not efficient, in contrast to ML estimators. Our approach exploits the simplicity of a linear SEM but we consider the original SEM formulation that uses Kullback-Leiber (KL) divergence as a goodness of fit while both the path coefficients and the error covariance matrix are estimated simultaneously making the original formulation problem non-convex.</p><p>The two proposed formulations were initially developed in prior work <ref type="bibr" target="#b47">[PS16,</ref><ref type="bibr" target="#b46">Pru17]</ref> and here we have adapted some details on the algorithms, provided mathematical proofs of important results and included two more experimental real world applications. Section 2 summarizes the mathematical formulation of the path analysis problem as the ML estimation with a quadratic equality constraint. Section 3 describes our convex formulation for confirmatory SEM and shows that the solution can be further used under the condition of having a low rank solution at the optimum. Another convex formulation for exploratory SEM is proposed in Section 3.2, where an 1 regularization is introduced in the cost objective. We show that sparse SEM solutions can be obtained by controlling the regularization parameter. To select the best causality structure, the model selection procedure is explained in Section 3.4. Then, proximal algorithms for solving the exploratory SEM are explained in Section 4, where we provide some examples of implementation in a large-scale. Numerical experiments in Section 5 demonstrate the important factors to the performance of our method from the simulated data. The results of applying our estimation formulation to learn the causal relations of air pollution data and brain regions are explained in Section 6. Finally, some mathematical proofs used in our analysis are provided in the Appendix.</p><p>Keywords: structural equation model, convex optimization, regularization, brain connectivity Notation. S n denotes the set of symmetric matrices of size n × n and S n + denotes the set of positive semidefinite matrices of size n × n. For a square matrix X, tr(X) is the trace of X and diag(X) is a diagonal matrix containing diagonal entries of X. A block symmetric matrix of size 2n × 2n is of the form:</p><formula xml:id="formula_0">X = X 1 X T 2 X 2 X 4</formula><p>, where X k is of size n × n. The notations X 0 and X 0 refer to X being positive definite, and positive semidefinite, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Path analysis in SEM</head><p>The SEM starts with a set of variables involved in the study, as the measured variables and latent variables. Measured variables are simply the ones that can be directly measured (physical quantities), while latent variables are those that cannot be directly (or exactly) measured, such as intelligence, attitude, etc. Each of these variables can be regarded as either endogenous or exogenous. An endogenous variable gets an influence from others, while an exogenous variable affects the other variables. A general mathematical model in SEM explains a linear relationship from latent variables to measured variables and also includes error terms of each variable <ref type="bibr" target="#b7">[Bol89,</ref><ref type="bibr" target="#b40">Mul09,</ref><ref type="bibr" target="#b24">Hoy95]</ref>.</p><p>In practice, we are commonly interested in the application of SEM in situations that only involve observable variables. For this reason, we focus on a special class of model in SEM that is described by the multiple linear regression: Y = c + AY + , where Y ∈ R n is the measured (or observed) variables, c ∈ R n is a constant vector representing a baseline and ∈ R n is the model error, assumed to be Gaussian distributed. The path matrix A = [A ij ] ∈ R n×n represents a dependence structure among variables in the model. Thus, if A ij = 0 then there is no path from Y j to Y i . In other words, a pattern of nonzero entries in A reveals a causal structure of variables in the model. If this structure is assumed from prior knowledge, then the problem of estimating A is called confirmatory SEM.</p><p>Let S be a sample covariance matrix of Y and Σ be the model-reproduced covariance matrix of Y , derived from the regression model, which is then given by (1);</p><formula xml:id="formula_1">Σ = (I -A) -1 Ψ(I -A) -T ,<label>(1)</label></formula><p>where Ψ = cov( ). The estimation problem in SEM is to find A and Ψ that minimize the KL divergence function d(S, Σ) = log det Σ + tr(SΣ -1 ) -log det S -n, meaning that Σ is close to S, while maintaining that Σ, A and Ψ are constrained by (1). Moreover, the structure of the path matrix is presumably encoded by a model hypothesis: (i) A ij = 0 if there is no link from Y j to Y i and (ii) we always have diag(A) = 0, since there is no path from Y i to itself. To specify the zero structure of A, we then define the associated index set I A ⊆ {1, 2, . . . , n} × {1, 2, . . . , n} with properties that i) (i, j) ∈ I A if A ij = 0 and ii) {(1, 1), (2, 2), . . . , (n, n)} ⊆ I A . In short, I A denotes the index set of hypothetical zero entries in A. Given an index set I A , we define a projection operator P : R n×n → R n×n , as shown in (2);</p><formula xml:id="formula_2">P (X) = X ij , (i, j) ∈ I A , 0, otherwise,<label>(2)</label></formula><p>and denote P c = I -P . The operators P c and P are both self-adjoint, i.e., tr(Y T P (X)) = tr(P (Y ) T X) and that P c (P (X)) = 0. These two projection operators are then used in the duality of our estimation formulations. With the definition of P , the estimation problem corresponding to the confirmatory SEM is shown in (3);</p><formula xml:id="formula_3">minimize -log det Σ -1 + tr(SΣ -1 ) -log det S -n subject to Σ -1 = (I -A) T Ψ -1 (I -A), P (A) = 0,<label>(3)</label></formula><p>with variables A ∈ R n×n , Ψ ∈ S n + and Σ ∈ S n + . The condition P (A) = 0 explains the zero constraint on the entries of A, and when there is no information on the path matrix, this condition reduces to diag(A) = 0. The problem in (3) is one of estimation formulations considered in an SEM context <ref type="bibr" target="#b7">[Bol89,</ref><ref type="bibr">§4]</ref>. Other cost objectives are also used, such as the ordinary or weighted LS. Estimation formulations in SEM typically consider the ML formulation using the KL divergence objective as in (3), because an ML estimator has the efficient property while it is known that a LS formulation does not always lead to an efficient estimator. Special case. If the constraint P (A) = 0 reduces to diag(A) = 0 (we allow A to have as many free parameters as possible), then we can make the cost objective zero by solving S -1 = (I -A) T Ψ -1 (I -A), where S is given and A and Ψ are free variables. In this case, solutions A of (3) are not unique; one can obtain A as dense (non-recursive model) or lower triangular matrix (recursive model). This could be problematic in reading a causality structure from a zero pattern in the estimated A. For this reason, it is common to assume some structure in A and a diagonal structure in Ψ (meaning the error terms are uncorrelated). Specifically, the degrees of freedom (df) are defined as in (4), df = the number of known parameters -the number of estimated parameters .</p><p>Referring to (3), the number of known parameters is the number of entries in the sample covariance matrix and is equal to n(n -1)/2 where n is the number of observed variables. The number of free parameters in (4) is the total number of entries in A plus the total number of entries in Ψ. One can use df as a guideline for identifying the uniqueness of solution. When the df is negative, the estimator may not be unique. We say that the model is identifiable if the df is nonnegative <ref type="bibr" target="#b48">[RM06,</ref><ref type="bibr">p. 35]</ref>.</p><p>Connection with Gaussian graphical models. Without the parametric covariance constraint in (1), the cost objective in (3) alone can be viewed as a problem of estimating the inverse covariance matrix of Gaussian vectors from a sample covariance S. When the objective function is added with an 1 -norm penalty as -log det X +tr(SX)+ i =j |X ij |, the problem is known as a graphical lasso [HTF09a, §17.3.2], which finds many extensions and algorithm developments <ref type="bibr" target="#b38">[MH12,</ref><ref type="bibr" target="#b3">BCP18]</ref>. It is worth nothing that dependence structures among variables decoded from SEMs and the graphical lasso are conceptually different. In graphical lasso, zeros in the inverse covariance matrix (X) of Gaussian vectors explain the conditional independence structure, while no parametric model of variables is assumed. On the other hand, linear SEM assumes a regression model and the interconnection structure of variables are explained from the significant entries of the path matrix A, not Σ. Combining the concept of a conditional independence graph and linear SEM model was proposed in <ref type="bibr" target="#b44">[PLB14]</ref>, which established a condition of undirected graphs on the inverse covariance Σ -1 , where Σ is an implied covariance in (1) from an underlying linear SEM with the assumption that Ψ is diagonal. Their formulation is to estimate A and Ψ so that a conditional independence graph can be recovered.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Convex formulations for path analysis in SEM</head><p>The non-convex property of (3) from the quadratic equality constraint is apparent and leads to the chance of obtaining local minima when solving the problem numerically. This section describes the first contribution of our work, where we propose alternative convex formulations and their dual problems for both confirmatory and exploratory SEM. We consider a special case of the path analysis problem where the covariance error is allowed to be diagonal, suggesting that the residual errors of the model were assumed to be uncorrelated. The solution to our formulations is useful when it is low rank at optimum, which was shown to occur under some mild conditions on a problem parameter. The solutions to our formulation and the original problem agree when the covariance of residual error is specified to be a multiple of the identity matrix, which is often the case in SEM applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Confirmatory SEM</head><p>Define a change of variables as shown in (5):</p><formula xml:id="formula_5">X = X 1 X T 2 X 2 X 4 , X 1 = Σ -1 , X 2 = I -A, X 4 = Ψ.</formula><p>(5)</p><p>Previous work <ref type="bibr" target="#b47">[PS16]</ref> applied a convex relaxation to the quadratic equality constraint of (3) and proposed the convex confirmatory SEM formulation minimize -log det X 1 + tr(SX 1 ) subject to X 0, 0 X 4 αI, P (X 2 ) = I,</p><p>with variables X ∈ S 2n , where S 0 and α &gt; 0 are given parameters. After relaxing the non-convex constraint, we introduced the inequality constraint X 4 αI to avoid a trivial solution in (6), such as X 4 can be arbitrarily large and X 2 = I. We justify that α can serve as an upper bound on the covariance error of residual in the SEM. Then (6) is a semidefinite programming that can be solved by many existing convex program solvers. It was previously noted <ref type="bibr" target="#b47">[PS16]</ref> that (3) and ( <ref type="formula" target="#formula_6">6</ref>) are no longer equivalent, but the convex formulation, that is solved more efficiently, can provide a useful initial solution when solving (3) numerically.</p><formula xml:id="formula_7">Let Z = Z 1 Z T 2 Z 2 Z 4 ∈ S 2n</formula><p>be the Lagrange multiplier of the constraint X 0 in (6). Then the dual problem of</p><formula xml:id="formula_8">the convex confirmatory SEM in (6) is minimize -log det(S -Z 1 ) -2 tr(Z 2 ) -α tr(Z 4 ) + n subject to Z 0, P c (Z 2 ) = 0,<label>(7)</label></formula><p>with variable Z ∈ S 2n . The constraint P c (Z 2 ) = 0 explains that the corresponding entries of block Z 2 to the zero entries in X 2 are free, and the other entries of Z 2 are all zero. If the condition P (X 2 ) = I reduces to diag(X 2 ) = I in (6), then in the dual, it is simplified to that Z 2 is diagonal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Trivial dual solutions</head><p>The proposed framework in (6) has two problem parameters: S 0 and α &gt; 0. (Note that if S 0 the problem could be unbounded below). The important theoretical results that α cannot be arbitrarily large as the solution X 1 in (6) becomes an impracticable approximate of Σ -1 in the original problem of (3) <ref type="bibr" target="#b47">[PS16]</ref>. For the completeness of this paper, we state the proposition in <ref type="bibr" target="#b47">[PS16]</ref> and provide the proof here. Let us start with the zero gradient condition of Karush-Kuhn-Tucher (KKT): X 1 = (S -Z) -1 , when Z = 0 is an optimal dual. Let X2 = X 2 -P (X 2 ) + I for any X 2 . Then the optimal primal solution X must satisfy the feasibility condition in (8),</p><formula xml:id="formula_9">X 1 = S -1 , X 1 XT 2 X -1 4 X2 , 0 ≺ X 4 αI.<label>(8)</label></formula><p>It suggests that under such acase (trivial dual solution) if α is too large then X 4 can be chosen sufficiently large that the choice of X 1 = S -1 is feasible and optimal, which becomes merely a simple estimate of Σ -1 and an unfavorable solution. Therefore, the proposition addresses a criterion of the choice of α to avoid such a solution.</p><p>Proposition 1. Let α c = n/ tr(S -1 ) (the harmonic mean of the eigenvalues of S 0). If α ≤ α c the feasibility problem in (8) has no solution.</p><p>See the proof in Appendix 9.1 that applies Farka's lemma to a semidefinite programming. To apply the formulation of (6), when samples of data, Y 1 , Y 2 , . . . , Y N are available, one computes S (the sample covariance of {Y k } N k=1 ) and choose α, which is as of now, suggested to be less than α c . We can show easily that as one simple choice, the minimum eigenvalue of S denoted as λ min (S) is always less than α c . Suppose</p><formula xml:id="formula_10">λ 1 ≤ λ 2 ≤ • • • ≤ λ n are eigenvalues of S. It follows that 1 λ1 + 1 λ2 + • • • + 1 λn ≤ n λ1 . Since tr(S -1 ) = n k=1 1/λ k , we have α c = n tr(S -1 ) = n 1 λ1 + 1 λ2 + • • • + 1 λn ≥ λ 1 = λ min (S).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Sparse SEM with 1 -norm regularization</head><p>In exploratory SEM analysis, one aims to discover a zero structure of A from the estimation process that reveals a causal structure of how one variable affects another. One existing approach performs a local search starting from a null model (all path coefficients are zero) and sequentially allows the coefficient corresponding to the largest Lagrangian multiplier to be nonzero [BHH + 00]. Another method is to also start from a null model and then add an extra path corresponding to the lowest minimized ML discrepancy function selected among all possible paths. This scheme is referred to as tree growth as the model grows by a single entry in A at a time [CGS + 11]. The most optimal, but not tractable, approach is to perform a simple brute-force method (or known as forest growth) that searches through all possible patterns of zero structures in A with a fixed number of paths and chooses the model corresponding to the lowest minimized ML [CGS + 11]. However, the number of all possible models grows exponentially to the number of variables (n), and so it is not feasible as the problem dimension increases.</p><p>In this section, we propose a convex formulation for the exploratory SEM problem by applying the widely-used sparse optimization with 1 -norm. An estimation problem with a 1 -norm penalty (lasso) problem has been wellunderstood as a convex relaxation of the 0 -norm penalty that aims to promote a sparsity of the solution <ref type="bibr" target="#b26">[HTF09b,</ref><ref type="bibr">§3]</ref>. This approach has been extended in various directions and applied to find a sparse dependence structure of variables. These include a 1 -penalty of an affine transformation, termed a generalized lasso [DSB + 16], estimation of the sparse vector autoregressive processes for inferring the Granger causality of time series <ref type="bibr" target="#b10">[BVN11]</ref>, joint sparse estimation of inverse covariance matrices to find a common conditional independence structure [THW + 16, MM16], and estimation of the sparse spectral density matrix to explain the conditional independence structures of time series</p><formula xml:id="formula_11">[JHG15, SV10]. Let h(A) = (i,j) / ∈I A |A ij |.</formula><p>When h is added to the estimation objective function, it is a form of 1 -like penalty function (or regularization) as it resembles the 1-norm of a matrix except, that only those entries not belonging to I A are penalized. It is well-known that an 1 -regularized problem returns a sparse solution; many entries of A are zero and the zero pattern can then reveal a causality structure of the variables. Users get to hypothesize about the known location of zeros in A, which is encoded as the index set I A . Without any prior knowledge about the zero locations in A at all, the constraint P (A) = diag(A) = 0 was applied. For (i, j) / ∈ I A , it is then uncertain if A ij would be zero or not, so the 1 norm on these entries is enforced. From the change of variables in (5), we have</p><formula xml:id="formula_12">A ij = -(X 2 ) ij for (i, j) / ∈ I A .</formula><p>The convex formulation for learning zero patterns in A is then proposed in (9); minimize -log det X 1 + tr(SX 1 ) + 2γ</p><formula xml:id="formula_13">(i,j) / ∈I A |(X 2 ) ij | subject to X 0, 0 X 4 αI, P (X 2 ) = I,<label>(9)</label></formula><p>with variables X ∈ S 2n . The parameter γ &gt; 0 is the regularization parameter that controls the sparsity of X 2 (and hence, in A).</p><formula xml:id="formula_14">Let Z = Z 1 Z T 2 Z 2 Z 4 ∈ S 2n be the Lagrange multiplier of X 0 in (9). The dual of (9) is maximize log det(S -Z 1 ) -2 tr(Z 2 ) -α tr(Z 4 ) + n, subject to Z 0, P c (Z 2 ) ∞ ≤ γ,<label>(10)</label></formula><p>with variable Z ∈ S 2n . The difference of the dual in (10) from that in (7) is that Z ij are not forced to be zero, but allowed to be less than γ for (i, j) / ∈ I A .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Choice of penalty parameter</head><p>We see that the sparsity of X 2 , and hence, of the optimal A can be controlled via γ. For example, the larger γ, the sparser A is. In Appendix 9.2, we established that there exists a critical value of γ, denoted by γ max , in the sense that if (11) holds;</p><formula xml:id="formula_15">γ ≥ γ max := (1/α) P c (αI -S) ∞<label>(11)</label></formula><p>then the optimal A in (9) is the zero matrix. Moreover, the value of γ max can be calculated in advance and depends only on α and S (problem parameters). This means it is unnecessary to vary γ arbitrarily in the problem, and so γ max can be used as an upper bound of the range of γ used for varying the sparsity patterns of A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related sparse SEM formulations</head><p>The goal of learning sparse coefficients in SEM is commonly found in recent literature. It is worth noting several related model structures that include exogenous (X), mediate (M ) or latent variables (η) and describe formulations that encourage zeros in the coefficients of these variables. The first related formulation is the model containing both dependent and latent variables <ref type="bibr" target="#b60">[ZTT16,</ref><ref type="bibr" target="#b23">HCW17]</ref> as Y = Λη + y and η = Πη + η . They aim to promote a sparsity in Π (coefficients of latent variables) where <ref type="bibr" target="#b60">[ZTT16]</ref> considered smoothly clipped absolute deviation (SCAD) formulation, while <ref type="bibr" target="#b23">[HCW17]</ref> applied various sparse penalties such as SCAD, minimax concave penalty and 1 -norm. The log-likelihood function certainly contains η, which is unobservable, so the ECM algorithm was employed to find a local maxima. Examples in [JGM16, ZTT16, HCW17] illustrate the case but with a small number of variables (up to 20 in Y and less than 10 in η). This framework is very useful when the latents are considered in the model, while we believe that our path analysis problem (that contains only observable variables) is not precisely a special case of the model in [ZTT16, HCW17] as they do not include a path matrix from Y to Y . Therefore, path analysis should be specifically solved where more efficient algorithms can be applied as a result of problem simplification and relaxed convexity. The second relevant work is the inclusion of mediate variables <ref type="bibr" target="#b51">[SJBG17]</ref> as</p><formula xml:id="formula_16">Y = cX + diag(b)M + y and M = a T X + m ,</formula><p>where Y and X are scalars and M is a multivariate mediator. The aim was to obtain a sparsity in b to regard only significant mediators and so they formulated the models into a path analysis so that regsem using the 1 -norm penalty in <ref type="bibr" target="#b28">[JGM16]</ref> could be applied. Examples are illustrated with the model having only five mediators. It is noted that their model regards Y and X as single variables. Moreover, when formulated as the path analysis problem: Z = AZ + with Z = (Y, X, M ), it could also be solved using our framework with a minor modification on the definition of I A as a block of A is restricted to I. The third model, by <ref type="bibr" target="#b12">[CBG11]</ref>, is described as Y = BY + F X + with the goal of estimating a sparse B. It is then obvious that our formulation is essentially their special case. However, the LS framework is chosen in <ref type="bibr" target="#b12">[CBG11]</ref> where noise covariance was not part of the variables and so their formulation is simplified and convex. Similarly, if both X and Y are observable, this model can be formulated as a path analysis with Z = (Y, X) and can be solved using our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Low rank solution</head><p>Solutions of the convex confirmatory SEM and the sparse SEM are useful for the original SEM problem if X 1 = X T 2 X -1 4 X 2 at optimum, as we can then use X 1 as an estimate of Σ -1 . This occurs if and only if rank(X) = n at optimum (note that X ∈ S 2n ). Therefore, we aimed to find a relation between α and low rank optimal solutions of ( <ref type="formula" target="#formula_6">6</ref>) and (9) from the optimality conditions. The result in section 3.1 suggested that if α is too large, then it is possible that rank(X) &gt; n, which is to be avoided.</p><p>Previous analysis <ref type="bibr" target="#b47">[PS16]</ref> showed that when an optimal X has rank n, then In Section 3.1 it was shown that the minimum eigenvalue of S always lies on the left of the harmonic mean of the eigenvalues of S. If we choose α = λ min (S), then it is often the case that rank(Z) = n. This advises us to include a constraint X 4 λ min (S)I S into the estimation problem. Since X 4 = Ψ, we can justify that the covariance error is controlled to be less than the covariance of the variables. Figure <ref type="figure" target="#fig_0">1</ref> illustrates that the error between X 1 and X T 2 X -1 4 X 2 increases as α increases and is zero when α is sufficiently small relatively to the minimum eigenvalue of S. It is also noted that when α is too large, then rank(X) &gt; n and there could be many optimal solutions X 4 that is strictly less than αI. In this case, the solution X 1 is not unique.</p><formula xml:id="formula_17">X 1 = X T 2 X -1 4 X 2 , rank(Z) = n, Z 4 0, X 4 = αI.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Exploratory SEM</head><p>As seen in Section 3.2, controlling the regularization parameter in the sparse SEM problem can provide path matrix solutions with various sparsity patterns. If γ is large then the path matrix A contains many zeros, resulting in a simple interpretation of the estimated causal structure but the goodness of fit becomes smaller. Therefore, choosing an appropriate value of γ is a trade-off between choosing the solution to explain a causal structure in a simple way and to best describe the data to a certain level. Two common approaches of selecting the regularization parameter implemented in many works on sparse learning are the cross-validation and model selection criterions <ref type="bibr" target="#b25">[HTF09a]</ref>, and examples of applying these approaches in a graphical lasso problem have been reported <ref type="bibr" target="#b59">[YL07,</ref><ref type="bibr" target="#b22">GLMZ11]</ref>. In a K-fold cross validation, we vary γ over a specified range and train the model on the training data fold, then the γ value that yielded the smallest test prediction error <ref type="bibr" target="#b25">[HTF09a]</ref> is chosen. The cross validation approach can be computationally intensive since the models are estimated from the data from K-folds and for several values of γ.</p><p>We adopted a model selection approach in this paper, which is a widely and commonly used approach in many regularized SEM problems [HCW17, ZTT16, SJBG17], where the value of γ corresponding to the model with the lowest score in the model selection criterion [HTF09a, §7] was chosen. These included the Akaike Information Criterion (AIC), the corrected AIC (AICc), Bayesian Information Criterion (BIC) [HTF09a, §7], Kullback Information Criterion (KIC) <ref type="bibr" target="#b11">[Cav99]</ref> and the corrected KIC (KICc) <ref type="bibr" target="#b49">[SB04]</ref>, where we calculated the log-likelihood function of samples Y 1 , Y 2 , . . . , Y N as L = N 2 -log det Σtr(S Σ-1 ) . These scores consisted of the two terms of the negative log-likelihood, representing the goodness of fit, and the model complexity. Using a large value of γ yielded a sparse model, so the model complexity is low but the fitting is worse. Therefore, a fair comparison among models with different sparsity patterns can be performed using these model selection criterions. These scores have different asymptotic behaviors. The BIC is known to prefer a simpler model, since the penalty on the model complexity is higher relatively to other criterions, while the AIC is the first model selection criterion, which is widely accepted and is known to perform poorly when the number of sample sizes is small relative to the number of effective parameters. The AICc and KICc scores were then developed to reduce the bias and improve the model selection in a small-sample setting.</p><p>To learn the best causal structure of path matrices, we can choose a range of γ values from 0 to γ max and then solved the sparse SEM in (9) for each of those values, resulting in estimated path matrices with different sparsity patterns ranging from the densest to the sparsest. Each of the estimated sparsity patterns was then used as a sparsity constraint on A in the convex confirmatory SEM in (6) and was solved for the optimal path matrix A to yield a candidate model. Estimation of the unrestricted model with the selected entries of A by sparse SEM also helps to reduce the bias of the estimator [HTF09a, §3.8.5]. This process was repeated for all the values of γ to obtain a set of candidate models, each of which was labeled by a model selection criterion score and the best model was the one with the minimum score. In short, the sparse SEM was used to select a finite number of sparsity patterns in A and the convex confirmatory SEM was used to provide the best estimate of the path matrix corresponding to the sparsity pattern selected from the model selection criterion score. This procedure is illustrated in Figure <ref type="figure" target="#fig_1">2</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Proximal algorithms</head><p>In this section, efficient numerical methods were applied to solve the two estimation formulations; the convex confirmatory SEM and the sparse SEM. We examine two proximal algorithms, namely, parallel proximal algorithm (PPXA) and alternating direction method of multipliers (ADMM), that requires a splitting technique on the cost objective of convex problems and introduces some auxiliary variables. Mostly, the key success of proximal algorithms is to add indicator functions corresponding to nonlinear constraints, where the update steps that require a computing of proximal operators can often turn into an almost analytical form. The ADMM algorithm is also examined in related problems, such as an estimation of dynamic SEM <ref type="bibr" target="#b5">[BMG14]</ref>, kernel-based SEM <ref type="bibr" target="#b50">[SBG17]</ref> or a SEM with an elastic net <ref type="bibr" target="#b56">[TSG17]</ref>. In contrast to our ML formulation, all of these works have used a LS objective, so no positive definite constraint of Ψ (error covariance) was needed. However, the algorithms applied to our problem are more involved due to the cone constraint. The main proximal operators involved in our problem are the projection operator of v on a set C. We follow the standard format and implementation guidelines in [CP11b, BPC + 10]. Moreover, the notation of the proximal operator: prox f (y) = argmin x f (x) + (1/2) y -x 2 2 was applied here <ref type="bibr" target="#b43">[PB14]</ref>. Applying the algorithms to both the confirmatory and sparse SEMs is quite similar, so only the algorithm detail for the sparse SEM is presented.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Algorithms for confirmatory SEM</head><p>The confirmatory SEM (6) has three constraints. We split the objective function into two terms by defining functions f : S 2n → R, g : S 2n → R that cooperate the constraints in (6) as indicator functions as</p><formula xml:id="formula_18">f (Z) = -log det(Z 1 ) + tr(SZ 1 ) + I{P (Z 2 ) = I} + I{0 Z 4 αI}, g(Z) = I{Z 0}.</formula><p>We denote I{Z ∈ C} as an indicator function with a function value of 0 if Z ∈ C and ∞ otherwise. The ADMM format of the problem in ( <ref type="formula" target="#formula_6">6</ref>) is to minimize f (X) + g(Z) subject to X -Z = 0 with variables X, Z ∈ S 2n . For PPXA, we extend the variable into X = (X 1 , X 2 ) where X 1 = X 2 . The iteration update of both algorithms required deriving the proximal operators of f and g, which could be obtained in a similar way to the updates in the sparse SEM problem. Hence, we omit the details here and present them in the next section (section 4.2) instead.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Algorithms for sparse SEM</head><p>In the sparse SEM, we extend the variable into a larger product space and split the cost objective function into the three terms, where each of them corresponds to a simple proximal operator. Let us define functions f i : S 2n → R for i = 1, 2 and 3, as given by:</p><formula xml:id="formula_19">f 1 (Z) = -log det(Z 1 ) + tr(SZ 1 ) + I{0 Z 4 αI}, f 2 (Z) = 2γ (i,j) / ∈I A |(Z 2 ) ij | + I{P (Z 2 ) = I}, f 3 (Z) = I{Z 0}.</formula><p>Then it is obvious that the problem of (9) is equivalent to the minimization of f 1 + f 2 + f 3 . If we extend the variable into a product space then the problem is also equivalent to the form of a global consensus problem, shown in (12); minimize</p><formula xml:id="formula_20">(X1,X2,X3)∈S 2n ×S 2n ×S 2n f 1 (X 1 ) + f 2 (X 2 ) + f 3 (X 3 ) subject to X 1 = X 2 = X 3 ,<label>(12)</label></formula><p>which can be solved by many existing algorithms, such as the parallel proximal algorithm (PPXA) [CP11b, §10], and ADMM [BPC + 10]. The update rules of these two algorithms are described in the Appendix 10 and require proximal operators of f 1 , f 2 and f 3 as follows.</p><p>Proximal operator of f 1 . Finding prox f1/ρ (Y ) involves solving the problem:</p><formula xml:id="formula_21">minimize Z∈S 2n -log det(Z 1 ) + tr(SZ 1 ) + (ρ/2) Y -Z 2 F subject to 0 Z 4 αI.</formula><p>The blocks Z 1 , Z 2 and Z 4 in Z can be optimized independently because the Frobenious norm term is separable. Firstly, Z 2 = Y 2 and Z 4 is obtained by projecting the block Y 4 into the set C = {Z | 0 Z 4 αI}. To find Z 1 , we derive the zero gradient condition of the objective function with respect to Z 1 as shown in (13);</p><formula xml:id="formula_22">ρZ 1 -Z -1 1 = ρY 1 -S,<label>(13)</label></formula><p>with an implicit constraint: Z 1 0. To solve for Z 1 0 that satisfies (13), we can apply the technique from [BPC + 10, §6.5]. We compute the eigenvalue decomposition: ρY 1 -S = QΛQ T , where Q is the eigenvector matrix and Λ is diagonal and contains the eigenvalues. Then, we define Z1 as a diagonal matrix with decomposition</p><formula xml:id="formula_23">Z 1 = Q Z1 Q T .</formula><p>Substituting these decompositions into (13) gives ρz 2 -λz -1 = 0 where z is the diagonal entries of Z1 . We can solve the quadratic equation to obtain the positive root as z = (λ + λ 2 + 4ρ)/2ρ and then Z 1 is guaranteed to be positive definite.</p><p>Proximal operator of f 2 . In this step, to find prox f2/rho (Y ), we are required to solve the problem of the form:</p><formula xml:id="formula_24">minimize Z 2γ (i,j) / ∈I A |Z ij | + (ρ/2) Y -Z 2 F , subject to P (Z 2 ) = I.</formula><p>The problem is separable again in blocks of Z and the optimized Z 1 and Z 4 are simply Y 1 and Y 4 , respectively. It is left to find the block Z 2 where the entries of (i, j) / ∈ I A can be solved from a simple problem: minimize z 2γ|z| + ρ|z -y| 2 , typically known as finding a proximal operator of f (x) = |x|. The solution of minimizing the above problem can be performed by element-wise soft thresholding, defined by (Z 2 ) ij = S γ/ρ ((Y 2 ) ij ), where the soft thresholding operator [BPC + 10] is defined by: F . The solution is, therefore, the projection of Y onto the positive definite cone.</p><formula xml:id="formula_25">S k (a) = a -k, for a &gt; k, S k (a) = 0, for |a| ≤ k, S k (a) = a + k, for a &lt; -k,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Choice of algorithm parameters</head><p>The choice of ADMM parameter, ρ, affects the convergence speed and so an optimal selection is still an open question. Recent progress includes an adaptive formula of ρ varying upon the primal and dual residuals [BPC + 10] or the rule motivated by the Barzilai-Borwein spectral method <ref type="bibr" target="#b57">[XFG17]</ref>. However, this paper did not follow this direction but rather an experimentally derived practical choice of ρ that led to a convergence by a reasonable number of iterations was used. When considering (9), we found that good choices of ρ varied with the problem scale, as determined by (S, α). When the minimum eigenvalue of S was very small (≤ 10 -3 ), the ADMM seemed to converge slowly for ρ = 10, 100. If the problem of (9) was scaled properly by β, the choice of ρ = β worked well in practice.</p><p>The PPXA parameters described in [CP11b, §10] are γ &gt; 0 and λ ∈ (0, 2). We found in the experiment that if the problem is scaled to have λ min (S) = 1, the choice of γ = 0.1 and λ = 1.8 yield comparatively smaller number of iterations in our problem. Hence, from the two observations of implementing the two algorithms, we needed a result where solutions from the scaled and original problems could be interchangeably obtained.</p><p>Proposition 2. Let (X, Z) be the primal and dual optimal solutions of (9) and (10), respectively, using the problem parameter (S, α). Moreover, let ( X, Z) be the primal and dual optimal solutions of (9) and (10), respectively, using the problem parameter ( S, α), where S = βS and α = βα. Then, (X, Z) and ( X, Z) are related by (14),</p><formula xml:id="formula_26">X = X 1 /β X T 2 X 2 βX 4 , Z = βZ 1 Z T 2 Z 2 Z 4 /β . (<label>14</label></formula><formula xml:id="formula_27">)</formula><p>In other words, we can obtain the optimal solution of the scaled sparse SEM instantly from the solution of the unscaled problem, and vice versa.</p><p>The proof of this result is described in Appendix 9.3. To apply Proposition (2), we scale the sparse SEM by β = 1/λ min (S), so that the minimum eigenvalue of S = βS is one and α = 1. As a result, we solve the scaled sparse SEM using ( S, α) with a heuristic choice of parameters described in section 4.3 and retrieve the solution to the original problem using (14).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Algorithm performance</head><p>We show the performance of PPXA algorithm in solving the sparse SEM in (9) with n = 100, 200, . . . , 500 using 50 samples of S for each n. We set 20% of entries in A to satisfy the constraint P (A) = 0 in random locations. The algorithm performances generally depended on the optimization problem parameters. In this experiment, we generated S as a covariance matrix of the n-dimensional random vector Y and varied the eigenvalues of S into small λ min (S) ∈ (0.08, 0.09) or large λ min (S) ∈ (0.47, 0.48) using 2n and 10n samples of Y , respectively. A slower convergence was expected when λ min (S) was smaller. The second factor in a convergence is the regularization parameter, so we varied γ as 0.05γ max (dense solution) or 0.8γ max (sparse solution). The algorithm performances were evaluated in terms of the number of iterations required to satisfy the stopping criterion and by the CPU times which are averaged over 50 runs. The program stops when the relative change of the cost objective and the relative change of the solution are less than 10 -5 (low accuracy). The specifications of the computer used in this experiment are: CPU: Intel Core I7-8559U (4.5 GHz), RAM: 16GB DDR4 BUS2400, HD: SSD 500GB, OS: Ubuntu 18.04LTS. The codes were programmed in MATLAB and parallel computing toolbox was used in this simulation.</p><p>A comparison of convergence between ADMM and PPXA are shown in Figure <ref type="figure" target="#fig_3">3</ref> where PPXA appears to take less number of iterations (and hence less CPU time) than ADMM to achieve the same accuracy. Hence, only results of PPXA algorithm are reported in Tables <ref type="table" target="#tab_0">1</ref> and<ref type="table" target="#tab_1">2</ref>. Table <ref type="table" target="#tab_1">2</ref> shows that when S was almost degenerated, a higher number of iterations was required to achieve a desired accuracy. In addition, the algorithm converges slightly slower when using a large γ to obtain sparse solutions but the effect of varying γ was not apparent when λ min (S) was large. Solving the confirmatory and sparse SEMs with dimension n involves the total number of variables in X. The main computational cost of the two proximal algorithms for both problems depend on the eigenvalue decomposition in a symmetric matrix of size 2n, which is O((2n) 3 ). A trial problem with n = 500 and a given pattern in A, resulting in a total of 250, 000 variables, required approximately 4 -9 minutes of computational time in low accuracy mode. Such a large-scale setting may not be feasible when applying an iterative method based on the use of a Hessian matrix. We note that it is not our goal to make an intensive comparison of available algorithms in this study. Both algorithms are available for publicly tested at <ref type="url" target="https://github.com/jitkomut/cvxsem">https://github.com/jitkomut/cvxsem</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results (generated data)</head><p>This section describes the performance of the exploratory SEM presented in Section 3.4 with the scheme in Figure <ref type="figure" target="#fig_1">2</ref>. The goal was to examine how well the nonzero and zero entries in the path matrix could be estimated from the data generated from the true model encoded with a pattern of the true path matrix. Throughout this section, we set n = 10 and choose A true having random sparsity patterns and generated measurements from Y = (I -A true ) -1 e, where e is normally distributed with a variance of 0.1. The matrix S was then computed as the sample covariance of Y . The options of the data generating process were the number of samples and the density of non-zero elements in A true . In the estimation process, as required by the constraint P (A) = 0, we made assumptions about zero locations in A, varied by patterns and as the percentage of all zeros. To examine the performance, we applied the typical measures of true positives (TP), true negative (TN), false positives (FP), false negatives (FN), TP rate and FP rate, where positives are the non-zero entries in A and the negatives are zeros in A, through a receiver operating characteristic (ROC) curve <ref type="bibr" target="#b1">[Alp14]</ref>[ §19.7].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Performance of the sparse SEM</head><p>Four main aspects that could influence the estimation results were explored. These factors were the sparsity density of the true model (A true ), the number of samples (N ), the number of known zero locations used in the estimation and the choice of model selection scores. The experiments were then designed to investigate the effects of these factors as explained below.</p><p>Table <ref type="table">3</ref>: Chosen values of γ by BIC scores, displayed as a scaling factor in (0, 1) of γ max . % assumed zeros in A N = 100 N = 100, 000 dense A true sparse A true dense A true sparse A true 0% of assumed zeros 1.1 × 10 -3 2.62 × 10 -2 1.0 × 10 -4 6.9 × 10 -4 20% of assumed zeros 1.4 × 10 -3 3.36 × 10 -2 1.0 × 10 -4 1.1 × 10 -3 50% of assumed zeros 1.8 × 10 -3 3.36 × 10 -2 0.0 × 10 -4 1.1 × 10 -3</p><p>1. The sparsity density of A true . To this end, A true was generated at the two sparsity levels of 50% and 80% and the relationship between the sparsity pattern of Â that minimizes the BIC score and the error rate was observed. Comparing the FN from Figures <ref type="figure" target="#fig_4">4</ref> and<ref type="figure" target="#fig_6">5</ref> for a moderate sample size (N = 100) revealed that our method gave a lower FN when A true was sparse and a lower FP when A true was dense. Unavoidable errors as FP (when A true is sparse) and FN (when A true is dense) were commonly seen since these type of errors occur against the hypothesis of the true model. Moreover, when A true was dense, using AIC leads to the minimum total error (Figure <ref type="figure" target="#fig_4">4</ref>) since this score is prone to use a dense model (which agrees with the assumption of the true model). Similarly, when A true was sparse, using the scores penalising more on the model complexity, and so BIC, AICc and KICc yielded a lower total error (Figure <ref type="figure" target="#fig_6">5</ref>).</p><p>2. The number of samples. In the experiments, we use N = 100 (moderate size) and N = 100, 000 (large sample size) to examine the asymptotic properties of the estimates. When N was large, the selected γ was closer to zero (Table <ref type="table">3</ref>) since the sparse SEM (as a regularized problem) should yield the solution closer to that of non-regularized problem. The selected γ was also larger when the true model was sparse. Moreover, the FP was not improved (since the solutions are denser), but the FN obviously decreased when N increased (Figures <ref type="figure" target="#fig_4">4</ref> and<ref type="figure" target="#fig_6">5</ref>), showing that our regularized formulation was robust to FN errors.</p><p>3. The percentage of known zero locations in the estimation. To examine this factor, the experiments were performed with known zeros of 0%, 20% and 50%. The first two values corresponded to the problem with a negative df where the sparse SEM solution could be not unique, implying that the estimated zero pattern may not be as accurate as when knowing more zero locations. This, in principle, should affect the FP, as supported by Figures <ref type="figure" target="#fig_4">4</ref> and<ref type="figure" target="#fig_6">5</ref>, where a greater knowledge about the true zero locations in A true decreased the FP and overall total error, but FN seemed to be indifferent.</p><p>4. The choice of model selection scores. We considered the AIC (tended to choose dense models), AICc (adjusted for finite sample size), BIC, KIC and KICc scores (tended to choose simpler models). Figures <ref type="figure" target="#fig_4">4</ref> and<ref type="figure" target="#fig_6">5</ref> supported that the AIC tended to yield the minimum total error when A true is dense, and conversely, the BIC, AICc and KICc tended to provide lower total errors than those of the other criterions when A true was sparse.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Comparison with an existing method</head><p>Regsem package in R was developed to solve sSEM with a regularization term, including both the ridge and the least absolute shrinkage and selection operator (lasso) <ref type="bibr" target="#b28">[JGM16]</ref>. Regsem uses the RAM (Recticular Action Model) notation to derive an implied covariance matrix. The parameters of the general SEM were translated into three matrices: the filter matrix (F ), the direct path matrix (A) and the covariance matrix of variables (Ψ). In detail, regsem can be used to solved an optimization problem shown in (15); minimize -log det Σ -1 + tr(SΣ -1 ) -log det S -n + γ</p><formula xml:id="formula_28">(i,j) / ∈I A |A ij | subject to Σ -1 = F -T (I -A) T Ψ -1 (I -A)F -1 , P (A) = 0, (<label>15</label></formula><formula xml:id="formula_29">)</formula><p>with variables Σ, A and Ψ. It generalizes (3) to include the (i) 1 -regularization penalty on A and (ii) the filter matrix, F , since the RAM model also considers latent variables. However, our problem aimed to find the relationship among the observed variables only. Therefore, to be able to make a comparison between Regsem and our method, we explored the structure of a recursive path model, which is the simplest model in RAM and is described by the set of linear equations: w = Hx + 1 , x = Jv + 2 , v = Kz + 3 , where w, x, v and z are the observed vectors and H, J and K are the coefficient matrices. These equations can be written in y = Ay + u, as displayed in Figure <ref type="figure" target="#fig_7">6</ref>, where A has a certain sparse structure. In the experiment, we then generated H true , J true and K true as a true description of the RAM model, which also implied A true . The measurements were generated from y = (I -A true ) -1 u, where y ∈ R 12 and u ∼ N (0, 1).   (a) False Positive error. The FP from all model selection criterions tended decrease with more about the zero location in in the estimation process, but increased when N grew as all model selection criterions tended to select a denser Â. In the of small N , the BIC, AICc and KICc provided a better accuracy as the true was sparse.     In our sparse SEM estimation process, the constraint P (A) = 0 was encoded according to the structure of A true derived in Figure <ref type="figure" target="#fig_7">6</ref> that corresponded to a df of 42. For the Regsem, it was based on the lavaan package, which is a general SEM software program. This command required at least four arguments of our model generated by the lavaan library, type of estimation formulation (lasso or ridge regression), entries to be penalized and a penalty parameter. The argument pars pen = c(1 : 24) in the regsem function was used to set the number of penalized entries in the path matrix A, which here was 24 entries, according to its structure in Figure <ref type="figure" target="#fig_7">6</ref>. We also use this function to find a lower bound of the regularization parameter by selecting the minimum value of γ that forced all entries in A to be zero. For each data trial, we solved both the sparse SEM and regularized SEM in (15) by varying 50 values of γ in the range of [0, γ max ].</p><p>The ROC curves averaged from 100 trials (Figure <ref type="figure" target="#fig_8">7</ref>) illustrate that our sparse SEM achieved a greater accuracy than Regsem in both the moderate and high sample size settings. Although the Regsem problem of (15) estimated the matrix variables in a more general model than our methods, when it comes to the special case that reduced to a path analysis problem and contained the observed variables only, our method, customized to a path analysis problem, should perform better. However, the Regsem model can be more advantageous when solving a more general SEM problem.</p><p>6 Real-world application</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Air pollution and weather data</head><p>We explored the relation structure among 11 climate variables (reenhouse gas (SO 2 , NO 2 , O 3 and CO), solar radiation, relative humidity, temperature, particulate matter (PM 10 ), pressure and wind speed) in residential sites of Bangkok, the capital of Thailand, during February 15, 2007 to May 15, 2009. The hourly data were measured from eight stations and were standardized to have a zero mean and unit variance. We split the data into training and validation sets at a 2 : 1 ratio.</p><p>In the estimation process, if identifiability of the model was encouraged, zero df, as in (4), should be obtained. Hence, it required setting 55 entries in the estimated A to be zero encoded in the constraint P (X 2 ) = I. In order to obtain a reasonable constraint, we performed a partial correlation analysis on Y using partialcorr in MATLAB to find insignificant relations among the variables via the zero pattern in the sample partial correlation matrix (using a statistical test at a significance level of 0.01) and imposed the corresponding entries in A to zero.</p><p>The estimation process was performed as described in Section 3.4 and the path matrices from the model selection scheme of each station were selected. The nonzero entries in A and their magnitude defined the graphical model and explained the relationships of variables and their strength of connections. These relationship patterns differed from station to station (Figure <ref type="figure">9</ref>) so we compared a common network from all stations using similarity scores ranging from 50% to 100% (Figure <ref type="figure" target="#fig_9">8</ref>). The similarity score is the percentage of the number of common links from all stations relative to the number of all links. Dominant connections with a similarity score of 100% were the: Firstly, the strong relation between CO and NO 2 was supported by the known combustion reaction from car congestion in city areas. This appeared in our finding as the red color between the two variables from all stations in Figure <ref type="figure">9</ref>. Secondly, the relationship between the RH and temperature is known to be inverse to each other, given that the moisture content in the air is constant. As the temperature rises, the capacity of air to hold water increases, so if the actual amount of water in the air does not change, then the RH decreases. Mathematical expressions explaining the relationship between these two variables can be found in <ref type="bibr" target="#b36">[Law05]</ref>. Our result revealed an inverse relationship between the RH and temperature, as noted in the blue value from all stations in Figure <ref type="figure">9</ref>. Thirdly, the positive dependency between temperature and radiation, as indicated by the red color in Figure <ref type="figure">9</ref>, agrees with the natural characteristics of solar radiation, where as the sun rises up, the air temperature is increased. Nevertheless, in the area of meteorology, the daily solar radiation is typically explained by an increasing diurnal air temperature range, ∆T (the difference between the maximum and minimum temperature) [LML + 09].</p><p>In addition to the above relationships, the estimated structures with a similarity score of 75% in Figure <ref type="figure" target="#fig_9">8</ref>  The findings from [JL99] using a regression analysis explained that the wind speed and temperature had an inverse dependence on radiation attenuation, RH and pollution had a direct influence on radiation attenuation and that the reduction of solar radiation in the rainy season due to the pollution wash-out effect was not significantly different from the dry season. The relations among temperature, RH and radiation are consistent with several other researches <ref type="bibr" target="#b31">[JL99]</ref>, where the RH was found to have a direct influence on the radiation.</p><p>The connection between PM 10 and O 3 corresponds to a previous finding [JCS + 15], although they considered PM 2.5 instead of PM 10 . It is known that vehicle emissions are the main source of CO and NO 2 , and diesel vehicles especially can emit particular matter. Moreover, areas containing burning process, such as fossil fuel combustion, can be major sources of NO 2 and SO 2 . The airborne PM in Bangkok can be produced in areas of high temperature, such as from automobiles and biomass burning in residential areas <ref type="bibr" target="#b15">[CNLK08]</ref>. These facts relate to our findings of connections between CO-PM 10 , PM 10 -Temperature and NO 2 -SO 2 , where the last two relationships are present altogether in many stations (Figure <ref type="figure">9</ref>). As the temperature increases, the air expands and its density in that area is reduced, resulting in a decreasing variation in the air pressure. This agrees with the minus sign of the coefficient in the path matrix from temperature to pressure (Figure <ref type="figure">9</ref>). We conclude that the dependence structure learned from our model mostly agrees with known characteristics of environmental variables and with similar findings from previous studies. The structure of the optimal path matrix from each station when zero constraints of A from a partial correlation analysis is applied.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">fMRI data</head><p>In this experiment, we aimed to explore a common brain network between the control and autism groups under a resting-state condition learned from the Autism Brain Imaging Data Exchange data set [MYL + 14, AS17]. This data set contains 1112 fMRI images, collected across more than 24 international brain imaging laboratories. We selected data trials that contain enough samples for estimation; 46 images from the autism group and 40 images from the control group, giving a total of 86 images, provided by University of Michigan.</p><p>The functional preprocessing was performed by the Preprocessed Connectomes Project <ref type="bibr" target="#b16">[CP11a]</ref>, using a Configurable Pipeline for the Analysis of Connectomes. This performs a structural preprocessing of skull-stripping using AFNIs 3dSkullStrip, three-issue type brain segmentation using FAST in FSL and skull-stripped brain normalisation to MNI152 with linear and non-linear registrations using ANTs. It then performs a slice timing correction and motion realignment, respectively. The image intensity was normalised by four-dimensional global mean and a band-pass filtering in the range of 0.01-0.1 Hz was applied. All images from every subject were transformed from the original to the MNI152 template. To reduce the data dimension, we averaged the time series over the region of interest (ROI) using an automated anatomical labeling template [TMLP + 02], which was fractionated to a functional resolution (3 × 3 × 3mm 3 ) via nearest-neighbour interpolation.</p><p>As a result, we obtained Y ∈ R 90×249 from 90 ROIs with 249 time points. A prior assumption on the zero locations of A was set as P (A) = diag(A) = 0. Path matrices, A ∈ R 90×90 , as estimated from our approach, could be represented as a graphical model containing 90 nodes whose labels were described in [FCZ + 15]. For each γ, we specified a common optimal path matrix by searching for positions of nonzero entry that appeared at a frequency of more than 90% from all subjects in each group. The most frequently appeared links were denoted as significant connections between brain regions. We computed a common optimal path matrix by discarding insignificant entries and averaging only the significant entries over all subjects. We selected three values of γ, as γ = 0.0025γ max , 0.0182γ max and 0.135γ max , to produce three common path matrices with different density levels. Figure <ref type="figure" target="#fig_13">10</ref> shows the brain networks from the autism groups, as constructed by BrainNet Viewer <ref type="bibr">[XWH]</ref>. Our method generally provided nested graph structures as γ varies, where noticeable connections in the sparse structure also existed in the moderately sparse and dense structures.</p><p>To draw some conclusions on the network differences between the two groups, we selected the sparsest structures and compared the brain graphs in Figure <ref type="figure" target="#fig_0">11</ref>. An interesting observation was that the autism brain graph has a lower number of connections. The connections found only in the control group were:</p><formula xml:id="formula_30">MFG(R) ← MFG(L), ORBsupmed(R) ← ORBsupmed(L), DCG(R) ← ACG(R), PHG(L) ← PHG(R), HIP(R) ← PHG(R), PCUN(R) ← CUN(R), PCUN(L) ← SPG(L), IPL(R) ← SPG(R), CUN(R) ← PCUN(R), ROL(R) ← HES(R), PUT(R) ← PAL(L), STG(R) ← HES(R), TPOmid(R) ← TPOmid(L), ITG(R) ← ITG(L), ORBmid(R) ↔ ORBsup(R), LING(L) ↔ CAL(L), SOG(R) ↔ CUN(L), MOG(R) ↔ SOG(L), IOG(R) ↔ MOG(R), PoCG(L) ↔ PoCG(R), IPL(L) ↔ SPG(L), SMG(L) ↔ SMG(R), CAU(L) ↔ CAU(R), STG(L) ↔ HES(L), STG(L) ↔ STG(R) and MTG(L) ↔ MTG(R).</formula><p>From the list above (see abbreviations in [FCZ + 15]), it is worth comparing our results with previous studies on  These experiments [CRZ + 17] were designed to study the disruptive change in the state and strength of connectivities in the autism group, referred to as abnormal connectivities, which were also found in ITG and STG area as well. This is consistent with a previous report [CRZ + 15], which concluded that the MTG region is implicatexd in facial expressions, gesture representation impairments and theory of mind impairments in autism. We also found that relations from the precuneus (PCUN), the basal ganglia and the anterior cingulate cortex [PCUN(R) ↔ CUN(R), CAU(L) ↔ CAU(R) and DCG(R) ← ACG(R)] were still missing from the brain network in the autism group. Caudate nucleus (CAU), one of the components in the basal ganglia, affects many nonmotor functions, such as procedural learning <ref type="bibr" target="#b42">[NHM09]</ref>, while the PCUN is involved in self-consciousness, such as reflective self-awareness <ref type="bibr" target="#b33">[KNL02]</ref>. These results agree with <ref type="bibr" target="#b53">[SWQ17]</ref>, where brain networks were expressed as conditional independence graphs and revealed that, in the autism group, the edges linking to the precuneus, the basal ganglia, the anterior cingulate cortex and the medial frontal cortex were mostly affected. Moreover, underconnectivity among the ROIs linking to the PCUN were reported in the autism group <ref type="bibr" target="#b14">[CKKJ06]</ref>.</p><p>In conclusion, our result represents that within the brain network some circuits relating to cognitive processes, such as social interaction, face and image recognition, learning process or working memory, are missing from the autism group but exist in the control group. Our findings are similar to many previous studies. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions</head><p>This paper proposed two convex formulations for confirmatory and exploratory SEM, which have applications in learning causality among variables based on path analysis models. Our formulations relaxed the quadratic matrix equality into an inequality, where the solution to this formulation can be a solution to the original problem under homoskedastic assumptions of noise in the model. The proposed scheme of exploratory SEM exploits the feature of sparse estimation, introduced by adding an 1 regularization to the estimation function. Causality structures encoded from sparsity patterns of the path matrix can be obtained by sweeping values of regularization parameters in a specific range, which was derived analytically. Applying efficient efficient proximal parallel algorithm allowed the problems to be solved at a large-scale, including hundreds of variables, while it is not common to see existing SEM softwares solve such cases. Numerical results showed that (i) the percentage of known zeros of A was a key factor for decreasing the FP, regardless of the sparsity density in the true model, (ii) FN could be improved the most by increasing the sample size, (iii) the total error mainly came from FP and (iv) the choice of model selection criterions depended on the sparsity density in the true model. That is, BIC, AICc and KICc provided a better accuracy when the true path matrix was sparse, while AIC performed best when the true model was dense. Comparison between our approach with Regsem method was set in a fair setting, while considering a special case of the SEM model. Our method yielded higher accuracies where the desirable results could have benefited from either our formulation or the algorithms. Results from real data sets showed that the causality structures findings coincided with previous studies of applications. Most relations between climate variables learned from our model can be supported from environmental facts and previous research findings in the literature. While in the brain studies, although a groundtruth causal network of brain activities is not completely known, our findings were comparable with the literature with some extents. The brain networks from the control and autism groups are different in some brain regions, including the temporal gyrus group, PCUN and CAU areas.</p><p>8 Acknowledgment 9 Mathematical proofs</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.1">Proof of Proposition 1</head><p>We applied a generalization of Farka's lemma to semidefinite programming <ref type="bibr" target="#b9">[BV04]</ref>.</p><p>Lemma 3. [BV04] The system: U 0, tr(GU ) &gt; 0, tr(F i U ) = 0, i = 1, 2, . . . , n, is a strong alternative for the nonstrict linear matrix inequality (LMI):</p><formula xml:id="formula_31">n i=1 x i F i + G 0, if the matrices F i satisfy n i=1 v i F i 0 implies that n i=1 v i F i = 0.</formula><p>Using the notation of Ã = I -X2 , the feasibility problem of (8) was expressed as an LMI as shown in (16);</p><formula xml:id="formula_32">  S -1 (I -Ã) T 0 I -Ã X 4 0 0 0 αI -X 4   0 ⇐⇒   -S -1 -I 0 -I 0 0 0 0 -αI   G +   0 ÃT 0 Ã 0 0 0 0 0   ij Ãij Fij +   0 0 0 0 -X 4 0 0 0 X 4   ij (X4)ij Hij 0.<label>(16)</label></formula><p>The matrices F ij and H ij are a common choice of standard basis matrices that make up the above summation. In detail, let E ij be a standard basis matrix for the set of n × n matrices with zero diagonals and S ij be a standard basis matrix for S n . In other words, the entries of E ij are all zero except that the (i, j) entry is 1. Similarly, the entries of S ij are all zero except that (i, j) and (j, i) entries are 1. Note that Ã only contains non-zero entries for (i, j) / ∈ I A , so the expressions of F ij and H ij are</p><formula xml:id="formula_33">F ij =   0 E T ij 0 E ij 0 0 0 0 0   , for (i, j) / ∈ I A , H ij =   0 0 0 0 -S ij 0 0 0 S ij   , for i ≥ j = 1, 2, . . . , n.</formula><p>From Lemma 3, the LMI of (16) has no solution if and only if ∃U 0, U = 0 such that tr(GU ) &gt; 0, tr(F ij U ) = 0, for (i, j) / ∈ I A , tr(H ij U ) = 0, for i ≥ j. In what follows, we show that there always exists such a matrix U under the condition α ≤ α c . For scalars γ and β with β ≥ 0 and γ = 0, we construct U 0 of the form:</p><formula xml:id="formula_34">U =   (γ 2 /β)I γI 0 γI βI 0 0 0 βI   .</formula><p>With this choice, we can easily check that tr(F ij U ) = 0 regardless of the choice of I A (as long as I A contains the indices of diagonal entries of A), and that tr(H ij U ) = 0. Moreover, the condition tr(GU ) &gt; 0 is expressed as in (17),</p><formula xml:id="formula_35">tr(S -1 ) β γ 2 + 2nβ tr(S -1 ) • γ + n tr(S -1 ) αβ 2 &lt; 0. (<label>17</label></formula><formula xml:id="formula_36">)</formula><p>The above quadratic polynomial in γ can be expressed in terms of α and α c as γ 2 + 2α c βγ + αα c β 2 ≤ 0. Therefore, if α ≤ α c then we can always choose any negative real value of γ in the interval -α</p><formula xml:id="formula_37">c β(1 + 1 -α/α c ), -α c β(1 -1 -α/α c ) ,</formula><p>so that (17) is satisfied with strict inequality. Lastly, we also see that ij Ãij F ij + ij (X 4 ) ij H ij 0 only implies that X 4 = 0 and Ã = 0 because its leading (1, 1) block is zero. From Farka's lemma, this concludes that if α ≤ α c the feasibility problem of (16) always has no solution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.2">Derivation of γ max</head><p>We will show that there exists a critical value of γ, denoted by γ max , such that if γ ≥ γ max , then the optimal solution of A obtained by A = I -X 2 in (9) is zero. The derivation of γ max is, in fact, derived from the KKT conditions of (9) and under an assumption that the optimal primal solution is low rank.</p><p>KKT conditions of (9). If strong duality holds, X and Z are optimal if and only if the following conditions hold.</p><p>• Zero gradient of the Lagrangian: X 1 = (S -Z 1 ) -1 .</p><p>• Primal and Dual feasibility: X 0, 0 X 4 αI, Z 0, P c (Z 2 ) ∞ ≤ γ.</p><p>• Complementary slackness condition: ZX = 0 and Z 4 (X 4 -αI) = 0.</p><p>If the optimal X has rank n, then it follows from the complementary slackness condition that rank Z = n and rank Z 4 = n, so Z 4 is invertible. This further implies from the slackness condition: Z 4 (X 4 -αI) = 0 that X 4 = αI.</p><p>Since we aim to characterize the dual feasibility condition when we obtain the sparsest solution of A, we set A = 0 (or equivalently X 2 = I) in the optimal solution, then X = X 1 I I αI . We see that X has rank n if and only if</p><formula xml:id="formula_38">X 1 = (1/α)I. The zero gradient of the Lagrangian condition, Z 1 = S -X -1 1 = S -αI, is substituted in the slackness condition: ZX = 0, S -αI Z T 2 Z 2 Z 4 X 1 I I αI = 0</formula><p>from which we can solve for Z 2 as Z 2 = (1/α)(αI -S) and the dual feasibility condition becomes γ ≥ (1/α) P c (αI -S) ∞ . In conclusion, we showed that if X 2 = I is the optimal solution to (9) and the optimal X has rank n, then γ must exceed a particular value. The KKT conditions are sufficient and necessary conditions for the optimality of a convex problem. As a result, we can set γ max = (1/α) P c (αI -S) ∞ as the critical value of γ, and conclude that for any γ ≥ γ max , the optimal solution A must be zero.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.3">Solution of a scaled sparse SEM</head><p>We provide a proof of Proposition 2. We show that if (X, Z) satisfies the KKT conditions of the unscaled problem in (9) using parameter (S, α) then ( X, Z) provided in the statement also satisfies the KKT conditions of the scaled problem (9) using parameter ( S, α).</p><p>Primal feasibility. Given that X 0 which is equivalent to X 1 -X T 2 X -1 4 X 2 0, X 4 0, by the Schur complement, then for any β &gt; 0, it follows that the previous inequalities are preserved: βX 4 0, X 1 /β -X T 2 (βX 4 ) -1 X 2 ) 0 which are equivalent to X 0 by the Schur complement when X is given in (14). Moreover, it is obvious that if X 4 αI then X4 = βX 4 βαI = αI.</p><p>Dual feasibility. Given that Z 0, and by the Schur complement, we have Z 1 -Z T 2 Z -1 4 Z 2 0, Z 4 0. It follows in the same way that those inequalities can be scaled by a positive scalar and, therefore, are equivalent to Z 4 /β 0, βZ 1 -Z T 2 (Z 4 /β) -1 Z 2 0. By the Schur complement, this means we also have Z 0 when Z is given in (14). Next, since Z2 = Z 2 , we immediately have P c ( Z2 ) ∞ ≤ γ.</p><p>Zero gradient of the Lagrangian. If X 1 = (S -Z 1 ) -1 , then we scale both sides by 1/β and obtain X 1 /β = (β(S -Z 1 )) -1 , which is the same as X1 = ( S -Z1 ) -1 .</p><p>Complementary slackness. By a simple algebra, we easily confirm that if ZX = 0 then for any β = 0, we also have X Z = 0 where ( X, Z) are given in (14). Moreover, if Z 4 (X 4 -αI) = 0 then (Z 4 /β)(βX 4 -βαI) = Z4 ( X4 -αI) = 0.</p><p>In addition to above results, we can check from (14) that if (X, Z) has low rank properties: X 4 = X 1 -X T 2 X -1 4 Z 2 and Z 4 = Z 1 -Z T 2 Z -1 4 Z 2 , then ( X, Z) also has a low rank.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10">Algorithm description</head><p>In this section, we describe the update rules of proximal algorithms used to solve the sparse SEM (9) when it is arranged in the format of (12). ADMM and PPXA algorithms require proximal steps as explained in section 4. An initial solution is selected to be X 0 = S -1 0 0 αI . The main variable is X where X and X + denote the current and next iteration variable, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.1">ADMM algorithm</head><p>Firstly, we initialize X = (X 0 , X 0 , X 0 ), Z = X 0 and Y = (0, 0, 0) ∈ S 2n × S 2n × S 2n . Repeat the following steps:</p><formula xml:id="formula_39">X + i = prox fi/ρ (Z -Y i /ρ</formula><p>), for i = 1, 2, 3,</p><formula xml:id="formula_40">Z + = (1/3) 3 i=1 X + i , Y + i = Y i + ρ(X + i -Z + ), for i = 1, 2, 3</formula><p>until the primal and dual residual norms are less than a threshold [BPC + 10, §7].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.2">PPXA algorithm</head><p>The update rule follows directly from [CP11b, §10] when we choose ω i = 1/3 for i = 1, 2 and 3 (uniform.) Firstly, we initialize Y = (X 0 , X 0 , X 0 ), X = X 0 , P = (X 0 , X 0 , X 0 ), P = (1/3)  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure1: The effect of α on the solutions of the convex confirmatory SEM. Left. rank(Z) as α varies. Right. Error between X 1 and the low rank approximation as α varies. Lines with the same color correspond to the result from using the same n and shown as results of many trials. copyright [2016] IEEE. Reprinted, with permission, from the Proceedings of 2016 55th Annual Conference of the Society of Instrument and Control Engineers of Japan (SICE).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Procedures of learning causal structure of path matrices in SEM. The best optimal causal structure of path matrices is chosen from a model that has the minimum model selection score.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>or equivalently S k (a) = max(|a| -k) • sign(a). Lastly, for (i, j) ∈ I A we must have P (Z 2 ) = I.Proximal operator of f 3 . This step solves the problem: minimize Z 0 Y -Z 2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Algorithm performances as n varies under the setting of small λ min (S) and γ = 0.05γ max .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Averaged (a) FP, (b) FN and (c) total error from 50 runs of a sample covariance matrix S, when A true was dense. The AIC provided the minimum error when N was small.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>False Negative (FN) error. Using more knowledge about zero location in Atrue in the estimation process barely affected the change of FN, but it improved when N grew.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Averaged (a) FP, (b) FN and (c) total error from 50 runs of sample covariance matrix S, when A true was sparse. The BIC, AICc and KICc provided lower total errors when N was small.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: The recursive path model used in the comparison of our model with Regsem.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: The ROC curve from Regsem and sparse SEM averaged from 100 trials of a (a) moderate (N = 100) or high (N = 1,000) sample size setting.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Common relationship pattern of variables from all stations with various similarity scores.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>included the relations between CO-PM 10 , PM 10 -O 3 , Temperature-O 3 , RH-O 3 , PM 10 -Temperature, NO 2 -SO 2 , RH-Radiation, RH → Pressure, Radiation-Pressure and Temperature-Pressure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>Figure9: The structure of the optimal path matrix from each station when zero constraints of A from a partial correlation analysis is applied.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>Moderately sparse network. (c) Dense network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: A common brain network from the autism group at three graph density levels of (a) sparse, (b) moderately sparse and (c) dense. The link widths varied with the magnitudes of entries in the estimated path matrix. The red and blue link represent the positive and negative magnitude, respectively.</figDesc><graphic coords="18,52.39,50.15,161.27,120.95" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head></head><label></label><figDesc>Figure11: Comparing the brain maps between the autism and control groups. Black edges are common links that appear in both groups, red edges are ones found only in the autism group and blue edges are links detected only in the control group.</figDesc><graphic coords="18,59.10,548.63,236.87,177.65" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>3=</head><label></label><figDesc>i=1 P i . Repeat the following steps:P + i = prox γfi/ωi (Y i ), for i = 1, 2Y i + λ(2 P + -X -P i ), for i = 1, 2, 3, X + = X + λ( P + -X)until a stopping criterion (relative change of cost objective) is less than a threshold.11 Brain Region of Interest (ROI)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Averaged CPU times and standard deviations (in parentheses) of PPXA algorithm.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">CPU time (seconds)</cell><cell></cell></row><row><cell>n</cell><cell cols="2">γ = 0.05γ max</cell><cell cols="2">γ = 0.8γ max</cell></row><row><cell></cell><cell cols="4">small λ min (S) large λ min (S) small λ min (S) large λ min (S)</cell></row><row><cell>100</cell><cell>3.92 (0.33)</cell><cell>3.14 (0.19)</cell><cell>6.19 (0.62)</cell><cell>3.42 (0.28)</cell></row><row><cell>200</cell><cell>21.83 (1.81)</cell><cell>17.73 (1.24)</cell><cell>35.78 (2.89)</cell><cell>20.06 (1.64)</cell></row><row><cell>300</cell><cell>66.41 (5.52)</cell><cell>52.74 (3.95)</cell><cell>109.24 (8.65)</cell><cell>60.63 (5.46)</cell></row><row><cell cols="5">400 147.94 (15.05) 115.42 (8.96) 245.30 (22.07) 134.58 (11.14)</cell></row><row><cell cols="5">500 290.48 (26.74) 222.03 (16.18) 495.80 (41.11) 267.63 (25.54)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Averaged number of iterations and standard deviations (in parentheses) of PPXA algorithm.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Number of iterations</cell><cell></cell></row><row><cell>n</cell><cell cols="2">γ = 0.05γ max</cell><cell cols="2">γ = 0.8γ max</cell></row><row><cell></cell><cell cols="4">small λ min (S) large λ min (S) small λ min (S) large λ min (S)</cell></row><row><cell>100</cell><cell>117 (9)</cell><cell>93 (2)</cell><cell>215 (19)</cell><cell>112 (5)</cell></row><row><cell>200</cell><cell>117 (6)</cell><cell>92 (2)</cell><cell>221 (12)</cell><cell>116 (3)</cell></row><row><cell>300</cell><cell>120 (7)</cell><cell>92 (2)</cell><cell>225 (10)</cell><cell>117 (2)</cell></row><row><cell>400</cell><cell>122 (8)</cell><cell>91 (3)</cell><cell>227 (11)</cell><cell>118 (2)</cell></row><row><cell>500</cell><cell>122 (6)</cell><cell>90 (3)</cell><cell>226 (8)</cell><cell>118 (2)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>False Positive (FP) error. When Atrue is dense, the FP from all model selection criterions tended to decrease with increasing knowledge about the zero location in Atrue, but increased when N was high as all model selection criterions tended to select the denser Â. In the case of a small N , AICc provided the minimum FP error.</figDesc><table><row><cell>0.09 0.1 0.11 0.12</cell><cell></cell><cell>0.1096</cell><cell>0.1180</cell><cell>0.1013</cell><cell>0.1142</cell><cell>0.1029</cell><cell>0.1062</cell><cell>0.1158</cell><cell>0.0976</cell><cell>0.1129</cell><cell>0.1007</cell><cell>0.1027</cell><cell>0.1111</cell><cell>0.0918</cell><cell>0.1082</cell><cell>0.0951</cell><cell>BIC AIC AICC KIC KICC</cell></row><row><cell>0.08</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.13 0.14 0.15</cell><cell></cell><cell cols="5">0.1320 0.1329 0.1329 0.1322 0.1322</cell><cell cols="5">0.1284 0.1287 0.1287 0.1287 0.1287</cell><cell cols="5">0.1238 0.1240 0.1240 0.1240 0.1240</cell><cell>BIC AIC AICC KIC KICC</cell></row><row><cell>0.12</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.11</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.1</cell><cell></cell><cell></cell><cell></cell><cell>1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>3</cell><cell></cell><cell></cell></row><row><cell>0.04 0.06 (a) 0.02 0.08 0.1</cell><cell></cell><cell>0.0573</cell><cell>0.0356</cell><cell>0.0876</cell><cell>0.0431</cell><cell>0.0773</cell><cell>0.0582</cell><cell>0.0360</cell><cell>0.0882</cell><cell>0.0433</cell><cell>0.0778</cell><cell>0.0576</cell><cell>0.0356</cell><cell>0.0887</cell><cell>0.0429</cell><cell>0.0778</cell><cell>BIC AIC AICC KIC KICC</cell></row><row><cell>1.5 2 2.5</cell><cell>×10 -3</cell><cell>0.0022</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.0022</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.0022</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>BIC AIC AICC KIC KICC</cell></row><row><cell>1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.5</cell><cell></cell><cell></cell><cell cols="4">0.0002 0.0002 0.0002 0.0002</cell><cell></cell><cell cols="4">0.0002 0.0002 0.0002 0.0002</cell><cell></cell><cell cols="4">0.0002 0.0002 0.0002 0.0002</cell></row><row><cell>0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">0.15 0.16 0.17 0.18 0.19 0.2</cell><cell>0.1669</cell><cell>0.1536</cell><cell>0.1889</cell><cell>0.1573</cell><cell>0.1802</cell><cell>0.1644</cell><cell>0.1518</cell><cell>0.1858</cell><cell>0.1562</cell><cell>0.1784</cell><cell>0.1602</cell><cell>0.1467</cell><cell>0.1804</cell><cell>0.1511</cell><cell>0.1729</cell><cell>BIC AIC AICC KIC KICC</cell></row><row><cell cols="2">0.14</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">0.13</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">0.13 0.14 0.15</cell><cell cols="5">0.1342 0.1331 0.1331 0.1324 0.1324</cell><cell cols="5">0.1307 0.1289 0.1289 0.1289 0.1289</cell><cell cols="5">0.1260 0.1242 0.1242 0.1242 0.1242</cell><cell>BIC AIC AICC KIC KICC</cell></row><row><cell cols="2">0.12</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">0.11</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">0.1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note><p><p>(b) False Negative (FN) error. When increasing the knowledge about the zero location in Atrue the estimation process barely affected the change of FN, but it was improved when N grows.</p>(c) Total error. Main portion of the total error comes from FP so it tended to decrease with a greater assumption about the true zero location in Atrue.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Names of region of interests (ROIs) in fMRI connectivity modeling according to Automated Anatomical Labeling (AAL) template.</figDesc><table><row><cell>No.</cell><cell>Name</cell><cell cols="2">No. name</cell></row><row><cell>1</cell><cell>Left precentral gyrus (PreCG.L)</cell><cell>2</cell><cell>Right precentral gyrus (PreCG.R)</cell></row><row><cell>3</cell><cell>Left superior frontal gyrus (SFGdor.L)</cell><cell>4</cell><cell>Right superior frontal gyrus (SFGdor.R)</cell></row><row><cell>5</cell><cell>Left superior frontal gyrus, orbital part (ORB-</cell><cell>6</cell><cell>Right superior frontal gyrus, orbital part</cell></row><row><cell></cell><cell>sup.L)</cell><cell></cell><cell>(ORBsup.R)</cell></row><row><cell>7</cell><cell>Left middle frontal gyrus (MFG.L)</cell><cell>8</cell><cell>Right middle frontal gyrus (MFG.R)</cell></row><row><cell>9</cell><cell>Left middle frontal gyrus, orbital part (ORB-</cell><cell>10</cell><cell>Right middle frontal gyrus, orbital part (ORB-</cell></row><row><cell></cell><cell>mid.L)</cell><cell></cell><cell>mid.R)</cell></row><row><cell>11</cell><cell>Left inferior frontal gyrus, pars opercularis</cell><cell>12</cell><cell>Right inferior frontal gyrus, pars opercularis</cell></row><row><cell></cell><cell>(IFGoperc.L)</cell><cell></cell><cell>(IFGoperc.R)</cell></row><row><cell>13</cell><cell>Left inferior frontal gyrus, pars triangularis</cell><cell>14</cell><cell>Right inferior frontal gyrus, pars triangularis</cell></row><row><cell></cell><cell>(IFGtriang.L)</cell><cell></cell><cell>(IFGtriang.R)</cell></row><row><cell>15</cell><cell>Left inferior frontal gyrus, pars orbitalis</cell><cell>16</cell><cell>Right inferior frontal gyrus, pars orbitalis</cell></row><row><cell></cell><cell>(ORBinf.L)</cell><cell></cell><cell>(ORBinf.R)</cell></row><row><cell>17</cell><cell>Left Rolandic operculum (ROL.L)</cell><cell>18</cell><cell>Right Rolandic operculum (ROL.R)</cell></row><row><cell>19</cell><cell>Left supplementary motor area (SMA.L)</cell><cell>20</cell><cell>Right supplementary motor area (SMA.R)</cell></row><row><cell>21</cell><cell>Left olfactory cortex (OLF.L)</cell><cell>22</cell><cell>Right olfactory cortex (OLF.R)</cell></row><row><cell>23</cell><cell>Left medial frontal gyrus (SFGmed.L)</cell><cell>24</cell><cell>Right medial frontal gyrus (SFGmed.R)</cell></row><row><cell>25</cell><cell>Left medial orbitofrontal cortex (ORB-</cell><cell>26</cell><cell>Right medial orbitofrontal cortex (ORB-</cell></row><row><cell></cell><cell>supmed.L)</cell><cell></cell><cell>supmed.R)</cell></row><row><cell>27</cell><cell>Left gyrus rectus (REC.L)</cell><cell>28</cell><cell>Right gyrus rectus (REC.R)</cell></row><row><cell>29</cell><cell>Left insula (INS.L)</cell><cell>30</cell><cell>Right insula (INS.R)</cell></row><row><cell>31</cell><cell>Left anterior cingulate gyrus (ACG.L)</cell><cell>32</cell><cell>Right anterior cingulate gyrus (ACG.R)</cell></row><row><cell>33</cell><cell>Left midcingulate area (DCG.L)</cell><cell>34</cell><cell>Right midcingulate area (DCG.R)</cell></row><row><cell>35</cell><cell>Left posterior cingulate gyrus (PCG.L)</cell><cell>36</cell><cell>Right posterior cingulate gyrus (PCG.R)</cell></row><row><cell>37</cell><cell>Left hippocampus (HIP.L)</cell><cell>38</cell><cell>Right hippocampus (HIP.R)</cell></row><row><cell>39</cell><cell>Left parahippocampal gyrus (PHG.L)</cell><cell>40</cell><cell>Right parahippocampal gyrus (PHG.R)</cell></row><row><cell>41</cell><cell>Left amygdala (AMYG.L)</cell><cell>42</cell><cell>Right amygdala (AMYG.R)</cell></row><row><cell>43</cell><cell>Left calcarine sulcus (CAL.L)</cell><cell>44</cell><cell>Right calcarine sulcus (CAL.R)</cell></row><row><cell>45</cell><cell>Left cuneus (CUN.L)</cell><cell>46</cell><cell>Right cuneus (CUN.L)</cell></row><row><cell>47</cell><cell>Left lingual gyrus (LING.L)</cell><cell>48</cell><cell>Right lingual gyrus (LING.R)</cell></row><row><cell>49</cell><cell>Left superior occipital (SOG.L)</cell><cell>50</cell><cell>Right superior occipital (SOG.R)</cell></row><row><cell>51</cell><cell>Left middle occipital gyrus (MOG.L)</cell><cell>52</cell><cell>Right middle occipital gyrus (MOG.R)</cell></row><row><cell>53</cell><cell>Left inferior occipital cortex (IOG.L)</cell><cell>54</cell><cell>Right inferior occipital cortex (IOG.R)</cell></row><row><cell>55</cell><cell>Left fusiform gyrus (FFG.L)</cell><cell>56</cell><cell>Right fusiform gyrus (FFG.R)</cell></row><row><cell>57</cell><cell>Left postcentral gyrus (PoCG.L)</cell><cell>58</cell><cell>Rightpostcentral gyrus (PoCG.R)</cell></row><row><cell>59</cell><cell>Left superior parietal lobule (SPG.L)</cell><cell>60</cell><cell>Right superior parietal lobule (SPG.R)</cell></row><row><cell>61</cell><cell>Left inferior parietal lobule (IPL.L)</cell><cell>62</cell><cell>Right inferior parietal lobule (IPL.R)</cell></row><row><cell>63</cell><cell>Left supramarginal gyrus (SMG.L)</cell><cell>64</cell><cell>Right supramarginal gyrus (SMG.R)</cell></row><row><cell>65</cell><cell>Left angular gyrus (ANG.L)</cell><cell>66</cell><cell>Right angular gyrus (ANG.R)</cell></row><row><cell>67</cell><cell>Left precuneus (PCUN.L)</cell><cell>68</cell><cell>Right precuneus (PCUN.R)</cell></row><row><cell>69</cell><cell>Left paracentral lobule (PCL.L)</cell><cell>70</cell><cell>Right paracentral lobule (PCL.R)</cell></row><row><cell>71</cell><cell>Left caudate nucleus (CAU.L)</cell><cell>72</cell><cell>Right caudate nucleus (CAU.R)</cell></row><row><cell>73</cell><cell>Left putamen (PUT.L)</cell><cell>74</cell><cell>Right putamen (PUT.R)</cell></row><row><cell>75</cell><cell>Left globus pallidus (PAL.L)</cell><cell>76</cell><cell>Right globus pallidus (PAL.R)</cell></row><row><cell>77</cell><cell>Left thalamus (THA.L)</cell><cell>78</cell><cell>Right thalamus (THA.R)</cell></row><row><cell>79</cell><cell>Left transverse temporal gyrus (HES.L)</cell><cell>80</cell><cell>Right transverse temporal gyrus (HES.R)</cell></row><row><cell>81</cell><cell>Left superior temporal gyrus (STG.L)</cell><cell>82</cell><cell>Right superior temporal gyrus (STG.R)</cell></row><row><cell>83</cell><cell>Left superior temporal pole (TPOsup.L)</cell><cell>84</cell><cell>Right superior temporal pole (TPOsup.R)</cell></row><row><cell>85</cell><cell>Left middle temporal gyrus (MTG.L)</cell><cell>86</cell><cell>Right right middle temporal gyrus (MTG.R)</cell></row><row><cell>87</cell><cell>Left middle temporal pole (TPOmid.L)</cell><cell>88</cell><cell>Right middle temporal pole (TPOmid.R)</cell></row><row><cell>89</cell><cell>Left inferior temporal gyrus (ITG.L)</cell><cell>90</cell><cell>Right inferior temporal gyrus (ITG.R)</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>We thank the <rs type="affiliation">Department of City Planning, Bangkok Metropolitan Administration and Pollution Control Department, Bangkok</rs>, for providing the relevant information on air pollution. This researh project was financially supported by a <rs type="funder">Chula Engineering</rs> research grant.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Stimulating the brain&apos;s language network: syntactic ambiguity resolution after TMS to the inferior frontal gyrus and middle temporal gyrus</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Acheson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hagoort</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Cognitive Neuroscience</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1664" to="1677" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">E</forename><surname>Alpaydin</surname></persName>
		</author>
		<title level="m">Introduction to Machine Learning</title>
		<imprint>
			<publisher>MIT press</publisher>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Adriana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Stewart</surname></persName>
		</author>
		<ptr target="http://fcon_1000.projects.nitrc.org/indi/abide/" />
		<title level="m">Autism Brain Imaging Data Exchange (ABIDE)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">A proximal approach for a class of matrix optimization problems</title>
		<author>
			<persName><forename type="first">A</forename><surname>Benfenati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Chouzenoux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pesquet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.07452</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Modulation of connectivity in visual pathways by attention: Cortical interactions evaluated with structural equation modelling and fMRI</title>
		<author>
			<persName><forename type="first">C</forename><surname>Büchel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J</forename><surname>Friston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><forename type="middle">E</forename><surname>Bullmore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Horwitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Honey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Brammer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sharma</surname></persName>
		</author>
		<idno>BHH + 00</idno>
	</analytic>
	<monogr>
		<title level="j">Cerebral cortex</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="289" to="301" />
			<date type="published" when="1997">1997. 2000</date>
		</imprint>
	</monogr>
	<note>NeuroImage</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Proximal-gradient algorithms for tracking cascades over social networks</title>
		<author>
			<persName><forename type="first">B</forename><surname>Baingana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Mateos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">B</forename><surname>Giannakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Selected Topics in Signal Processing</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="563" to="575" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Superior temporal gyrus, language function, and autism. Developmental</title>
		<author>
			<persName><forename type="first">]</forename><forename type="middle">E D</forename><surname>Bmn + 07</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bigler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">S</forename><surname>Mortensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Neeley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ozonoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Krasny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Provencal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Mcmahon</surname></persName>
		</author>
		<author>
			<persName><surname>Lainhart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuropsychology</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="217" to="238" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Structural equations with latent variables</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Bollen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989">1989</date>
			<publisher>John Wiley &amp; Sons</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Distributed optimization and statistical learning via the alternating direction method of multipliers</title>
		<author>
			<persName><forename type="first">]</forename><forename type="middle">S</forename><surname>Bpc + 10</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Peleato</surname></persName>
		</author>
		<author>
			<persName><surname>Eckstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends in Machine Learning</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="122" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Vandenberghe</surname></persName>
		</author>
		<ptr target="www.stanford.edu/~boyd/cvxbook" />
		<title level="m">Convex Optimization</title>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Causal network inference via group sparse regularization</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bolstad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">D</forename><surname>Van Veen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Nowak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2628" to="2641" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A large-sample model selection criterion based on Kullback&apos;s symmetric divergence</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Cavanaugh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistics &amp; Probability Letters</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="333" to="343" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Gene network inference via sparse structural equation modeling with genetic perturbations</title>
		<author>
			<persName><forename type="first">X</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Bazerque</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">B</forename><surname>Giannakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE International Workshop on Genomic Signal Processing and Statistics (GENSIPS)</title>
		<imprint>
			<biblScope unit="page" from="66" to="69" />
			<date type="published" when="2011">2011. 2011</date>
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Vector autoregression, structural equation modeling, and their synthesis in neuroimaging data analysis</title>
		<author>
			<persName><forename type="first">]</forename><forename type="middle">G</forename><surname>Cgs + 11</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">S</forename><surname>Glen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Saad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">H</forename><surname>Thomason</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">W</forename><surname>Gotlib</surname></persName>
		</author>
		<author>
			<persName><surname>Cox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers in biology and medicine</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="1142" to="1155" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Functional connectivity in a baseline restingstate network in autism</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">L</forename><surname>Cherkassky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Kana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">A</forename><surname>Keller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Just</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeuroReport</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">16</biblScope>
			<biblScope unit="page" from="1687" to="1690" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Levels and major sources of PM 2.5 and PM 10 in Bangkok Metropolitan Region</title>
		<author>
			<persName><forename type="first">N</forename><surname>Chuersuwan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nimrat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lekphet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kerdkumrai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Environment international</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="671" to="677" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Cameron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Pierre</surname></persName>
		</author>
		<ptr target="http://preprocessed-connectomes-project.org/abide/" />
		<title level="m">Preprocessed Connectomes Project (PCP)</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Proximal splitting methods in signal processing</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">L</forename><surname>Combettes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Pesquet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fixed-point algorithms for inverse problems in science and engineering</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="185" to="212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Autism: reduced connectivity between cortical areas involved in face expression, theory of mind, and the sense of self</title>
		<author>
			<persName><forename type="first">]</forename><forename type="middle">W</forename><surname>Crz + 15</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">T</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rolls</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><surname>Gu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Brain</title>
		<imprint>
			<biblScope unit="volume">138</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1382" to="1393" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Functional connectivity decreases in autism in emotion, self, and face circuits identified by knowledge-based enrichment analysis</title>
		<author>
			<persName><forename type="first">]</forename><forename type="middle">W</forename><surname>Crz + 17</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">T</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rolls</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeuroImage</title>
		<imprint>
			<biblScope unit="volume">148</biblScope>
			<biblScope unit="page" from="169" to="178" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Generalized lasso with under-determined regularization matrices</title>
		<author>
			<persName><forename type="first">]</forename><forename type="middle">J</forename><surname>Dsb + 16</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Soussen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Brie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Idier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal processing</title>
		<imprint>
			<biblScope unit="volume">127</biblScope>
			<biblScope unit="page" from="239" to="246" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Dynamic brain architectures in local brain activity and functional network efficiency associate with efficient reading in bilinguals</title>
		<author>
			<persName><forename type="first">]</forename><forename type="middle">G</forename><surname>Fcz + 15</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">C</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeuroImage</title>
		<imprint>
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="page" from="103" to="118" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Joint estimation of multiple graphical models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Levina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Michailidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">98</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="15" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A penalized likelihood method for structural equation modeling</title>
		<author>
			<persName><forename type="first">P</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Weng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychometrika</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="329" to="354" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">H</forename><surname>Hoyle</surname></persName>
		</author>
		<title level="m">Structural equation modeling: Concepts, issues, and applications</title>
		<imprint>
			<publisher>Sage Publications</publisher>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<author>
			<persName><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Friedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Elements of Statistical Learning</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Friedman</surname></persName>
		</author>
		<title level="m">The Elements of Statistical Learning: Data Mining, Inference and Prediction</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
	<note>2nd edition</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">The impact of weather changes on air quality and health in the United States in 1994-2012</title>
		<author>
			<persName><forename type="first">I</forename><surname>Jhun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Coull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hubbell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Koutrakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Environmental Research Letters</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">84009</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Regularized structural equation modeling</title>
		<author>
			<persName><forename type="first">R</forename><surname>Jacobucci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J</forename><surname>Grimm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Mcardle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Structural equation modeling: a multidisciplinary journal</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="555" to="566" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Graphical lasso based model selection for time series</title>
		<author>
			<persName><forename type="first">A</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hannak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Goertz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1781" to="1785" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Exploratory structural equation modeling of resting-state fMRI: applicability of group models to individual subjects</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Kelley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Craddock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">E</forename><surname>Holtzheimer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">W</forename><surname>Dunlop</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">B</forename><surname>Nemeroff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">S</forename><surname>Mayberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">P</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeuroImage</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="778" to="787" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Global radiation attenuation by air pollution and its effects on the thermal climate in Mexico city</title>
		<author>
			<persName><forename type="first">E</forename><surname>Jauregui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Luyando</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Climatology</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="683" to="694" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><surname>Jöreskog</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sörbom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Du</forename><surname>Toit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Du</forename><surname>Toit</surname></persName>
		</author>
		<title level="m">LISREL 8: New statistical features. Scientific Software International</title>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Reflective self-awareness and conscious states: PET evidence for a common midline parietofrontal core</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">W</forename><surname>Kjaer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Nowak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">C</forename><surname>Lou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeuroImage</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1080" to="1086" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">An Introduction to Brain and Behavior</title>
		<author>
			<persName><forename type="first">B</forename><surname>Kolb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">Q</forename><surname>Whishaw</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>Worth Publishers</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Unified structural equation modeling approach for the analysis of multisubject, multivariate functional MRI data</title>
		<author>
			<persName><forename type="first">]</forename><forename type="middle">J</forename><surname>Kzc + 07</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Bentler</surname></persName>
		</author>
		<author>
			<persName><surname>Ernst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Human Brain Mapping</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="85" to="93" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">The relationship between relative humidity and the dewpoint temperature in moist air: A simple conversion and applications</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Jensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Porter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bulletin of the American Meteorological Society</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1433" to="1446" />
			<date type="published" when="2005">2005. 2009</date>
		</imprint>
	</monogr>
	<note>Agricultural and Forest Meteorology</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Structural equation modeling and its application to network analysis in functional brain imaging</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Mcintosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Gonzalez-Lima</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Human Brain Mapping</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="2" to="22" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">The graphical lasso: New insights and alternatives</title>
		<author>
			<persName><forename type="first">R</forename><surname>Mazumder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Electronic journal of statistics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">2125</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Joint structural estimation of multiple graphical models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Michailidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="5777" to="5824" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Linear causal modeling with structural equations</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Mulaik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>CRC Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">The Autism brain imaging data exchange: Towards a large-scale evaluation of the intrinsic brain architecture in autism</title>
		<author>
			<persName><forename type="first">]</forename><forename type="middle">A</forename><surname>Myl + 14</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">G</forename><surname>Di Martino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">X</forename><surname>Denio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Castellanos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Alaerts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Y</forename><surname>Assaf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bookheimer</surname></persName>
		</author>
		<author>
			<persName><surname>Dapretto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Molecular psychiatry</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">659</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Molecular Neuropharmacology: A Foundation for Clinical Neuroscience</title>
		<author>
			<persName><forename type="first">E</forename><surname>Nestler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hyman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Malenka</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>McGraw-Hill Medical</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
	<note>second edition</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Proximal algorithms</title>
		<author>
			<persName><forename type="first">N</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Boyd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends in Optimization</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="127" to="239" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">High-dimensional learning of linear causal networks via inverse covariance estimation</title>
		<author>
			<persName><forename type="first">P</forename><surname>Po-Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Loh</surname></persName>
		</author>
		<author>
			<persName><surname>Bühlmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3065" to="3105" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Modeling dynamic functional neuroimaging data using structural equation modeling</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">R</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Laird</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Ingham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Structural Equation Modeling: a Multidisciplinary Journal</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="147" to="162" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Convex formulations for path analysis problems in structural equation modeling</title>
		<author>
			<persName><forename type="first">A</forename><surname>Pruttiakaravanich</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
		<respStmt>
			<orgName>Chulalongkorn University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Master&apos;s thesis</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">A convex formulation for path analysis in structural equation modeling</title>
		<author>
			<persName><forename type="first">A</forename><surname>Pruttiakaravanich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Songsiri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeddings of the 55th Annual Conference of the Society of Instrument and Control Engineers of Japan (SICE)</title>
		<meeting>eeddings of the 55th Annual Conference of the Society of Instrument and Control Engineers of Japan (SICE)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="282" to="287" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">A First Course in Structural Equation Modeling</title>
		<author>
			<persName><forename type="first">T</forename><surname>Raykov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Marcoulides</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>Lawrence Eribaum Associates, Inc</publisher>
		</imprint>
	</monogr>
	<note>second edition</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">A small sample model selection criterion based on kullback&apos;s symmetric divergence</title>
		<author>
			<persName><forename type="first">A</forename><surname>Seghouane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bekara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3314" to="3323" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Kernel-based structural equation models for topology identification of directed networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Baingana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">B</forename><surname>Giannakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2503" to="2516" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Exploratory mediation analysis via regularization</title>
		<author>
			<persName><forename type="first">S</forename><surname>Serang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jacobucci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">C</forename><surname>Brimhall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J</forename><surname>Grimm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Structural equation modeling: a multidisciplinary journal</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="733" to="744" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Topology selection in graphical models of autoregressive processes</title>
		<author>
			<persName><forename type="first">J</forename><surname>Songsiri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Vandenberghe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="2671" to="2705" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">A constrained, weighted-l1 minimization approach for joint discovery of heterogeneous neural connectivity graphs</title>
		<author>
			<persName><forename type="first">C</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.04090</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Multiple Gaussian graphical estimation with jointly sparse penalty</title>
		<author>
			<persName><forename type="first">]</forename><forename type="middle">Q</forename><surname>Thw + 16</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xi</surname></persName>
		</author>
		<author>
			<persName><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Processing</title>
		<imprint>
			<biblScope unit="volume">128</biblScope>
			<biblScope unit="page" from="88" to="97" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Automated anatomical labeling of activations in SPM using a macroscopic anatomical parcellation of the MNI MRI single-subject brain</title>
		<author>
			<persName><forename type="first">]</forename><forename type="middle">N</forename><surname>Tmlp + 02</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Tzourio-Mazoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Landeau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Papathanassiou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Crivello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Etard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Delcroix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mazoyer</surname></persName>
		</author>
		<author>
			<persName><surname>Joliot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeuroImage</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="273" to="289" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Network topology inference via elastic net structural equation models</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Traganitis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">B</forename><surname>Giannakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th European Signal Processing Conference (EUSIPCO)</title>
		<meeting>the 25th European Signal Processing Conference (EUSIPCO)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="146" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A T</forename><surname>Figueiredo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Goldstein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07246v5[cs.LG</idno>
		<title level="m">Adaptive ADMM with spectral penalty parameter selection</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<ptr target="http://www.nitrc.org/projects/bnv/" />
		<title level="m">BrainNet Viewer: A Network visualization tool for human brain connectomics</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Model selection and estimation in the gaussian graphical model</title>
		<author>
			<persName><forename type="first">M</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">94</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">19</biblScope>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Latent variable selection in structural equation models</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Multivariate Analysis</title>
		<imprint>
			<biblScope unit="volume">152</biblScope>
			<biblScope unit="page" from="190" to="205" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
