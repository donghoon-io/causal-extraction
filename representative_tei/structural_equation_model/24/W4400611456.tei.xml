<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Variational Bayes for Mixture of Gaussian Structural Equation Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2024-07-12">July 12, 2024</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Khue-Dung</forename><surname>Dang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Melbourne</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Luca</forename><surname>Maestrini</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Australian National University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Francis</forename><forename type="middle">K C</forename><surname>Hui</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Australian National University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Variational Bayes for Mixture of Gaussian Structural Equation Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2024-07-12">July 12, 2024</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2407.08140v1[stat.ME]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-21T20:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>approximate inference</term>
					<term>latent variables</term>
					<term>mean field variational Bayes</term>
					<term>missing data</term>
					<term>variational information criteria</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Structural equation models (SEMs) are commonly used to study the structural relationship between observed variables and latent constructs. Recently, Bayesian fitting procedures for SEMs have received more attention thanks to their potential to facilitate the adoption of more flexible model structures, and variational approximations have been shown to provide fast and accurate inference for Bayesian analysis of SEMs. However, the application of variational approximations is currently limited to very simple, elemental SEMs. We develop mean-field variational Bayes algorithms for two SEM formulations for data that present non-Gaussian features such as skewness and multimodality. The proposed models exploit the use of mixtures of Gaussians, include covariates for the analysis of latent traits and consider missing data. We also examine two variational information criteria for model selection that are straightforward to compute in our variational inference framework. The performance of the MFVB algorithms and information criteria is investigated in a simulated data study and a real data application.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Multivariate data such as those arising from behavioural, educational, medical and social sciences studies can be analysed through structural equation models (SEMs). These models are commonly used to test hypotheses regarding the structural relationship between predictive factors of interest and multiple observable outcomes by means of latent factors <ref type="bibr" target="#b18">(Kaplan, 2009)</ref>.</p><p>For example, this article considers a study on child growth and development where investigators are interested in assessing the impact of environmental factors such as prenatal alcohol exposure on a child cognitive function. "Cognition", or "cognitive function" is a complex and multi-faceted construct that is difficult to measure directly and social scientists typically rely on a battery of different but related tests to provide a measure of general mental ability. The resulting cognition scores can be represented by latent factors which can in turn be modelled as a function of various predictors of interest.</p><p>In our motivation study, investigators are interested in assessing the impact of prenatal alcohol exposure on cognitive function of children. <ref type="bibr" target="#b16">Jacobson et al. (2004)</ref> and <ref type="bibr" target="#b8">Dang et al. (2023)</ref> used simple SEM formulations based on a Gaussian response assumption and a simple latent factor structure to study the dose-response relationship between prenatal alcohol exposure and child cognition. In particular, <ref type="bibr" target="#b8">Dang et al. (2023)</ref> adopted a Bayesian SEM to analyse the effect of prenatal alcohol exposure on cognition. However, their model was not able to capture some prominent features of the outcomes such as skewness and multimodality. Here we make use of mixtures of Gaussians to better capture the structure of the data and improve model fitting.</p><p>The application of our framework is not limited to our motivating example but is relevant to the general class of data that are typically examined using SEMs, which often exhibit non-Gaussian structures. We focus on a model where each outcome is assumed to follow a mixture of Gaussians and also show that this formulation is preferrable to other proposals assuming a mixture of Gaussians on the latent variables.</p><p>Bayesian analysis of SEMs is receiving increasing attention as frequentist approaches such as those based on maximum likelihood estimation and weighted least squares may encounter computational and theoretical problems for small sample sizes or non-Gaussian data as in the case of the data examined in this work. Bayesian formulations of SEMs facilitate incorporation of useful information via prior distributions and can accommodate more flexible model structures, such as those with crossed-loadings <ref type="bibr" target="#b20">(Lee, 2007)</ref>. Another important advantage of Bayesian approaches is the ability to get information on non-Gaussian features such as multimodality in marginal densities <ref type="bibr" target="#b3">(Arhonditsis et al., 2006)</ref>. Markov chain Monte Carlo (MCMC) methods are typically used for fitting and inference of Bayesian SEMs but they may suffer from slow convergence and long running times, making them less appealing when it comes to performing tasks such as tuning of priors, sensitivity analysis or model selection. <ref type="bibr" target="#b7">Dang and Maestrini (2022)</ref> investigated the use of variational approximations for SEMs and showed that this approximation technique can provide fast and reliable fitting for basic SEMs. Recent works have also envisages a growing interest in the application of variational inference to structural equation modelling <ref type="bibr" target="#b31">(Zhang et al., 2022;</ref><ref type="bibr" target="#b19">Kaplan, 2023;</ref><ref type="bibr" target="#b10">Fazio and B ürkner, 2024)</ref>.</p><p>In this work, we build upon the base variational approximation framework outlined by <ref type="bibr" target="#b7">Dang and Maestrini (2022)</ref> and demonstrate the use of variational Bayes to solve a wider range of problems modelled through SEMs. We consider models that contemplate the presence of missing data, include covariates to explain latent factors and, more importantly, that make use of mixtures of Gaussian densities on the outcomes. This is different from more common model formulations used to deal with non-standard features of outcomes or latent variables such as mixtures of SEMs (e.g., <ref type="bibr" target="#b27">Vermunt and Magidson, 2005)</ref>, infinite mixtures for nonparametric modelling of latent traits (e.g., <ref type="bibr" target="#b9">Dunson, 2006)</ref> or skewed distributions (e.g., <ref type="bibr" target="#b4">Asparouhov and Muthén, 2016)</ref>. We also consider an alternative model where a Gaussian mixture is applied to the latent variables and the conditional distribution of the outcomes is assumed to be Gaussian. We then utilize a variational version of the Akaike information criterion and propose a variational formulation of the Watanabe-Akaike information criterion to perform model selection. Through the application of these criteria we demonstrate that the model including a Gaussian mixture on the responses better captures the features of the real data in exam, besides requiring simpler prior hyperparameter tuning and initialisation of the variational algorithms compared to the model where the mixture is placed on the latent factors. Our simulation experiments also demonstrate that the variational information criteria are effective tools for selecting a reliable model for inference on the main parameters of interests in SEM analysis.</p><p>In Section 2 we formulate the main model of interest with a Gaussian mixture on the responses and an alternative formulation where the mixture is placed on the latent factors rather than the outcomes. In Section 3 we introduce tools for performing model comparison in the context of variational inference. Section 4 provides a numerical study to assess the models through the proposed selection criteria. The analysis of the prenatal alcohol exposure data is provided in Section 5. Conclusions and future directions are outlined in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Structural equation models with Gaussian-mixture outcomes</head><p>Consider a set of latent factors η i associated to individuals indexed by i = 1, . . . , N and each regressed over a p×1 vector of covariates x i and corresponding vector of coefficients β of equal length. Let y ij denote the jth outcome, with j = 1, . . . , M , observed for the ith individual and assumed to be conditionally distributed according to a Gaussian mixture of H j components.</p><p>The proposed model is</p><formula xml:id="formula_0">y ij | µ jh , λ j , η i , ψ 2 jh ind. ∼ H j h=1 w jh N µ jh + λ j η i , ψ 2 jh , η i | β, σ 2 ind. ∼ N (x T i β, σ 2 ), µ jh ind. ∼ N (µ µ jh , σ 2 µ jh ), λ j ind. ∼ N (µ λ , σ 2 λ ), ψ 2 jh ind. ∼ Inverse-Gamma(α ψ 2 , β ψ 2 ), β ∼ N (µ β , Σ β ), σ 2 ∼ Inverse-Gamma(α σ 2 , β σ 2 ), w j ∼ Dirichlet(H j , α w ),<label>(1)</label></formula><p>for i = 1, . . . , N , j = 1, . . . , M and h = 1, . . . , H j . Here Dirichlet(H j , α w ) denotes a symmetric Dirichlet distribution of order H j with concentration parameter α w &gt; 0. Note that we allow for the hyperparameters µ µ jh and σ 2 µ jh of the Gaussian priors on µ jh to be different and this will be motivated later on in our application. Our study involves a single unobserved component, cognitive function, therefore the latent factors η i are univariate. Nonetheless, the model above could be easily extended to accommodate for multiple unobserved components (see <ref type="bibr">Dang and Maestrini, 2022, Section 7)</ref>. The vectors x i include individual measurements of covariates (in our study, alcohol exposure and a propensity score) that affect the latent variables and are assumed to not contain an intercept term. Consistently with the features of the data in exam, the number of mixture components H j can vary across the M outcomes so that different outcomes can potentially be modelled with different numbers of mixture components or with a Gaussian distribution when H j = 1. To facilitate fitting, model (1) can be rewritten in terms of auxiliary variables a ij = (a ij1 , . . . , a ijH j ) T as</p><formula xml:id="formula_1">p(y ij | µ jh , λ j , η i , ψ 2 jh , a ij ) = H j h=1 ψ -1 jh (2π) -1/2 exp - (y ij -µ jh -λ j η i ) 2 2ψ 2 jh a ijh , a ij ind. ∼ Multinomial(1; w j ), η i | β, σ 2 ind. ∼ N (x T i β, σ 2 ), µ jh ind. ∼ N (µ µ , σ 2 µ ), λ j ind. ∼ N (µ λ , σ 2 λ ), ψ 2 jh ind. ∼ Inverse-Gamma(α ψ 2 , β ψ 2 ), β ∼ N (µ β , Σ β ), σ 2 ∼ Inverse-Gamma(α σ 2 , β σ 2 ), w j ∼ Dirichlet(H j , α w ).<label>(2)</label></formula><p>In many real data applications including the one considered in this work, some outcomes may be unobserved for certain individuals. Assuming that the data are missing at random, one straightforward way to account for this type of missing observations is to use a full information maximum likelihood approach, which corresponds to taking only the contribution to the likelihood from the observed responses into account <ref type="bibr" target="#b2">(Arbuckle, 1996;</ref><ref type="bibr" target="#b11">Finkbeiner, 1979;</ref><ref type="bibr" target="#b22">Merkle et al., 2021)</ref>. This can be implemented by introducing the set S obs of (i, j) pairs corresponding to outcomes y ij that are observed and rewriting model (1) in terms of the outcomes y ij such that (i, j) ∈ S obs . In a similar way, the likelihood function can be written by replacing products over all pairs of i = 1, . . . , N and j = 1, . . . , M with products over all (i, j) ∈ S obs . Given that the elements of y i are independent conditionally on the latent variables, the likelihood function arising from the conditional distribution of the outcomes in (2) under the missing data situation described above can be written as</p><formula xml:id="formula_2">p(y|µ, λ, η, ψ 2 , a) = i j:(i,j)∈S obs p(y ij |µ ij , λ j , η i , ψ 2 jh , a ij ),</formula><p>which reflects the fact we have at least one outcome observed for each individual i.  <ref type="formula" target="#formula_0">1</ref>) and ( <ref type="formula" target="#formula_13">7</ref>) in the context of the cognitive data study including 16 outcomes and 2 covariates ('Exposure' and 'Propensity Score'). The outcomes 'IQ 1' to 'AorLM 6' are described in Section 5. The number 1 over the arrow pointing to 'IQ 1' means that the factor loading connecting the latent variable 'Cognition' and the outcome 'IQ 1' is set to 1.</p><p>In the literature of SEMs and their applications, the relationship between observed outcomes and latent variables is often represented through a path diagram. Figure <ref type="figure" target="#fig_0">1</ref> shows the path diagram for our cognitive data examined through model (1). The observed outcomes are represented as rectangles to which the arrows departing from the only latent variable, cognition (represented as an ellipsis) are pointing. The diagram also shows the two exogenous covariates that are supposed to affect cognition: a propensity score and alcohol exposure. A more detailed description of the data is provided in Section 5. of the number of Gaussian mixture components. As for mainstream variational approximations, MFVB boils down to finding an approximating density q for which the Kullback-Leibler divergence between the approximating density itself and the posterior density function is minimized, subject to convenient restrictions on q. If y is a generic vector of data, θ ∈ Θ represents all model parameters and q is an arbitrary density function defined over Θ, the logarithm of the marginal likelihood satisfies log p(y) ≥ log p(y; q) ≡ q(θ) log p(y, θ) q(θ) dθ,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Model fitting via VB</head><p>where p(y; q) is a marginal likelihood lower-bond depending on q. It can easily be shown that maximizing this lower-bound is equivalent to minimizing the Kullback-Leibler divergence KL(q(θ) ∥ p(θ|y)) = q(θ) log q(θ) p(θ|y) dθ.</p><p>If the approximating density is factorized according to a partition (θ 1 , . . . , θ K ) of θ such that q(θ) = K k=1 q(θ k ), then the optimal approximating densities satisfy</p><formula xml:id="formula_4">q * (θ k ) ∝ exp E q(θ\θ k ) {log p(θ k |y, θ\θ k )} , k = 1, . . . , K,<label>(4)</label></formula><p>where E q(θ\θ k ) denotes the expectation with respect to all the approximating densities except q(θ k ) and θ\θ k represents the entries of θ with θ k omitted. It can also be shown that the maximization of the lower-bound can be performed via a coordinate ascent scheme converging to a local maximizer of the lower bound under mild regularity conditions. A more detailed introduction to MFVB as presented above is provided, for example, in Section 2.2 of <ref type="bibr" target="#b23">Ormerod and Wand (2010)</ref>.</p><p>As an illustration, consider the full conditional density of µ jh , for j = 1, . . . , M and h = 1, . . . , H j , arising from the log likelihood function of model (1):</p><formula xml:id="formula_5">p(µ jh | rest) ∝ exp   - µ 2 jh 2   i:(i,j)∈S obs a ijh ψ 2 jh + 1 σ 2 µ jh   + µ jh    i:(i,j)∈S obs a ijh (y ij -λ j η i ) ψ 2 jh + µ µ jh σ 2 µ jh      .</formula><p>Furthermore, consider the following factorization of the variational approximating density:</p><formula xml:id="formula_6">q(µ, λ, η, ψ 2 , β, σ 2 , a, w) = q(µ)q(λ)q(ψ 2 )q(β)q(σ 2 ) N i=1 q(η i )q(a)q(w) = q(σ 2 )q(β) M j=1 H j h=1 {q(ψ 2 jh )q(µ jh )} M j=2 q(λ j ) N i=1 q(η i ) (i,j)∈S obs q(a ij ) M j=1</formula><p>q(w j ).</p><p>(5)</p><p>From application of (4) and given the factorization in (5), the optimal approximating density for µ jh q * (µ jh ) is N µ q(µ jh ) , σ 2 q(µ jh ) , j = 1, . . . , M, h = 1, . . . , H j , with µ q(µ jh ) = σ 2 q(µ jh )</p><p>i:(i,j)∈S obs µ q(a ijh ) µ q(1/ψ 2 jh ) y ij -µ q(λ j ) µ q(η i ) +</p><formula xml:id="formula_7">µ µ jh σ 2 µ jh and σ 2 q(µ jh ) = i:(i,j)∈S obs µ q(a ijh ) µ q(1/ψ 2 jh ) + 1 σ 2 µ jh -1 ,<label>(6)</label></formula><p>where µ q(a ijh ) , µ q(1/ψ 2 jh ) , µ q(λ j ) and µ q(η i ) are defined in Algorithm 1. The optimal approximating densities of the remaining parameters can be derived in a similar manner and have the following forms:</p><formula xml:id="formula_8">q * (β) is N µ q(β) , Σ q(β) , q * (λ j ) is N µ q(λ j ) , σ 2 q(λ j ) , j = 2, . . . , M, q * (η i ) is N µ q(η i ) , σ 2 q(η i ) , i = 1, . . . , N, q * (ψ 2 jh ) is Inverse-Gamma α q(ψ 2 jh ) , β q(ψ 2 jh ) , j = 1, . . . , M, h = 1, . . . , H j , q * (σ 2 ) is Inverse-Gamma α q(σ 2 ) , β q(σ 2 ) , q * (a ij ) is Multinomial 1; µ q(a ij ) , i = 1, . . . , N, k = 1, . . . , K, q * (w j ) is Dirichlet(H j , α q(w j ) ), j = 1, . . . , M,</formula><p>where <ref type="bibr" target="#b28">Wand (2017)</ref>, and the quantities on which the approximating densities parameters depend are given in Algorithm 1.</p><formula xml:id="formula_9">G [ v T 1 v T 2 ] T ; Q, r, s ≡ -(1/8)tr Q{vec -1 (v 2 )} -1 [v 1 v T 1 {vec -1 (v 2 )} -1 -2I] -(1/2)r T {vec -1 (v 2 )} -1 v 1 -(1/2)s, for a d × 1 vector v 1 , d 2 × 1 vector v 2 such that vec -1 (v 2 ) is symmetric, d × d matrix Q, d × 1 vector r and s ∈ R, as defined in</formula><p>Following standard practice in the SEM literature, we fix λ 1 to 1 and introduce the constraint λ j &gt; 0 to ensure model identifiability.</p><p>An important aspect of the proposed modelling framework is the choice of the number of Gaussian mixture components H j . This can be performed, for instance, resorting to software such as the package 'mixtools' <ref type="bibr" target="#b5">(Benaglia et al., 2009)</ref> in R (R Core Team, 2024) to cluster the measurements of each outcome and then using the estimated number of clusters as H j . Visual inspection of the empirical density plots of outcome could also help identifying or confirming the number of mixture components.</p><p>Algorithm 1 Algorithm for fitting model (1) via MFVB.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data Input: y</head><formula xml:id="formula_10">i , i = 1, . . . , N , vectors of length M . Hyperparameter Input: µ µ jh , µ λ ∈ R, σ µ jh , σ λ , α ψ 2 , β ψ 2 , α σ 2 , β σ 2 ∈ R + , α w = 1, µ β ∈ R p , Σ β being a p × p symmetric positive definite matrix. Initialize: µ q(µ jh ) , µ q(log(ψ 2 jh )) ∈ R, µ q(µ 2 jh ) , µ q(1/ψ 2 jh ) ∈ R + , j = 1, . . . , M , h = 1, . . . , H j ; µ q(λj ) ∈ R, µ q(λ 2 j ) ∈ R + , j = 2, . . . , M ; µ q(ηi) ∈ R, µ q(η 2 i ) ∈ R + , i = 1, . . . , N ; µ q(1/σ 2 ) ∈ R + , µ q(β) ∈ R, µ q(log(w jh )) ∈ R -, j = 1, . . . , M , h = 1, . . . , H j ; α q(σ 2 ) = N +2α σ 2 2</formula><p>, µ q(λ1) = µ q(λ 2 1 ) = 1. Cycle until convergence:</p><formula xml:id="formula_11">For j = 1, . . . , M : For i = 1, . . . , N , h = 1, . . . , H j : τ ijh ←-µ q(log(w jh )) -1 2 µ q(log(ψ 2 jh )) -1 2 log(2π) -1 2 µ q(1/ψ 2 jh ) y 2 ij + µ q(µ 2 jh ) +µ q(λ 2 j ) µ q(η 2 i ) -2y ij µ q(µ jh ) -2y ij µ q(λj ) µ q(ηi) + 2µ q(µ jh ) µ q(λj ) µ q(ηi) For i = 1, . . . , N , h = 1, . . . , H j : µ q(a ijh ) ←-exp(τ ijh ) Hj h=1 exp(τ ijh ) If j &gt; 1: σ 2 q(λj ) ←- N i=1 Hj h=1 µ q(a ijh ) µ q(η 2 i ) µ q(1/ψ 2 jh ) + 1 σ 2 λ -1 µ q(λj ) ←-σ 2 q(λj ) N i=1 Hj h=1 µ q(a ijh ) µ q(ηi) µ q(1/ψ 2 jh ) y ij -µ q(µ jh ) + µ λ σ 2 λ µ q(λ 2 j ) ←-σ 2 q(λj ) + µ 2 q(λj )</formula><p>For h = 1, . . . , H j :</p><formula xml:id="formula_12">σ 2 q(µ jh ) ←- N i=1 µ q(a ijh ) µ q(1/ψ 2 jh ) + 1/σ 2 µ jh -1 µ q(µ jh ) ←-σ 2 q(µ jh ) N i=1 µ q(a ijh ) µ q(1/ψ 2 jh ) y ij -µ q(λj ) µ q(ηi) + µ µ jh /σ 2 µ jh µ q(µ 2 jh ) ←-σ 2 q(µ jh ) + µ 2 q(µ jh ) α q(ψ 2 jh ) ←-1 2 N i=1 µ q(a ijh ) + α ψ 2 β q(ψ 2 jh ) ←-β ψ 2 + 1 2 N i=1 µ q(a ijh ) y 2 ij + µ q(µ 2 jh ) + µ q(λ 2 j ) µ q(η 2 i ) -2y ij µ q(µ jh ) -2y ij µ q(λj ) µ q(ηi) + 2µ q(µ jh ) µ q(λj ) µ q(ηi) µ q(log(ψ 2 jh )) ←-log β q(ψ 2 jh ) -digamma α q(ψ 2 jh ) ; µ q(1/ψ 2 jh ) ←-α q(ψ 2 jh ) β q(ψ 2 jh ) If H &gt; 1: [α q(wj ) ] h ←- N i=1 µ q(a ijh ) + α w For h = 1, . . . , H j : µ q(log(w jh )) ←-digamma [α q(wj ) ] h -digamma Hj h=1 [α q(wj ) ] h For i = 1, . . . , N : σ 2 q(ηi) ←- M j=1 Hj h=1 µ q(a ijh ) µ q(λ 2 j ) µ q(1/ψ 2 jh ) + µ q(1/σ 2 ) -1 µ q(ηi) ←-σ 2 q(ηi) µ q(1/σ 2 ) x T i µ q(β) + M j=1 Hj h=1 µ q(a ijh ) µ q(λj ) (y ij -µ q(µ jh ) )µ q(1/ψ 2 jh ) µ q(η 2 i ) ←-σ 2 q(ηi) + µ 2 q(ηi) Σ q(β) ←-{µ q(1/σ 2 ) ( N i=1 x i x T i ) + Σ -1 β } -1 µ q(β) ←-Σ q(β) µ q(1/σ 2 ) N i=1 µ q(ηi) x i + µ β Σ -1 β η q(β) ←-Σ -1 q(β) µ q(β) -1 2 vec(Σ -1 q(β) ) T µ q(1/σ 2 ) ←-α q(σ 2 ) - N i=1 G η q(β) ; x i x T i , µ q(ηi) x i , µ q(η 2 i ) + β σ 2 Relevant Output: α q(ψ 2 jh ) β q(ψ 2 jh ) , µ q(log(w jh )) , j = 1, . . . , M , h = 1, . . . , H j ; µ q(λj ) , σ 2 q(λj ) , j = 1, . . . , M ; µ q(ηi) , σ 2 q(ηi) , i = 1, . . . , N ; α q(σ 2 ) , β q(σ 2 ) = α q(σ 2 ) /µ q(1/σ 2 ) , µ q(β) , Σ q(β) .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Alternative model</head><p>Non-standard and dynamic patterns in SEMs or latent variable models in general could potentially be handled with model formulations that are different to the one provided in Section 2. Various works have investigated the use of mixtures of SEMs (e.g., <ref type="bibr" target="#b32">Zhu and Lee, 2001;</ref><ref type="bibr" target="#b27">Vermunt and Magidson, 2005)</ref>; in <ref type="bibr" target="#b4">Asparouhov and Muthén (2016)</ref> mixtures of SEMs were adopted in conjunction with skewed distributions for the observations and latent variables. The use of an infinite mixture of Dirichlet processes was proposed in <ref type="bibr" target="#b9">Dunson (2006)</ref> to characterize the latent response distributions of a factor analytic model nonparametrically.</p><p>Here we examine a second SEM formulation where a mixture of Gaussian distribution is imposed to the latent factors and the outcomes are assumed to follow a Gaussian distribution. This alternative formulation resembles the one of <ref type="bibr" target="#b9">Dunson (2006)</ref> in a sense that the non-Gaussian features of the data are captured by applying a mixture distribution to the latent variables. The intent here, however is to characterize the effect of distinct subpopulations that may be naturally present (e.g., low and high alcohol drinkers) on the latent factors (cognition in our case), rather than introducing nonparametric components into the model to merely obtain a better model fit. While this model may seem a particularly attractive option to characterize subpopulations of the surveyed individuals, different outcomes can present different multimodal or other non-Gaussian features that can be better captured using model (1) instead.</p><p>Furthermore, fitting a mixture of Gaussians distribution on the latent space may introduce futher identifiability issues related to the estimation of the Gaussian densities parameters and weights.</p><p>The alternative model we propose and study takes the following form for individual i = 1, . . . , N , outcome j = 1, . . . , M and mixture component k = 1, . . . , K:</p><formula xml:id="formula_13">y ij | ν j , λ j , η i , ψ 2 j ∼ N (ν j + λ j η i , ψ 2 j ), η i | β 1 , . . . , β K , σ 2 1 , . . . , σ 2 K ind. ∼ K k=1 w k N (x T i β k , σ 2 k )<label>(7)</label></formula><p>plus priors on ν j , λ j , ψ 2 j , β k , σ 2 k and w such as those used in model (1). As done for model (1), we can set λ 1 = 1 and λ j &gt; 0 for identifiability purposes. In this case identifiability is also a particularly relevant issue affecting the mixture component of the model, which can be solved by applying additional constraints. Similarly to <ref type="bibr" target="#b9">Dunson (2006)</ref>, one could set ν 1 to a constant in addition to λ 1 , or use a good initialisation strategy to facilitate convergence, as the one described later on in the real data application. In the supplementary material we provide a variational inference scheme (Algorithm S.1) for fitting model ( <ref type="formula" target="#formula_13">7</ref>) via MFVB according to an auxiliary variable representation of the mixture such as the one in (2) and a variational approximating density similar to (5). To facilitate convergence and reduce computational times, we recommend fitting a mixture regression to the data (e.g., using the R package 'mixtools') and using the estimates of the Gaussian mixture parameters and weights to initialize β k , σ 2 k and w k , for k = 1, . . . , K.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Comparing models</head><p>A major aspect of model fitting for the proposed SEM formulations is the choice of the numbers of mixture components H j , j = 1, . . . , M , for (1) and K for (7). It is also of interest to identify which model between the baseline formulation (1) and the alternative (7) better fits the data. To this scope, we introduce a variational version of the Watanabe-Akaike information criterion (WAIC; <ref type="bibr" target="#b29">Watanabe, 2010;</ref><ref type="bibr" target="#b12">Gelman et al., 2014)</ref> and also consider the variational Akaike information criterion (VAIC) proposed by <ref type="bibr" target="#b30">You et al. (2014)</ref>. Commonly used information criteria typically involve the log posterior density of the observed data given a point estimate and a correction for bias due to overfitting. The point estimate can be the maximum likelihood estimate (as in the Akaike information criterion) or the posterior mean (for the deviance information criterion, DIC; <ref type="bibr" target="#b25">Spiegelhalter et al., 2002)</ref>, and the bias correction usually corresponds to a measure of the effective number of parameters.</p><p>Unlike DIC, WAIC can be considered a fully Bayesian approach, as its formulation uses the average over the posterior distribution from the model rather than conditioning on a point estimate. To derive a variational version of WAIC, we start from the following formulation provided by <ref type="bibr" target="#b12">Gelman et al. (2014)</ref>, which we define for a generic model with a data vector y of length N and parameter vector θ: WAIC ≡ -2(lppd-P WAIC ), with lppd ≡ N i=1 log p(y i |θ)p(θ|y)dθ representing the logarithmic pointwise predictive density and</p><formula xml:id="formula_14">P WAIC ≡ 2 N i=1 log(E p(θ|y) p(y i |θ)) -E p(θ|y) (log p(y i |θ)</formula><p>) being a correction for the effective number of parameters introduced to adjust for overfitting. Both lppd and P WAIC can in practice be obtained using a large number R of samples from the posterior p(θ|y) via MCMC. Noting that lppd = N i=1 log(E p(θ|y) p(y i |θ)), a variational version of WAIC immediately arises if the expectation in this expression is computed with respect to the variational density q(θ) instead of p(θ|y). We then define the variational WAIC as VWAIC ≡ -2(vlppd -P VWAIC ),</p><formula xml:id="formula_15">with vlppd ≡ N i=1 log(E q(θ) p(y i |θ)) ≈ N i=1 log 1 R R s=1 p(y i |θ s )</formula><p>and</p><formula xml:id="formula_16">P VWAIC ≡ 2 N i=1 log(E q(θ) p(y i |θ)) -E q(θ) (log p(y i |θ)) ≈ 2 N i=1 log 1 R R s=1 p(y i |θ s ) -1 R R s=1 (log p(y i |θ s )) ,<label>(8)</label></formula><p>and where each θ s , s = 1, . . . , R, is a vector of the same length as θ sampled from q(θ).</p><p>The other information criterion we use to compare model fits in the context of variational approximations is VAIC. This criterion can be thought to be an approximation to DIC and is defined as follows:</p><p>VAIC ≡ -2(log p(y| θ) -P V AIC ), with θ ≡ E q(θ) (θ) and P V AIC ≡ 2 log p(y| θ) -E q(θ) (log p(y|θ)) .</p><p>(9)</p><p>In the context of MFVB as presented in this work, E q (θ) can be obtained directly from the approximating densities, for instance using µ q(µ jh ) for the optimal approximating density of µ jh in (6). As for VWAIC, the expectation E q(θ) (log p(y|θ)) can be obtained as an approximation from a large number R of draws from the approximating density q(θ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Numerical study</head><p>The purpose of this section is to study the performance of our MVFB algorithms and the proposed variational information criteria through a simulation exercise. We generated 100 data sets according to model (1) with N = 1000 individuals, M = 4 outcomes for each individual, and H 1 = 1, H 2 = 2, H 3 = 2 and H 4 = 1 components for the Gaussian mixtures. As a covariate vector we used x i = (x 1i , x 2i ) T where the x 1i 's and x 2i 's were respectively generated from N (3, 4) and U (0, 5) distributions, for i = 1, . . . , N . We also set β = (1, 2) T , λ 1 = 1, λ 2 = 0.8, λ 3 = 0.5 and λ 4 = 0.2 as indicated by the values over the arrows of the Figure <ref type="figure">2</ref> path diagram, which shows the relationship between the covariates, the only latent factor and the four outcomes. We then ran MFVB using Algorithm 1 and MCMC implemented with the package 'rstan' <ref type="bibr">(Stan Development Team, 2024)</ref> to fit model (1) setting the H j 's to their true values and applying weakly informative priors. Specifically, following the notation in (1) we set µ λ = 1, The MFVB and MCMC fits were used to construct 95% credible intervals for the model parameters to then compute the number of times the true values fell inside the corresponding intervals. These coverage values are presented as percentages in Table <ref type="table">1</ref>, while Table <ref type="table" target="#tab_1">2</ref> presents mean squared errors between the parameter estimates and their true values. For MFVB, the parameter estimates were chosen to be the means of the optimal approximating densities. As an example, the value of µ q(µ jh ) as given in ( <ref type="formula" target="#formula_7">6</ref>) and obtained at convergence of the MFVB algorithm was used as an estimate of µ jh . For MCMC, we ran 1 chain of 6, 000 iterations and discarded 3, 000 as burn-in. The MCMC parameter estimates were obtained as means of the remaining 3, 000 samples.</p><formula xml:id="formula_17">σ 2 λ = 1, α ψ 2 ≈ 2.39, β ψ 2 ≈ 8.69, α σ 2 = β σ 2 = 1, µ µ jh = 0, σ 2 µ jh =</formula><p>From Tables <ref type="table">1</ref> and<ref type="table" target="#tab_1">2</ref> it is clear that the mean squared errors of MFVB are comparable to those of MCMC, but the coverage values of MFVB are not very close to the 95% target. For this reason, we tested the use of nonparametric bootstrap as described in <ref type="bibr" target="#b7">Dang and Maestrini (2022)</ref> to improve the coverage of MFVB. For each simulated dataset, we obtained 20 bootstrap datasets and ran MFVB on each of them. We then produced percentile bootstrap confidence intervals according to the procedure described in <ref type="bibr" target="#b7">Dang and Maestrini (2022)</ref>  We also compared the means of the MFVB approximating densities for λ 2 , λ 3 , λ 4 , β 1 and β 2 with the posterior means from MCMC. This comparison is shown in Figure <ref type="figure" target="#fig_2">3</ref> in form of scatterplots for the parameters of interest, with each point corresponding to a pair of MFVB and MCMC estimates from a simulated dataset. The scatterplots show a strong alignment between the MFVB and MCMC estimates.</p><p>In addition to the above, we fitted two other models to the same simulated datasets: model (1) but with all the H j 's set to 1 so that the outcomes were attributed normal distributions; the alternative model ( <ref type="formula" target="#formula_13">7</ref>) with K = 2. For each of these two models and the previously considered model we computed VWAIC and VAIC as defined in ( <ref type="formula" target="#formula_16">8</ref>) and ( <ref type="formula">9</ref>) with R = 1, 000. The first model, i.e. model ( <ref type="formula" target="#formula_0">1</ref>) with H 1 = 1, H 2 = 2, H 3 = 2 and H 4 = 1, produced the lowest values of VAIC and VWAIC for all the simulated datasets, meaning that it was always preferable over the other two models in exam.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Cognitive data</head><p>High levels of prenatal alcohol exposure are known to cause fetal alcohol syndrome, a distinct pattern of craniofacial anomalies, growth restriction, and cognitive and behavioural deficits <ref type="bibr" target="#b6">(Carter et al., 2016;</ref><ref type="bibr" target="#b14">Hoyme et al., 2005</ref><ref type="bibr" target="#b13">Hoyme et al., , 2016;;</ref><ref type="bibr" target="#b16">Jacobson et al., 2004</ref><ref type="bibr" target="#b17">Jacobson et al., , 2008;;</ref><ref type="bibr" target="#b21">Mattson et al., 2019)</ref>  However, some individuals that are subject to prenatal alcohol exposure, especially those at lower levels of exposure, may show cognitive and/or behavioural impairment without exhibiting the characteristic craniofacial dysmorphology. This condition is known as alcohol-related neurodevelopmental disorder. Although the presence of this disorder is often associated with a history of maternal alcohol consumption, the full extent of the dose-response relationship between prenatal alcohol exposure and child cognitive function is yet not well understood.</p><p>Studying this relationship is crucial in having a proper diagnosis and appropriate treatment and intervention strategy for affected children.</p><p>Cognitive function is an ensemble of mental processes that cannot be observed directly and in children or other individuals it is commonly assessed using neuropsychological test results.</p><p>SEMs are therefore suitable models to relate these measures with the unobserved cognition latent variable. <ref type="bibr" target="#b8">Dang et al. (2023)</ref> used data from studies involving six cohorts in the United</p><p>States to analyse the effect of prenatal alcohol exposure on child cognition, and their results suggested a strong negative effect of alcohol exposure. The model used by <ref type="bibr" target="#b8">Dang et al. (2023)</ref> was a multi-cohort SEM which allowed to treat different sets of outcomes across cohorts. In this work we restrict our attention to one of the six studies, specifically the Detroit cohort that is available online at <ref type="url" target="https://doi.org/10.7910/DVN/BOAXR8">https://doi.org/10.7910/DVN/BOAXR8</ref>.</p><p>In the Detroit study, the mothers were interviewed prenatally or shortly after delivery about their drinking habits during pregnancy, and the children were followed longitudinally to assess their intelligence quotient (IQ), academic achievement in reading and arithmetic, learning and memory abilities (AorLM) and executive function (EF). These cognitive domains were assessed using several tests and the study resulted in 16 measured outcomes from 377 children. The first 3 outcomes are related to IQ tests, the 4th to 10th outcomes are related to EF and the remaining 6 are measures of AorLM. We name these outcomes according to the description provided by the online data source, as shown in the Figure <ref type="figure" target="#fig_0">1</ref> path diagram. For example, "IQ 1" and "EF 3" stand for the outcomes of the first IQ test and the third EF test. The mothers' consumption of beer, wine and liquor during pregnancy were summarised in terms of ounces of absolute alcohol consumed per day (1 ounce absolute alcohol equals 2 standard drinks of alcohol), which is denoted as "aadxp". Following <ref type="bibr" target="#b8">Dang et al. (2023)</ref>, the covariates used to study the data are log(aadxp + 1) and a pre-computed propensity score that accounts for other confounders (see <ref type="bibr" target="#b0">Akkaya Hocagil et al., 2021,</ref> for more details about this propensity score).</p><p>The model used by <ref type="bibr" target="#b8">Dang et al. (2023)</ref> to study each cohort assumed that the outcomes are jointly normal given the latent variable, and therefore was unable to fully capture non-Gaussian features such as skewness and multimodality of some challenging outcomes. In such a situation, model ( <ref type="formula" target="#formula_0">1</ref>) is particularly suitable to study the data since it allows to model each outcome using mixtures of Gaussians with varying numbers of components. Alternatively, one can assume that the non-Gaussian features are due to the presence of distinct groups with distinct drinking habits (for instance, a group of people less affected by alcohol exposure, and one strongly affected) and assume a Gaussian mixture on the latent variable space as in model ( <ref type="formula" target="#formula_13">7</ref>), although this implies the actual presence of a grouped structure in the data. In this section we consider both model formulations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Fitted models and priors</head><p>One of the advantages of formulating SEMs in a Bayesian framework is that it is possible to incorporate prior knowledge about the parameters via the prior distribution. Given the relatively large number of observed outcomes, 16, and small number of participants, 377, with many test results missing for several participants (the median percentage of missing data across outcomes is 22.944%), choosing priors adequately plays an important role also in facilitating identifiability and convergence of Bayesian inference algorithms. Following previous analyses of the same data <ref type="bibr" target="#b8">(Dang et al., 2023;</ref><ref type="bibr" target="#b15">Jacobson et al., 2021)</ref>, the observations corresponding to each outcome were standardized to have standard deviation of 15, which is approximately that of IQ.</p><p>Referring to the priors of model ( <ref type="formula" target="#formula_0">1</ref>) and their hyperparameters, we set our priors as follows.</p><p>For the latent factor parameters that are not constrained to 1 for identifiability we set µ λ = 1 and σ 2 λ = 1. For the mixture component-specific intercepts we use σ 2 µ jh = 10 4 and for each j we set the µ µ jh 's to the estimated intercepts of a mixture of regressions of the jth outcome against the covariates obtained through the function 'regmixEM' from the package 'mixtools' if H j &gt; 1, or the intercept of a simple regression of the jth outcome against the covariates when H j = 1. Based on the estimates of the regressions used to initialize the µ µ jh 's and the fact that the outcomes were standardised to have standard deviation equal to 15, we set α ψ 2 = α σ 2 = 6 and β ψ 2 = β σ 2 = 500 so that the Inverse Gamma priors on ψ 2 jh and σ 2 had means and variances respectively equal to 100 and 2, 500. To impose a weakly informative prior on w jh , we set α w = 10. Finally, we set µ β = 0 and Σ β to be a diagonal matrix with diagonal elements equal to 100.</p><p>We also fitted the alternative model ( <ref type="formula" target="#formula_13">7</ref>) using K = 2 as the number of components of the Gaussian mixture on the latent variable and a Dirichlet prior with α w = 5 for the components weights, i.e. a smaller value of α w compared to the one used for model (1) as the fitting of the Gaussian mixture was more challenging on the latent space than on the outcomes. The priors and corresponding hyperparameters for β, λ j , ψ j and σ 2 k were kept the same as the ones used for model (1). We used a N (0, 10 4 ) prior for each ν j .</p><p>We considered and fitted four models as follows: 1) model ( <ref type="formula" target="#formula_0">1</ref> Table <ref type="table">3</ref>: VWAIC and VAIC of the models fitted in each setting, based on 10,000 samples generated from the variational approximating densities. components from visual inspection of the empirical density functions of the outcomes, which resulted in (H 1 , . . . , H <ref type="table">16 ) = (1,</ref><ref type="table">1,</ref><ref type="table" target="#tab_1">2,</ref><ref type="table">1,</ref><ref type="table" target="#tab_1">2,</ref><ref type="table" target="#tab_1">2,</ref><ref type="table" target="#tab_1">2,</ref><ref type="table">1,</ref><ref type="table">1,</ref><ref type="table">1,</ref><ref type="table" target="#tab_1">2,</ref><ref type="table" target="#tab_1">2,</ref><ref type="table">1,</ref><ref type="table" target="#tab_1">2,</ref><ref type="table">1,</ref><ref type="table">1) (option 1);</ref><ref type="table" target="#tab_1">2) model (</ref> <ref type="formula" target="#formula_0">1</ref>) with all the H j 's set to 1, i.e. with the mixtures collapsing to Gaussian densities (option 2); 3) model ( <ref type="formula" target="#formula_0">1</ref>) choosing the number of components through the function densityMclust from the R package 'mclust', which resulted in (H 1 , . . . , H 16 ) = (1, 1, 2, 1, 2, 2, 2, 1, 1, 3, 2, 2, 1, 2, 1, 1) (option 3); 4) model ( <ref type="formula" target="#formula_13">7</ref>) with a mixture of 2 components on the latent factor (option 4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Results</head><p>We fitted the four options listed above via MFVB using Algorithms 1 and S.1, and for each model computed VAIC and VWAIC as described in Section 3 by approximating the expectation E q(θ) (log p(y|θ)) in both ( <ref type="formula" target="#formula_16">8</ref>) and ( <ref type="formula">9</ref>) using R = 10, 000 random draws from the variational approximating densities. The values collected in Table <ref type="table">3</ref> indicate that option 1, i.e. model ( <ref type="formula" target="#formula_0">1</ref>) with the number of components chosen from visual inspection of the outcomes distributions, is the preferred option. Option 3 gave similar values of the information criteria to option 1 and this is due to the fact that the only difference between the two options is a different number of components for the 10 th outcome, with H 10 being equal to 1 in the case of option 1 and 3 for option 3.</p><p>It is interesting to note from Table <ref type="table">4</ref> that the estimates of the regression coefficients of options 1, 2, and 3 (which all refer to model (1) fittings) are very similar, meaning that fitting a Gaussian mixture on the outcomes can improve the predictive power without significantly affecting inference. Similarly, Table <ref type="table">5</ref> reporting the estimates of β 1 , β 2 and w k from model ( <ref type="formula" target="#formula_13">7</ref>) considered in option 4 suggests that the effects of the two covariates averaged over the two Gaussian mixture components are close to those from options 1, 2 and 3. We also note the similarity between the results presented here and the pooled effect size estimate of prenatal alcohol exposure shown in Table <ref type="table">4</ref> of <ref type="bibr" target="#b1">Akkaya Hocagil et al. (2022)</ref>, where the small discrepancy may be due to some differences in the set of outcomes being used.</p><p>Another way to check whether a model fits the data well is by looking at posterior predictive densities. Recall that the posterior predictive distribution of a new data point ỹ given the observed data y is p(ỹ|y) = p(ỹ|θ)p(θ|y)dθ. In most cases it is not possible to compute p(ỹ|y) in an analytic form and a simple way to visualize this distribution is by generating new data Option 1 Option 2 Option 3 β1 -0.36646 -0.44148 -0.38889 (0.61319) (0.59784) (0.61857) β2 -6.89255 -7.04235 -6.97791</p><p>(1.93116) (1.88431) (1.94755) Table <ref type="table">4</ref>: Estimates of the elements of β and their standard deviations (in brackets) for the models of options 1, 2 and 3 obtained as means and standard deviations from the variational approximating density of β.</p><formula xml:id="formula_18">1st component 2nd component β1,1 0.29766 β2,1 -1.07275 (0.80827) (0.85658) β2,1 -4.59911 β2,2 -8.46553 (2.54153) (2.63300) ŵ1 0.48368 ŵ2 0.51632</formula><p>Table <ref type="table">5</ref>: Estimates of the elements of β 1 , β 2 and their standard deviations (in brackets) for the two mixture components of model ( <ref type="formula" target="#formula_13">7</ref>) according to option 4 obtained as means and standard deviations from the variational approximating densities of β 1 and β 2 . The estimates of the weights are also provided and these coincide with the means of the corresponding approximating densities.</p><p>sets with values of θ drawn from the posterior. These simulated data can be plotted together with the observed data to assess the quality of the model fit. We performed such checks using the models of options 1 and 2. However, instead of drawing θ from the posterior p(θ|y), we generated data from the approximating density q(θ) as specified in Section 2.1 with the optimal parameters obtained from Algorithm 1 at convergence. Figure <ref type="figure" target="#fig_4">4</ref> shows the kernel density estimates of 300 drawings from the posterior predictive distribution displayed over histograms of the measurements of a selection of four outcomes that showed non-Gaussian features. From the plots it is evident that for the last outcome, whose histogram looks roughly bell shaped, the two models fit the data equally well. However, when the data show non-Gaussian features such as skewness as for the remaining outcomes, the model of option 1 captures the shape of the data better than that of option 2. Similar plots for all outcomes and models are provided in the supplementary material S.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>We have introduced two SEM formulations that make use of mixtures of Gaussian densities to better capture non-Gaussian features of the data and take into account missing data. We have also proposed a new variational information criterion that can be computed from the output of MFVB algorithms at convergence, VWAIC, to compare candidate models. Through a simulation study, we have shown that both VWAIC and the alternative VAIC we have also examined are able to identify the correct model. We have fitted the models to data from a study on prenatal alcohol exposure and child cognition in Detroit using MFVB and shown that the model with Gaussian mixtures on the outcomes is suitable for capturing the non-Gaussian features of the data.</p><p>Our real data application is also an example of incorporating exogenous covariates in a SEM, although it has to be noted that the small number of participants with a high level of alcohol exposure in the Detroit study could hinder estimation of the relationship between alcohol exposure and cognition. Considering data from multiple studies or cohorts would allow to fit more flexible structures, including those with non-linear or semi-parametric regression components, and obtain more reliable estimates of the effect of prenatal alcohol exposure on cognition. The variational framework we have presented here could easily be extended to such multi-group settings, but we leave this for future investigation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplement for:</head><p>Variational Bayes for Mixture of Gaussian</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Structural Equation Models</head><p>BY KHUE-DUNG DANG 1 * , LUCA MAESTRINI 2 AND FRANCIS K.C. HUI 2 1 University of Melbourne and 2 Australian National University</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S.1 Derivations for model with Gaussian mixture on the responses</head><p>The joint likelihood function of model ( <ref type="formula" target="#formula_0">1</ref>) is</p><formula xml:id="formula_19">p(y, µ, λ, η, ψ 2 , β, σ 2 , a, w) = (i,j)∈S obs H j h=1 (2πψ 2 jh ) -1/2 exp - (y ij -µ jh -λ j η i ) 2 2ψ 2 jh a ijh × N i=1 (2πσ 2 ) -1/2 exp - (η i -x T i β) 2 2σ 2 × M j=2 (2πσ 2 λ ) -1/2 exp - (λ j -µ λ ) 2 2σ 2 λ × M j=1 H j h=1 (2πσ 2 µ jh ) -1/2 exp - (µ jh -µ µ jh ) 2 2σ 2 µ jh × β ψ 2 α ψ 2 Γ α ψ 2 -1 (ψ 2 jh ) -α ψ 2 -1 exp - β ψ 2 ψ 2 jh ×(2π) -p/2 |Σ β | -1/2 exp - 1 2 (β -µ β ) T Σ -1 β (β -µ β ) × (β σ 2 ) α σ 2 {Γ (α σ 2 )} -1 (σ 2 ) -α σ 2 -1 exp - β σ 2 σ 2 × (i,j)∈S obs 1 a ij1 ! . . . a ijH j ! w a ij1 j1 . . . w a ijH j jH j × M j=1    Γ(H j α w ) Γ(α w ) H j H j h=1 w αw-1 jh    .</formula><p>The logarithm of the joint likelihood function arising from model ( <ref type="formula" target="#formula_0">1</ref>) is log p(y; µ, η, λ, ψ 2 , β, σ 2 , a, w) = (i,j)∈S obs</p><formula xml:id="formula_20">H j h=1 - 1 2 log(2π) - (y ij -µ jh -λ j η i ) 2 2ψ 2 jh a ijh - N + 2α σ 2 + 2 2 log(σ 2 ) - 1 2σ 2 N i=1 η 2 i -2η i x T i β + (x T i β) 2 + 2β σ 2 - 1 2 (i,j)∈S obs H j h=1 a ijh log(ψ 2 jh ) - M j=1 H j h=1 α ψ 2 + 1 log(ψ 2 jh ) -β ψ 2 M j=1 H j h=1 1 ψ 2 jh - 1 2σ 2 λ M j=2 λ 2 j + µ λ σ 2 λ M j=2 λ j - 1 2σ 2 µ jh M j=1 H j h=1 µ 2 jh + µ µ jh σ 2 µ jh M j=1 H j h=1 µ jh - 1 2 (β -µ β ) T Σ -1 β (β -µ β ) + (i,j)∈S obs H j h=1 {a ijh log(w jh ) -log(a ijh !)} + (α w -1) M j=1 H j h=1 log(w jh ) + const.</formula><p>In order to achieve a tractable MFVB approximation for model (1), we factorize the density approximating the posterior as in equation ( <ref type="formula">5</ref>).</p><p>For j = 1, . . . , M and h = 1, . . . , H j ,</p><formula xml:id="formula_21">p(µ jh | rest) ∝ exp   - µ 2 jh 2   i:(i,j)∈S obs a ijh ψ 2 jh + 1 σ 2 µ jh   + µ jh    i:(i,j)∈S obs a ijh (y ij -λ j η i ) ψ 2 jh + µ µ jh σ 2 µ jh      ,</formula><p>from which it follows that q * (µ jh ) is N µ q(µ jh ) , σ 2 q(µ jh ) , j = 1, . . . , M, h = 1, . . . , H j , with µ q(µ jh ) = σ 2 q(µ jh )</p><p>i:(i,j)∈S obs µ q(a ijh ) µ q(1/ψ 2 jh ) y ij -µ q(λ j ) µ q(η i ) +</p><formula xml:id="formula_22">µ µ jh σ 2 µ jh and σ 2 q(µ jh ) = i:(i,j)∈S obs µ q(a ijh ) µ q(1/ψ 2 jh ) + 1 σ 2 µ jh -1 .</formula><p>Without loss of generality, we fix λ 1 to 1 for ensuring identifiability. For j = 2, . . . , M ,</p><formula xml:id="formula_23">p(λ j | rest) ∝ exp - λ 2 j 2 i:(i,j)∈S obs H j h=1 a ijh η 2 i ψ 2 jh + 1 σ 2 λ + λ j i:(i,j)∈S obs H j h=1 a ijh η i y ij -µ jh ψ 2 jh + µ λ σ 2 λ ,</formula><p>from which it follows that q * (λ j ) is N µ q(λ j ) , σ 2 q(λ j ) , j = 2, . . . , M, with µ q(λ j ) = σ 2 q(λ j ) i:(i,j)∈S obs</p><formula xml:id="formula_24">H j h=1 µ q(a ijh ) µ q(η i ) µ q(1/ψ 2 jh ) y ij -µ q(µ jh ) + µ λ σ 2 λ and σ 2 q(λ j ) = i:(i,j)∈S obs H j h=1 µ q(a ijh ) µ q(η 2 i ) µ q(1/ψ 2 jh ) + 1 σ 2 λ -1</formula><p>.</p><formula xml:id="formula_25">For i = 1, . . . , N , p(η i | rest) ∝ exp - η 2 i 2 j:(i,j)∈S obs H j h=1 a ijh λ 2 j ψ 2 jh + 1 σ 2 + η i j:(i,j)∈S obs H j h=1 a ijh λ j y ij -µ jh ψ 2 jh + x T i β σ 2 , from which it follows that q * (η i ) is N µ q(η i ) , σ 2 q(η i ) , 1 = 1, . . . , N, with µ q(η i ) = σ 2 q(η i ) µ q(1/σ 2 ) x T i µ q(β) + j:(i,j)∈S obs H j h=1 µ q(a ijh ) µ q(λ j ) (y ij -µ q(µ jh ) )µ q(1/ψ 2 jh )</formula><p>and σ 2 q(η i ) = j:(i,j)∈S obs H j h=1 µ q(a ijh ) µ q(λ 2 j ) µ q(1/ψ 2 jh ) + µ q(1/σ 2 ) -1</p><p>.</p><p>For j = 1, . . . , M and h = 1, . . . , H j ,</p><formula xml:id="formula_26">p(ψ 2 jh | rest) ∝ (ψ 2 jh ) - i:(i,j)∈S obs a ijh 2 +α ψ 2 +1 exp - 1 ψ 2 jh β ψ 2 + i:(i,j)∈S obs a ijh 2 (y ij -µ jh -λ j η i ) 2 , from which it follows that q * (ψ 2 jh ) is Inverse-Gamma α q(ψ 2 jh ) , β q(ψ 2 jh ) , j = 1, . . . , M, h = 1, . . . , H j , with α q(ψ 2 jh ) ≡ 1 2 i:(i,j)∈S obs µ q(a ijh ) + α ψ 2 and β q(ψ 2 jh ) ≡ β ψ 2 + 1 2 i:(i,j)∈S obs µ q(a ijh ) y 2 ij + µ q(µ 2 jh ) + µ q(λ 2 j ) µ q(η 2 i ) -2y ij µ q(µ jh )</formula><p>-2y ij µ q(λ j ) µ q(η i ) + 2µ q(µ jh ) µ q(λ j ) µ q(η i ) .</p><p>Next,</p><formula xml:id="formula_27">p(σ 2 | rest) ∝ (σ 2 ) -(N +2α σ 2 +2)/2 exp - 1 2σ 2 N i=1 η 2 i -2η i x T i β + (x T i β) 2 + 2β σ 2 .</formula><p>Following Wand (2017), we define the function</p><formula xml:id="formula_28">G     v 1 v 2   ; Q, r, s   ≡ - 1 8 tr Q{vec -1 (v 2 )} -1 [v 1 v T 1 {vec -1 (v 2 )} -1 -2I] - 1 2 r T {vec -1 (v 2 )} -1 v 1 - 1 2 s.</formula><p>for a d×1 vector v 1 and a d 2 ×1 vector v 2 such that vec -1 (v 2 ) is symmetric. This function arises from the fact that if θ is a d × 1 Multivariate Normal random vector with natural parameter vector η, then</p><formula xml:id="formula_29">E θ - 1 2 θ T Qθ -2r T θ + s = E θ - 1 2 θ T Qθ + r T θ - 1 2 s = G(η; Q, r, s).</formula><p>Given the expression of p(σ 2 | rest) given above, we want to calculate</p><formula xml:id="formula_30">E µ q(β) - 1 2 µ q(η 2 i ) -2µ q(η i ) x T i β + (x T i β) 2 = E µ q(β) - 1 2 (β T x i x T i β -2µ q(η i ) x T i β + µ q(η 2 i )</formula><p>, which is related to the expectation involving the G function with θ = β, Q = x i x T i , r = µ q(η i ) x i and s = µ q(η 2 i ) . It means this involves calculation of G η q(β) ; x i x T i , µ q(η i ) x i , µ q(η 2 i ) , where η q(β) is defined later. The optimal approximating density arising from p(σ 2 | rest) is then</p><formula xml:id="formula_31">q * (σ 2 ) is Inverse-Gamma α q(σ 2 ) , β q(σ 2 ) with α q(σ 2 ) ≡ N + 2α σ 2 2 and β q(σ 2 ) ≡ -N i=1 G η q(β) ; x i x T i , µ q(η i ) x i , µ q(η 2 i ) + β σ 2 .</formula><p>Using the property tr(A T B) = vec(B) T vec(A) and noticing that x</p><formula xml:id="formula_32">T i ββ T x i = tr(x T i ββ T x i ) = tr(x i x T i ββ T ) = vec(ββ T ) T vec(x i x T i ) and β T Σ -1 β β = tr(β T Σ -1 β β) = tr(Σ -1 β ββ T ) = vec(ββ T ) T vec(Σ -1 β ) we have p(β | rest) ∝ exp - 1 2σ 2 N i=1 x T i ββ T x i -2η i x T i β - 1 2 β T Σ -1 β β + β T Σ -1 β µ β = exp        β vec(ββ T )    T    1 σ 2 N i=1 η i x i + Σ -1 β µ β - 1 2σ 2 vec( N i=1 x i x T i ) - 1 2 vec(Σ -1 β )        .</formula><p>Hence, using (S.4) of <ref type="bibr" target="#b28">Wand (2017)</ref>,</p><formula xml:id="formula_33">q * (β) is N µ q(β) , Σ q(β) with µ q(β) ≡ Σ q(β) µ q(1/σ 2 ) N i=1 µ q(η i ) x i + Σ -1 β µ β and Σ q(β) ≡ µ q(1/σ 2 ) N i=1 x i x T i + Σ -1 β -1</formula><p>, from which we define</p><formula xml:id="formula_34">η q(β) ≡    Σ -1 q(β) µ q(β) -1 2 vec(Σ -1 q(β) )    . For (i, j) ∈ S obs , p(a ij | rest) ∝ H j h=1 1 a ijh ! w jh (ψ 2 jh ) -1/2 (2π) -1/2 exp - (y ij -µ jh -λ j η i ) 2 2ψ 2 jh a ijh</formula><p>, from which it follows that q * (a ij ) is Multinomial 1; µ q(a ij ) , i = 1, . . . , N, k = 1, . . . , K, with µ q(a ijh ) ≡ exp(τ ijh )</p><p>H j h=1 exp(τ ijh ), h = 1, . . . , H j , and τ ijh ≡ µ q(log(w jh )) -1 2 µ q(log(ψ 2 jh )) -1 2 log(2π) -1 2 µ q(1/ψ 2 jh ) y 2 ij + µ q(µ 2 jh ) + µ q(λ 2 j ) µ q(η 2 i ) -2y ij µ q(µ jh ) -2y ij µ q(λ j ) µ q(η i ) + 2µ q(µ jh ) µ q(λ j ) µ q(η i ) .</p><p>Note that p(w j | rest) ∝ from which it follows that q * (w j ) is Dirichlet(H j , α q(w j ) ), with α q(w j ) = (i,j)∈S obs µ q(a ij1 ) + α w , . . . , (i,j)∈S obs µ q(a ijH j ) + α w .</p><p>This provides µ q(log(w jh )) = digamma [α q(w j ) ] hdigamma H j h=1 [α q(w j ) ] h .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S.2 Derivations for model with Gaussian mixture on latent factors</head><p>Another way to represent model ( <ref type="formula" target="#formula_13">7</ref>) is, for i = 1, . . . , N , j = 1, . . . , M and k = 1, . . . , K, y ij | ν j , λ j , η i , ψ 2 j ∼ N (ν j + λ j η i , ψ 2 j ), p(η i | β 1 , . . . , β K , σ 2 1 , . . . , σ 2 K , a i ) = K k=1 σ -1 k (2π) -1/2 exp -</p><formula xml:id="formula_35">(η i -x T i β k ) 2 2σ 2 k a ik , a i ind.</formula><p>∼ Multinomial(1; w), ν j ind.</p><p>∼ N (µ ν , σ 2 ν ), λ j ind.</p><p>∼ N (µ λ , σ 2 λ ), ψ 2 j ind.</p><p>∼ Inverse-Gamma(α ψ 2 , β ψ 2 ),</p><formula xml:id="formula_36">β k ∼ N (µ β , Σ β ), σ 2 k ∼ Inverse-Gamma(α σ 2 , β σ 2 ), w ∼ Dirichlet(K, α w ). (S.1)</formula><p>In order to achieve a tractable MFVB approximation for model ( <ref type="formula" target="#formula_13">7</ref>), we factorize the density approximating the posterior as follows:</p><p>q(ν, λ, η, ψ 2 1 , . . . , ψ 2 M , β 1 , . . . , β K , σ 2 1 , . . . , σ 2 K , a, w)</p><p>= q(w) N i=1 {q(η i )q(a i )} M j=1 q(ν j )q(ψ 2 j ) M j=2 q(λ j ) K k=1 q(β k )q(σ 2 k ) . q * (ν j ) is N µ q(ν j ) , σ 2 q(ν j ) , j = 1, . . . , M, with µ q(ν j ) = σ 2 q(ν j )</p><p>i:(i,j)∈S obs µ q(1/ψ 2 j ) y ij -µ q(λ j ) µ q(η i ) + µ ν σ 2 ν and σ 2 q(ν j ) = i:(i,j)∈S obs µ q(1/ψ 2 j ) +</p><formula xml:id="formula_37">1 σ 2 ν -1</formula><p>.</p><p>Without loss of generality, we fix λ 1 to 1 for ensuring identifiability. q * (λ j ) is N µ q(λ j ) , σ 2 q(λ j ) , j = 2, . . . , M, with µ q(λ j ) = σ 2 q(λ j )</p><p>i:(i,j)∈S obs µ q(η i ) µ q(1/ψ 2 j ) y ij -µ q(ν j ) + µ λ σ 2 λ and σ 2 q(λ j ) = i:(i,j)∈S obs µ q(η 2 i ) µ q(1/ψ 2 j ) + a ik x T i β k σ 2 k , η q(β k ) is defined later. The optimal approximating density arising from p(σ 2 k | rest) is then q * (σ 2 k ) is Inverse-Gamma α q(σ 2 k ) , β q(σ 2 k ) , k = 1, . . . , K, with α q(σ 2 k ) ≡ N i=1 µ q(a ik ) + 2α σ 2 2 and β q(σ 2 k ) ≡ -N i=1 µ q(a ik ) G η q(β k ) ; x i x T i , µ q(η i ) x i , µ q(η 2 i )</p><p>+ β σ 2 .</p><p>For k = 1, . . . , K,</p><formula xml:id="formula_38">p(β k | rest) ∝ exp - 1 2σ 2 k N i=1 a ik x T i β k β T k x i -2η i x T i β k - 1 2 β T k Σ -1 β β k + β T k Σ -1 β µ β = exp        β k vec(β k β T k )    T     1 σ 2 k N i=1 a ik η i x i + Σ -1 β µ β - 1 2σ 2 k vec( N i=1 a ik x i x T i ) - 1 2 vec(Σ -1 β )         .</formula><p>Hence, using (S.4) or Wand (2017, JASA), q * (β k ) is N µ q(β k ) , Σ q(β k ) , k = 1, . . . , K, with µ q(β k ) ≡ Σ q(β k ) µ q(1/σ 2 k ) N i=1 µ q(a ik ) µ q(η i ) x i + Σ -1 β µ β and Σ q(β k ) ≡ µ q(1/σ 2 k ) N i=1 µ q(a ik )</p><formula xml:id="formula_39">x i x T i + Σ -1 β -1</formula><p>, from which we define</p><formula xml:id="formula_40">η q(β k ) ≡    Σ -1 q(β k ) µ q(β k )</formula><p>-1 2 vec(Σ -1 q(β k ) )    .</p><p>For i = 1, . . . , N and k = 1, . . . , K,</p><formula xml:id="formula_41">p(a i | rest) ∝ K k=1 1 a ik ! w k (σ 2 k ) -1/2 (2π) -1/2 exp - η 2 i -2η i x T i β k + (x T i β k ) 2 2σ 2 k a ik</formula><p>, from which it follows that q * (a i ) is Multinomial 1; µ q(a i ) , i = 1, . . . , N, with µ q(a ik ) ≡ exp(τ ik ) K k=1 exp(τ ik ), k = 1, . . . , K and τ ik ≡ µ q(log(w k )) -1 2 µ q(log(σ 2 k )) -1 2 log(2π) + µ q(1/σ 2 k ) G η q(β k ) ; x i x T i , µ q(η i ) x i , µ q(η 2 i )</p><p>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The path diagram for both models (1) and (7) in the context of the cognitive data study including 16 outcomes and 2 covariates ('Exposure' and 'Propensity Score'). The outcomes 'IQ 1' to 'AorLM 6' are described in Section 5. The number 1 over the arrow pointing to 'IQ 1' means that the factor loading connecting the latent variable 'Cognition' and the outcome 'IQ 1' is set to 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Figure 2: Path diagram for model (1) according to the specifications of the simulation example. The numbers on the errors are the true values used to generate the data sets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Scatterplots of the mean of the MFVB approximating densities versus the MCMC posterior means for the main parameters of interest (latent variables and covariates coeffiecients). Each point corresponds to a pair of MFVB and MCMC estimates from one of the 100 simulated datasets. The y = x line is represented in red.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure4: Posterior predictive densities for four representative outcomes of the Detroit data plotted over histograms of the outcome measurements. The top row refers to the model where the conditional densities of the outcomes are all assumed to be Gaussian according to option 2, and the bottom row corresponds to model (1) fitted according to option 1. Each grey line is the kernel density estimate of one of the 300 drawings of y from the posterior predictive distribution; their averages are represented as blue lines.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>For j = 2, . . . , M , p(λ j | rest) ∝ exp -</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>.</head><label></label><figDesc>For i = 1, . . . , N , p(η i | rest) ∝ exp -</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="12,148.54,38.00,317.48,212.97" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>and computed the coverage of these bootstrap intervals for each parameter. The results of MFVB implemented in conjunction with bootrap are stored in Table1and indicate great improvement over the coverage of plain MFVB, with the coverage values being much closer to the MCMC target.The performance of MFVB could of course be boosted by increasing the number of bootstrap Mean squared errors of MFVB and MCMC computed from fitting model (1) as described in the text to 100 simulated datasets.</figDesc><table><row><cell></cell><cell></cell><cell>λ 2</cell><cell>λ 3</cell><cell>λ 4</cell><cell>β 1</cell><cell>β 2</cell><cell cols="3">µ 11 µ 21 µ 22 µ 31 µ 32</cell></row><row><cell>MFVB</cell><cell></cell><cell cols="8">0.36 0.59 0.59 0.70 0.61 0.44 0.46 0.60 0.50 0.84</cell></row><row><cell cols="10">MFVB-Bootstrap 0.86 0.92 0.83 0.89 0.84 0.85 0.84 0.78 0.92 0.91</cell></row><row><cell>MCMC</cell><cell></cell><cell cols="8">0.96 0.97 0.96 0.94 0.93 0.92 0.95 0.96 0.99 0.98</cell></row><row><cell></cell><cell></cell><cell cols="2">µ 41 ψ 2 11</cell><cell>ψ 2 21</cell><cell>ψ 2 22</cell><cell>ψ 2 31</cell><cell>ψ 2 32</cell><cell>ψ 2 41</cell><cell>σ 2</cell></row><row><cell>MFVB</cell><cell></cell><cell cols="8">0.58 0.86 0.76 0.89 0.89 0.95 0.95 0.74</cell></row><row><cell cols="10">MFVB-Bootstrap 0.87 0.89 0.85 0.92 0.86 0.88 0.86 0.93</cell></row><row><cell>MCMC</cell><cell></cell><cell cols="8">0.92 0.97 0.94 0.97 0.93 0.96 0.95 0.98</cell></row><row><cell cols="10">Table 1: Coverage of MFVB, MFVB with 20 bootstrap iterations and MCMC, computed from</cell></row><row><cell cols="8">fitting model (1) as described in the text to 100 simulated datasets.</cell><cell></cell></row><row><cell></cell><cell>λ 2</cell><cell>λ 3</cell><cell>λ 4</cell><cell>β 1</cell><cell></cell><cell>β 2</cell><cell>µ 11</cell><cell>µ 21</cell><cell>µ 22</cell><cell>µ 31</cell></row><row><cell>MFVB</cell><cell cols="9">0.00033 0.00037 0.00009 0.00130 0.00293 0.04802 0.02825 0.02854 0.04056</cell></row><row><cell cols="10">MCMC 0.00034 0.00038 0.00009 0.00131 0.00297 0.04865 0.02799 0.02843 0.04001</cell></row><row><cell></cell><cell>µ 32</cell><cell>µ 41</cell><cell>ψ 2 11</cell><cell>ψ 2 21</cell><cell></cell><cell>ψ 2 22</cell><cell>ψ 2 31</cell><cell>ψ 2 32</cell><cell>ψ 2 41</cell><cell>σ 2</cell></row><row><cell>MFVB</cell><cell cols="9">0.02911 0.00736 0.05131 0.04578 0.02406 0.12209 0.06523 0.00240 0.02879</cell></row><row><cell cols="10">MCMC 0.02848 0.00739 0.05165 0.03913 0.02354 0.12483 0.06498 0.00241 0.02897</cell></row></table><note><p>datasets.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>λ 2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>λ 3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>λ 4</cell></row><row><cell>MFVB</cell><cell>0.76 0.78 0.80 0.82 0.84</cell><cell>0.78</cell><cell>0.80</cell><cell>0.82</cell><cell>0.84</cell><cell>MFVB</cell><cell>0.48 0.50 0.52 0.54</cell><cell cols="3">0.48 0.50 0.52 0.54</cell><cell>MFVB</cell><cell>0.18 0.22 0.20</cell><cell>0.18</cell><cell>0.20</cell><cell>0.22</cell></row><row><cell></cell><cell></cell><cell></cell><cell>MCMC</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>MCMC</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>MCMC</cell></row><row><cell></cell><cell></cell><cell></cell><cell>β 1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>β 2</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>1.10</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>1.05</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>2.10</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>MFVB</cell><cell>1.00</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>MFVB</cell><cell>2.00</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.95</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1.90</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>0.95</cell><cell>1.00</cell><cell>1.05</cell><cell>1.10</cell><cell></cell><cell></cell><cell>1.90</cell><cell>2.00</cell><cell>2.10</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>MCMC</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>MCMC</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="7">Acknowledgements</head><p><rs type="person">Khue-Dung Dang</rs> was supported by the <rs type="person">Andrew Sisson</rs> support package from the <rs type="institution">School of Mathematics and Statistics at the University of Melbourne</rs>.</p></div>
			</div>			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The joint likelihood function of model (S.1) is p(y, ν, λ, η, ψ 2 1 , . . . , ψ 2 M , β 1 , . . . , β K , σ 2 1 , . . . , σ 2 K , a, w)</p><p>The logarithm of the joint likelihood function arising from model ( <ref type="formula">7</ref>) is</p><p>from which it follows that</p><p>and σ 2 q(η i ) = j:(i,j)∈S obs µ q(λ 2 j ) µ q(1/ψ 2 j ) + K k=1 µ q(a ik ) µ q(1/σ 2 k )</p><p>-1</p><p>.</p><p>from which it follows that</p><p>Given this expression, we want to calculate</p><p>, which is related to the expectation involving the G function with</p><p>, where Note that</p><p>This provides µ q(log(w</p><p>All the above derivations produce Algorithm S.1 for fitting model (S.1). Data Input: y i , i = 1, . . . , N , vectors of length M .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S.3 Additional results for the real data study</head><p>being a p × p symmetric positive definite matrix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cycle until convergence:</head><p>For k = 1, . . . , K:</p><p>For k = 1, . . . , K: µ q(a ik ) ←-exp(τ ik ) K k=1 exp(τ ik )</p><p>σ 2 q(ηi) ←j:(i,j)∈S obs µ q(λ 2 j ) µ q(1/ψ 2 j )</p><p>For j = 1, . . . , M :</p><p>If j &gt; 1: </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Propensity score analysis for a semi-continuous exposure variable: a study of gestational alcohol exposure and childhood cognition</title>
		<author>
			<persName><forename type="first">Akkaya</forename><surname>Hocagil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Jacobson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">W</forename><surname>Jacobson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Ryan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society: Series A (Statistics in Society)</title>
		<imprint>
			<biblScope unit="volume">184</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1390" to="1413" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A hierarchical meta-analysis for settings involving multiple outcomes across multiple cohorts</title>
		<author>
			<persName><forename type="first">Akkaya</forename><surname>Hocagil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ryan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Jacobson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">W</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Day</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">L</forename><surname>Coles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Carmichael Olson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jacobson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Stat</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">462</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Full information estimation in the presence of incomplete data</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Arbuckle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advanced Structural Equation Modeling: Issues and Techniques</title>
		<editor>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Marcoulides</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Schumacker</surname></persName>
		</editor>
		<meeting><address><addrLine>Mahwah, NJ</addrLine></address></meeting>
		<imprint>
			<publisher>Erlbaum</publisher>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="243" to="277" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Exploring ecological patterns with structural equation modeling and Bayesian analysis</title>
		<author>
			<persName><forename type="first">G</forename><surname>Arhonditsis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Stow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Steinberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kenney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Lathrop</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mcbride</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Reckhow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ecological Modelling</title>
		<imprint>
			<biblScope unit="volume">192</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="385" to="409" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Structural equation models and mixture models with continuous nonnormal skewed distributions</title>
		<author>
			<persName><forename type="first">T</forename><surname>Asparouhov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Muthén</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Structural Equation Modeling: A Multidisciplinary Journal</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="19" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">mixtools: An R package for analyzing finite mixture models</title>
		<author>
			<persName><forename type="first">T</forename><surname>Benaglia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chauveau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Hunter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Statistical Software</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1" to="29" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Fetal alcohol growth restriction and cognitive impairment</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Carter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Jacobson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Molteno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">C</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Meintjes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">W</forename><surname>Jacobson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pediatrics</title>
		<imprint>
			<biblScope unit="volume">138</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Fitting structural equation models via variational approximations</title>
		<author>
			<persName><forename type="first">K.-D</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Maestrini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Structural Equation Modeling: A Multidisciplinary Journal</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="839" to="853" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Bayesian modelling of effects of prenatal alcohol exposure on child cognition based on data from multiple cohorts</title>
		<author>
			<persName><forename type="first">K.-D</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Ryan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Akkaya Hocagil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">L</forename><surname>Day</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Coles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Carmichael Olson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">W</forename><surname>Jacobson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Jacobson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Australian &amp; New Zealand Journal of Statistics</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="167" to="186" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Bayesian dynamic modeling of latent trait distributions</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Dunson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biostatistics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="551" to="568" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>Fazio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-C</forename></persName>
		</author>
		<idno type="arXiv">arXiv:2404.14124</idno>
		<title level="m">Gaussian distributional structural equation models: A framework for modeling latent heteroscedasticity</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Estimation for the multiple factor model when data are missing</title>
		<author>
			<persName><forename type="first">C</forename><surname>Finkbeiner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychometrika</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="409" to="420" />
			<date type="published" when="1979">1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Understanding predictive information criteria for Bayesian models</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gelman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vehtari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistics and Computing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="997" to="1016" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Updated clinical guidelines for diagnosing fetal alcohol spectrum disorders</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">E</forename><surname>Hoyme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">O</forename><surname>Kalberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Elliott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Blankenship</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Buckley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A.-S</forename><surname>Marais</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">K</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Abdul-Rahman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pediatrics</title>
		<imprint>
			<biblScope unit="volume">138</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A practical clinical approach to diagnosis of fetal alcohol spectrum disorders: clarification of the 1996 institute of medicine criteria</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">E</forename><surname>Hoyme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>May</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">O</forename><surname>Kalberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kodituwakku</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Gossage</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M</forename><surname>Trujillo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Buckley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Aragon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Khaole</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pediatrics</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="39" to="47" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Effects of prenatal alcohol exposure on cognitive and behavioral development: Findings from a hierarchical meta-analysis of data from six prospective longitudinal us cohorts</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Jacobson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Akkaya-Hocagil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Ryan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">C</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">C</forename><surname>Olson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Coles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">L</forename><surname>Day</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">W</forename><surname>Jacobson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Alcoholism: Clinical and Experimental Research</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2040" to="2058" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Maternal age, alcohol abuse history, and quality of parenting as moderators of the effects of prenatal alcohol exposure on 7.5-year intellectual function</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">W</forename><surname>Jacobson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Jacobson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Sokol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Chiodo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Corobana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Alcoholism: Clinical and Experimental Research</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1732" to="1745" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Impaired eyeblink conditioning in children with fetal alcohol syndrome</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">W</forename><surname>Jacobson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Stanton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Molteno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Burden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Fuller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">E</forename><surname>Hoyme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">K</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Khaole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Jacobson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Alcoholism: Clinical and Experimental Research</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="365" to="372" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Kaplan</surname></persName>
		</author>
		<title level="m">Structural Equation Modeling: Foundations and Extensions</title>
		<meeting><address><addrLine>Thousand Oaks</addrLine></address></meeting>
		<imprint>
			<publisher>Sage Publications</publisher>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
	<note>2nd edition</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Bayesian Statistics for the Social Sciences</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kaplan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
			<publisher>Guilford Publications</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">S.-Y</forename><surname>Lee</surname></persName>
		</author>
		<title level="m">Structural Equation Modeling: A Bayesian Approach</title>
		<imprint>
			<publisher>John Wiley &amp; Sons</publisher>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Fetal alcohol spectrum disorders: a review of the neurobehavioral deficits associated with prenatal alcohol exposure</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">N</forename><surname>Mattson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Bernes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">R</forename><surname>Doyle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Alcoholism: Clinical and Experimental Research</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1046" to="1062" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Efficient Bayesian structural equation modeling in Stan</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">C</forename><surname>Merkle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Fitzsimmons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uanhoro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Goodrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Statistical Software</title>
		<imprint>
			<biblScope unit="volume">100</biblScope>
			<biblScope unit="page" from="1" to="22" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Explaining variational approximations</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Ormerod</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Wand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The American Statistician</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="140" to="153" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">R: A Language and Environment for Statistical Computing</title>
		<author>
			<persName><forename type="first">Team</forename><surname>Core</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">R Foundation for Statistical Computing</title>
		<meeting><address><addrLine>Vienna, Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Bayesian measures of model complexity and fit</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Spiegelhalter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">G</forename><surname>Best</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">P</forename><surname>Carlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Van Der Linde</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society: Series B (Statistical Methodology)</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="583" to="639" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">RStan: the R interface to Stan</title>
		<imprint>
			<date type="published" when="2024">2024</date>
			<publisher>Stan Development Team</publisher>
		</imprint>
	</monogr>
	<note>R package version 2.32.6</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Structural equation models: Mixture models</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Vermunt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Magidson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Encyclopedia of Statistics in Behavioral Science</title>
		<imprint>
			<publisher>Wiley</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="1922" to="1927" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Fast approximate inference for arbitrarily large semiparametric regression models via message passing</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Wand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">112</biblScope>
			<biblScope unit="issue">517</biblScope>
			<biblScope unit="page" from="137" to="168" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Asymptotic equivalence of Bayes cross validation and widely applicable information criterion in singular learning theory</title>
		<author>
			<persName><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">116</biblScope>
			<biblScope unit="page" from="3571" to="3594" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">On variational Bayes estimation and variational information criteria for linear regression models</title>
		<author>
			<persName><forename type="first">C</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Ormerod</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mueller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Australian &amp; New Zealand Journal of Statistics</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="73" to="87" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A model comparison approach to posterior predictive model checks in Bayesian confirmatory factor analysis</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Templin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Mintz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Structural Equation Modeling: A Multidisciplinary Journal</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="339" to="349" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A Bayesian analysis of finite mixtures in the LISREL model</title>
		<author>
			<persName><forename type="first">H.-T</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-Y</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychometrika</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="133" to="152" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
