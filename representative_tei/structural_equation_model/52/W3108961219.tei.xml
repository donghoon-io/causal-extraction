<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Conditional variance penalties and domain shift robustness</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2020-11-23">23 November 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Christina</forename><surname>Heinze-Deml</surname></persName>
							<email>heinzedeml@stat.math.ethz.ch</email>
							<idno type="ORCID">0000-0002-2489-9415</idno>
							<affiliation key="aff1">
								<orgName type="department">Seminar for Statistics</orgName>
								<orgName type="institution">ETH Zurich</orgName>
								<address>
									<settlement>Zurich</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Nicolai</forename><surname>Meinshausen</surname></persName>
							<email>meinshausen@stat.math.ethz.ch</email>
							<affiliation key="aff1">
								<orgName type="department">Seminar for Statistics</orgName>
								<orgName type="institution">ETH Zurich</orgName>
								<address>
									<settlement>Zurich</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Christina Heinze-Deml</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Conditional variance penalties and domain shift robustness</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-11-23">23 November 2020</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1007/s10994-020-05924-1</idno>
					<note type="submission">Received: 15 July 2019 / Revised: 1 October 2020 / Accepted: 15 October 2020 /</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-28T23:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Domain shift</term>
					<term>Dataset shift</term>
					<term>Causal models</term>
					<term>Distributional robustness</term>
					<term>Anticausal prediction</term>
					<term>Image classification</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>When training a deep neural network for image classification, one can broadly distinguish between two types of latent features of images that will drive the classification. We can divide latent features into (i) 'core' or 'conditionally invariant' features C whose distribu- tion C|Y , conditional on the class Y, does not change substantially across domains and (ii) 'style' features S whose distribution S|Y can change substantially across domains. Exam- ples for style features include position, rotation, image quality or brightness but also more complex ones like hair color, image quality or posture for images of persons. Our goal is to minimize a loss that is robust under changes in the distribution of these style features. In contrast to previous work, we assume that the domain itself is not observed and hence a latent variable. We do assume that we can sometimes observe a typically discrete identifier or " ID variable". In some applications we know, for example, that two images show the same person, and ID then refers to the identity of the person. The proposed method requires only a small fraction of images to have ID information. We group observations if they share the same class and identifier (Y, ID) = (y, id) and penalize the conditional vari- ance of the prediction or the loss if we condition on (Y, ID) . Using a causal framework, this conditional variance regularization (CoRe) is shown to protect asymptotically against shifts in the distribution of the style variables in a partially linear structural equation model. Empirically, we show that the CoRe penalty improves predictive accuracy substantially in settings where domain changes occur in terms of image quality, brightness and color while we also look at more complex changes such as changes in movement and posture.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Deep neural networks (DNNs) have achieved outstanding performance on prediction tasks like visual object and speech recognition <ref type="bibr" target="#b36">(Krizhevsky et al. 2012;</ref><ref type="bibr" target="#b58">Szegedy et al. 2015;</ref><ref type="bibr" target="#b28">He et al. 2015)</ref>. Issues can arise when the learned representations rely on dependencies that vanish in test distributions (see for example <ref type="bibr" target="#b49">Quionero-Candela et al. (2009)</ref>, <ref type="bibr" target="#b60">Torralba and Efros (2011)</ref>, <ref type="bibr" target="#b15">Csurka (2017)</ref> and references therein). Such domain shifts can be caused by changing conditions such as color, background or location changes. Predictive performance is then likely to degrade. For example, consider the analysis presented in <ref type="bibr" target="#b37">Kuehlkamp et al. (2017)</ref> which is concerned with the problem of predicting a person's gender based on images of their iris. The results indicate that this problem is more difficult than previous studies have suggested due to the remaining effect of cosmetics after segmenting the iris from the whole image. <ref type="foot" target="#foot_0">1</ref> Previous analyses obtained good predictive performance on certain datasets but when testing on a dataset only including images without cosmetics accuracy dropped. In other words, the high predictive performance previously reported relied to a significant extent on exploiting the confounding effect of mascara on the iris segmentation which is highly predictive for gender. Rather than the desired ability of discriminating based on the iris' texture the systems would mostly learn to detect the presence of cosmetics.</p><p>More generally, existing biases in datasets used for training machine learning algorithms tend to be replicated in the estimated models <ref type="bibr" target="#b10">(Bolukbasi et al. 2016)</ref>. For an example involving Google's photo app, see <ref type="bibr" target="#b13">Crawford (2016)</ref> and <ref type="bibr" target="#b18">Emspak (2016)</ref>. In Sect. 5 we show many examples where unwanted biases in the training data are picked up by the trained model. As any bias in the training data is in general used to discriminate between classes, these biases will persist in future classifications, raising also considerations of fairness and discrimination <ref type="bibr" target="#b4">(Barocas and Selbst 2016)</ref>.</p><p>Addressing the issues outlined above, we propose Conditional variance Regularization (CoRe) to give differential weight to different latent features. Conceptually, we take a causal view of the data generating process and categorize the latent data generating factors into 'conditionally invariant' (core) and 'orthogonal' (style) features, as in <ref type="bibr" target="#b23">Gong et al. (2016)</ref>. The core and style features are unobserved and can in general be highly nonlinear transformations of the observed input data. It is desirable that a classifier only extracts the latent core features from the input data as they pertain to the target of interest in a stable and coherent fashion. Basing a prediction on the core features alone yields stable predictive accuracy even if the style features are altered. Under suitable assumptions, CoRe yields an estimator which is approximately invariant under changes in the conditional distribution of the style features (conditional on the class labels) and it is asymptotically robust with respect to domain shifts, arising through interventions on the style features. CoRe relies on the fact that for certain datasets we can observe grouped observations in the sense that we observe the same object under different conditions. For instance, such grouping information is available (i) in natural image data when several pictures of the same person are taken; (ii) in medical imaging when several images belonging to the same patient are made;</p><p>(iii) in speech recognition when multiple recordings from the same speaker are available; (iv) in video data where nearby frames showing the same objects can be exploited to group observations; (v) in data augmentation where a transformed data point can be grouped together with the original one.</p><p>We will show examples for the first and last category. For the last category, we will show that pairing the augmented data with the original image they were generated from helps to improve accuracy and robustness with respect to the chosen transformation.</p><p>Rather than pooling over all examples, CoRe exploits knowledge about this grouping, i.e., that a number of instances relate to the same object. By penalizing between-object variation of the prediction less than variation of the prediction for the same object, we can steer the prediction to be based more on the latent core features and less on the latent style features. While the proposed methodology can be motivated from the desire the achieve representational invariance with respect to the style features, the causal framework we use throughout this work allows to precisely formulate the distribution shifts we aim to protect against.</p><p>The remainder of this manuscript is structured as follows: Sect. 1.1 starts with a few motivating examples, showing simple settings where the style features change in the test distribution such that standard empirical risk minimization approaches would fail. In Sect. 1.2 we review related work, introduce notation in Sect. 2 and in Sect. 3 we formally introduce conditional variance regularization CoRe. In Sect. 4, CoRe is shown to be asymptotically equivalent to minimizing the risk under a suitable class of strong interventions in a partially linear classification setting, provided one chooses sufficiently strong CoRe penalties. We also show that the population CoRe penalty induces domain shift robustness for general loss functions to first order in the intervention strength. The size of the conditional variance penalty can be shown to determine the size of the distribution class over which we can expect distributional robustness. In Sect. 5 we evaluate the performance of CoRe in a variety of experiments.</p><p>To summarize, our contributions are the following:</p><p>(i) Causal framework and distributional robustness We build on the causal framework from <ref type="bibr" target="#b23">Gong et al. (2016)</ref> to define distributional shifts for style variables. This allows us to formulate the objective of interest in terms of distributional robust inference. Specifically, the distribution class, on which the estimator should achieve a guaranteed performance bound, consists of those distributions that are generated by interventions on the latent style variables in a causal model. Our framework allows that the domain variable itself is latent. (ii) Conditional variance penalties We introduce conditional variance penalties and show two robustness properties in Theorems 1 and 2. (iii) Software We illustrate our ideas using synthetic and real-data experiments. A Ten-sorFlow implementation of CoRe as well as code to reproduce some of the experimental results are available at <ref type="url" target="https://githu">https ://githu</ref> b.com/chris tinah einze /core.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Motivating examples</head><p>To motivate the methodology we propose, consider the examples shown in Figs. <ref type="figure" target="#fig_16">1</ref> and<ref type="figure">2</ref>.</p><p>Example 1 shows a setting where a nonlinear decision boundary is required. Here, the core 1 3</p><p>feature corresponds to the distance from the origin while the style feature corresponds to the angle between the x 1 -axis and the vector from the origin to (x 1 , x 2 ) . Panel (a) shows a subsample of the training data where class 1 is associated with red points, dark blue points correspond to class 0. Panel (b) additionally shows a subsample of the test data where the style-i.e. the distribution of the angle-is intervened upon: class 1 is associated with orange squares, cyan squares correspond to class 0. Clearly, a circular decision boundary yields optimal performance on both training and test set but is unlikely to be found by a standard classification algorithm when only using the training set for the estimation. We will return to these examples in Sect. 3.4. Secondly, we introduce a strong dependence between the class label and the style feature "image quality" in the third example by manipulating the face images from the CelebA dataset <ref type="bibr" target="#b40">(Liu et al. 2015)</ref>  neural network to distinguish between people wearing glasses or not works well on test data that are drawn from the same distribution (with error rates below 2%) but fails entirely on the shown test data, with error rates worse than 65%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Related work</head><p>For general distributional robustness, the aim is to learn for a given set F of distributions, twice differentiable and convex loss , and prediction f (x) . The set F is the set of distributions on which one would like the estimator to achieve a guaranteed performance bound.</p><p>Causal inference can be seen to be a specific instance of distributional robustness, where we take F to be the class of all distributions generated under do-interventions on the pre- dictors X <ref type="bibr" target="#b44">(Meinshausen 2018;</ref><ref type="bibr" target="#b52">Rothenh√§usler et al. 2018)</ref>. Causal models thus have the defining advantage that the predictions will be valid even under arbitrarily large interventions on all predictor variables <ref type="bibr" target="#b26">(Haavelmo 1944;</ref><ref type="bibr" target="#b1">Aldrich 1989;</ref><ref type="bibr" target="#b47">Pearl 2009;</ref><ref type="bibr" target="#b54">Sch√∂lkopf et al. 2012;</ref><ref type="bibr" target="#b48">Peters et al. 2016;</ref><ref type="bibr" target="#b70">Zhang et al. 2013</ref><ref type="bibr" target="#b69">Zhang et al. , 2015;;</ref><ref type="bibr" target="#b68">Yu et al. 2017;</ref><ref type="bibr" target="#b51">Rojas-Carulla et al. 2018;</ref><ref type="bibr" target="#b43">Magliacane et al. 2018)</ref>. There are two difficulties in transferring these results to the setting of domain shifts in image classification. The first hurdle is that the classification task is typically anti-causal since the image we use as a predictor is a descendant of the true class of the object we are interested in rather than the other way around. The second challenge is that the input data consists of pixel intensities and we do not want (or could) guard against arbitrary interventions on any or all variables but only would like to guard against a shift of the unobserved style features. It is hence not immediately obvious how standard causal inference can be used to guard against large domain shifts.</p><p>Another line of work uses a class of distributions of the form F = F (F 0 ) with with ùúñ &gt; 0 a small constant and D(F, F 0 ) being, for example, a -divergence (Namkoong  and Duchi 2017; Ben-Tal et al. 2013; Bagnell 2005; Volpi et al. 2018) or a Wasserstein distance <ref type="bibr" target="#b55">(Shafieezadeh-Abadeh et al. 2017;</ref><ref type="bibr" target="#b56">Sinha et al. 2018;</ref><ref type="bibr" target="#b21">Gao et al. 2017)</ref>. The distribution F 0 can be the true (but generally unknown) population distribution P from which the data were drawn or its empirical counterpart P n . The distributionally robust targets in Eq. ( <ref type="formula">2</ref>) can often be expressed in penalized form <ref type="bibr" target="#b21">(Gao et al. 2017;</ref><ref type="bibr" target="#b56">Sinha et al. 2018;</ref><ref type="bibr" target="#b66">Xu et al. 2009)</ref>. A Wasserstein-ball is a suitable class of distributions for example in the context of adversarial examples <ref type="bibr" target="#b56">(Sinha et al. 2018;</ref><ref type="bibr" target="#b59">Szegedy et al. 2014;</ref><ref type="bibr" target="#b24">Goodfellow et al. 2015)</ref>.</p><p>In this work, we do not try to achieve robustness with respect to a set of distributions that are pre-defined by a Kullback-Leibler divergence or a Wasserstein metric as in Eq. ( <ref type="formula">2</ref>). Instead, we try to achieve robustness against a set of distributions that are generated by interventions on latent style variables in a causal model (we will make this precise in Sect. 2). We will formulate the class of distributions over which we try to achieve robustness as in Eq. ( <ref type="formula">1</ref>) but with the class of distributions in Eq. ( <ref type="formula">2</ref>) now replaced with the class of distributions F defined as {F ‚à∂ D style (F, F 0 ) ‚â§ }, where F 0 is again the distribution the training data are drawn from. The difference to standard distributional robustness approaches listed below Eq. ( <ref type="formula">2</ref>) is now that the metric D style measures the shift of the (1) argmin sup</p><formula xml:id="formula_0">F‚ààF E F ( (Y, f (X))) (2) F (F 0 ) ‚à∂= {distributions F such that D(F, F 0 ) ‚â§ },</formula><p>orthogonal style features. We do not know a priori which features are prone to distributional shifts and which features have a stable (conditional) distribution. The metric is hence not known a priori and needs to be inferred in a suitable sense from the data.</p><p>Similar to this work in terms of their goals are the work of <ref type="bibr" target="#b23">Gong et al. (2016)</ref> and Domain-Adversarial Neural Networks (DANN) proposed in <ref type="bibr" target="#b20">Ganin et al. (2016)</ref>, an approach motivated by the work of <ref type="bibr" target="#b7">Ben-David et al. (2007)</ref>. The main idea of <ref type="bibr" target="#b20">Ganin et al. (2016)</ref> is to learn a representation that contains no discriminative information about the origin of the input (source or target domain). This is achieved by an adversarial training procedure: the loss on domain classification is maximized while the loss of the target prediction task is minimized simultaneously. The data generating process assumed in <ref type="bibr" target="#b23">Gong et al. (2016)</ref> is similar to our model, introduced in Sect. 2.1, where we detail the similarities and differences between the models (cf. Fig. <ref type="figure">3</ref>). <ref type="bibr" target="#b23">Gong et al. (2016)</ref> identify the conditionally independent features by adjusting a transformation of the variables to minimize the squared MMD distance between distributions in different domains. <ref type="foot" target="#foot_1">2</ref> The fundamental difference between these very promising methods and our approach is that we use a different data basis. The domain identifier is explicitly observable in <ref type="bibr" target="#b23">Gong et al. (2016)</ref> and <ref type="bibr" target="#b20">Ganin et al. (2016)</ref>, while it is latent in our approach. In contrast, we exploit the presence of an identifier variable ID that relates to the identity of an object (for example identifying a person). In other words, we do not assume that we have data from different domains but just different realizations of the same object under different interventions. This also differentiates this work from latent domain adaptation papers from the computer vision literature <ref type="bibr" target="#b30">(Hoffman et al. 2012;</ref><ref type="bibr" target="#b22">Gong et al. 2013)</ref>. Further related work is discussed in Sect. 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(a)</head><formula xml:id="formula_1">Domain D Y S(‚àÜ) ‚àÜ C image X(‚àÜ) (b) Domain D Y S(‚àÜ) ‚àÜ ID C image X(‚àÜ)</formula><p>Fig. <ref type="figure">3</ref> Observed quantities are shown as shaded nodes; nodes of latent quantities are transparent. Left: data generating process for the considered model as in <ref type="bibr" target="#b23">Gong et al. (2016)</ref>, where the effect of the domain on the orthogonal features S is mediated via unobserved noise . The style interventions and all its descendants are shown as nodes with dashed borders to highlight variables that are affected by style interventions. Right: our setting. The domain itself is unobserved but we can now observe the (typically discrete) ID variable we use for grouping. The arrow between ID and Y can be reversed, depending on the sampling scheme 1 3</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Setting</head><p>We introduce the assumed underlying causal graph and some notation before discussing notions of domain shift robustness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Causal graph</head><p>Let Y ‚àà Y be a target of interest. Typically Y = ‚Ñù for regression or Y = {1, ‚Ä¶ , K} in clas- sification with K classes. Let X ‚àà ‚Ñù p be predictor variables, for example the p pixels of an image. The causal structural model for all variables is shown in panel (b) of Fig. <ref type="figure">3</ref>. The domain variable D is latent, in contrast to <ref type="bibr" target="#b23">Gong et al. (2016)</ref> whose model is shown in panel (a) of Fig. <ref type="figure">3</ref>. We add the ID variable to the graph. In Fig. <ref type="figure">3</ref>, Y ‚Üí ID but in some set- tings it might be more plausible to consider ID ‚Üí Y . For the proposed method both options are possible. Together with Y, the ID variable is used to group observations. It is typically discrete and relates to the identity of the underlying object. The variable can be assumed to be latent in the setting of <ref type="bibr" target="#b23">Gong et al. (2016)</ref>.</p><p>The rest of the graph is in analogy to <ref type="bibr" target="#b23">Gong et al. (2016)</ref>. The prediction is anti-causal, that is the predictor variables X that we use for ≈∂ are non-ancestral to Y. In other words, the class label is here seen to be causal for the image and not the other way around. <ref type="foot" target="#foot_2">3</ref> The causal effect from the class label Y on the image X is mediated via two types of latent variables: the so-called core or 'conditionally invariant' features C and the orthogonal or style fea- tures S . The distinguishing factor between the two is that external interventions are pos- sible on the style features but not on the core features. If the interventions have different distributions in different domains, then the conditional distributions C|Y = y, ID = id are invariant for all (y, id) while S|Y = y, ID = id can change. The style variable can include point of view, image quality, resolution, rotations, color changes, body posture, movement etc. and will in general be context-dependent. <ref type="foot" target="#foot_3">4</ref> The style intervention variable influences both the latent style S , and hence also the image X. In potential outcome notation, we let S( = ) be the style under intervention = and X(Y, ID, = ) the image for class Y, identity ID and style intervention . The latter is sometimes abbreviated as X( = ) for notational simplicity. Finally, f (X( = )) is the prediction under the style intervention = . For a formal justification of using a causal graph and potential outcome notation simultaneously see <ref type="bibr" target="#b50">Richardson and Robins (2013)</ref>.</p><p>To be specific, if not mentioned otherwise we will assume a causal graph as follows. For independent Y , ID , style in ‚Ñù, ‚Ñù, ‚Ñù q respectively with positive density on their support and continuously differentiable functions k y , k id , and</p><formula xml:id="formula_2">k style , k core , k x ,</formula><p>The distribution of style is assumed to be identical across domains, while can change. In more generality, one could discard and instead allow the distribution of style to change.</p><p>Here the assumption is slightly more restrictive because of additivity of in the structural equation for the style variable. The core features are here assumed to be a deterministic function of Y and ID to allow for theoretical analysis. In more generality (and as indicated in the graph), these would also be non-deterministic relations. The theoretical results will also require positive density for the style features in an -ball around the origin, as made precise in assumption (A1) later.</p><p>The prediction ≈∑ for y, given X = x , is of the form f (x) for a suitable function f with parameters ‚àà ‚Ñù d , where the parameters correspond to the weights in a DNN, for example.</p><p>We would like to stress that the above model is fairly general and subsumes many simpler ones as special cases. To give a concrete example, consider the task of classifying a health condition Y from medical images X. Style features could be, for example, technical noise, orientation or resolution. The unobserved domain D could correspond to different hospitals or doctors. Due to the usage of different measuring devices in each of these locations, the conditional distribution S|Y will change substantially across different domains. In contrast, the core features C , i.e. those image features that carry the actual signal, will remain invariant conditional on the underlying health condition Y.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Data</head><p>We assume we have n data points (x i , y i , id i ) for i = 1, ‚Ä¶ , n , where the observations id i with i = 1, ‚Ä¶ , n of variable ID can also contain unobserved values. Let m ‚â§ n be the number of unique realizations of (Y, ID) and let G 1 , ‚Ä¶ , G m be a partition of {1, ‚Ä¶ , n} such that, for each j ‚àà {1, ‚Ä¶ , m} , the realizations (y i , id i ) are identical<ref type="foot" target="#foot_4">foot_4</ref> for all i ‚àà G j . While our prime application is classification, regression settings with continuous Y can be approximated in this framework by slicing the range of the response variable into distinct bins in analogy to the approach in sliced inverse regression <ref type="bibr" target="#b39">(Li 1991)</ref>. The cardinality of G j is denoted by n j ‚à∂= |G j | ‚â• 1 . Then n = ‚àë i n i is again the total number of samples and c = nm is the total number of grouped observations in the following sense: if we count all samples in a group except the first one we have, if summing over all groups, a total of c = nm = ‚àë n i=1 (n i -1) observations left that are 'grouped' with the first example in their corresponding group.</p><p>Typically n i = 1 for most samples and occasionally n i ‚â• 2 but one can also envisage scenarios with larger groups of the same identifier (y, id).</p><p>(3)</p><formula xml:id="formula_3">Y ‚Üê k y (D, Y ) identifier ID ‚Üê k id (Y, ID ) core or conditionally invariant features C ‚Üê k core (Y, ID) style or orthogonal features S ‚Üê k style (Y, ID, style ) + image X ‚Üê k x (C, S).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Domain shift robustness</head><p>In this section, we clarify against which classes of distributions we hope to achieve robustness. Let be a suitable loss that maps y and ≈∑ = f ùúÉ (x) to ‚Ñù + . The risk under dis- tribution F and parameter is given by Let F 0 be the joint distribution of (ID, Y, S) in the training distribution. A new domain and explicit interventions on the style features can now shift the distribution of (ID, Y, S) to F. We can measure the distance between distributions F 0 and F in different ways. Below we will define the distance considered in this work and denote it by D style (F, F 0 ) . Once defined, we get a class of distributions and the goal will be to optimize a worst-case loss over this distribution class in the sense of Eq. ( <ref type="formula">1</ref>), where larger values of afford protection against larger distributional changes. The relevant loss for distribution class F is then In the limit of arbitrarily strong interventions on the style features S , the loss is given by Minimizing the loss L ‚àû ( ) with respect to guarantees an accuracy in prediction which will work well across arbitrarily large shifts in the conditional distribution of the style features.</p><p>A natural choice to define D style is to use a Wasserstein-type distance (see e.g. Villani  2003). We will first define a distance D y,id for the conditional distributions and then set D(F 0 , F) = E(D Y,ID ) , where the expectation is with respect to random ID and labels Y. The distance D y,id between the two conditional distributions of S will be defined as a Wasserstein W 2 2 (F 0 , F)-distance for a suitable cost function c(x, x) . Specifically, let Œ† y,id be the couplings between the conditional distributions of S and S , meaning measures sup- ported on ‚Ñù q √ó ‚Ñù q such that the marginal distribution over the first q components is equal to the distribution of S and the marginal distribution over the remaining q components equal to the distribution of S . Then the distance between the conditional distributions is defined as where c ‚à∂ ‚Ñù q √ó ‚Ñù q ‚Ü¶ ‚Ñù + is a nonnegative, lower semi-continuous cost function. Here, we focus on a Mahalanobis distance as cost</p><formula xml:id="formula_4">E F (Y, f (X)) . (4) F = {F ‚à∂ D style (F 0 , F) ‚â§ } (5) L ( ) = sup F‚ààF E F Y, f X . (6) L ‚àû ( ) = lim ‚Üí‚àû sup F‚ààF E F Y, f X . S|Y = y, ID = id and S|Y = y, ID = id, D y,id = min M‚ààŒ† y,id E c(x, x) , 1<label>3</label></formula><p>The cost of a shift is hence measured against the variability under the distribution F 0 , Œ£ y,id = Var(S|Y, ID). <ref type="foot" target="#foot_5">6</ref>Clearly, since the core and style features are unobserved, we cannot directly optimize the loss (5) but need to infer the metric D style from the input data X. In the next section, we will show how this can be achieved. Intuitively, the model in Fig. <ref type="figure">3</ref> implies that the variance conditional on Y, ID stems from the difference in the style features. Hence, we would like to minimize the conditional variance of the prediction or loss when we condition on Y, ID . This enforces the desired invariance with respect to the style features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Conditional variance regularization</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Pooled estimator</head><p>Let (x i , y i ) for i = 1, ‚Ä¶ , n be the observations that constitute the training data and ≈∑i = f ùúÉ (x i ) the prediction for y i . The standard approach is to simply pool over all available observations, ignoring any grouping information that might be available. The pooled estimator thus treats all examples identically by summing over the empirical loss as where the first part is simply the empirical loss over the training data, In the second part, pen( ) is a complexity penalty, for example a squared 2 -norm of the weights in a convolutional neural network as a ridge penalty.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">CoRe estimator</head><p>The CoRe estimator is defined in Lagrangian form for penalty ‚â• 0 as The penalty ƒàùúÉ is a conditional variance penalty of the form</p><formula xml:id="formula_5">c 2 (x, x) = (x -x) t Œ£ -1 y,id (x -x). (7) Œ∏pool = argmin ùúÉ √ä ùìÅ(Y, f ùúÉ (X)) + ùõæ ‚ãÖ pen(ùúÉ), √ä (Y, f ùúÉ (X)) = 1 n n ‚àë i=1 y i , f ùúÉ (x i ) . (8) Œ∏core (ùúÜ) = argmin ùúÉ √ä ùìÅ(Y, f ùúÉ (X)) + ùúÜ ‚ãÖ ƒàùúÉ . (9) conditional-variance-of-prediction: ƒàf ,ùúà,ùúÉ ‚à∂= √ä ÔøΩ Var(f ùúÉ (X)|Y, ID) ùúà</formula><p>where typically ‚àà {1‚àï2, 1} . For = 1‚àï2 , we also refer to the respective penalties as "conditional-standard-deviation" penalties. In practice in the context of classification and DNNs, we apply the penalty (9) to the predicted logits. The conditional-variance-of-loss penalty (10) takes a similar form to <ref type="bibr" target="#b46">Namkoong and Duchi (2017)</ref>. The crucial difference of our approach to <ref type="bibr" target="#b46">Namkoong and Duchi (2017)</ref> is that we penalize with the expected conditional variance or standard deviation. The fact that we take a conditional variance is here important as we try to achieve distributional robustness with respect to interventions on the style variables. Conditioning on ID allows to guard specifically against these interven- tions. An unconditional variance penalty, in contrast, can achieve robustness against a predefined class of distributions such as a ball of distributions defined in a Kullback-Leibler or Wasserstein metric. The population CoRe estimator is defined as in Eq. ( <ref type="formula">8</ref>) where empirical estimates are replaced by their respective population quantities.</p><p>Before showing numerical examples, we discuss the estimation of the expected conditional variance in Sect.  <ref type="formula" target="#formula_4">3</ref>) is shown in Sect. 4.1. Furthermore, we discuss the population limit of Œ∏core (ùúÜ) in Sect. 4.2, where we show that the regularization parameter ‚â• 0 is proportional to the size of the future style interventions that we want to guard against for future test data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Estimating the expected conditional variance</head><p>Recall that G j ‚äÜ {1, ‚Ä¶ , n} contains samples with identical realizations of (Y, ID) for j ‚àà {1, ‚Ä¶ , m} . For each j ‚àà {1, ‚Ä¶ , m} , define ŒºùúÉ,j as the arithmetic mean across all f (x i ), i ‚àà G j . The canonical estimator of the conditional variance ƒàf ,1,ùúÉ is then and analogously for the conditional-variance-of-loss, defined in Eq. ( <ref type="formula">10</ref>) <ref type="foot" target="#foot_6">7</ref> . If there are no groups of samples that share the same identifier (y, id) , we define ƒàf ,1,ùúÉ to vanish. The CoRe estimator is then identical to pooled estimation in this special case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Motivating examples (continued)</head><p>We revisit the first example from Sect. 1.1. Figure <ref type="figure">4</ref> shows subsamples of the training and test set with the estimated decision boundaries for different values of the penalty parameter when using a 2-layer fully connected neural network. Here, n = 20,000 and c = 500 . Additionally, grouped examples that share the same (y, id) are visualized: two grouped observations are connected by a line or curve, respectively. Ten such groups are shown. Panel (a) shows the decision boundaries for = 0 , equivalent to the pooled estimator, and for CoRe with ‚àà {0, 0.05, 0.1, 1} . The pooled estimator misclassifies a large number of (10) conditional-variance-of-loss:</p><formula xml:id="formula_6">ƒà ,ùúà,ùúÉ ‚à∂= √ä ÔøΩ Var( (Y, f ùúÉ (X))|Y, ID) ùúà , ƒàf ,1,ùúÉ ‚à∂= 1 m m ‚àë j=1 1 |G j | ‚àë i‚ààG j (f ùúÉ (x i ) -ŒºùúÉ,j ) 2 , where ŒºùúÉ,j = 1 |G j | ‚àë i‚ààG j f ùúÉ (x i )</formula><p>test points as can be seen in panel (b), suffering from a test error of ‚âà 58% . In contrast, the decision boundary of the CoRe estimator with = 1 aligns with the direction along which the grouped observations vary, classifying the test set with almost perfect accuracy (test error is ‚âà 0%).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Domain shift robustness for the CoRe estimator</head><p>We show two properties of the CoRe estimator. First, consistency is shown under the risk definition (6) for an infinitely large conditional variance penalty and the logistic loss in a partially linear structural equation model. Second, the population CoRe estimator is shown to achieve distributional robustness against shift interventions in a first order expansion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Asymptotic domain shift robustness under strong interventions</head><p>We analyze the loss under strong domain shifts, as given in Eq. ( <ref type="formula">6</ref>), for the pooled and the CoRe estimator in a one-layer network for binary classification (logistic regression) in an asymptotic setting of large sample size and strong interventions.</p><p>Assume the structural equation for the image X ‚àà ‚Ñù p is linear in the style features S ‚àà ‚Ñù q (with generally p ‚â´ q ) and we use logistic regression to predict the class label Y ‚àà {-1, 1} . Let the interventions ‚àà ‚Ñù q act additively on the style features S (this is only for notational convenience) and let the style features S act in a linear way on the image X via a matrix W ‚àà ‚Ñù p√óq (this is an important assumption without which results are more involved). The core or 'conditionally invariant' features are C ‚àà ‚Ñù r , where in general r ‚â§ p but this is not important for the following. For independent</p><formula xml:id="formula_7">Y , ID , style in ‚Ñù, ‚Ñù, ‚Ñù q (a) Example 1, training set. -4 -2 0 2 4 -4 -2 0 2 4 0 0.05 0.1 1 X 1 X 2 Y=0 (train) Y=1 (train) (b) Example 1, test set. -4 -2 0 2 4 -4 -2 0 2 4 0 0.05 0.1 1 X 1 X 2 Y=0 (train) Y=0 (test) Y=1 (train) Y=1 (test)</formula><p>Fig. <ref type="figure">4</ref> The decision boundary as function of the penalty parameters for Example 1 from Fig. <ref type="figure" target="#fig_3">1</ref>. There are ten pairs of samples visualized that share the same identifier (y, id) and these are connected by a curve in the figures. The decision boundary associated with a solid line corresponds to = 0 , the standard pooled estimator that ignores the groupings. The broken lines are decision boundaries for increasingly strong penalties, taking into account the groupings in the data. Here, we only show a subsample of the data to avoid overplotting 1 3 respectively with positive density on their support and continuously differentiable func-</p><formula xml:id="formula_8">tions k y , k id , k style , k core , k x ,</formula><p>We assume a logistic regression as a prediction of Y from the image data X:</p><p>Given training data with n samples, we estimate with Œ∏ and use here a logistic loss</p><formula xml:id="formula_9">(y i , x i ) = log(1 + exp(-y i (x t i ))</formula><p>). The formulation of Theorem 1 relies on the following assumptions.</p><p>Assumption 1 We require the following conditions:</p><p>(A1) Assume the conditional distribution S|Y = y, ID = id under the training distribution F 0 has positive density (with respect to the Lebesgue measure) in an -ball in 2 -norm around the origin for some ùúñ &gt; 0 for all y ‚àà Y and id ‚àà I. (A2) Assume the matrix W has full rank q. (A3) Let M ‚â§ n be the number of unique realizations among n iid samples of (Y, ID) and let p n ‚à∂= P(M ‚â§ nq) . Assume that p n ‚Üí 1 for n ‚Üí ‚àû.</p><p>Assumption (A1) is a key assumption about the style variations we observe in the training set. It requires that we observe some variance in those directions that we expect to be subject to domain shifts in the future. If, on the other hand, the conditional variance in a particular direction is vanishing, we also expect it to vanish in the future. A violation of this assumption would imply that the guarantee of the CoRe regularization no longer holds. Assumption (A3) guarantees that the number c = nm of grouped examples is at least as large as the dimension of the style variables. If we have too few or no grouped examples (small c), we cannot estimate the conditional variance accurately. Under these assumptions we can prove domain shift robustness. </p><formula xml:id="formula_10">class Y ‚Üê k y (D, Y ) identifier ID ‚Üê k id (Y, ID ) core or conditionally invariant features C ‚Üê k core (Y, ID) style or orthogonal features S ‚Üê k style (Y, ID, style ) + image X ‚Üê k x (C) + WS. f (x) ‚à∂= exp(x t ) 1 + exp(x t ) . L ‚àû ( Œ∏pool ) = ‚àû. L ‚àû ( Œ∏core ) ‚Üí p inf ùúÉ L ‚àû (ùúÉ).</formula><p>A proof is given in "Appendix A". The respective ridge penalties in both estimators ( <ref type="formula">7</ref>) and ( <ref type="formula">8</ref>) are assumed to be zero for the proof, but the proof can easily be generalized to include ridge penalties that vanish sufficiently fast for large sample sizes. The Lagrangian regularizer is assumed to be infinite for the CoRe estimator to achieve domain shift robustness under these strong interventions. The next section considers the population CoRe estimator in a setting with weak interventions and finite values of the penalty parameter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Population domain shift robustness under weak interventions</head><p>The previous theorem states that the CoRe estimator can achieve domain shift robustness under strong interventions for an infinitely strong penalty in an asymptotic setting. An open question is how the loss (5), behaves under interventions of small to medium size and correspondingly smaller values of the penalty. Here, we aim to minimize this loss for a given value of and show that domain shift robustness can be achieved to first order with the population CoRe estimator using the conditional-standard-deviation-of-loss penalty, i.e., Eq. ( <ref type="formula">10</ref>) with = 1‚àï2 , by choosing an appropriate value of the penalty . Below we will show this appropriate choice of the penalty weight is = ‚àö .</p><p>Assumption 2 We require the following conditions:</p><p>(B1) Define the loss under a deterministic shift as where the expectation is with respect to random (ID, Y, S) ‚àº F ùúÉ , with F defined by the deterministic shift intervention S = S + ùõø and (ID, Y, S) ‚àº F 0 . Assume that for all ‚àà Œò , h ( ) is twice continuously differentiable with bounded second derivative for a deterministic shift ‚àà ‚Ñù q . (B2) The spectral norm of the conditional variance Œ£ y,id of S|Y, ID under F 0 is assumed to be smaller or equal to some ‚àà ‚Ñù for all y ‚àà Y and id ‚àà I.</p><p>The first assumption (B1) ensures that the loss is well behaved under interventions on the style variables. The second assumption (B2) allows to take the limit of small conditional variances in the style variables.</p><p>If setting = ‚àö and using the conditional-standard-deviation-of-loss penalty, the CoRe estimator optimizes according to</p><p>The next theorem shows that this is to first order equivalent to minimizing the worst-case loss over the distribution class F . The following result holds for the population CoRe esti- mator, see below for a discussion about consistency.</p><formula xml:id="formula_11">L ( ) = sup F‚ààF E F Y, f (X) h ( ) ‚à∂= E F [ (Y, f (X))], Œ∏core ( ‚àö ùúâ) = argmin ùúÉ √äF 0 ÔøΩ ùìÅ(Y, f ùúÉ (X)) ÔøΩ + ‚àö ùúâ ‚ãÖ ƒàùìÅ,1‚àï2,ùúÉ .</formula><p>Theorem 2 The supremum of the loss over the class of distribution F is to first-order given by the expected loss under distribution F 0 with an additional conditional-standard-deviation-of-loss penalty C ,1‚àï2, A proof is given in "Appendix B". The objective of the population CoRe estimator matches thus to first order the loss under domain shifts if we set the penalty weight = ‚àö . Larger anticipated domain shifts thus require naturally a larger penalty in the CoRe estimation. The result is possible as we have chosen the Mahalanobis distance to measure shifts in the style variable and define F , ensuring that the strength of shifts on style variables are measured against the natural variance on the training distribution F 0 .</p><p>In practice, the choice of involves a somewhat subjective choice about the strength of the distributional robustness guarantee. A stronger distributional robustness property is traded off against a loss in predictive accuracy if the distribution is not changing in the future. One option for choosing is to choose the largest penalty weight before the validation loss increases considerably. This approach would provide the best distributional robustness guarantee that keeps the loss of predictive accuracy in the training distribution within a pre-specified bound. <ref type="foot" target="#foot_7">8</ref>As a caveat, the result takes the limit of small conditional variance of S in the train- ing distribution and small additional interventions. Under larger interventions higher-order terms could start to dominate, depending on the geometry of the loss function and f . A further caveat is that the result looks at the population CoRe estimator. For finite sample sizes, we would optimize a noisy version on the rhs of (12). To show domain shift robustness in an asymptotic sense, we would need additional uniform convergence (in ) of both the empirical loss and the conditional variance in that for n ‚Üí ‚àû, While this is in general a reasonable assumption to make, the validity of the assumption will depend on the specific function class and on the chosen estimator of the conditional variance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>We perform an array of different experiments, showing the applicability and advantage of the conditional variance penalty for two broad settings:</p><p>1. Settings where we do not know what the style variables correspond to but still want to protect against a change in their distribution in the future.</p><p>In the examples we show</p><formula xml:id="formula_12">(12) sup F‚ààF E F ÔøΩ ùìÅ ÔøΩ Y, f (X) ÔøΩÔøΩ = E F 0 ÔøΩ ùìÅ ÔøΩ Y, f (X) ÔøΩÔøΩ + ‚àö ‚ãÖ C ùìÅ,1‚àï2, + O(max{ , }). sup ùúÉ | √äF 0 (Y, f ùúÉ (X)) -E F 0 Y, f ùúÉ (X) | ‚Üí p 0, and sup ùúÉ | ƒà ,1‚àï2,ùúÉ -C ,1‚àï2,ùúÉ | ‚Üí p 0.</formula><p>cases where the style variable ranges from fashion (Sect. 5.2), image quality (Sect. 5.3), movement (Sect. 5.4) and brightness ("Appendix D.1"), which are all not known explicitly to the method. We also include genuinely unknown style variables in Sect. 5.1 (in the sense that they are unknown not only to the methods but also to us as we did not explicitly create the style interventions). 2. Settings where we do know what type of style interventions we would like to protect against. This is usually dealt with by data augmentation (adding images which are, say, rotated or shifted compared to the training data if we want to protect against rotations or translations in the test data; see for example <ref type="bibr" target="#b53">Sch√∂lkopf et al. (1996)</ref>). The conditional variance penalty is here exploiting that some augmented samples were generated from the same original sample and we use as ID variable the index of the original image. We show that this approach generalizes better than simply pooling the augmented data, in the sense that we need fewer augmented samples to achieve the same test error. This setting is shown in Sect. 5.5.</p><p>We compare against the pooled estimator which has the same architecture as the network to which we add the CoRe penalty. For both the pooled and the CoRe estimator we apply an 2 penalty as regularization. We would like to stress that the related work discussed in Sects. 1.2 and 6 cannot be directly compared to the CoRe estimator as these approaches cannot exploit the ID information but rely on having data from different domains avail- able at training time instead. Since this is a different problem setting, we can only compare against the pooled estimator which is a standard approach to classification. As a downside, our approach requires availability of an ID variable, which might not always be available <ref type="foot" target="#foot_8">9</ref>To further understand the behavior of the CoRe penalty, we perform a number of analyses and ablation studies to show (i) How sensitive the performance of CoRe is to the value of the penalty weight (Sects. 5.1.1, 5.2); (ii) How the CoRe penalty differs from a standard 2 penalty (Sect. 5.1.1); (iii) How the value of the CoRe penalty can be used as a qualitative measure for the presence of sample bias (Sects. 5.1.1, 5.2); (iv) How sensitive the performance of both the CoRe and the pooled estimator is to label shift in the grouped observations (Sect. 5.2.1); (v) How the relative performance of both estimators is affected when using pre-trained InceptionV3 features (Sect. 5.2.2); (vi) How sensitive the performance is to different grouping strategies (Sects. 5. Details of the network architectures can be found in Appendix "Appendix C". All reported error rates are averaged over five runs of the respective method. A TensorFlow <ref type="bibr" target="#b0">(Abadi et al. 2015)</ref> implementation of CoRe can be found at <ref type="url" target="https://githu">https ://githu</ref> b.com/chris tinah einze /core.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Eyeglasses detection with small sample size</head><p>In this example, we explore a setting where training and test data are drawn from the same distribution, so we might not expect a distributional shift between the two. However, we consider a small training sample size which gives rise to statistical fluctuations between training and test data. We assess to which extent the conditional variance penalty can help to improve test accuracies in this setting. Specifically, we use a subsample of the CelebA dataset <ref type="bibr" target="#b40">(Liu et al. 2015)</ref> and try to classify images according to whether or not the person in the image wears glasses. For construction of the ID variable, we exploit the fact that several photos of the same person are available and set ID to be the identifier of the person in the dataset. Figure <ref type="figure" target="#fig_5">5</ref> shows exam- ples from both the training and the test dataset. The conditional variance penalty is estimated across groups of observations that share a common (Y, ID) . Here, this corresponds to pictures of the same person where all pictures show the person either with glasses (if Y = 1 ) or all pictures show the person without glasses ( Y = 0 ). Statistical fluctuations between training and test set could for instance arise if by chance the background of eyeglass wearers is darker in the training sample than in test samples, the eyeglass wearers happen to be outdoors more often or might be more often female than male etc. Below, we present the following analyses. First, we look at five different datasets and analyze the effect of adding the CoRe penalty (using conditional-variance-of-prediction) to the cross-entropy loss. Second, we focus on one dataset and compare the four different variants of the CoRe penalty in Eqs. ( <ref type="formula">9</ref>) and (10) with ‚àà {1‚àï2, 1}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">CoRe penalty using the conditional variance of the predicted logits</head><p>We consider five different training sets which are created as follows. For each person in the standard CelebA training data we count the number of available images and select the 50 identities for which most images are available individually. We partition these 50 identities into 5 disjoint subsets of size 10 and consider the resulting 5 datasets, Given a training dataset, the standard approach would be to pool all examples. The only additional information we exploit is that some observations can be grouped. If using a 5-layer convolutional neural network with a standard ridge penalty (details can be found in Table <ref type="table" target="#tab_4">5</ref>) and pooling all data, the test error on unseen images ranges from 18.08 to 25.97%. Exploiting the group structure with the CoRe penalty (in addition to a ridge penalty) results in test errors ranging from 14.79 to 21.49%, see Table <ref type="table" target="#tab_0">1</ref>. The relative improvements when using the CoRe penalty range from 9 to 28.6%.</p><p>The test error is not very sensitive to the weight of the CoRe penalty as shown in Fig. <ref type="figure" target="#fig_16">6a</ref>: for a large range of penalty weights, adding the CoRe penalty decreases the test error compared to the pooled estimator (identical to a CoRe penalty weight of 0). This holds true for various ridge penalty weights.</p><p>While the test error rates shown in Fig. <ref type="figure" target="#fig_6">6</ref> suggest already that the CoRe penalty differentiates itself clearly from a standard ridge penalty, we examine next the differential effect of the CoRe penalty on the between-and within-group variances. Concretely, the variance of the predictions can be decomposed as where the first term on the rhs is the within-group variance that CoRe penalizes, while a ridge penalty would penalize both the within-and also the between-group variance (the second term on the rhs above). In Fig. <ref type="figure" target="#fig_6">6b</ref> we show the ratio between the CoRe penalty and the between-group variance where groups are defined by conditioning on (Y, ID) . Specifi- cally, the ratio is computed as</p><formula xml:id="formula_13">Var(f (X)) = E Var(f (X)|Y, ID) + Var E(f (X)|Y, ID) ,</formula><p>The results shown in Fig. <ref type="figure" target="#fig_6">6b</ref> are computed on dataset 1 (DS 1). While increasing ridge penalty weights do lead to a smaller value of the CoRe penalty, the between-group variance is also reduced such that the ratio between the two terms does not decrease with larger weights of the ridge penalty.<ref type="foot" target="#foot_9">foot_9</ref> With increasing weight of the CoRe penalty, the variance ratio decreases, showing that the CoRe penalty indeed penalizes the within-group variance more than the between-group variance. Table <ref type="table" target="#tab_0">1</ref> also reports the value of the CoRe penalty after training when evaluated for the pooled and the CoRe estimator on the training and the test set. As a qualitative measure to assess the presence of sample bias in the data (provided the model assumptions hold), we can compare the value the CoRe penalty takes after training when evaluated for the pooled estimator and the CoRe estimator. The difference yields a measure for the extent the respective estimators are functions of . If the respective hold-out values are both small, this would indicate that the style features are not very predictive for the target variable. If, on the other hand, the CoRe penalty evaluated for the pooled estimator takes a much larger value than for the CoRe estimator (as in this case), this would indicate the presence of sample bias. The results can be seen to be fairly insensitive to the ridge penalty. b The variance ratio (13) on test data as a function of both the CoRe and ridge penalty weights. The CoRe penalty can be seen to penalize the within-group variance selectively, whereas a strong ridge penalty decreases both the within-and betweengroup variance</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">Other CoRe penalty types</head><p>We now compare all CoRe penalty types, i.e., penalizing with (i) the conditional variance of the predicted logits ƒàf ,1,ùúÉ , (ii) the conditional standard deviation of the predicted log- its ƒàf ,1‚àï2,ùúÉ , (iii) the conditional variance of the loss ƒàl,1,ùúÉ and (iv) the conditional standard deviation of the loss ƒàl,1‚àï2,ùúÉ . For this comparison, we use the training dataset 1 (DS 1) from above. Table <ref type="table" target="#tab_1">2</ref> contains the test error (training error was 0% for all methods) as well as the value the respective CoRe penalty took after training on the training set and the test set. The four CoRe penalty variants' performance differences are not statistically significant.</p><p>Hence, we mostly focus on the conditional variance of the predicted logits ƒàf ,1,ùúÉ in the other experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.3">Discussion</head><p>While the distributional shift in this example arises due to statistical fluctuations which will diminish as the sample size grows, the following examples are more concerned with biases that will persist even if the number of training and test samples is very large. A second difference to the subsequent examples is the grouping structure-in this example, we consider only a few identities, namely m = 10 , with a relatively large number n i of associated observations (about thirty observations per individual). In the following examples, m is much larger while n i is typically smaller than five.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Gender classification with unknown confounding</head><p>In the following set of experiments, we work again with the CelebA dataset and the 5-layer convolutional neural network architecture described in Table <ref type="table" target="#tab_4">5</ref>. This time we consider the problem of classifying whether the person shown in the image is male or female. We create a confounding in training and test set I by including mostly images of men wearing glasses and women not wearing glasses. In test set 2 the association between gender and glasses is flipped: women always wear glasses while men never To compute the conditional variance penalty, we use again images of the same person. The ID variable is, in other words, the identity of the person and gender Y is con- stant across all examples with the same ID . Conditioning on (Y, ID) is hence identical to conditioning on ID alone. Another difference to the other experiments is that we consider a binary style feature here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Label shift in grouped observations</head><p>We compare six different datasets that vary with respect to the distribution of Y in the grouped observations. In all training datasets, the total number of observations is 16982 and the total number of grouped observations is 500. In the first dataset, 50% of the grouped observations correspond to males and 50% correspond to females. In the remaining 5 datasets, we increase the number of grouped observations with Y = "man" , denoted by , to 75%, 90%, 95%, 99% and 100%, respectively. Table <ref type="table" target="#tab_2">3</ref> shows the performance obtained for these datasets when using the pooled estimator compared to the CoRe estimator with ƒàf ,1,ùúÉ . The results show that both the pooled estimator as well as the CoRe estimator perform better if the distribution of Y in the grouped observations is more balanced. The CoRe estimator improves the error rate of the pooled estimator by ‚âà 28 -39% on a relative scale. Figure <ref type="figure" target="#fig_8">8</ref> shows the performance for = 50% as a function of the CoRe penalty weight. Significant improvements can be obtained across a large range of values for the CoRe penalty and the ridge penalty. Test errors become more sensitive to the chosen value of the CoRe penalty for very large values of the ridge penalty weight as the overall amount of regularization is already large. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Using pre-trained Inception V3 features</head><p>To verify that the above conclusions do not change when using more powerful features, we here compare 2 -regularized logistic regression using pre-trained Inception V3 features<ref type="foot" target="#foot_10">foot_10</ref> with and without the CoRe penalty. Table <ref type="table">4</ref> shows the results for = 0.5 . While the results show that both the pooled estimator as well as the CoRe estimator perform better using pre-trained Inception features, the relative improvement with the CoRe penalty is still 28% on test set 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.3">Ablation experiments</head><p>In Sect. D.3.1, we report results for the following two additional baselines: (i) we group all examples sharing the same class label and penalize with the conditional variance of the predicted logits, computed over these two groups; (ii) we penalize the overall variance of the predicted logits, i.e., a form of unconditional variance regularization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Eyeglasses detection with known and unknown image quality intervention</head><p>We now revisit the second example from Sect. 1.1. We again use the CelebA dataset and consider the problem of classifying whether the person in the image is wearing eyeglasses.</p><p>Here, we modify the images in the following way: in the training set and in test set 1, we sample the image quality<ref type="foot" target="#foot_11">foot_11</ref> for all samples {i ‚à∂ y i = 1} (all samples that show glasses) from We compare six different datasets that vary with respect to the distribution of Y in the grouped observations. Specifically, we vary the proportion of images showing men between = 0.5 and = 1 . In all training datasets, the total number of observations is 16,982 and the total number of grouped observations is 500. Both the pooled estimator as well as the CoRe estimator perform better if the distribution of Y in the grouped observations is more balanced. The CoRe estimator improves the error rate of the pooled estimator by ‚âà 28 -39% on a relative scale. a Gaussian distribution with mean = 30 and standard deviation = 10 . Samples with y i = 0 (no glasses) are unmodified. In other words, if the image shows a person wearing glasses, the image quality tends to be lower. In test set 2, the quality is reduced in the same way for y i = 0 samples (no glasses), while images with y i = 1 are not changed. Figure <ref type="figure" target="#fig_9">9</ref> shows examples from the training set and test sets 1 and 2. For the CoRe penalty, we calculate the conditional variance across images that share the same ID if Y = 1 , that is across images that show the same person wearing glasses on all images. Observations with Y = 0 (not wearing glasses) are not grouped. Two examples are shown in the red box of Fig. <ref type="figure" target="#fig_9">9</ref>.</p><p>Here, we have c = 5000 grouped observations among a total sample size of n = 20,000.</p><p>Figure <ref type="figure" target="#fig_9">9</ref> shows misclassification rates for CoRe and the pooled estimator on test sets 1 and 2. The pooled estimator (only penalized with an 2 penalty) achieves low error rates of 2% on test set 1, but suffers from a 65% misclassification error on test set 2, as now the relation between Y and the implicit S variable (image quality) has been flipped. The CoRe estimator has a larger error of 13% on test set 1 as image quality as a feature is penalized by CoRe implicitly and the signal is less strong if image quality has been removed as a dimension. However, in test set 2 the performance of the CoRe estimator is 28% and improves substantially on the 65% error of the pooled estimator. The reason is again the same: the CoRe penalty ensures that image quality is not used as a feature to the same extent as for the pooled estimator. This increases the test error slightly if the samples are generated from the same distribution as training data (as here for test set 1) but substantially improves the test error if the distribution of image quality, conditional on the class label, is changed on test data (as here for test set 2).</p><p>Eyeglasses detection with known image quality intervention To compare to the above results, we repeat the experiment by changing the grouped observations as follows. Above, we grouped images that had the same person ID when Y = 1 . We refer to this scheme of grouping observations with the same (Y, ID) as 'Grouping setting 2'. Here, we use an explicit augmentation scheme and augment c = 5000 images with Y = 1 in the following way: each image is paired with a copy of itself and the image quality is adjusted as described above. In other words, the only difference between the two images is that image quality differs slightly, depending on the value that was drawn from the Gaussian distribution with mean = 30 and standard deviation = 10 , determining the strength of the image quality intervention. Both the original and the copy get the same value of identifier variable ID . We call this grouping scheme 'Grouping setting 1'. Com- pare the left panels of Figs. 9 and 10 for examples.</p><p>While we used explicit changes in image quality in both above and here, we referred to grouping setting 2 as 'unknown image quality interventions' as the training sample as in the left panel of Fig. <ref type="figure" target="#fig_9">9</ref> does not immediately reveal that image quality is the important style variable. In contrast, the augmented data samples (grouping setting 1) we use here differ only in their image quality for a constant (Y, ID). Figure <ref type="figure" target="#fig_10">10</ref> shows examples and results. The pooled estimator performs more or less identical to the previous dataset. The explicit augmentation did not help as the association between image quality and whether eyeglasses are worn is not changed in the pooled data after including the augmented data samples. The misclassification error of the CoRe estimator is substantially better than the error rate of the pooled estimator. The error rate on test set 2 of 13% is also improving on the rate of 28% of the CoRe estimator in grouping setting 2. We see that using grouping setting 1 works best since we could explicitly control that only S ‚â° image quality varies between grouped examples. In grouping setting 2, dif- ferent images of the same person can vary in many factors, making it more challenging to isolate image quality as the factor to be invariant against.</p><p>A similar example where S ‚â° brightness is summarized in "Appendix D.1".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Stickmen image-based age classification with unknown movement interventions</head><p>In this example we consider synthetically generated stickmen images; see Fig. <ref type="figure" target="#fig_3">11</ref> for some examples. The target of interest is Y ‚àà {adult, child} . The core feature C is here the height of each person. The class Y is causal for height and height cannot be easily intervened on or change in different domains. Height is thus a robust predictor for differentiating between children and adults. As style feature we have here the movement of a person (distribution of angles between body, arms and legs). For the training data we created a dependence between age and the style feature 'movement', which can be thought to arise through a hidden common cause D , namely the place of observation. For instance, the images of children might mostly show children playing while the images of adults typically show them in more "static" postures. The left panel of Fig. <ref type="figure" target="#fig_3">11</ref> shows examples from the training set where large movements are associated with children and small movements are associated with adults. Test set 1 follows the same distribution, as shown in the middle panel. A standard CNN will exploit this relationship between movement and the label Y of interest, whereas this is discouraged by the conditional variance penalty of CoRe. The latter is pairing images of the same person in slightly different movements as shown by the red boxes in the leftmost panel of Fig. <ref type="figure" target="#fig_3">11</ref>. If the learned model exploits this dependence between movement and age for predicting Y, it will fail when presented images of, say, dancing adults. The right panel of Fig. <ref type="figure" target="#fig_3">11</ref> shows such examples (test set 2). The standard CNN suffers in this case from a 41% misclassification rate, as opposed to the 3% on test set 1 data. For as few as c = 50 paired observations, the network with an added CoRe penalty, in contrast, achieves also 4% on test set 1 data and succeeds in achieving an 9% performance on test set 2, whereas the pooled estimator fails on this dataset with a test error of 41%. These results suggest that the learned representation of the pooled estimator uses movement as a predictor for age while CoRe does not use this feature due to the conditional variance regularization. Importantly, including more grouped examples would not improve the performance of the pooled estimator as these would be subject to the same bias and hence also predominantly have examples of heavily moving children and "static" adults (also see Fig. <ref type="figure">23</ref> which shows results for c ‚àà {20, 500, 2000}).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">MNIST: more sample efficient data augmentation</head><p>The goal of using CoRe in this example is to make data augmentation more efficient in terms of the required samples. In data augmentation, one creates additional samples by modifying the original inputs, e.g. by rotating, translating, or flipping the images <ref type="bibr" target="#b53">(Sch√∂lkopf et al. 1996)</ref>. In other words, additional samples are generated by interventions on style features. Using this augmented data set for training results in invariance of the estimator with respect to the transformations (style features) of interest. For CoRe we can use the grouping information that the original and the augmented samples belong to the same object. This enforces the invariance with respect to the style features more strongly compared to normal data augmentation which just pools all samples. We assess this for the style feature 'rotation' on <ref type="bibr">MNIST (LeCun et al. 1998</ref>) and only include c = 200 augmented training examples for m = 10,000 original samples, resulting in a total sample size of n = 10200 . The degree of the rotations is sampled uniformly at random from <ref type="bibr">[35,</ref><ref type="bibr">70]</ref>. Figure <ref type="figure" target="#fig_1">12</ref> shows examples from the training set. By using CoRe the average test error on rotated examples is reduced from 22% to 10%. Very few augmented sample are thus sufficient to lead to stronger rotational invariance. The standard approach of creating augmented data and pooling all images requires, in contrast, many more samples to achieve the same effect. Additional results for m ‚àà {1000, 10,000} and c ranging from 100 to 5000 can be found in Fig. <ref type="figure">22</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Elmer the Elephant</head><p>In this example, we want to assess whether invariance with respect to the style feature 'color' can be achieved. In the children's book 'Elmer the elephant'<ref type="foot" target="#foot_12">foot_12</ref> one instance of a colored elephant suffices to recognize it as being an elephant, making the color 'gray' no longer an integral part of the object 'elephant'. Motivated by this process of concept formation, we would like to assess whether CoRe can exclude 'color' from its learned representation by penalizing conditional variance appropriately. We work with the 'Animals with attributes 2' (AwA2) dataset <ref type="bibr" target="#b65">(Xian et al. 2017</ref>) and consider classifying images of horses and elephants. We include additional examples by adding grayscale images for c = 250 images of elephants. These additional examples do not distinguish themselves strongly from the original training data as the elephant images are already close to grayscale images. The total training sample size is 1850.</p><p>Figure <ref type="figure" target="#fig_3">13</ref> shows examples and misclassification rates from the training set and test sets for CoRe and the pooled estimator on different test sets. Examples from these and more test sets can be found in Fig. <ref type="figure">24</ref>. Test set 1 contains original, colored images only. In test set 2 images of horses are in grayscale and the colorspace of elephant images is modified, effectively changing the color gray to red-brown. We observe that the pooled estimator does not perform well on test set 2 as its learned representation seems to exploit the fact that 'gray' is predictive for 'elephant' in the training set. This association is no longer valid for test set 2. In contrast, the predictive performance of CoRe is hardly affected by the changing color distributions. More details can be found in "Appendix D.7".</p><p>It is noteworthy that a colored elephant can be recognized as an elephant by adding a few examples of a grayscale elephant to the very lightly colored pictures of natural elephants. If we just pool over these examples, there is still a strong bias that elephants are gray. The CoRe estimator, in contrast, demands invariance of the prediction for instances of the same elephant and we can learn color invariance with a few added grayscale images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Further related work</head><p>Encoding certain invariances in estimators is a well-studied area in computer vision and machine learning with an extensive body of literature. While a large part of this work assumes the desired invariance to be known, fewer approaches aim to learn the required invariances from data and the focus often lies on geometric transformations of the input data or explicitly creating augmented observations <ref type="bibr" target="#b57">(Sohn and Lee 2012;</ref><ref type="bibr" target="#b32">Khasanova and Frossard 2017;</ref><ref type="bibr" target="#b27">Hashimoto et al. 2017;</ref><ref type="bibr" target="#b17">Devries and Taylor 2017)</ref>. The main difference between this line of work and CoRe is that we do not require to know the style feature explicitly, the set of possible style features restricted to a particular class of transformations and we do not aim to create augmented observations in a generative framework.</p><p>Recently, various approaches have been proposed that leverage causal motivations for deep learning or use deep learning for causal inference, related to e.g. the problems of cause-effect inference and generative adversarial networks <ref type="bibr" target="#b12">(Chalupka et al. 2014;</ref><ref type="bibr">Lopez-Paz et al. 2017;</ref><ref type="bibr" target="#b41">Lopez-Paz and Oquab 2017;</ref><ref type="bibr" target="#b25">Goudet et al. 2017;</ref><ref type="bibr" target="#b3">Bahadori et al. 2017;</ref><ref type="bibr" target="#b9">Besserve et al. 2018;</ref><ref type="bibr" target="#b35">Kocaoglu et al. 2018)</ref>. <ref type="bibr" target="#b33">Kilbertus et al. (2017)</ref> exploit causal reasoning to characterize fairness considerations in machine learning. Distinguishing between the protected attribute and its proxies, they derive causal non-discrimination criteria. The resulting algorithms avoiding proxy discrimination require classifiers to be constant as a function of the proxy variables in the causal graph, thereby bearing some structural similarity to our style features.</p><p>Distinguishing between core and style features can be seen as some form of disentangling factors of variation. Estimating disentangled factors of variation has gathered a lot of interested in the context of generative modeling. As in CoRe, <ref type="bibr" target="#b11">Bouchacourt et al. (2018)</ref> exploit grouped observations. In a variational autoencoder framework, they aim to separate style and content-they assume that samples within a group share a common but unknown value for one of the factors of variation while the style can differ. <ref type="bibr" target="#b16">Denton and Birodkar (2017)</ref> propose an autoencoder framework to disentangle style and content in videos using an adversarial loss term where the grouping structure induced by clip identity is exploited. Here we try to solve a classification task directly without estimating the latent factors explicitly as in a generative framework.</p><p>In the computer vision literature, various works have used identity information to achieve pose invariance in the context of face recognition <ref type="bibr" target="#b5">(Bartlett and Sejnowski 1996;</ref><ref type="bibr" target="#b61">Tran et al. 2017)</ref>. More generally, the idea of exploiting various observations of the same underlying object is related to multi-view learning <ref type="bibr" target="#b67">(Xu et al. 2013</ref>). In the context of adversarial examples, <ref type="bibr" target="#b31">Kannan et al. (2018)</ref> recently proposed the defense "Adversarial logit pairing" which is methodologically equivalent to the CoRe penalty C f ,1, when using the squared error loss. Several empirical studies have shown mixed results regarding the performance on ‚àû perturbations <ref type="bibr" target="#b19">(Engstrom et al. 2018;</ref><ref type="bibr" target="#b45">Mosbach et al. 2018</ref>), so far this setting has not been analyzed theoretically and hence it is an open question whether a CoRetype penalty constitutes an effective defense against adversarial examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>Distinguishing the latent features in an image into CoRe and style features, we have proposed conditional variance regularization (CoRe) to achieve robustness with respect to interventions on the style or "orthogonal" features. The main idea of the CoRe estimator is to exploit the fact that we often have instances of the same object in the training data. By demanding invariance of the classifier amongst a group of instances that relate to the same object, we can achieve invariance of the classification performance with respect to interventions on style features such as image quality, fashion type, color, or body posture. The training also works despite sampling biases in the data.</p><p>There are two main application areas:</p><p>1. If the style features are known explicitly, we can achieve the same classification performance as standard data augmentation approaches with substantially fewer augmented samples, as shown for example in Sect. 5.5. 2. Perhaps more interesting are settings in which it is unknown what the style features are, with examples in Sects. 5.1, 5.2, 5.3, 5.4 and "Appendix D.1". CoRe regularization forces predictions to be based on features that do not vary strongly between instances of the same object. We could show in the examples and in Theorems 1 and 2 that this regularization achieves distributional robustness with respect to changes in the distribution of the (unknown) style variables.</p><p>An interesting line of work would be to use larger models such as Inception or large ResNet architectures <ref type="bibr" target="#b58">(Szegedy et al. 2015;</ref><ref type="bibr" target="#b29">He et al. 2016</ref>). These models have been trained to be invariant to an array of explicitly defined style features. In Sect. 5.2.2 we include results which show that using Inception V3 features does not guard against interventions on more implicit style features. We would thus like to assess what benefits CoRe can bring for training Inception-style models end-to-end, both in terms of sample efficiency and in terms of generalization performance. While we showed some examples where the necessary grouping information is available, an interesting possible future direction would be to use video data since objects display temporal constancy and the temporal information can hence be used for grouping and conditional variance regularization. Beyond that our results show that it can be worthwhile to collect ID information when new datasets are created. As CoRe only requires a subset of the observa- tions to have ID annotations, in many cases this information might be cheap to collect while it can improve performance substantially when future test data is subject to domain shifts. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proof of Theorem 1</head><p>First part To show the first part, namely that with probability 1, we need to show that W t Œ∏pool ‚â† 0 with probability 1. The reason this is sufficient is as follows: if W t ‚â† 0 , then L ‚àû ( ) = ‚àû as we can then find a v ‚àà ‚Ñù q such that ‚à∂= t Wv ‚â† 0 . Assume without limitation of generality that v is normed such that</p><formula xml:id="formula_14">E(E(v t Œ£ -1 y,id v|Y = y, ID = id)) = 1 . Setting = v for ‚àà ‚Ñù , we have that (ID, Y, S + ) is in the class F | | if the distribution of (ID, Y, S) is equal to F 0 . Furthermore, x( ) t = x( = 0) t + . Hence log(1 + exp(-y ‚ãÖ x( ) t )) ‚Üí ‚àû for either ‚Üí ‚àû or ‚Üí -‚àû.</formula><p>To show that W t Œ∏pool ‚â† 0 with probability 1, let Œ∏ * be the oracle estimator that is con- strained to be orthogonal to the column space of W:</p><p>We show W t Œ∏pool ‚â† 0 by contradiction. Assume hence that W t Œ∏pool = 0 . If this is indeed the case, then the constraint W t = 0 in ( <ref type="formula">14</ref>) becomes non-active and we have Œ∏pool = Œ∏ * . This would imply that taking the directional derivative of the training loss with respect to any ‚àà ‚Ñù p in the column space of W should vanish at the solution Œ∏ * . In other words, define the gradient as g( ) = ‚àá L n ( ) ‚àà ‚Ñù p . The implication is then that for all in the columnspace of W, and we will show the latter condition is violated almost surely.</p><p>As we work with the logistic loss and Y ‚àà {-1, 1} , the loss is given by (y i , f (x i )) = log(1 + exp(-y i x t i )). Define r i ( ) ‚à∂= y i ‚àï(1 + exp(y i x t i )) . For all i = 1, ‚Ä¶ , n we have r i ‚â† 0 . Then</p><p>The training images can be written according to the model as x i = x 0 i + Ws i , where X 0 ‚à∂= k x (C, X ) are the images in absence of any style variation. Since the style features only have an effect on the column space of W in X, the oracle estimator Œ∏ * is identical under the true training data and the (hypothetical) training data x 0 i , i = 1, ‚Ä¶ , n in absence of style variation. As X -X 0 = WS , Eq. ( <ref type="formula">16</ref>) can also be written as Since is in the column-space of W , there exists u ‚àà ‚Ñù q such that = Wu and we can write (17) as</p><formula xml:id="formula_15">L ‚àû ( Œ∏pool ) = ‚àû, (14) Œ∏ * = argmin ùúÉ‚à∂W t ùúÉ=0 L n (ùúÉ) with L n (ùúÉ) ‚à∂= 1 n n ‚àë i=1 (y i , f ùúÉ (x i )). (<label>15</label></formula><formula xml:id="formula_16">) ùõø t g( Œ∏ * ) = 0 (16) g( Œ∏ * ) = 1 n n ‚àë i=1 r i ( Œ∏ * )x i . (17) ùõø t g( Œ∏ * ) = 1 n n ‚àë i=1 r i ( Œ∏ * )(x 0 i ) t ùõø + 1 n n ‚àë i=1 r i ( Œ∏ * )(s i ) t W t ùõø. (<label>18</label></formula><formula xml:id="formula_17">) ùõø t g( Œ∏ * ) = 1 n n ‚àë i=1 r i ( Œ∏ * )(x 0 i ) t Wu + 1 n n ‚àë i=1 r i ( Œ∏ * )(s i ) t W t Wu.</formula><p>From (A2) we have that the eigenvalues of W t W are all positive. Also r i ( Œ∏ * ) is not a function of the interventions s i , i = 1, ‚Ä¶ , n since, as above, the estimator Œ∏ * is identical whether trained on the original data x i or on the intervention-free data x 0 i , i = 1, ‚Ä¶ , n . If we con- dition on everything except for the random interventions by conditioning on (x 0 i , y i ) for i = 1, ‚Ä¶ , n , then the rhs of ( <ref type="formula" target="#formula_16">18</ref>) can be written as where a ‚àà ‚Ñù q is fixed (conditionally) and B = 1 n ‚àë n i=1 r i ( Œ∏ * )(s i ) t W t W ‚àà ‚Ñù q is a random vector and B ‚â† -a ‚àà ‚Ñù q with probability 1 by (A1) and (A2) Hence the left hand side of ( <ref type="formula" target="#formula_16">18</ref>) is not identically 0 with probability 1 for any given in the column-space of W. This shows that the implication ( <ref type="formula" target="#formula_15">15</ref>) is incorrect with probability 1 and hence completes the proof of the first Invariant parameter space Before continuing with the second part of the proof, some definitions. Let I be the invariant parameter space For all ‚àà I , the loss (6) for any F ‚àà F is identical to the loss under F 0 . That is for all ‚â• 0,</p><p>The optimal predictor in the invariant space I is If f is only a function of the core features C , then ‚àà I . The challenge is that the core fea- tures are not directly observable and we have to infer the invariant space I from data.</p><p>Second part For the second part, we first show that with probability at least p n , as defined in (A3), Œ∏core = Œ∏ * with Œ∏ * defined as in ( <ref type="formula">14</ref>). The invariant space for this model is the linear subspace I = { ‚à∂ W t = 0} and by their respective definitions, Since we use</p><formula xml:id="formula_18">I n = I n ( ) with = 0, This implies that for ‚àà I n , f (x i ) = f (x i ÔøΩ ) if i, i ÔøΩ ‚àà G j for some j ‚àà {1, ‚Ä¶ , m}. 14 Since f (x) = f (x ÔøΩ ) implies (x -x ÔøΩ ) t = 0 , it follows that (x i -x i ÔøΩ ) t = 0 if i, i ÔøΩ ‚àà G j</formula><p>for some j ‚àà {1, ‚Ä¶ , m} and hence a t u + B t u,</p><formula xml:id="formula_19">I ‚à∂= { ‚à∂ f (x( )) is constant as function of ‚àà ‚Ñù q for all x ‚àà ‚Ñù p }. if ‚àà I, then sup F‚ààF E F Y, f X = E F 0 Y, f X . (19) * = argmin E F 0 (Y, f (X)) such that ‚àà I. Œ∏ * = argmin ùúÉ 1 n n ‚àë i=1 (y i , f ùúÉ (x i )) such that ùúÉ ‚àà I, Œ∏core = argmin ùúÉ 1 n n ‚àë i=1 (y i , f ùúÉ (x i )) such that ùúÉ ‚àà I n . I n = ùúÉ ‚à∂ √ä( V ar(f ùúÉ (X)|Y, ID)) = 0 .</formula><p>14 Recall that (y i , id i ) = (y i ÔøΩ , id i ÔøΩ ) if i, i ÔøΩ ‚àà G j as the subsets G j , j = 1, ‚Ä¶ , m , collect all observations that have a unique realization of (Y, ID)</p><p>Since S has a linear influence on X in (11), x ix i ÔøΩ = W( ii ÔøΩ ) if i, i ‚Ä≤ are in the same group G j of observations for some j ‚àà {1, ‚Ä¶ , m} . Note that the number of grouped examples nm is equal to or exceeds the rank q of W with probability p n , using (A3), and p n ‚Üí 1 for n ‚Üí ‚àû . By (A2), it follows then with probability at least p n that I n ‚äÜ {ùúÉ ‚à∂ W t ùúÉ = 0} = I . As, by definition, I ‚äÜ I n is always true, we have with p n that I = I n . Hence, with probability p n (and p n ‚Üí 1 for n ‚Üí ‚àû ), Œ∏core = Œ∏ * . It thus remains to be shown that Since Œ∏ * is in I, we have (y, x( )) = (y, x 0 ) , where x 0 are the previously defined data in absence of any style variance. Hence that is the estimator is unchanged if we use the (hypothetical) data x i , i = 1, ‚Ä¶ , n as train- ing data. The population optimal parameter vector defined in (19) as is for all ‚â• 0 identical to Hence ( <ref type="formula">21</ref>) and ( <ref type="formula">22</ref>) can be written as By uniform convergence of L (0)  n to the population loss L (0) , we have L (0) ( Œ∏ * ) ‚Üí p L (0) (ùúÉ * ) . By definition of I and * , we have</p><formula xml:id="formula_20">L * ‚àû = L ‚àû ( * ) = L (0) ( * ) . As Œ∏ * is in I, we also have L ‚àû ( Œ∏ * ) = L (0) ( Œ∏ * ) . Since, from above, L (0) ( Œ∏ * ) ‚Üí p L (0) (ùúÉ * ) , this also implies L ‚àû ( Œ∏ * ) ‚Üí p L ‚àû (ùúÉ * ) = L * ‚àû .</formula><p>Using the previously established result that Œ∏core = Œ∏ * with prob- ability at least p n and p n ‚Üí 1 for n ‚Üí ‚àû , this completes the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proof of Theorem 2</head><p>Let F 0 be the training distribution of (ID, Y, S) and F a distribution for (ID, Y, S) in F . By definition of F , we can write S = S + ùõ• for a suitable random variable ‚àà ‚Ñù q with Vice versa: if we can write S = S + ùõ• with ‚àà U , then the distribution is in F . While X under F 0 can be written as X( = 0) , the distribution of X under F is of the form X( )</p><formula xml:id="formula_21">I n ‚äÜ ùúÉ ‚à∂ (x i -x i ÔøΩ ) t ùúÉ = 0 if i, i ÔøΩ ‚àà G j for some j ‚àà {1, ‚Ä¶ , m} . (20) L ‚àû ( Œ∏ * ) ‚Üí p inf ùúÉ L ‚àû (ùúÉ). (21) Œ∏ * = argmin ùúÉ 1 n n ‚àë i=1 (y i , f ùúÉ (x 0 i )) such that ùúÉ ‚àà I, (22) * = argmin E F 0 (Y, f (X)) such that ‚àà I. argmin sup F‚ààF E F (Y, f (X)) such that ‚àà I. Œ∏ * = argmin ùúÉ‚à∂ùúÉ‚ààI L (0) n (ùúÉ) with L (0) n (ùúÉ) ‚à∂= 1 n n ‚àë i=1 (y i , f ùúÉ (x 0 i )) ùúÉ * = argmin ùúÉ‚à∂ùúÉ‚ààI L (0) (ùúÉ) with L (0) (ùúÉ) ‚à∂= E[ (Y, f ùúÉ (X 0 ))]. ‚àà U , where U = { ‚à∂ E(E( t Œ£ -1 Y,ID |Y, ID)) ‚â§ }.</formula><p>or, alternatively, X( ‚àö U) with U ‚àà U 1 . Adopting from now on the latter constraint that U ‚àà U 1 , and using (B2), where ‚àáh is the gradient of h ( ) with respect to , evaluated at ‚â° 0 . Hence The proof is complete if we can show that On the one hand, This follows for a matrix Œ£ with Cholesky decomposition Œ£ = V t V, On the other hand, the conditional-variance-of-loss can be expanded as which completes the proof. </p><formula xml:id="formula_22">E F ÔøΩ ÔøΩ Y, f (X) ÔøΩ = E F 0 ÔøΩ h (0) ÔøΩ + ‚àö E F 0 ÔøΩ (‚àáh ) t U ÔøΩ + o( ), sup F‚ààF E F ÔøΩ h ) ÔøΩ = E F 0 ÔøΩ h (0) ÔøΩ + ‚àö sup U‚ààU 1 E F 0 ÔøΩ (‚àáh ) t U ÔøΩ + o( ). C ,1‚àï2, = sup U‚ààU 1 E F 0 (‚àáh ) t U + O( ). sup U‚ààU 1 E F 0 (‚àáh ) t U = E F 0 ‚àö (‚àáh ) t Œ£ Y,ID (‚àáh ) . max u‚à∂u t Œ£ -1 u‚â§1 (‚àáh ) t u = max w‚à∂‚Äñw‚Äñ 2 2 ‚â§1 (‚àáh ) t V t w = ‚ÄñV(‚àáh)‚Äñ 2 = ‚àö (‚àáh) t Œ£(‚àáh). C ,1‚àï2, = E F 0 ÔøΩ‚àö Var( (Y, f (X))ÔøΩY, ID) ÔøΩ = E F 0 ÔøΩÔøΩ (‚àáh ) t Œ£ Y,ID (‚àáh ) ÔøΩ + O( ),</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Network architectures</head><p>We implemented the considered models in TensorFlow <ref type="bibr" target="#b0">(Abadi et al. 2015)</ref>. The model architectures used are detailed in Table <ref type="table" target="#tab_4">5</ref>. CoReCoRe and the pooled estimator use the same network architecture and training procedure; merely the loss function differs by the CoRe regularization term. In all experiments we use the Adam optimizer <ref type="bibr" target="#b34">(Kingma and Ba 2015)</ref>. All experimental results are based on training the respective model five times (using the same data) to assess the variance due to the randomness in the training procedure. In each epoch of the training, the training data x i , i = 1, ‚Ä¶ , n are randomly shuffled, keeping the grouped observations (x i ) i‚ààI j for j ‚àà {1, ‚Ä¶ , together to ensure that mini batches will contain grouped observations. In all experiments the mini batch size is set to 120. For small c this implies that not all mini batches contain grouped observations, making the optimiza- tion more challenging.  whether the image was taken outdoors or indoors. If it was taken outdoors, then the person tends to wear (sun-)glasses more often and the image tends to be brighter. If the image was taken indoors, then the person tends not to wear (sun-)glasses and the image tends to be darker. In other words, the style variable S here equivalent to brightness and the structure of the data generating process is equivalent to the one shown in Fig. <ref type="figure">3</ref>. Figure <ref type="figure" target="#fig_12">14</ref> shows examples from the training set and test sets. As previously, we compute the conditional variance over images of the same person, sharing the same class label (and the CoRe estimator is hence not using the knowledge that brightness is important). Two alternatives for constructing grouped observations in this setting are discussed further below. We use c = 2000 and n = 20,000 . For the brightness intervention, we sample the value for the magnitude of the brightness increase resp. decrease from an exponential distribution with mean = 20 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Additional experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Eyeglasses detection: known and unknown brightness interventions</head><p>In the training set and test set 1, we sample the brightness value as b i,j = [100 + y i e i,j ] + where e i,j ‚àº Exp( -1 ) and y i ‚àà {-1, 1} , where y i = 1 indicates presence of glasses and y i = -1 indicates absence. <ref type="foot" target="#foot_13">15</ref> For test set 2, we use instead b i,j = [100y i e i,j ] + , so that the relation between brightness and glasses is flipped.</p><p>Figure <ref type="figure" target="#fig_12">14</ref> shows misclassification rates for CoRe and the pooled estimator on different test sets. Examples from all test sets can be found in Fig. <ref type="figure" target="#fig_5">15</ref>. First, we notice that the pooled estimator performs better than CoRe on test set 1. This can be explained by the fact that it can exploit the predictive information contained in the brightness of an image while CoRe is restricted not to do so. Second, we observe that the pooled estimator does not perform well on test set 2 as its learned representation seems to use the image's brightness as a predictor for the response which fails when the brightness distribution in the test set differs significantly from the training set. In contrast, the predictive performance of CoRe is hardly affected by the changing brightness distributions.</p><p>We now discuss two alternatives for constructing different test sets and we vary the number of grouped observations in c ‚àà {200, 2000, 5000} as well as the strength of the brightness interventions in ‚àà {5, 10, 20} , all with sample size n = 20,000 . Generation of training and test sets 1 and 2 were already described above. Here, we consider additionally test set 3 where all images are left unchanged (no brightness interventions at all) and in test set 4 the brightness of all images is increased. Furthermore, we consider three different ways of grouping images. Above, we used images of the same person to create a grouped observation by sampling a different value for the brightness intervention. We refer to this as 'Grouping setting 2' here. An alternative is to use the same image of the same person in different brightnesses (drawn from the same distribution) as a group over which the conditional variance is calculated. We call this 'Grouping setting 1' and it can be useful if we know that we want to protect against brightness interventions in the future. For comparison, we also evaluate grouping with an image of a different person (but sharing the same class label) as a baseline ('Grouping setting 3'). Examples from the training sets using grouping settings 1, 2 and 3 can be found in Fig. <ref type="figure" target="#fig_5">15</ref>.</p><p>Results for all grouping settings, ‚àà {5, 10, 20} and c ‚àà {200, 5000} can be found in Fig. <ref type="figure" target="#fig_14">16</ref>. We see that using grouping setting 1 works best since we could explicitly control that only S ‚â° brightness varies between grouping examples. In grouping setting 2, different images of the same person can vary in many factors, making it more challenging to isolate brightness as the factor to be invariant against. Lastly, we see that if we group images of different persons ('Grouping setting 3'), the difference between CoRe estimator and the pooled estimator becomes much smaller than in the previous settings. Figure <ref type="figure" target="#fig_7">17</ref> shows some examples of misclassified observations for Grouping setting 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Eyeglasses detection with small sample size</head><p>Figure <ref type="figure" target="#fig_8">18</ref> shows the numerator and the denominator of the variance ratio defined in Eq. ( <ref type="formula">13</ref>) separately as a function the CoRe penalty weight. In conjunction with Fig. <ref type="figure" target="#fig_6">6b</ref>, we observe that a ridge penalty decreases both the within-and between-group variance while the CoRe penalty penalizes the within-group variance selectively. 1 3</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Gender classification</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Additional baselines: Unconditional variance regularization and grouping by class label</head><p>As additional baselines, we consider the following two schemes: (i) we group all examples sharing the same class label and penalize with the conditional variance of the Panel a shows the numerator of the variance ratio defined in Eq. ( <ref type="formula">13</ref>) on test data as a function of both the CoRe and ridge penalty weights. Panel b shows the equivalent plot for the denominator. A ridge penalty decreases both the within-and between-group variance while the CoRe penalty penalizes the within-group variance selectively (the latter can be seen more clearly in Fig. <ref type="figure" target="#fig_6">6b</ref> predicted logits, computed over these two groups; (ii) we penalize the overall variance of the predicted logits, i.e., a form of unconditional variance regularization. Figure <ref type="figure" target="#fig_9">19</ref> shows the performance of these two approaches. In contrast to the CoRe penalty, regularizing with the variance of the predicted logits conditional on Y only does not yield performance improvements on test set 2, compared to the pooled estimator (corresponding to a penalty weight of 0). Interestingly, using baseline (i) without a ridge penalty does yield an improvement on test set I, compared to the pooled estimator with various strengths of the ridge penalty.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Additional results</head><p>Table <ref type="table" target="#tab_7">6</ref> additionally reports the standard errors for the results discussed in Sect. 5.2.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Eyeglasses detection: image quality intervention</head><p>Here, we show further results for the experiments introduced in Sect. 5.3. Specifically, we consider interventions of different strengths by varying the mean of the quality intervention in ‚àà {30, 40, 50} . Recall that we use ImageMagick to modify the image quality. In the training set and in test set 1, we sample the image quality value as q i,j ‚àº N( , = 10) and apply the command convert -quality q_ij input. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Stickmen image-based age classification</head><p>Here, we show further results for the experiment introduced in Sect. 5.4. Recall that test set 1 follows the same distribution as the training set. In test sets 2 and 3 large movements are associated with both children and adults, while the movements are heavier in test set 3 than in test set 2. The pooled estimator fails to achieve good predictive performance on test sets 2 and 3 as it seems to use "movement" as a predictor for "age" (Fig. <ref type="figure">23</ref>). Elmer the Elephant</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>: in the training set images of class "wearing glasses" are associated with a lower image quality than images of class "not wearing glasses". Examples are shown in Fig. 2a. In the test set, this relation is reversed, i.e. images showing persons wearing glasses are of higher quality than images of persons without glasses, with examples in Fig. 2b. We will return to this example in Sect. 5.3 and show that training a convolutional (a) Example 1, training set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 Fig. 2</head><label>12</label><figDesc>Fig. 1 Motivating example 1: The distributions are shifted in the test data by style interventions where style is the polar angle. Standard estimators achieve error rates of 0% on the training data and test data drawn from the same distribution as the training data (panel a). On the shown test set where the distribution of the style conditional on Y has changed the error rates are &gt; 50% (panel b)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>3.3 and return to the simple examples of Sect. 1.1 in Sect. 3.4. Domain shift robustness in a classification setting for a partially linear version of the structural equation model (</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Theorem 1 (</head><label>1</label><figDesc>Asymptotic domain shift robustness under strong interventions) Under model (11) and Assumption 1, with probability 1, the pooled estimator (7) has infinite loss (6) under arbitrarily large shifts in the distribution of the style features, The CoRe estimator (8) Œ∏core with ‚Üí ‚àû is domain shift robust under strong interventions in the sense that for n ‚Üí ‚àû,(11)    </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>3, "Appendix D.1", D.3.1, D.4); (vii) how sensitive the performance is as a function of the strength of the domain shift and the number of grouped observations ("Appendices D.1, D.4, D.5, D.6").</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5</head><label>5</label><figDesc>Fig. 5 Eyeglass detection for CelebA dataset with small sample size. The goal is to predict whether a person wears glasses or not. Random samples from training and test data are shown. Groups of observations in the training data that have common (Y, ID) here correspond to pictures of the same person with either glasses on or off. These are labelled by red boxes in the training data and the conditional variance penalty is calculated across these groups of pictures</figDesc><graphic coords="17,49.63,57.72,340.10,102.62" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6</head><label>6</label><figDesc>Fig.6Eyeglass detection, trained on a small subset (DS1) of the CelebA dataset with disjoint identities. a Average test error as a function of both the CoRe penalty on x-axis and various levels of the ridge penalty. The results can be seen to be fairly insensitive to the ridge penalty. b The variance ratio (13) on test data as a function of both the CoRe and ridge penalty weights. The CoRe penalty can be seen to penalize the within-group variance selectively, whereas a strong ridge penalty decreases both the within-and betweengroup variance</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 7</head><label>7</label><figDesc>Fig. 7 Classification for Y ‚àà {woman, man} . There is an unknown confounding here as men are very likely to wear glasses in training and test set 1 data, while it is women that are likely to wear glasses in test set 2. Estimators that pool all observations are making use of this confounding and hence fail for test set 2. The conditional variance penalty for the CoRe estimator is computed over groups of images of the same person (and consequently same class label), such as the images in the red box on the left. The number of grouped examples c is 500. We vary the proportion of males in the grouped examples between 50 and 100% (cf. Sect. 5.2.1)</figDesc><graphic coords="21,49.57,57.72,340.22,92.18" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 8</head><label>8</label><figDesc>Fig. 8 Classification for Y ‚àà {woman, man} with = 0.5 . Panels a and b show the test error on test data sets 1 and 2 respectively as a function of the CoRe and ridge penalty. Panels c and d show the variance ratio (13) (comparing within-and between-group variances) for females and males separately</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 9</head><label>9</label><figDesc>Fig. 9 Eyeglass detection for CelebA dataset with image quality interventions (which are unknown to any procedure used). The JPEG compression level is lowered for Y = 1 (glasses) samples on training data and test set 1 and lowered for Y = 0 (no glasses) samples for test set 2. To the human eye, these interventions are barely visible but the CNN that uses pooled data without CoRe penalty has exploited the correlation between image quality and outcome Y to achieve a (arguably spurious) low test error of 2% on test set 1. However, if the correlation between image quality and Y breaks down, as in test set 2, the CNN that uses pooled data without a CoRe penalty has a 65% misclassification rate. The training data on the left show paired observations in two red boxes: these observations share the same label Y and show the same person ID . They are used to compute the conditional variance penalty for the CoRe estimator that does not suffer from the same degradation in performance for test set 2</figDesc><graphic coords="24,52.83,99.82,334.46,79.34" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 10</head><label>10</label><figDesc>Fig. 10 Eyeglass detection for CelebA dataset with image quality interventions. The only difference to Fig. 9 is in the training data where the paired images now use the same underlying image in two different JPEG compressions.The compression level is drawn from the same distribution. The CoRe penalty performs better than for the experiment in Fig.9since we could explicitly control that only S ‚â° image quality varies between grouped examples. On the other hand, the performance of the pooled estimator is not changed in a noticeable way if we add augmented images as the (spurious) correlation between image quality and outcome Y still persists in the presence of the extra augmented images. Thus, the pooled estimator continues to be susceptible to image quality interventions</figDesc><graphic coords="25,52.47,99.57,334.46,79.58" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 12</head><label>12</label><figDesc>Fig. 12 Data augmentation for MNIST images. The left shows training data with a few rotated images. Evaluating on only rotated images from the test set, a standard network achieves only 22% accuracy. We can add the CoRe penalty by computing the conditional variance over images that were generated from the same original image. The test error is then lowered to 10% on the test data of rotated images</figDesc><graphic coords="27,81.30,98.47,277.34,73.10" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 14</head><label>14</label><figDesc>Fig. 14 Eyeglass detection for CelebA dataset with brightness interventions (which are unknown to any procedure used). On training data and test set 1 data, images where people wear glasses tend to be brighter whereas on test set 2 images where people do not wear glasses tend to be brighter</figDesc><graphic coords="34,51.99,99.32,335.42,79.10" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>( a )</head><label>a</label><figDesc>Fig. 15 Examples from the CelebA eyeglasses detection with brightness interventions, grouping settings 1-3 with ‚àà {5, 10, 20} . In all rows, the first three images from the left have y ‚â° no glasses ; the remaining three images have y ‚â° glasses . Connected images are grouped examples. In panels a-c, row 1 shows examples from the training set, rows 2-4 contain examples from test sets 2-4, respectively. Panels d-i show examples from the respective training sets</figDesc><graphic coords="35,49.59,73.20,338.66,190.22" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Fig. 16</head><label>16</label><figDesc>Fig. 16 Misclassification rates for the CelebA eyeglasses detection with brightness interventions, grouping settings 1-3 with c ‚àà {200, 2000, 5000} and the mean of the exponential distribution ‚àà {5, 10, 20}</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Fig. 17 Fig. 18</head><label>1718</label><figDesc>Fig. 17 CelebA eyeglasses detection with brightness interventions, grouping setting 1. Examples of misclassified observations from the test sets</figDesc><graphic coords="38,104.07,122.46,243.26,83.06" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>( a )</head><label>a</label><figDesc>Fig. 19Classification for Y ‚àà {woman, man} with = 0.5 , using the baselines which (i) penalize the variance of the predicted logits conditional on the class label Y only; and (ii) penalize the overall variance of the predicted logits (cf. Sect. D.3.1). For baseline (i), panels (a) and (b) show the test error on test data sets 1 and 2 respectively as a function of the "baseline penalty weight" for various ridge penalty strengths. For baseline (ii), the equivalent plots are shown in panels (c) and (d). In contrast to the CoRe penalty, regularizing with these two baselines does not yield performance improvements on test set 2, compared to the pooled estimator (corresponding to a penalty weight of 0)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>( a )Fig. 21 Fig. 22 Fig. 23 a</head><label>a212223</label><figDesc>Fig. 20 Examples from the CelebA image quality datasets, grouping settings 1-3 with ‚àà {30, 40, 50} . In all rows, the first three images from the left have y ‚â° no glasses ; the remaining three images have y ‚â° glasses . Connected images are grouped observations over which we calculate the conditional variance. In panels a-c, row 1 shows examples from the training set, rows 2-4 contain examples from test sets 2-4, respectively. Panels d-i show examples from the respective training sets</figDesc><graphic coords="41,51.51,73.16,336.86,189.02" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head></head><label></label><figDesc>Figure D.10b shows results for different numbers of grouping examples. For c = 20 the misclassification rate of CoRe estimator has a large variance. For c ‚àà {50, 500, 2000} , the CoRe estimator shows similar results. Its performance is thus not sensitive to the number of grouped examples, once there are sufficiently many grouped observations in the training set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Fig. 24 Fig. 25</head><label>2425</label><figDesc>Fig. 24 Examples from the subsampled and augmented AwA2 dataset (Elmer-the-Elephant dataset). Row 1 shows examples from the training set, rows 2-5 show examples from test sets 1-4, respectively</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc>Eyeglass detection, trained on small subsets (DS1-DS5) of the CelebA dataset with disjoint identities For each experimental setting, bold marks the best performing methodWe report training and test error as well as the value of the CoRe penalty ƒàf ,1,ùúÉ on the training and the test set after training, evaluated for both the pooled estimator ("5-layer CNN") and the CoRe estimator, applied to training the same architecture ("+ CoRe "). The weights of the ridge and the CoRe penalty were chosen based on their performance on the validation set</figDesc><table><row><cell></cell><cell>Method</cell><cell>Error</cell><cell></cell><cell>Penalty value</cell><cell></cell></row><row><cell></cell><cell></cell><cell>Training</cell><cell>Test</cell><cell>Training</cell><cell>Test</cell></row><row><cell>DS 1</cell><cell>5-layer CNN</cell><cell>0.0% (0.00%)</cell><cell>18.08% (0.24%)</cell><cell>19.14 (1.70)</cell><cell>18.86 (1.87)</cell></row><row><cell></cell><cell>+ CoRe</cell><cell>0.0% (0.00%)</cell><cell>15.08% (0.43%)</cell><cell>0.01 (0.01)</cell><cell>0.70 (0.05)</cell></row><row><cell>DS 2</cell><cell>5-layer CNN</cell><cell>0.0% (0.00%)</cell><cell>23.81% (0.51%)</cell><cell>6.20 (0.35)</cell><cell>6.97 (0.46)</cell></row><row><cell></cell><cell>+ CoRe</cell><cell>0.0% (0.00%)</cell><cell>17.00% (0.75%)</cell><cell>0.00 (0.00)</cell><cell>0.41 (0.04)</cell></row><row><cell>DS 3</cell><cell>5-layer CNN</cell><cell>0.0% (0.00%)</cell><cell>18.61% (0.52%)</cell><cell>7.33 (1.40)</cell><cell>7.91 (1.13)</cell></row><row><cell></cell><cell>+ CoRe</cell><cell>0.0% (0.00%)</cell><cell>14.79% (0.89%)</cell><cell>0.00 (0.00)</cell><cell>0.26 (0.03)</cell></row><row><cell>DS 4</cell><cell>5-layer CNN</cell><cell>0.0% (0.00%)</cell><cell>25.97% (0.24%)</cell><cell>6.19 (0.43)</cell><cell>7.13 (0.54)</cell></row><row><cell></cell><cell>+ CoRe</cell><cell>0.0% (0.00%)</cell><cell>21.12% (0.40%)</cell><cell>0.00 (0.00)</cell><cell>0.63 (0.04)</cell></row><row><cell>DS 5</cell><cell>5-layer CNN</cell><cell>0.0% (0.00%)</cell><cell>23.64% (0.64%)</cell><cell>20.20 (2.46)</cell><cell>24.85 (3.56)</cell></row><row><cell></cell><cell>+ CoRe</cell><cell>0.0% (0.00%)</cell><cell>21.49% (1.27%)</cell><cell>0.00 (0.00)</cell><cell>0.59 (0.10)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2</head><label>2</label><figDesc>Eyeglass detection, trained on a small subset (DS1) of the CelebA dataset with disjoint identitiesWe report training and test error as well as the value of the CoRe penalties ƒàf ,1,ùúÉ , ƒàf ,1‚àï2,ùúÉ , ƒàl,1,ùúÉ and ƒàl,1‚àï2,ùúÉ on the training and the test set after training, evaluated for both the pooled estimator and the CoRe estimator. The weights of the ridge and the CoRe penalty were chosen based on their performance on the validation set. The four CoRe penalty variants' performance differences are not statistically significant Examples from the training and test sets 1 and 2 are shown in Fig.7. The training set, test set 1 and 2 are subsampled such that they are balanced with respect to Y, resulting in16,982, 4224 and 1120 observations, respectively.   </figDesc><table><row><cell>Method</cell><cell>Error</cell><cell>Penalty value</cell><cell></cell></row><row><cell></cell><cell>Test</cell><cell>Training</cell><cell>Test</cell></row><row><cell>5-layer CNN</cell><cell>18.08% (0.24%)</cell><cell>19.14 (1.70)</cell><cell>18.86 (1.87)</cell></row><row><cell>5-layer CNN + CoRe w/ ƒàf ,1,ùúÉ</cell><cell>15.08% (0.43%)</cell><cell>0.01 (0.01)</cell><cell>0.70 (0.05)</cell></row><row><cell>5-layer CNN + CoRe w/ ƒàf ,1‚àï2,ùúÉ</cell><cell>15.34% (0.83%)</cell><cell>0.03 (0.01)</cell><cell>0.89 (0.03)</cell></row><row><cell>5-layer CNN + CoRe w/ ƒàl,1,ùúÉ</cell><cell>15.12% (0.27%)</cell><cell>0.00 (0.00)</cell><cell>0.38 (0.03)</cell></row><row><cell>5-layer CNN + CoRe w/ ƒàl,1‚àï2,ùúÉ</cell><cell>15.59% (0.36%)</cell><cell>0.00 (0.00)</cell><cell>0.35 (0.02)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3</head><label>3</label><figDesc>Classification for Y ‚àà {woman, man}For each experimental setting, bold marks the best performing method</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Table 6 in the Appendix additionally contains the standard error of all shown results</figDesc><table><row><cell></cell><cell>Method</cell><cell>Error</cell><cell></cell><cell></cell><cell cols="2">Penalty value</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="4">Train (%) Test 1 (%) Test 2 (%) Train</cell><cell cols="2">Test: Females Test: Males</cell></row><row><cell>= .5</cell><cell>5-layer CNN</cell><cell>0.00</cell><cell>2.00</cell><cell>38.54</cell><cell>22.77</cell><cell>74.05</cell><cell>30.67</cell></row><row><cell></cell><cell>+ CoRe</cell><cell>6.43</cell><cell>5.85</cell><cell>24.07</cell><cell>0.01</cell><cell>1.61</cell><cell>0.93</cell></row><row><cell cols="2">= .75 5-layer CNN</cell><cell>0.00</cell><cell>1.98</cell><cell>43.41</cell><cell>8.23</cell><cell>32.98</cell><cell>11.76</cell></row><row><cell></cell><cell>+ CoRe</cell><cell>7.61</cell><cell>6.99</cell><cell>27.05</cell><cell>0.00</cell><cell>1.44</cell><cell>0.62</cell></row><row><cell>= .9</cell><cell>5-layer CNN</cell><cell>0.00</cell><cell>2.00</cell><cell>47.64</cell><cell>9.47</cell><cell>40.51</cell><cell>14.37</cell></row><row><cell></cell><cell>+ CoRe</cell><cell>8.76</cell><cell>7.74%</cell><cell>30.63</cell><cell>0.00</cell><cell>1.26</cell><cell>0.42</cell></row><row><cell cols="2">= .95 5-layer CNN</cell><cell>0.00</cell><cell>1.89</cell><cell>48.96</cell><cell>13.62</cell><cell>61.01</cell><cell>21.26</cell></row><row><cell></cell><cell>+ CoRe</cell><cell>10.45</cell><cell>9.35</cell><cell>29.57</cell><cell>0.00</cell><cell>0.42</cell><cell>0.16</cell></row><row><cell cols="2">= .99 5-layer CNN</cell><cell>0.00</cell><cell>1.70</cell><cell>50.11</cell><cell>20.66</cell><cell>70.80</cell><cell>27.80</cell></row><row><cell></cell><cell>+ CoRe</cell><cell>11.10</cell><cell>10.51</cell><cell>32.91</cell><cell>0.00</cell><cell>0.00</cell><cell>0.00</cell></row><row><cell>= 1</cell><cell>5-layer CNN</cell><cell>0.00</cell><cell>1.93</cell><cell>49.41</cell><cell cols="2">821.32 2524.77</cell><cell>1253.21</cell></row><row><cell></cell><cell>+ CoRe</cell><cell>11.12</cell><cell>10.11</cell><cell>35.68</cell><cell>0.00</cell><cell>0.02</cell><cell>0.01</cell></row><row><cell cols="3">Table 4 Classification for Y ‚àà {woman, man} with = 0.5 Here, we compared 2 -regularized logistic regression</cell><cell>Method</cell><cell></cell><cell>Error Train (%)</cell><cell>Test 1 (%)</cell><cell>Test 2 (%)</cell></row><row><cell cols="3">based on Inception V3 features with and without the CoRe penalty</cell><cell cols="2">Inception V3 Inception V3 + CoRe</cell><cell>5.74 6.15</cell><cell>5.53 5.85</cell><cell>30.29 21.70</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="5">For each experimental setting, bold marks the best performing method</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="5">The CoRe estimator improves the performance of the pooled estimator by ‚âà 28% on a relative scale</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5</head><label>5</label><figDesc>Details of the model architectures used</figDesc><table><row><cell>Dataset</cell><cell>Optimizer</cell><cell></cell><cell>Architecture</cell></row><row><cell>MNIST</cell><cell>Adam</cell><cell>Input</cell><cell>28 √ó 28 √ó 1</cell></row><row><cell></cell><cell></cell><cell>CNN</cell><cell>Conv 5 √ó 5 √ó 16 , 5 √ó 5 √ó 32</cell></row><row><cell></cell><cell></cell><cell></cell><cell>(same padding, strides = 2 , ReLu activation),</cell></row><row><cell></cell><cell></cell><cell></cell><cell>fully connected, softmax layer</cell></row><row><cell>Stickmen</cell><cell>Adam</cell><cell>Input</cell><cell>64 √ó 64 √ó 1</cell></row><row><cell></cell><cell></cell><cell>CNN</cell><cell>Conv 5 √ó 5 √ó 16 , 5 √ó 5 √ó 32 , 5 √ó 5 √ó 64 , 5 √ó 5 √ó 128</cell></row><row><cell></cell><cell></cell><cell></cell><cell>(same padding, strides = 2 , leaky ReLu activation),</cell></row><row><cell></cell><cell></cell><cell></cell><cell>fully connected, softmax layer</cell></row><row><cell>CelebA</cell><cell>Adam</cell><cell>Input</cell><cell>64 √ó 48 √ó 3</cell></row><row><cell>(all experiments</cell><cell></cell><cell>CNN</cell><cell>Conv 5 √ó 5 √ó 16 , 5 √ó 5 √ó 32 , 5 √ó 5 √ó 64 , 5 √ó 5 √ó 128</cell></row><row><cell>using CelebA)</cell><cell></cell><cell></cell><cell>(same padding, strides = 2 , leaky ReLu activation),</cell></row><row><cell></cell><cell></cell><cell></cell><cell>fully connected, softmax layer</cell></row><row><cell>AwA2</cell><cell>Adam</cell><cell>Input</cell><cell>32 √ó 32 √ó 3</cell></row></table><note><p>CNN Conv 5 √ó 5 √ó 16 , 5 √ó 5 √ó 32 , 5 √ó 5 √ó 64 , 5 √ó 5 √ó 128 (same padding, strides = 2 , leaky ReLu activation), fully connected, softmax layer</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Open Access This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://creat iveco mmons .org/licen ses/by/4.0/.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6</head><label>6</label><figDesc>Classification for Y ‚àà {woman, man}We compare six different datasets that vary with respect to the distribution of Y in the grouped observations. Specifically, we vary the proportion of images showing men between = 0.5 and = 1 . In all training datasets, the total number of observations is 16982 and the total number of grouped observations is 500. Both the pooled estimator as well as the CoRe estimator perform better if the distribution of Y in the grouped observations is more balanced. The CoRe estimator improves the error rate of the pooled estimator by ‚âà 28</figDesc><table><row><cell>on a relative scale</cell></row><row><cell>-39%</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Segmenting eyelashes from the iris is not entirely accurate which implies that the iris images can still contain parts of eyelashes, occluding the iris. As mascara causes the eyelashes to be thicker and darker, it is difficult to entirely remove the presence of cosmetics from the iris images.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>The distinction between 'conditionally independent' features and 'conditionally transferable' (which is the former modulo location and scale transformations) is for our purposes not relevant as we do not make a linearity assumption in general.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>If an existing image is classified by a human, then the image is certainly ancestral for the attached label. If the label Y refers, however, to the underlying true object (say if you generate images by asking people to take pictures of objects or if you record the status of cells after performing a gene knockout), then the more fitting model is the one where Y is ancestral for X. We here focus on modeling the underlying true object since ultimately the goal is to deploy the trained system in the real world.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>The type of features we regard as style and which ones we regard as core features can conceivably change depending on the circumstances-for instance, is the color "gray" an integral part of the object "elephant" or can it be changed so that a colored elephant is still considered to be an elephant?</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4"><p>Observations where the ID variable is unobserved are not grouped, that is each such observation is counted as a unique observation of (Y, ID).</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5"><p>As an example, if the change in distribution for S is caused by random shift-interventions , then S ‚Üê S + ùõ• , and the distance D style induced in the distributions is ensuring that the strength of the shifts is measured against the natural variability Œ£ y,id of the style features.D style (F 0 , F) ‚â§ E E( t Œ£ -1y,id |Y = y, ID = id) ,</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6"><p>The right hand side can also be interpreted as the graph Laplacian<ref type="bibr" target="#b6">(Belkin et al. 2006</ref>) of an appropriately weighted graph that fully connects all observations i ‚àà G j for each j ‚àà {1, ‚Ä¶ , m}.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_7"><p>If some labeled test data are avaiable and our goal is to perform optimally for samples that come from the same distribution as those test samples, then we can and should adjust the parameter to minimize estimated test error.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_8"><p>For example, the home office dataset in<ref type="bibr" target="#b62">(Venkateswara et al. 2017</ref>) contains images of various objects (kettle, clock etc) from different domains (real world photographs, cliparts, art images and product images). If we aim to predict the category of an object, it seems difficult to identify an appropriate ID variable in the dataset. However, if we aim instead to predict properties of an object (does it use electricity?), then we can use as ID the object category (kettle, clock etc.)</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_9"><p>In Fig.18in the Appendix, the numerator and the denominator are plotted separately as a function of the CoRe penalty weight.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11" xml:id="foot_10"><p>Retrieved from https ://tfhub .dev/googl e/image net/incep tion_v3/featu re_vecto r/1.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12" xml:id="foot_11"><p>We use ImageMagick (https ://www.image magic k.org) to change the level of the JPEG compression</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="13" xml:id="foot_12"><p>https ://en.wikip edia.org/wiki/Elmer _the_Patch work_Eleph ant.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="15" xml:id="foot_13"><p>Specifically, we use ImageMagick (https ://www.image magic k.org) and modify the brightness of each image by applying the command convert -modulate b_ij,100,100 input.jpg output. jpg to the image.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>We thank <rs type="person">Brian McWilliams</rs>, <rs type="person">Jonas Peters</rs>, and <rs type="person">Martin Arjovsky</rs> for helpful comments and discussions and <rs type="institution">CSCS</rs> for provision of computational resources. A preliminary version of this work was presented at the <rs type="institution" subtype="infrastructure">NIPS 2017 Interpretable ML Symposium</rs> and we thank participants of the symposium for very helpful discussions.</p></div>
			</div>
			<div type="funding">
<div><p>Funding Open access funding provided by <rs type="institution">Swiss Federal Institute of Technology Zurich</rs>.</p></div>
			</div>
			<listOrg type="infrastructure">
				<org type="infrastructure">					<orgName type="extracted">NIPS 2017 Interpretable ML Symposium</orgName>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Publisher's Note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Harp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kudlur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Levenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Man√©</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Talwar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Vi√©gas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Warden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wattenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wicke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zheng</surname></persName>
		</author>
		<ptr target="https://www.tensorflow.org/,softwareavail-ablefromtensorflow.org" />
		<title level="m">TensorFlow: Large-scale machinelearning on heterogeneous systems</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Autonomy</title>
		<author>
			<persName><forename type="first">J</forename><surname>Aldrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Oxford Economic Papers</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="15" to="34" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Robust supervised learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bagnell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the national conference on artificial intelligence</title>
		<meeting>the national conference on artificial intelligence<address><addrLine>Menlo Park, CA; Cambridge, MA; London</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press; MIT Press</publisher>
			<date type="published" when="1999">2005. 1999</date>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page">714</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Bahadori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chalupka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">F</forename><surname>Stewart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno>arXiv :17020 2604</idno>
		<title level="m">Causal regularization</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Big Data&apos;s Disparate Impact</title>
		<author>
			<persName><forename type="first">S</forename><surname>Barocas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Selbst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">California Law Review</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Viewpoint invariant face recognition using independent component analysis and attractor networks</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Sejnowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th International Conference on Neural Information Processing Systems</title>
		<meeting>the 9th International Conference on Neural Information Processing Systems<address><addrLine>Cambridge, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page">96</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Manifold regularization: A geometric framework for learning from labeled and unlabeled examples</title>
		<author>
			<persName><forename type="first">M</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Niyogi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Sindhwani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="2399" to="2434" />
			<date type="published" when="2006-11">2006. Nov</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Analysis of representations for domain adaptation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ben-David</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Robust solutions of optimization problems affected by uncertain probabilities</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ben-Tal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Den Hertog</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>De Waegenaere</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Melenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Rennen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Management Science</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="341" to="357" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Group invariance principles for causal generative models</title>
		<author>
			<persName><forename type="first">M</forename><surname>Besserve</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shajarisales</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Sch√∂lkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Janzing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st International Conference on Artificial Intelligence and Statistics (AISTATS)</title>
		<meeting>the 21st International Conference on Artificial Intelligence and Statistics (AISTATS)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">84</biblScope>
			<biblScope unit="page" from="557" to="565" />
		</imprint>
	</monogr>
	<note>PMLR, Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Man is to computer programmer as woman is to homemaker? Debiasing word embeddings</title>
		<author>
			<persName><forename type="first">T</forename><surname>Bolukbasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Saligrama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">T</forename><surname>Kalai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">29</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Multi-level variational autoencoder: Learning disentangled representations from grouped observations</title>
		<author>
			<persName><forename type="first">D</forename><surname>Bouchacourt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tomioka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nowozin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Visual Causal Feature Learning</title>
		<author>
			<persName><forename type="first">K</forename><surname>Chalupka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Eberhardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Uncertainty in Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Artificial intelligence&apos;s white guy problem. The New York Times</title>
		<author>
			<persName><forename type="first">K</forename><surname>Crawford</surname></persName>
		</author>
		<ptr target="https://www.nytimes.com/2016/06/26/opinion/sunday/artificial-intelligences-white-guy-problem.html" />
		<imprint>
			<date type="published" when="2016-06-25">2016. June 25 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<ptr target="http://www.imagemagick.org/Usage/color_mods/#color_mods" />
		<title level="m">For more details</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A comprehensive survey on domain adaptation for visual applications</title>
		<author>
			<persName><forename type="first">G</forename><surname>Csurka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Domain Adaptation in Computer Vision Applications</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Unsupervised learning of disentangled representations from video</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">L</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Birodkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Dataset augmentation in feature space</title>
		<author>
			<persName><forename type="first">T</forename><surname>Devries</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR Workshop Track</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">How a machine learns prejudice</title>
		<author>
			<persName><forename type="first">J</forename><surname>Emspak</surname></persName>
		</author>
		<ptr target="https://www.scientificamerican.com/article/how-a-machine-learns-prejudice/" />
	</analytic>
	<monogr>
		<title level="j">Scientific American</title>
		<imprint>
			<date type="published" when="2016-12-29">2016. December 29 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Evaluating and understanding the robustness of adversarial logit pairing</title>
		<author>
			<persName><forename type="first">L</forename><surname>Engstrom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ilyas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Athalye</surname></persName>
		</author>
		<idno>arXiv :18071 0272</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Domain-adversarial training of neural networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ustinova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ajakan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Germain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Laviolette</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2096" to="2030" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">R</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kleywegt</surname></persName>
		</author>
		<idno>arXiv :17120 6050</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Reshaping visual datasets for domain adaptation</title>
		<author>
			<persName><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Sha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="1286" to="1294" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Domain adaptation with conditional transferable components</title>
		<author>
			<persName><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Glymour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Sch√∂lkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Explaining and harnessing adversarial examples</title>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on learning representations</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Learning functional causal models with generative neural networks</title>
		<author>
			<persName><forename type="first">O</forename><surname>Goudet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kalainathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Caillou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sebag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tritas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Tubaro</surname></persName>
		</author>
		<idno>arXiv :17090 5321</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">The probability approach in econometrics</title>
		<author>
			<persName><forename type="first">T</forename><surname>Haavelmo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Econometrica</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="1" to="S115" />
			<date type="published" when="1944">1944</date>
		</imprint>
	</monogr>
	<note>supplement</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Unsupervised transformation learning via convex relaxations</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">B</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Duchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="6875" to="6883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 IEEE international conference on computer vision (ICCV)</title>
		<meeting>the 2015 IEEE international conference on computer vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE conference on computer vision and pattern recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Discovering latent domains for multisource domain adaptation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kulis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2012</title>
		<meeting><address><addrLine>Berlin Heidelberg; Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="702" to="715" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<idno>arXiv :18030 6373</idno>
		<title level="m">Adversarial logit pairing</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Graph-based isometry invariant representation learning</title>
		<author>
			<persName><forename type="first">R</forename><surname>Khasanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Frossard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th international conference on machine learning</title>
		<meeting>the 34th international conference on machine learning</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1847" to="1856" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Avoiding discrimination through causal reasoning</title>
		<author>
			<persName><forename type="first">N</forename><surname>Kilbertus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rojas Carulla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Parascandolo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Janzing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Sch√∂lkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="656" to="666" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<title level="m">Adam: A method for stochastic optimization. International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">CausalGAN: Learning causal implicit generative models with adversarial training</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kocaoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Snyder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dimakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vishwanath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on learning representations</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">25</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Gender-from-iris or gender-from-mascara?</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kuehlkamp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Becker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Bowyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Sliced inverse regression for dimension reduction</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">C</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">414</biblScope>
			<biblScope unit="page" from="316" to="327" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Deep learning face attributes in the wild</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Computer Vision (ICCV)</title>
		<meeting>International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Revisiting classifier two-sample tests</title>
		<author>
			<persName><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Oquab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on learning representations (ICLR)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Discovering causal signals in images</title>
		<author>
			<persName><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Nishihara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Sch√∂lkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Domain adaptation by using causal inference to predict invariant conditional distributions</title>
		<author>
			<persName><forename type="first">S</forename><surname>Magliacane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Van Ommen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Claassen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bongers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Versteeg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mooij</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Causality from a distributional robustness point of view</title>
		<author>
			<persName><forename type="first">N</forename><surname>Meinshausen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Data Science Workshop</title>
		<imprint>
			<biblScope unit="page" from="6" to="10" />
			<date type="published" when="2018">2018. 2018</date>
			<publisher>DSW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Logit pairing methods can fool gradient-based attacks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Mosbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Andriushchenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Trost</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Klakow</surname></persName>
		</author>
		<idno>arXiv :18101 2042</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Variance-based regularization with convex objectives</title>
		<author>
			<persName><forename type="first">H</forename><surname>Namkoong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Duchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2975" to="2984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
		<title level="m">Causality: Models, reasoning, and inference</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
	<note>nd ed.</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Causal inference using invariant prediction: identification and confidence intervals</title>
		<author>
			<persName><forename type="first">J</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>B√ºhlmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Meinshausen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society, Series B</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="page" from="947" to="1012" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Dataset shift in machine learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Quionero-Candela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Schwaighofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Lawrence</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>The MIT Press</publisher>
			<pubPlace>Cambridge</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Single world intervention graphs (SWIGs): A unification of the counterfactual and graphical approaches to causality</title>
		<author>
			<persName><forename type="first">T</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Robins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Center for the Statistics and the Social Sciences</title>
		<imprint>
			<biblScope unit="volume">128</biblScope>
			<date type="published" when="2013-04-30">2013. 30 April 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Causal transfer in machine learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Rojas-Carulla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Sch√∂lkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Peters</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>To appear in</note>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Rothenh√§usler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>B√ºhlmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Meinshausen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Peters</surname></persName>
		</author>
		<idno>arXiv :18010 6229</idno>
		<title level="m">Anchor regression: heterogeneous data meets causality</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Incorporating invariances in support vector learning machines</title>
		<author>
			<persName><forename type="first">B</forename><surname>Sch√∂lkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Burges</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Neural Networks -ICANN</title>
		<imprint>
			<biblScope unit="volume">96</biblScope>
			<biblScope unit="page" from="47" to="52" />
			<date type="published" when="1996">1996</date>
			<publisher>Springer</publisher>
			<pubPlace>Berlin Heidelberg; Berlin, Heidelberg</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">On causal and anticausal learning</title>
		<author>
			<persName><forename type="first">B</forename><surname>Sch√∂lkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Janzing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Sgouritsa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mooij</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th international conference on machine learning (ICML)</title>
		<meeting>the 29th international conference on machine learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1255" to="1262" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Shafieezadeh-Abadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kuhn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Esfahani</surname></persName>
		</author>
		<idno>arXiv :17101 0016</idno>
		<title level="m">Regularization via mass transportation</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Certifiable distributional robustness with principled adversarial training</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Namkoong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Duchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on learning representations</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Learning invariant representations with local transformations</title>
		<author>
			<persName><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th international coference on international conference on machine learning</title>
		<meeting>the 29th international coference on international conference on machine learning<address><addrLine>Omnipress, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1339" to="1346" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer vision and pattern recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Intriguing properties of neural networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on learning representations</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Unbiased look at dataset bias</title>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Disentangled representation learning gan for pose-invariant face recognition</title>
		<author>
			<persName><forename type="first">L</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of IEEE computer vision and pattern recognition</title>
		<meeting>eeding of IEEE computer vision and pattern recognition<address><addrLine>Honolulu, HI</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Deep hashing network for unsupervised domain adaptation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Venkateswara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Eusebio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Panchanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE) Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Villani</surname></persName>
		</author>
		<title level="m">Topics in optimal transportation</title>
		<meeting><address><addrLine>Providence</addrLine></address></meeting>
		<imprint>
			<publisher>American Mathematical Society</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">58</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Generalizing to unseen domains via adversarial data augmentation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Volpi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Namkoong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Sener</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Murino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<idno>arXiv :18051 2018</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Lampert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
		<idno>arXiv :17070 0600</idno>
		<title level="m">Zero-shot learning-A comprehensive evaluation of the good, the bad and the ugly</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Robust regression and lasso</title>
		<author>
			<persName><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Caramanis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mannor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1801" to="1808" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<idno>arXiv :13045 634</idno>
		<title level="m">A survey on multi-view learning</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<author>
			<persName><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<idno>arXiv :17070 9724</idno>
		<title level="m">Transfer learning with label noise</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Multi-source domain adaptation: A causal view</title>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Sch√∂lkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence</title>
		<meeting>the Twenty-Ninth AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Domain adaptation under target and conditional shift</title>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Sch√∂lkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Muandet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
