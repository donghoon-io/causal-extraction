<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Distance Metrics for Measuring Joint Dependence with Application to Causal Inference</title>
				<funder ref="#_UyyemnG">
					<orgName type="full">National Science Foundation</orgName>
					<orgName type="abbreviated">NSF</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2018-06-18">June 18, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Shubhadeep</forename><surname>Chakraborty</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Statistics</orgName>
								<orgName type="department" key="dep2">Department of Statistics</orgName>
								<orgName type="institution" key="instit1">Texas A&amp;M University and Xianyang Zhang</orgName>
								<orgName type="institution" key="instit2">Texas A&amp;M University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Distance Metrics for Measuring Joint Dependence with Application to Causal Inference</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2018-06-18">June 18, 2018</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:1711.09179v2[stat.ME]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-28T23:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Bootstrap</term>
					<term>Directed Acyclic Graph</term>
					<term>Distance Covariance</term>
					<term>Interaction Dependence</term>
					<term>Ustatistic</term>
					<term>V-statistic</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Many statistical applications require the quantification of joint dependence among more than two random vectors. In this work, we generalize the notion of distance covariance to quantify joint dependence among d ≥ 2 random vectors. We introduce the high order distance covariance to measure the so-called Lancaster interaction dependence. The joint distance covariance is then defined as a linear combination of pairwise distance covariances and their higher order counterparts which together completely characterize mutual independence. We further introduce some related concepts including the distance cumulant, distance characteristic function, and rankbased distance covariance. Empirical estimators are constructed based on certain Euclidean distances between sample elements. We study the large sample properties of the estimators and propose a bootstrap procedure to approximate their sampling distributions. The asymptotic validity of the bootstrap procedure is justified under both the null and alternative hypotheses. The new metrics are employed to perform model selection in causal inference, which is based on the joint independence testing of the residuals from the fitted structural equation models. The effectiveness of the method is illustrated via both simulated and real datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Measuring and testing dependence is of central importance in statistics, which has found applications in a wide variety of areas including independent component analysis, gene selection, graphical modeling and causal inference. Statistical tests of independence can be associated with widely many dependence measures. Two of the most classical measures of association between two ordinal random variables are Spearman's rho and Kendall's tau. However, tests for (pairwise) independence using these two classical measures of association are not consistent, and only have power for alternatives with monotonic association. Contingency table-based methods, and in particular the power-divergence family of test statistics <ref type="bibr" target="#b18">(Read and Cressie, 1988)</ref>, are the best known general purpose tests of independence, but are limited to relatively low dimensions, since they require a partitioning of the space in which each random variable resides. Another classical measure of dependence between two random vectors is the mutual information <ref type="bibr" target="#b4">(Cover and Thomas, 1991)</ref>, which can be interpreted as the Kullback-Leibler divergence between the joint density and the product of the marginal densities. The idea originally dates back to the 1950's, in groundbreaking works by <ref type="bibr" target="#b23">Shannon and Weaver (1949)</ref>, <ref type="bibr" target="#b15">Mcgill (1954)</ref> and <ref type="bibr" target="#b3">Fano (1961)</ref>. Mutual information completely characterizes independence and generalizes to more than two random vectors. However, test based on mutual information involves distributional assumptions for the random vectors and hence is not robust to model misspecification.</p><p>In the past fifteen years, kernel-based methods have received considerable attention in both the statistics and machine learning literature. For instance, <ref type="bibr" target="#b0">Bach and Jordan (2002)</ref> derived a regularized correlation operator from the covariance and cross-covariance operators and used its largest singular value to conduct independence test. <ref type="bibr" target="#b7">Gretton et al. (2005;</ref><ref type="bibr">2007)</ref> introduced a kernel-based independence measure, namely the Hilbert-Schmidt Independence Criterion (HSIC), to test for independence of two random vectors. This idea was recently extended by <ref type="bibr">Sejdinovic et al. (2013)</ref> and <ref type="bibr" target="#b17">Pfister et al. (2018)</ref> to quantify the joint independence among more than two random vectors.</p><p>Along with a different direction, <ref type="bibr" target="#b27">Székely et al. (2007)</ref>, in their seminal paper, introduced the notion of distance covariance (dCov) and distance correlation as a measure of dependence between two random vectors of arbitrary dimensions. Given the theoretical appeal of the population quantity and the striking simplicity of the sample version, the idea has been widely extended and analyzed in various ways in <ref type="bibr" target="#b29">Székely and Rizzo (2012;</ref><ref type="bibr">2014)</ref>, <ref type="bibr" target="#b13">Lyons (2013)</ref>, <ref type="bibr">Sejdinovic et al. (2013)</ref>, <ref type="bibr" target="#b6">Dueck et al. (2014)</ref>, <ref type="bibr" target="#b1">Bergsma et al. (2014)</ref>, <ref type="bibr" target="#b32">Wang et al. (2015)</ref>, and <ref type="bibr" target="#b10">Huo and Székely (2016)</ref>, to mention only a few. The dCov between two random vectors X ∈ R p and Y ∈ R q with finite first moments is defined as the positive square root of</p><formula xml:id="formula_0">dCov 2 (X, Y ) = 1 c p c q R p+q |f X,Y (t, s) -f X (t)f Y (s)| 2 |t| 1+p p |s| 1+q q dtds,</formula><p>where f X , f Y and f X,Y are the individual and joint characteristic functions of X and Y respectively, | • | p is the Euclidean norm of R p , c p = π (1+p)/2 / Γ((1 + p)/2) is a constant with Γ(•) being the complete gamma function. An important feature of dCov is that it fully characterizes independence because dCov(X, Y ) = 0 if and only if X and Y are independent.</p><p>Many statistical applications require the quantification of joint dependence among d ≥ 2 random variables (or vectors). Examples include model diagnostic checking for directed acyclic graph <ref type="bibr">(DAG)</ref> where inferring pairwise independence is not enough in this case (see more details in Section 6), and independent component analysis which is a means for finding a suitable representation of multivariate data such that the components of the transformed data are mutually independent. In this paper, we shall introduce new metrics which generalize the notion of dCov to quantify joint dependence of d ≥ 2 random vectors. We first introduce the notion of high order dCov to measure the so-called Lancaster interaction dependence <ref type="bibr" target="#b11">(Lancaster, 1969)</ref>. We generalize the notion of Brownian covariance <ref type="bibr" target="#b28">(Székely et al., 2009)</ref> and show that it coincides with the high order distance covariance. We then define the joint dCov (Jdcov) as a linear combination of pairwise dCov and their high order counterparts. The proposed metric provides a natural decomposition of joint dependence into the sum of lower order and high order effects, where the relative importance of the lower order effect terms and the high order effect terms is determined by a user-chosen number. In the population case, Jdcov is equal to zero if and only if the d random vectors are mutually independent, and thus completely characterizes joint independence. It is also worth mentioning that the proposed metrics are invariant to permutation of the variables and they inherit some nice properties of dCov, see Section 2.2.</p><p>Following the idea of <ref type="bibr" target="#b24">Streitberg (1990)</ref>, we introduce the concept of distance cumulant and distance characteristic function, which leads us to an equivalent characterization of independence of the d random vectors. Furthermore, we establish a scale invariant version of Jdcov and discuss the concept of rank-based distance measures, which can be viewed as the counterparts of Spearman's rho to dCov and JdCov.</p><p>JdCov and its scale-invariant versions can be conveniently estimated in finite sample using V-statistics or their bias-corrected versions. We study the asymptotic properties of the estimators, and introduce a bootstrap procedure to approximate their sampling distributions. The asymptotic validity of the bootstrap procedure is justified under both the null and alternative hypotheses. The new metrics are employed to perform model selection in a causal inference problem, which is based on the joint independence testing of the residuals from the fitted structural equation models. We compare our tests with the bootstrap version of the d-variate HSIC (dHSIC) test recently introduced in Pfister et al. ( <ref type="formula">2018</ref>) and the mutual independence test proposed by <ref type="bibr" target="#b14">Matteson and Tsay (2017)</ref>. Finally we remark that although we focus on Euclidean space valued random variables, our results can be readily extended to general metric spaces in view of the results in <ref type="bibr" target="#b13">Lyons (2013)</ref>.</p><p>The rest of the paper is organized as follows. Section 2.1 introduces the high order distance covariance and studies its basic properties. Section 2.2 describes the JdCov to quantity joint dependence.</p><p>Sections 2.3-2.4 further introduce some related concepts including the distance cumulant, distance characteristic function, and rank-based distance covariance. We study the estimation of the distance metrics in Section 3 and present a joint independence test based on the proposed metrics in Section 4. For a complex number a, denote by ā its conjugate. Let f i be the characteristic function</p><formula xml:id="formula_1">of X i , i.e., f i (t) = E[e ı t,X i ] with t ∈ R p i . Define w p (t) = (c p |t| 1+p p ) -1 with c p = π (1+p)/2 / Γ((1 + p)/2). Write dw = (c p 1 c p 2 . . . c p d |t 1 | 1+p 1 p 1 • • • |t d | 1+p d p d ) -1 dt 1 • • • dt d . Let I d</formula><p>k be the collection of k-tuples of indices from {1, 2, . . . , d} such that each index occurs exactly once. Denote by a the integer part of a ∈ R.</p><formula xml:id="formula_2">Write X ⊥ ⊥ Y if X is independent of Y.</formula><p>2 Measuring joint dependence</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">High order distance covariance</head><p>We briefly review the concept of Lancaster interactions first introduced by <ref type="bibr" target="#b11">Lancaster (1969)</ref>. The Lancaster interaction measure associated with a multidimensional probability distribution of d random variables {X 1 , . . . , X d } with the joint distribution F = F 1,2,...,d , is a signed measure ∆F given by</p><formula xml:id="formula_3">∆F = (F * 1 -F 1 )(F * 2 -F 2 ) • • • (F * d -F d ) ,<label>(1)</label></formula><p>where after expansion, a product of the form</p><formula xml:id="formula_4">F * i F * j • • • F * k denotes the corresponding joint distribution function F i,j,...,k of {X i , X j , . . . , X k }. For example for d = 4, the term F * 1 F * 2 F 3 F 4 stands for F 12 F 3 F 4 , F * 1 F 2 F 3 F 4 stands for F 1 F 2 F 3 F 4 , etc.</formula><p>In particular for d = 3, (1) simplifies to</p><formula xml:id="formula_5">∆F = F 123 -F 1 F 23 -F 2 F 13 -F 3 F 12 + 2F 1 F 2 F 3 .<label>(2)</label></formula><p>In light of the Lancaster interaction measure, we introduce the concept of dth order dCov as follows.</p><p>Definition 2.1. The dth order dCov is defined as the positive square root of</p><formula xml:id="formula_6">dCov 2 (X 1 , . . . , X d ) = R p 0 E d i=1 (f i (t i ) -e ı t i ,X i ) 2 dw,<label>(3)</label></formula><p>When d = 2, it reduces to the dCov in <ref type="bibr" target="#b27">Székely et al. (2007)</ref>.</p><p>The term E[ d i=1 (f i (t i ) -e ı t i ,X i )] in the definition of dCov is a counterpart of the Lancaster interaction measure in (1) with the joint distribution functions replaced by the joint characteristic functions. When d = 3, dCov 2 (X 1 , X 2 , X 3 ) &gt; 0 rules out the possibility of any factorization of the joint distribution. To see this, we note that X</p><formula xml:id="formula_7">1 ⊥ ⊥ (X 2 , X 3 ), X 2 ⊥ ⊥ (X 1 , X 3 ) or X 3 ⊥ ⊥ (X 1 , X 2 ) all lead to dCov 2 (X 1 , X 2 , X 3 ) = 0. On the other hand, dCov 2 (X 1 , X 2 , X 3 ) = 0 implies that f 123 (t 1 , t 2 , t 3 ) -f 1 (t 1 )f 2 (t 2 )f 3 (t 3 ) =f 1 (t 1 )f 23 (t 2 , t 3 ) + f 2 (t 2 )f 13 (t 1 , t 3 ) + f 3 (t 3 )f 12 (t 1 , t 2 ) -3f 1 (t 1 )f 2 (t 2 )f 3 (t 3 )</formula><p>for t i ∈ R p i almost everywhere. In this case, the "higher order effect" i.e., f 123 (t 1 , t 2 , t 3 )-f 1 (t 1 )f 2 (t 2 )f 3 (t 3 ) can be represented by the "lower order/pairwise effects"</p><formula xml:id="formula_8">f ij (t i , t j ) -f i (t i )f j (t j ) for 1 ≤ i = j ≤ 3.</formula><p>However, this does not necessarily imply that X 1 , X 2 and X 3 are jointly independent. In other words when d = 3 (or more generally when d ≥ 3), joint independence of X 1 , X 2 and X 3 is not a necessary condition for dCov to be zero. To address this issue, we shall introduce a new distance metric to quantify any forms of dependence among X in Section 2.2.</p><p>In the following, we present some basic properties of high order dCov. Define the bivariate function</p><formula xml:id="formula_9">U i (x, x ) = E|x -X i | + E|X i -x | -|x -x | -E|X i -X i | for x, x ∈ R p i with 1 ≤ i ≤ d.</formula><p>Our definition of dCov is partly motivated by the following lemma.</p><formula xml:id="formula_10">Lemma 2.1. For 1 ≤ i ≤ d, U i (x, x ) = R p i (f i (t) -e ı t,x )(f i (-t) -e -ı t,x ) w p i (t)dt.</formula><p>By Lemma 2.1 and Fubini's theorem, the dth order (squared) dCov admits the following equivalent representation,</p><formula xml:id="formula_11">dCov 2 (X 1 , . . . , X d ) = R p 0 E d i=1 (f i (t i ) -e ı t i ,X i ) 2 dw = R p 0 E d i=1 (f i (t i ) -e ı t i ,X i ) E d i=1 (f i (t i ) -e ı t i ,X i ) dw = E d i=1 U i (X i , X i ) . (4)</formula><p>This suggests that similar to dCov, its high order counterpart has an expression based on the moments of U i s, which results in very simple and applicable empirical formulas, see more details in Section 3.</p><p>Remark 2.1. From the definition of dCov in <ref type="bibr" target="#b27">Székely et al. (2007)</ref>, it might appear that its most natural generalization to the case of d = 3 would be to define a measure in the following way</p><formula xml:id="formula_12">1 c p c q c r R p+q+r |f X,Y,Z (t, s, u) -f X (t)f Y (s)f Z (u)| 2 |t| 1+p p |s| 1+q q |u| 1+r r dtdsdu ,</formula><p>where X ∈ R p , Y ∈ R q and Z ∈ R r . Assuming that the integral above exists, one can easily verify that such a measure completely characterizes joint independence among X, Y and Z. However, it does not admit a nice equivalent representation as in (4) (unless one considers a different weighting function).</p><p>We exploit this equivalent representation of the dth order dCov to propose a V-statistic type estimator of the population quantity (see Section 3) which is much simpler to compute rather than evaluating an integral as in the original definition in (3).</p><p>Remark 2.2. <ref type="bibr" target="#b28">Székely et al. (2009)</ref> introduced the notion of covariance with respect to a stochastic process. Theorem 8 in <ref type="bibr" target="#b28">Székely et al. (2009)</ref> shows that population distance covariance coincides with the covariance with respect to Brownian motion (or the so-called Brownian covariance). The Brownian covariance of two random variables</p><formula xml:id="formula_13">X ∈ R p and Y ∈ R q with E(|X| 2 + |Y | 2 ) &lt; ∞ is defined as the positive square root of W 2 (X, Y ) = Cov 2 W (X, Y ) = E[X W X W Y W Y W ] ,</formula><p>where W and W are independent Brownian motions with zero mean and covariance function C(t, s) = |s| + |t| -|s -t| on R p and R q respectively, and</p><formula xml:id="formula_14">X W = W (X) -E[ W (X)|W ] . Conditional on W (or W ), X W (or Y W ) is an i.i.d. copy of X W (or Y W ).</formula><p>Then following Theorem 8</p><p>in <ref type="bibr" target="#b28">Székely et al. (2009)</ref> and Definition 2.1, we have</p><formula xml:id="formula_15">dCov 2 (X, Y ) = W 2 (X, Y ) . Now for d ≥ 2 random variables {X 1 , X 2 , . . . , X d } where X i ∈ R p i , 1 ≤ i ≤ d, we can generalize</formula><p>the notion of Brownian covariance as the positive square root of</p><formula xml:id="formula_16">W 2 (X 1 , . . . , X d ) = E d i=1 X i W i X i W i ,</formula><p>where W i 's are independent Brownian motions on</p><formula xml:id="formula_17">R p i , 1 ≤ i ≤ d. Property (2) in Proposition 2.1</formula><p>below establishes the connection between the higher order distance covariances and the generalized notion of Brownian covariance.</p><p>Similar to dCov, our definition of high order dCov possesses the following important properties.</p><p>Proposition 2.1. We have the following properties regarding dCov(X 1 , X 2 , . . . , X d ):</p><p>(1) For any a i ∈ R p i , c i ∈ R, and orthogonal transformations</p><formula xml:id="formula_18">A i ∈ R p i ×p i , dCov 2 (a 1 +c 1 A 1 X 1 , . . . , a d + c d A d X d ) = d i=1 |c i | dCov 2 (X 1 , . . . , X d ).</formula><p>Moreover, dCov is invariant to any permutation of {X 1 , X 2 , . . . , X d }.</p><p>(2) Under Assumption 3.1 (see Section 3), the dth order dCov exists and</p><formula xml:id="formula_19">W 2 (X 1 , . . . , X d ) = dCov 2 (X 1 , . . . , X d ) .</formula><p>Property (1) shows that dCov is invariant to translation, orthogonal transformation and permutation on X i s. In property (2), the existence of the dth order dCov follows from (4) and application of Fubini's Theorem and Hölder's inequality. The equality with Brownian covariance readily follows from the proof of Theorem 7 in <ref type="bibr" target="#b28">Székely et al. (2009)</ref>.</p><p>Theorem 7 in <ref type="bibr" target="#b27">Székely et al. (2007)</ref> shows the relationship between distance correlation and the correlation coefficient for bivariate normal distributions. We extend that result in case of multivariate normal random variables with zero mean, unit variance and pairwise correlation ρ. Proposition 2.2 below establishes a relationship between the correlation coefficient and higher order distance covariances for multivariate normal random variables.</p><formula xml:id="formula_20">Proposition 2.2. Suppose (X 1 , X 2 , . . . , X d ) ∼ N (0, Σ), where Σ = (σ i,j ) d i,j=1 with σ ii = 1 for 1 ≤ i ≤ d and σ ij = ρ for 1 ≤ i = j ≤ d. When d = 2k -1 or d = 2k, dCov 2 (X 1 , . . . , X d ) = O(|ρ| 2k ) for k ≥ 2.</formula><p>Proposition 9.1 in the supplementary materials shows some additional properties of the dth order dCov.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Joint distance covariance</head><p>In this subsection, we introduce a new joint dependence measure called the joint dCov (Jdcov), which is designed to capture all types of interaction dependence among the d random vectors. To achieve this goal, we define JdCov as the linear combination of all kth order dCov for 1 ≤ k ≤ d.</p><p>Definition 2.2. The JdCov among {X 1 , . . . , X d } is given by</p><formula xml:id="formula_21">JdCov 2 (X 1 , . . . , X d ; C 2 , . . . , C d ) =C 2 (i 1 ,i 2 )∈I d 2 dCov 2 (X i 1 , X i 2 ) + C 3 (i 1 ,i 2 ,i 3 )∈I d 3 dCov 2 (X i 1 , X i 2 , X i 3 ) + • • • + C d dCov 2 (X 1 , . . . , X d ),<label>(5)</label></formula><p>for some nonnegative constants C i ≥ 0 with 2 ≤ i ≤ d.</p><p>Proposition 2.3 below states that JdCov completely characterizes joint independence among</p><formula xml:id="formula_22">{X 1 , . . . , X d }. Proposition 2.3. Suppose C i &gt; 0 for 2 ≤ i ≤ d. Then JdCov 2 (X 1 , . . . , X d ; C 2 , . . . , C d ) = 0 if and only if {X 1 , . . . , X d } are mutually independent.</formula><p>Next we show that by properly choosing C i s, JdCov 2 (X 1 , . . . , X d ; C 2 , . . . , C d ) has a relatively simple expression, which does not require the evaluation of 2 d -d -1 dCov terms in its original definition <ref type="bibr" target="#b26">(5)</ref>. Specifically, let C i = c d-i for c ≥ 0 in the definition of JdCov and denote JdCov 2 (X 1 , . . . , X d ; c) = JdCov 2 (X 1 , . . . , X d ; c d-2 , c d-1 , . . . , 1). Then, we have the following result.</p><p>Proposition 2.4. For any c ≥ 0,</p><formula xml:id="formula_23">JdCov 2 (X 1 , . . . , X d ; c) = E d i=1 (U i (X i , X i ) + c) -c d . In particular, JdCov 2 (X 1 , X 2 ; c) = E[U 1 (X 1 , X 1 )U 2 (X 2 , X 2 )] = dCov 2 (X 1 , X 2 ).</formula><p>By <ref type="bibr" target="#b26">(5)</ref>, the dependence measured by JdCov can be decomposed into the main effect term</p><formula xml:id="formula_24">(i 1 ,i 2 )∈I d 2 dCov 2 (X i 1 , X i 2 )</formula><p>quantifying the pairwise dependence as well as the higher order effect terms</p><formula xml:id="formula_25">(i 1 ,i 2 ,...,i k )∈I d k dCov 2 (X i 1 , X i 2 , .</formula><p>. . , X i k ) quantifying the multi-way interaction dependence among any k-tuples. The choice of c reflects the relative importance of the main effect and the higher order effects.</p><p>For c ≥ 1, C i = c d-i is nonincreasing in i. Thus, the larger c we select, the smaller weights we put on the higher order terms. In particular, we have</p><formula xml:id="formula_26">lim c→+∞ c 2-d JdCov 2 (X 1 , . . . , X d ; c) = (i 1 ,i 2 )∈I d 2 dCov 2 (X i 1 , X i 2 ),</formula><p>that is JdCov reduces to the main effect term as c → +∞. We remark that the main effect term fully characterizes joint dependence in the case of elliptical distribution and it has been recently used in <ref type="bibr" target="#b34">Yao et al. (2018)</ref> to test mutual independence for high-dimensional data. On the other hand, JdCov becomes the dth order dCov as c → 0, i.e., lim c→0 JdCov 2 (X 1 , . . . , X d ; c) = dCov 2 (X 1 , . . . , X d ).</p><p>The choice of c depends on the types of interaction dependence of interest as well as the specific scientific problem, and thus is left for the user to decide.</p><p>It is worth noting that JdCov 2 (X 1 , . . . , X d ; c) depends on the scale of X i . To obtain a scaleinvariant metric, one can normalize U i by the corresponding distance variance. Specifically, when dCov(X i ) := dCov(X i , X i ) &gt; 0, the resulting quantity is given by,</p><formula xml:id="formula_27">JdCov 2 S (X 1 , . . . , X d ; c) = E d i=1 U i (X i , X i ) dCov(X i ) + c -c d ,</formula><p>which is scale-invariant. Another way to obtain a scale-invariant metric is presented in Section 2.4</p><p>based on the idea of rank transformation.</p><p>Below we present some basic properties of JdCov, which follow directly from Proposition 2.1.</p><p>Proposition 2.5. We have the following properties regarding JdCov:</p><p>(1) For any a i ∈ R p i , c 0 ∈ R, and orthogonal transformations</p><formula xml:id="formula_28">A i ∈ R p i ×p i , JdCov 2 (a 1 +c 0 A 1 X 1 , . . . , a d + c 0 A d X d ; |c 0 |c) = |c 0 | d JdCov 2 (X 1 , . . . , X d ; c). Moreover, JdCov is invariant to any permutation of {X 1 , X 2 , . . . , X d }.</formula><p>(2) For any a i ∈ R p i , c i = 0, and orthogonal transformations</p><formula xml:id="formula_29">A i ∈ R p i ×p i , JdCov 2 S (a 1 +c 1 A 1 X 1 , . . . , a d + c d A d X d ; c) = JdCov 2 S (X 1 , . . . , X d ; c).</formula><p>Remark 2.3. A natural question to ask is what should be a data driven way to choose the tuning parameter c. Although we leave it for future research, here we present a heuristic idea of choosing c.</p><p>In the discussion below Proposition 2.4, we pointed out that choosing c &gt; 1 (or &lt; 1) puts lesser (or higher) weightage on the higher order effects. Note that if the data is Gaussian, testing for the mutual independence of {X 1 , . . . , X d } is equivalent to testing for their pairwise independences. In that case, intuitively one should choose a larger (&gt; 1) value of c. If, however, the data is non-Gaussian, it might be of interest to look into higher order dependencies and thus a smaller (&lt; 1) choice of c makes sense.</p><p>To summarize, a heuristic way to choose the tuning parameter c could be :</p><formula xml:id="formula_30">Choose c      &gt; 1, if {X 1 , . . . , X d } are jointly Gaussian &lt; 1, if {X 1 , . . . , X d } are not jointly Gaussian.<label>(6)</label></formula><p>There is a huge literature on testing for joint normality of random vectors. It has been shown that the test based on energy distance is consistent against fixed alternatives <ref type="bibr" target="#b25">(Székely and Rizzo, 2004</ref>) and shows higher empirical power compared to several competing tests <ref type="bibr" target="#b26">(Székely and Rizzo, 2005;</ref><ref type="bibr" target="#b13">2013)</ref>.</p><p>Suppose p is the p-value of the energy distance based test for joint normality of {X 1 , . . . , X d } at level α. We expect c to increase (or decrease) from 1 as p &gt; (or &lt;) α, so one heuristic choice of c can be</p><formula xml:id="formula_31">c = 1 + sign(p -α) × |p -α| 1/4 ,<label>(7)</label></formula><p>where sign(x) = 1, 0 or -1 depending on whether x &gt; 0, x = 0 or x &lt; 0. For example, p = (0.001, 0.03, 0.0499, 0.0501, 0.1, 0.3) and α = 0.05 yields c = (0.53, 0.62, 0.9, 1.1, 1.47, 1.71).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Distance cumulant and distance characteristic function</head><p>As noted in <ref type="bibr" target="#b24">Streitberg (1990)</ref>, for d ≥ 4, the Lancaster interaction measure fails to capture all possible factorizations of the joint distribution. For example, it may not vanish if (X 1 , X 2 ) ⊥ ⊥ (X 3 , X 4 ). <ref type="bibr" target="#b24">Streitberg (1990)</ref> corrected the definition of Lancaster interaction measure using a more complicated construction, which essentially corresponds to the cumulant version of dCov in our context. Specifically, <ref type="bibr" target="#b24">Streitberg (1990)</ref> proposed a corrected version of Lancaster interaction as follows</p><formula xml:id="formula_32">∆F = π (-1) |π|-1 (|π| -1)! D∈π F D ,</formula><p>where π is a partition of the set {1,2,. . . ,d}, |π| denotes the number of blocks of the partition π and F D denotes the joint distribution of {X i : i ∈ D}. It has been shown in <ref type="bibr" target="#b24">Streitberg (1990)</ref> that ∆F = 0 whenever F is decomposable. Our definition of joint distance cumulant of {X 1 , . . . , X d } below can be viewed as the dCov version of Streitberg's correction.</p><formula xml:id="formula_33">Definition 2.3. The joint distance cumulant among {X 1 , . . . , X d } is defined as cum(X 1 , . . . , X d ) = π (-1) |π|-1 (|π| -1)! D∈π E i∈D U i (X i , X i ) ,<label>(8)</label></formula><p>where π runs through all partitions of {1, 2, . . . , d}.</p><p>It is not hard to verify that cum(X 1 , . . . , X d ) = 0 if {X 1 , . . . , X d } can be decomposed into two mutually independent groups say (X i ) i∈π 1 and (X j ) j∈π 2 with π 1 and π 2 being a partition of {1, 2, . . . , d}.</p><p>We further define the distance characteristic function.</p><p>Definition 2.4. The joint distance characteristic function among {X 1 , . . . , X d } is defined as</p><formula xml:id="formula_34">dcf (t 1 , . . . , t d ) = E exp ı d i=1 t i U i (X i , X i ) ,<label>(9)</label></formula><p>for t 1 , . . . , t d ∈ R.</p><p>The following result shows that distance cumulant can be interpreted as the coefficient of the Taylor expansion of the log distance characteristic function.</p><p>Proposition 2.6. The joint distance cumulant cum(X i 1 , . . . , X is ) is given by the coefficient of ı s s k=1 t i k in the Taylor expansion of log {dcf (t 1 , . . . , t d )}, where {i 1 , . . . , i s } is any subset of {1, 2, . . . , d} with s ≤ d.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Our next result indicates that the mutual independence among</head><formula xml:id="formula_35">{X 1 , . . . , X d } is equivalent to the mutual independence among {U 1 (X 1 , X 1 ), . . . , U d (X d , X d )}. Proposition 2.7. The random variables {X 1 , . . . , X d } are mutually independent if and only if dcf (t 1 , . . . , t d ) = d i=1 dcf (t i ) for t i almost everywhere, where dcf (t i ) = E[exp{ıt i U i (X i , X i )}].</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Rank-based metrics</head><p>In this subsection, we briefly discuss the concept of rank-based distance measures. For simplicity, we assume that X i s are all univariate and remark that our definition can be generalized to the case where X i s are random vectors without essential difficulty. The basic idea here is to apply the monotonic transformation based on the marginal distribution functions to each X j , and then use the dCov or JdCov to quantify the interaction and joint dependence of the coordinates after transformation.</p><p>Therefore it can be viewed as the counterpart of Spearman's rho to dCov or JdCov.</p><p>Let F j be the marginal distribution function for X j . The squared rank dCov and JdCov among {X 1 , . . . , X d } are defined respectively as</p><formula xml:id="formula_36">dCov 2 R (X 1 , . . . , X d ) = dCov 2 (F 1 (X 1 ), . . . , F d (X d )), JdCov 2 R (X 1 , . . . , X d ; c) = JdCov 2 (F 1 (X 1 ), . . . , F d (X d ); c).</formula><p>The rank-based dependence metrics enjoy a few appealing features: (1) they are invariant to monotonic component wise transformations;</p><p>(2) they are more robust to outliers and heavy tail of the distribution;</p><p>(3) their existence require very weak moment assumption on the components of X . In Section 5, we shall compare the finite sample performance of JdCov 2 R with that of JdCov and JdCov S .</p><p>Comparison of various distance metrics for measuring joint dependence of d ≥ 2 random vectors of arbitrary dimensions :</p><formula xml:id="formula_37">Distance metrics Complete characterization Permutation Scale of joint independence invariance invariance dHSIC × × × (for fixed bandwidth) T M T × × × × × × High order dCov × × × (Captures Lancaster interactions) × × × JdCov × × × JdCov S JdCov R</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Estimation</head><p>We now turn to the estimation of the joint dependence metrics. Given n samples {X j } n j=1 with X j = (X j1 , . . . , X jd ), we consider the plug-in estimators based on the V-statistics as well as their bias-corrected versions to be described below. Denote by fi (t i ) = n -1 n j=1 e ı t i ,X ji the empirical characteristic function for X i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Plug-in estimators</head><formula xml:id="formula_38">For 1 ≤ k, l ≤ n, let U i (k, l) = n -1 n v=1 |X ki -X vi | + n -1 n u=1 |X ui -X li | -|X ki -X li | - n -2 n u,v=1 |X ui -X vi | be the sample estimate of U i (X ki , X li ).</formula><p>The V-statistic type estimators for dCov, JdCov and its scale-invariant version are defined respectively as,</p><formula xml:id="formula_39">dCov 2 (X 1 , . . . , X d ) = 1 n 2 n k,l=1 d i=1 U i (k, l) 2 , (<label>10</label></formula><formula xml:id="formula_40">) JdCov 2 (X 1 , . . . , X d ; c)) = 1 n 2 n k,l=1 d i=1 U i (k, l) + c -c d ,<label>(11)</label></formula><formula xml:id="formula_41">JdCov 2 S (X 1 , . . . , X d ; c) = 1 n 2 n k,l=1 d i=1 U i (k, l) dCov(X i ) + c -c d ,<label>(12)</label></formula><p>where dCov 2 (X i ) = n -2 n k,l=1 U i (k, l) 2 is the sample (squared) dCov. The following lemma shows that the V-statistic type estimators are equivalent to the plug-in estimators by replacing the characteristic functions and the expectation in the definitions of dCov and JdCov with their sample counterparts.</p><p>Lemma 3.1. The sample (squared) dCov can be rewritten as,</p><formula xml:id="formula_42">dCov 2 (X 1 , . . . , X d ) = R p 0 1 n n k=1 d i=1 ( fi (t i ) -e ı t i ,X ki ) 2 dw.</formula><p>(13)</p><p>Moreover, we have</p><formula xml:id="formula_43">JdCov 2 (X 1 , . . . , X d ; c) =c d-2 (i 1 ,i 2 )∈I d 2 dCov 2 (X i 1 , X i 2 ) + c d-3 (i 1 ,i 2 ,i 3 )∈I d 3 dCov 2 (X i 1 , X i 2 , X i 3 ) + • • • + dCov 2 (X 1 , . . . , X d ). (<label>14</label></formula><formula xml:id="formula_44">)</formula><p>Remark 3.1. Consider the univariate case where p i = 1 for all 1 ≤ i ≤ d. Let F i be the empirical distribution based on {X ji } n j=1 and define Z ji = F i (X ji ). Then, the rank-based metrics defined in Section 2.4 can be estimated in a similar way by replacing X ji with Z ji in the definitions of the above estimators.</p><p>Remark 3.2. The distance cumulant can be estimated by</p><formula xml:id="formula_45">cum(X 1 , . . . , X d ) = π (-1) |π|-1 (|π| -1)! D∈π 1 n 2 n k,l=1 i∈D U i (k, l) .</formula><p>However, the combinatorial nature of distance cumulant implies that detecting interactions of higher order requires significantly more costly computation.</p><p>We study the asymptotic properties of the V-statistic type estimators under suitable moment assumptions.</p><p>Proposition 3.1. Under Assumption 3.1 , we have as n → ∞,</p><formula xml:id="formula_46">dCov 2 (X 1 , • • • , X d ) a.s -→ dCov 2 (X 1 , • • • , X d ), JdCov 2 (X 1 , • • • , X d ; c) a.s -→ JdCov 2 (X 1 , . . . , X d ; c), JdCov 2 S (X 1 , • • • , X d ; c) a.s -→ JdCov 2 S (X 1 , . . . , X d ; c),</formula><p>where " a.s -→ " denotes the almost sure convergence. <ref type="bibr" target="#b27">Székely et al. (2007)</ref>. Suppose X i s are mutually independent. Then Assumption 3.1 is fulfilled</p><formula xml:id="formula_47">When d = 2, Assumption 3.1 reduces to the condition that E|X 1 | &lt; ∞ and E|X 2 | &lt; ∞ in Theorem 2 of</formula><formula xml:id="formula_48">provided that E|X i | &lt; ∞ for all i. More generally, if E|X i | (d+1)/2 &lt; ∞ for 1 ≤ i ≤ d, then Assumption 3.1 is satisfied.</formula><p>Let Γ(•) denote a complex-valued zero mean Gaussian random process with the covariance function</p><formula xml:id="formula_49">R(t, t ) = d i=1 f i (t i -t i ) -f i (t i )f i (-t i ) , where t = (t 1 , t 2 , . . . , t d ), t = (t 1 , t 2 , . . . , t d ) ∈ R p 1 × R p 2 × • • • × R p d . Proposition 3.2. Suppose X 1 , X 2 , . . . , X d are mutually independent, and E|X i | &lt; ∞ for 1 ≤ i ≤ d. Then we have n dcov 2 (X 1 , X 2 , • • • , X d ) d -→ Γ 2 = +∞ j=1 λ j Z 2 j ,</formula><p>where</p><formula xml:id="formula_50">||Γ|| 2 = Γ(t 1 , t 2 , . . . , t d ) 2 dw, Z j i.i.d</formula><p>∼ N (0, 1) and λ j &gt; 0 depends on the distribution of X . As a consequence, we have</p><formula xml:id="formula_51">n Jdcov 2 (X 1 , X 2 , • • • , X d ; c) d -→ +∞ j=1 λ j Z 2 j ,</formula><p>with λ j &gt; 0 and Z j i.i.d</p><p>∼ N (0, 1).</p><p>Proposition 3.2 shows that both dcov 2 and Jdcov 2 converge to weighted sum of chi-squared random variables, where the weights depend on the marginal characteristic functions in a complicated way.</p><p>Since the limiting distribution is non-pivotal, we will introduce a bootstrap procedure to approximate their sampling distributions in the next section.</p><p>It has been pointed out in the literature that the computational complexity of dCov is O(n 2 ) if it is implemented directly according to its definition. The computational cost of the V-statistic type estimators and the bias-corrected estimators for JdCov are both of the order O(n 2 p 0 ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Bias-corrected estimators</head><p>It is well known that V-statistic leads to biased estimation. To remove the bias, one can construct an estimator for the dth order dCov based on a dth order U-statistic. However, the computational complexity for the dth order U-statistic is of the order O(dn d ), which is computationally prohibitive when n and d are both large. Adopting the U-centering idea in <ref type="bibr" target="#b31">Székely and Rizzo (2014)</ref>, we propose bias-corrected estimators which do not bring extra computational cost as compared to the plug-in estimators. Specifically, for 1</p><formula xml:id="formula_52">≤ i ≤ d, we define the U-centered version of |X ki -X li | as U i (k, l) = 1 n -2 n u=1 |X ui -X li | + 1 n -2 n v=1 |X ki -X vi | -|X ki -X li | - 1 (n -1)(n -2) n u,v=1 |X ui -X vi | when k = l, and U i (k, l) = 0 when k = l. One can verify that v =k U i (k, v) = u =l U i (u, l) = 0, which mimics the double-centered property E[U i (X i , X i )|X i ] = E[U i (X i , X i )|X i ] = 0 for its population coun- terpart. Let dCov 2 (X i , X j ) = k =l U i (k, l) U j (k, l)/{n(n -3)} and write dCov(X i ) = dCov(X i , X i ).</formula><p>We define the bias-corrected estimators as,</p><formula xml:id="formula_53">JdCov 2 (X 1 , . . . , X d ; c) = 1 n(n -3) n k,l=1 d i=1 U i (k, l) + c - n n -3 c d , JdCov 2 S (X 1 , . . . , X d ; c) = 1 n(n -3) n k,l=1 d i=1 U i (k, l) dCov(X i ) + c - n n -3 c d .</formula><p>Direct calculation yields that</p><formula xml:id="formula_54">JdCov 2 (X 1 , . . . , X n ; c) = c d-2 (i,j)∈I d 2 dCov 2 (X i , X j ) + higher order terms. (<label>15</label></formula><formula xml:id="formula_55">)</formula><p>It has been shown in Proposition 1 of Székely and Rizzo (2014) that dCov 2 (X i , X j ) is an unbiased estimator for dCov 2 (X i , X j ). In the supplementary material, we provide an alternative proof which simplifies the arguments in <ref type="bibr" target="#b31">Székely and Rizzo (2014)</ref>. Our argument relies on a new decomposition of U i (k, l), which provides some insights on the U-centering idea. See Lemma 9.1 and Proposition 9.2 in the supplementary material. In view of (15) and Proposition 9.2, the main effect in JdCov 2 (X 1 , . . . , X n ; c) can be unbiasedly estimated by the main effect of JdCov 2 (X 1 , . . . , X n ; c). However, it seems very challenging to study the impact of U-centering on the bias of the high order effect terms. We shall leave this problem to our future research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Testing for joint independence</head><p>In this section, we consider the problem of testing the null hypothesis</p><formula xml:id="formula_56">H 0 : X 1 , . . . , X d are mutually independent (16)</formula><p>against the alternative H A : negation of H 0 . For the purpose of illustration, we use n JdCov 2 as our test statistic and set</p><formula xml:id="formula_57">φ n (X 1 , . . . , X n ) :=      1 if n JdCov 2 (X 1 , . . . , X d ) &gt; c n , 0 if n JdCov 2 (X 1 , . . . , X d ) ≤ c n ,<label>(17)</label></formula><p>where the threshold c n remains to be chosen. Consequently, we define a decision rule as follows: reject</p><formula xml:id="formula_58">H 0 if φ n = 1 and fail to reject H 0 if φ n = 0.</formula><p>Below we introduce a bootstrap procedure to approximate the sampling distribution of n JdCov under H 0 . Let F i be the empirical distribution function based on the data points {X ji } n j=1 . Conditional on the original sample, we define X * j = (X * j1 , . . . , X * jd ), where X * ji are generated independently from F i for 1 ≤ i ≤ d. Let {X * j } n j=1 be n bootstrap samples. Then we can compute the bootstrap statistics dCov 2 * and JdCov 2 * in the same way as dCov 2 and JdCov 2 based on {X * j } n j=1 . In particular, we note that the bootstrap version of the dth order dCov is given by</p><formula xml:id="formula_59">n dCov 2 * (X 1 , . . . , X d ) = Γ * n 2 = Γ * n (t 1 , . . . , t d ) 2 dw, where Γ * n (t) = n -1/2 n j=1 d i=1 ( f * i (t i ) -e ı t i ,X * ji ).</formula><p>Denote by " d * -→ " the weak convergence in the bootstrap world conditional on the original sample</p><formula xml:id="formula_60">{X j } n j=1 . Proposition 4.1. Suppose E|X i | &lt; ∞ for 1 ≤ i ≤ d. Then n dCov 2 * (X 1 , . . . , X d ) d * -→ +∞ j=1 λ j Z 2 j , n JdCov 2 * (X 1 , . . . , X d ) d * -→ +∞ j=1 λ j Z 2 j ,</formula><p>almost surely as n → ∞.</p><p>Proposition 4.1 shows that the bootstrap statistic is able to imitate the limiting distribution of the test statistic. Thus, we shall choose c n to be the 1 -α quantile of the distribution of n JdCov 2 * conditional on the sample {X j } n j=1 . The validity of the bootstrap-assisted test can be justified as follows.</p><p>Proposition 4.2. For all α ∈ (0, 1), the α-level bootstrap-assisted test has asymptotic level α when testing H 0 against H A . In other words, under H 0 , lim sup</p><formula xml:id="formula_61">n→∞ P ( φ n (X 1 , . . . , X n ) = 1 ) = α .</formula><p>Proposition 4.3. For all α ∈ (0, 1), the α-level bootstrap-assisted test is consistent when testing H 0 against H A . In other words, under H A , lim n→∞ P ( φ n (X 1 , . . . , X n ) = 1 ) = 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Numerical studies</head><p>We investigate the finite sample performance of the proposed methods. Our first goal is to test the joint independence among the variables {X 1 , . . . , X d } using the new dependence metrics, and compare the performance with some existing alternatives in the literature in terms of size and power.</p><p>Throughout the simulation, we set c = 0.5, 1, 2 in JdCov and implement the bootstrap-assisted test based on the bias-corrected estimators. We compare our tests with the dHSIC-based test in <ref type="bibr" target="#b17">Pfister et al. (2018)</ref> and the mutual independence test proposed in <ref type="bibr" target="#b14">Matteson and Tsay (2017)</ref>, which is defined as</p><formula xml:id="formula_62">T M T := d-1 i=1 dCov 2 (X i , X (i+1):d ), (<label>18</label></formula><formula xml:id="formula_63">)</formula><p>where X (i+1):d = {X i+1 , X i+2 , . . . , X d }. We consider both Gaussian and non-Gaussian distributions and study the following models, motivated from <ref type="bibr">Sejdinovic et al. (2013)</ref> and <ref type="bibr" target="#b34">Yao et al. (2018)</ref>.</p><p>Example 5.1. [Gaussian copula model] The data X = (X 1 , . . . , X d ) are generated as follows:</p><p>1. X ∼ N (0, I d );</p><p>2. X = Z 1/3 and Z ∼ N (0, I d );</p><p>3. X = Z 3 and Z ∼ N (0, I d ).</p><p>Example 5.2. [Multivariate Gaussian model] The data X = (X 1 , . . . , X d ) are generated from the multivariate normal distribution with the following three covariance matrices Σ = (σ ij (ρ)) d i,j=1 with ρ = 0.25:</p><p>1. AR(1): σ ij = ρ |i-j| for all i, j ∈ {1, . . . , d};</p><p>2. Banded:</p><formula xml:id="formula_64">σ ii = 1 for i = 1, . . . , d; σ ij = ρ if 1 ≤ |i -j| ≤ 2 and σ ij = 0 otherwise; 3. Block: Define Σ block = (σ ij ) 5 i,j=1 with σ ii = 1 and σ ij = ρ if i = j. Let Σ = I d/5 ⊗ Σ block</formula><p>, where ⊗ denotes the Kronecker product. Example 5.4. In this example, we consider a triplet of random vectors (X, Y, Z) on</p><formula xml:id="formula_65">R p × R p × R p , with X, Y i.i.d</formula><p>∼ N (0, I p ). We focus on the following cases :</p><formula xml:id="formula_66">1. Z 1 = sign(X 1 Y 1 ) W and Z 2:p ∼ N (0, I p-1 )</formula><p>, where W follows an exponential distribution with mean √ 2;</p><p>2. Z 2:p ∼ N (0, I p-1 ) and</p><formula xml:id="formula_67">Z 1 =            X 2 1 + , with probability 1/3, Y 2 1 + , with probability 1/3, X 1 Y 1 + , with probability 1/3,</formula><p>where ∼ U (-1, 1).</p><p>We conduct tests for joint independence among the random variables described in the above examples. For each example, we draw 1000 simulated datasets and perform tests of joint independence with 500 bootstrap resamples. We try small and moderate sample sizes, i.e., n = 50, 100 or 200. Figure <ref type="figure" target="#fig_4">1</ref> and Figure <ref type="figure" target="#fig_5">2</ref> display the proportion of rejections (out of 1000 simulation runs) for the five different tests, based on the statistics JdCov 2 , JdCov 2 S , JdCov 2 R , dHSIC and T M T . The detailed figures are reported in Tables <ref type="table">2</ref> and<ref type="table">3</ref> in the supplementary materials.</p><p>In Example 5.1, the data generating scheme suggests that the variables are jointly independent.</p><p>The plots in Figure <ref type="figure" target="#fig_4">1</ref>  S outperforms all the others in a majority of the cases. In Examples 5.3 and 5.4, the power becomes higher when c decreases to 0.5. These results are consistent with our statistical intuition and the discussions in Section 2.2. For the Gaussian copula model, only the main effect term matters, so a larger c is preferable. For non-Gaussian models, the high order terms kick in and hence a smaller c may lead to higher power.</p><p>Remark 5.1. We have considered U-statistic type estimators of JdCov 2 , JdCov 2 S and JdCov 2 R so far in all the above computations, as they remove the bias due to the main effects (see Section 3.2).</p><p>However it might be interesting to see if the bias correction has any empirical impact. We conduct tests for joint independence of the random variables in some of the above examples, this time using the V-statistic type estimators (described in Section 3.1). Table <ref type="table">4 (</ref> The plots in Figure <ref type="figure" target="#fig_4">1</ref> and Figure <ref type="figure" target="#fig_5">2</ref> reveal some interesting features. In Example 5.2 we have Gaussian data, so a larger c is preferable. Clearly the proportion of rejections are a little higher (or lower) in most of the cases when we choose c in the data-driven way (c turns out to be around 1.6 or 1.7), than when c is subjectively chosen to be 0.5 (or 2). On the contrary, in Example 5.3, the data is non-Gaussian and a smaller c is preferable. Evidently choosing c in the data-driven way leads to nearly equally good power compared to when c = 0.5, and higher power compared to when c = 2.</p><p>6 Application to causal inference</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Model diagnostic checking for Directed Acyclic Graph (DAG)</head><p>We employ the proposed metrics to perform model selection in causal inference which is based on the joint independence testing of the residuals from the fitted structural equation models. Specifically, given a candidate DAG G, we let Par(j) denote the index associated with the parents of the jth node.</p><p>Following <ref type="bibr" target="#b16">Peters et al. (2014)</ref> and <ref type="bibr" target="#b2">Bühlmann et al. (2014)</ref>, we consider the structural equation models with additive components</p><formula xml:id="formula_68">X j = k∈Par(j) f j,k (X k ) + j , j = 1, 2, . . . , d,<label>(19)</label></formula><p>where the noise variables 1 , . . . , d are jointly independent variables. Given n observations {X i } n i=1 with X i = (X i1 , . . . , X id ), we use generalized additive regression <ref type="bibr" target="#b33">(Wood and Augustin, 2002)</ref> to regress X j on all its parents {X k , k ∈ Par(j)} and denote the resulting residuals by  where fj,k is the B-spline estimator for f j,k . To check the goodness of fit of G, we test the joint independence of the residuals. Let T n be the statistic (e.g. JdCov 2 , JdCov 2 S or JdCov 2 R ) to test the joint dependence of ( 1 , . . . , d ) constructed based on the fitted residuals ˆ i = (ˆ i1 , . . . , ˆ id ) for 1 ≤ i ≤ n.</p><formula xml:id="formula_69">ˆ ij = X ij - k∈Par(j) fj,k (X ik ), 1 ≤ j ≤ d, 1 ≤ i ≤ n,<label>(a)</label></formula><p>Following the idea presented in <ref type="bibr" target="#b22">Sen and Sen (2014)</ref>, it seems that T n might have a limiting distribution different from the one mentioned in Proposition 3.2. So to approximate the sampling distribution of T n , we introduce the following residual bootstrap procedure.  <ref type="bibr" target="#b17">Pfister et al. (2018)</ref> proposed to bootstrap the residuals directly and used the bootstrapped residuals to construct the test statistic. In contrast, we suggest the use of the above residual bootstrap to capture the estimation effect caused by replacing f j,k with the estimate fj,k .</p><formula xml:id="formula_70">-1 B b=1 {T * b,n &gt; T n }.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Real data example</head><p>We now apply the model diagnostic checking procedure for DAG to one real world dataset. A population of women who were at least 21 years old, of Pima Indian heritage and living near Phoenix, Arizona, was tested for diabetes according to World Health Organization criteria. The data were collected by the US National Institute of Diabetes and Digestive and Kidney Diseases. We downloaded the data from <ref type="url" target="https://archive.ics.uci.edu/ml/datasets/Pima+Indians+Diabetes">https://archive.ics.uci.edu/ml/datasets/Pima+Indians+Diabetes</ref>. We focus only on the following five variables : Age, Body Mass Index (BMI), 2-Hour Serum Insulin (SI), Plasma Glucose Concentration (glu) and Diastolic Blood Pressure (DBP). Further, we only selected the instances with non-zero values, as it seems that zero values encode missing data. This yields n = 392 samples. Now, age is likely to affect all the other variables (but of course not the other way round). Moreover, serum insulin also has plausible causal effects on BMI and plasma glucose concentration. We try to determine the correct causal structure out of 48 candidate DAG models and perform model diagnostic checking for each of the 48 models, as illustrated in Section 6.1. We first center each of the variables and scale them so that l 2 norm of each of the variables is √ n. We perform the mutual independence test of residuals based on the statistics JdCov 2 , JdCov 2 S and JdCov 2 R with c = 1, and compare with the bootstrap-assisted version of the dHSIC-based test proposed in <ref type="bibr" target="#b17">Pfister et al. (2018)</ref> and T M T . For each of the tests, we implement the residual bootstrap to obtain the p-value with B = 1000. Figure <ref type="figure" target="#fig_7">3</ref> shows the selected DAG models corresponding to the largest p-values from each of the five tests.  Figure <ref type="figure" target="#fig_7">3a</ref> shows the model with the maximum p-value among all the 48 candidate DAG models, when the test for joint independence of the residuals is conducted based on JdCov 2 , JdCov 2 S and JdCov 2 R and T M T . This graphical structure goes in tune with the biological evidences of causal relationships among these five variables. Figure <ref type="figure" target="#fig_7">3b</ref> stands for the model with the maximum p-value when the test is based on dHSIC. Its only difference with Figure <ref type="figure" target="#fig_7">3a</ref> is that, it has an additional edge from glu to DBP, indicating a causal effect of Plasma Glucose Concentration on Diastolic Blood Pressure. We are unsure of any biological evidence that supports such a causal relationship in reality.</p><p>Remark 6.1. In view of Remark 2.3, it might be intriguing to take into account the heuristic datadriven way of determining c in the above example, instead of setting c at a default value of 1. Our findings indicate that choosing c in the data-driven way leads to a slightly different result. The tests based on dHSIC and JdCov 2 S select the DAG model shown in Figure <ref type="figure" target="#fig_7">3b</ref> (considering the maximum p-value among all the 48 candidate DAG models), whereas Figure <ref type="figure" target="#fig_7">3a</ref> is the DAG model selected when the test is based on JdCov 2 , JdCov 2 R and T M T . The proposed tests (based on JdCov 2 and JdCov 2 R ) still perform well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">A simulation study</head><p>We conduct a simulation study based on our findings in the previous real data example. To save the computational cost, we focus our attention on three of the five variables, viz. Age, glu and DBP.</p><p>In the correct causal structure among these three variables, there are directed edges from Age to glu and Age to DBP. We consider the additive structural equation models</p><formula xml:id="formula_71">X j = k∈Par(j) fj,k (X k ) + e j , j = 1, 2, 3,<label>(20)</label></formula><p>where X 1 , X 2 , X 3 correspond to Age, glu and DBP (after centering and scaling) respectively, and fj,k denotes the estimated function from the real data. Note that X 1 is the only variable without any parent. In Section 6.2, we get from our numerical studies that the standard deviation of X 1 is 1.001, and the standard deviations of the residuals when X 2 and X 3 are regressed on X 1 (according to the structural equation models in ( <ref type="formula" target="#formula_68">19</ref>), are 0.918 and 0.95, respectively. In this simulation study, we simulate X 1 from a zero mean Gaussian distribution with standard deviation 1. For X 2 and X 3 , we simulate the noise variables from zero mean Gaussian distributions with standard deviations 0.918 and 0.95, respectively. The same n = 392 is considered for the number of generated observations, and based on this simulated dataset we perform the model diagnostic checking for 27 candidate DAG models. The number of bootstrap replications is set to be B = 100 (to save the computational cost).</p><p>This procedure is repeated 100 times to note how many times out of 100 that the five tests select the correct model, based on the largest p-value. The results in Table <ref type="table">1</ref> indicate that the proposed tests with c = 1 and the dHSIC-based test outperform T M T .</p><p>Table <ref type="table">1</ref>: The number of times (out of 100) that the true model is being selected.</p><formula xml:id="formula_72">JdCov 2 JdCov 2 S JdCov 2 R dHSIC T M T 45 61 54 52 32</formula><p>Remark 6.2. A natural question to raise is why do we bootstrap the residuals and not test for the joint independence of the estimated residuals directly, to check for the goodness of fit of the DAG model. From the idea in <ref type="bibr" target="#b22">Sen and Sen (2014)</ref>, it appears that the joint distance covariance of the estimated residuals might have a limiting distribution different from the one stated in Proposition 3.2.</p><p>We leave the formulation of a rigorous theory in support of that for future research. We present below the models selected most frequently (out of 100 times) by the different test statistics if we repeat the simulation study done above in Section 6.3 without using residual bootstrap to re-estimate f j,k .</p><p>We immediately see that joint independence tests of the estimated residuals based on all of the five statistics we consider, select a DAG model that is meaningless and far away from the correct one. In the context of Remark 6.2, if we repeat the simulation study done in Section 6.3 (choosing c in the heuristic way), we still reach the same conclusion presented in Remark 6.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Discussions</head><p>Huo and Székely ( <ref type="formula">2016</ref>) proposed an O(n log n) algorithm to compute dCov of univariate random variables. In a more recent work, <ref type="bibr" target="#b9">Huang and Huo (2017)</ref> introduced a fast method for multivariate cases which is based on random projection and has computational complexity O(nK log n), where K is the number of random projections. One of the possible directions for future research is to come up with a fast algorithm to compute JdCov. When p i = 1, we can indeed use the method in <ref type="bibr" target="#b10">Huo and Székely (2016)</ref> to compute JdCov. But their method may be inefficient when d is large and it is not applicable to the case where p i &gt; 1. Another direction is, to introduce the notion of Conditional</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Supplementary materials</head><p>The supplementary materials contain some additional proofs of the main results in the paper and tabulated numerical results from Section 5.</p><p>Proof of Lemma 2.1. By Lemma 1 of <ref type="bibr" target="#b27">Székely et al. (2007)</ref>, we have</p><formula xml:id="formula_73">RHS = R p Ee ı t,X i -X i + e ı t,x-x -Ee t,x-X i -Ee ı t,X i -x w p i (t)dt =E R p cos( t, i -X i ) -1 + cos( t, x -x ) -1 + 1 -cos( t, x -X i ) + 1 -cos( t, X i -x ) w p i (t)dt + ı R E sin( t, X i -X i ) + sin( t, x -x ) -sin( t, x -X i ) -sin( t, X i -x ) w p i (t)dt =E|x -X i | + E|X i -x | -|x -x | -E|X i -X i | = U i (x, x ).</formula><p>Here we have used the fact that R {sin( t, X -X ) + sin( t, x -x ) -sin( t, x -X ) -sin( t, Xx )}w p i (t)dt = 0. ♦ Proof of Proposition 2.1. To show (1), notice that for a i , c i and orthogonal transformations</p><formula xml:id="formula_74">A i ∈ R p i ×p i , E i∈S U i (a i + c i A i X i , a i + c i A i X i ) = i∈S |c i | E i∈S U i (X i , X i ),</formula><p>where S ⊂ {1, 2, . . . , d}. The conclusion follows directly. ♦ Proof of Proposition 2.2. The proof is essentially similar to the proof of Lemma 1.2 in the supplementary material of <ref type="bibr" target="#b34">Yao et al. (2018)</ref>. It is easy to verify that</p><formula xml:id="formula_75">E d i=1 U i (X i , X i ) = C R d |A| 2 dt 1 t 2 1 . . . dt d t 2 d ,</formula><p>where C is some constant and</p><formula xml:id="formula_76">A = e -t 2 1 +•••+t 2 d 2 e -ρt 1 t 2 + e -ρt 1 t 3 + . . . d 2 similar terms -e -ρt 1 t 2 --ρt 1 t 3 -ρt 2 t 3 -e -ρt 1 t 2 -ρt 1 t 4 -ρt 2 t 4 -. . . d 3 similar terms + e -ρt 1 t 2 --ρt 1 t 3 -ρt 1 t 4 -ρt 2 t 3 -ρt 2 t 4 -ρt 3 t 4 + . . . d 4 similar terms + . . . -(d -1) . (<label>21</label></formula><formula xml:id="formula_77">)</formula><p>For example, if d ≥ 4 and we use the Taylor's expansion</p><formula xml:id="formula_78">e x = 1 + x + x 2 2! + ∞ l=3</formula><p>x l l! , then keeping in mind the multinomial theorem</p><formula xml:id="formula_79">(a 1 + • • • + a q ) 2 = 2 l 1 ,...,lq=0 l 1 +•••+lq=2 2! l 1 ! . . . l q ! , q ≥ 2 ,</formula><p>it is easy to check that the leading terms and their coefficients (upto some constants) are</p><p>Leading terms Coefficients (upto some constants)</p><formula xml:id="formula_80">t i t j 1 -d-2 1 + d-2 2 -. . . (= 0 for d &gt; 2) t 2 i t 2 j 1 -d-2 1 + d-2 2 -. . . (= 0 for d &gt; 2) t 2 i t j t k -1 + d-3 1 -d-3 2 + . . . (= 0 for d &gt; 3) t i t j t k t l 1 -d-4 1 + d-4 2 -. . . (= 1 if d = 4, = 0 for d &gt; 4)</formula><p>.</p><p>To get a non-trivial upper bound for E d i=1 U i (X i , X i ), we need to consider the Taylor's expansion</p><formula xml:id="formula_81">e x = 1 + x + x 2 2! + • • • + x k k! + ∞ l=k+1 x l l! , when d = 2k -1 or d = 2k, k ≥ 2,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>and the only leading term</head><p>in the Taylor's expansion of ( <ref type="formula" target="#formula_76">21</ref>) that would lead to a term with non-vanishing coefficient, is x k k! . To see this, note that when d = 4, i.e., k = 2, it is shown in Lemma 1.2 in the supplementary material of <ref type="bibr" target="#b34">Yao et al. (2018)</ref> that the only non-vanishing term is t 1 t 2 t 3 t 4 (upto some constants). Likewise for d = 5 and 6, the only non-vanishing leading terms (upto some constants) are : d k The only non-vanishing term (upto some constants) 5 3</p><p>(</p><formula xml:id="formula_82">t i t j ) 1 (t i t a ) 1 (t l t m ) 1 = t 2 i t j t a t l t m 6 3 (t i t j ) 1 (t a t l ) 1 (t m t n ) 1 = t i t j t a t l t m t n , i = j = a = l = m = n .</formula><p>In general when d = 2k -1, for k ≥ 2, the only non-vanishing term (upto some constants) is</p><formula xml:id="formula_83">t 2 i 1 t i 2 . . . t i d</formula><p>, where (i 1 , . . . , i d ) is any permutation of (1, 2, . . . , d). Suppose P d denotes the set of all possible permutations of (1, 2, . . . , d). Then</p><formula xml:id="formula_84">E d i=1 U i (X i , X i ) = c 0 R d |e -t 2 1 +•••+t 2 d 2 ( c 1 ρ k (i 1 ,...,i d )∈P d t 2 i 1 t i 2 . . . t i d + R ) | 2 dt 1 t 2 1 . . . dt d t 2 d = A 0 + A 1 + A 2 + A 3 ,</formula><p>where</p><formula xml:id="formula_85">A 0 = c0 ρ 2k (i 1 ,...,i d )∈P d R d e -(t 2 1 +•••+t 2 d ) t 4 i 1 t 2 i 2 . . . t 2 i d dt 1 t 2 1 . . . dt d t 2 d , A 1 = c1 |ρ| k (i 1 ,...,i d )∈P d R d e -(t 2 1 +•••+t 2 d ) t 2 i 1 t i 2 . . . t i d × R dt 1 t 2 1 . . . dt d t 2 d , A 2 = c2 ρ 2k (i 1 ,...,i d )∈P d R d e -(t 2 1 +•••+t 2 d ) t 3 i 1 t 3 i 2 t 2 i 3 . . . t 2 i d dt 1 t 2 1 . . . dt d t 2 d ,<label>and</label></formula><formula xml:id="formula_86">A 3 = c3 R d e -(t 2 1 +•••+t 2 d ) × R 2 dt 1 t 2 1 . . . dt d t 2 d , c 0 , c 1 , c0 , c1</formula><p>, c2 and c3 being some constants and R being the remainder term from the Taylor's expansion. Following the similar arguments of <ref type="bibr" target="#b34">Yao et al. (2018)</ref>, it can be shown that</p><formula xml:id="formula_87">A 0 = O(|ρ| 2k ) , A 1 = O(|ρ| 2k+1 ) , A 2 = O(|ρ| 2k ) and A 3 = O(|ρ| 2k+2 ) . Thus for d = 2k -1, k ≥ 2, E d i=1 U i (X i , X i ) = O(|ρ| 2k ) .</formula><p>And when d = 2k, for k ≥ 2, the only non-vanishing term (upto some constants) is</p><formula xml:id="formula_88">t 1 t 2 . . . t d . Consequently E d i=1 U i (X i , X i ) = c 0 R d |e -t 2 1 +•••+t 2 d 2 ( c 1 ρ k t 1 t 2 . . . t d + R ) | 2 dt 1 t 2 1 . . . dt d t 2 d = A 0 + A 1 + A 2 ,</formula><p>where</p><formula xml:id="formula_89">A 0 = c 0 ρ 2k R d e -(t 2 1 +•••+t 2 d ) t 2 1 t 2 2 . . . t 2 d dt 1 t 2 1 . . . dt d t 2 d , A 1 = c 1 |ρ| k R d e -(t 2 1 +•••+t 2 d ) t 1 t 2 . . . t d × R dt 1 t 2 1 . . . dt d t 2 d ,<label>and</label></formula><formula xml:id="formula_90">A 2 = c 2 R d e -(t 2 1 +•••+t 2 d ) × R 2 dt 1 t 2 1 . . . dt d t 2 d , c 0 , c 1 , c 0 , c<label>1</label></formula><p>and c 2 being some constants and R being the remainder term from the Taylor's expansion.</p><p>Again following the similar arguments of <ref type="bibr" target="#b34">Yao et al. (2018)</ref>, it can be shown that</p><formula xml:id="formula_91">A 0 = O(|ρ| 2k ) , A 1 = O(|ρ| 2k+1 ) and A 2 = O(|ρ| 2k+2 ) . Thus for d = 2k, k ≥ 2, E d i=1 U i (X i , X i ) = O(|ρ| 2k ) ,</formula><p>which completes the proof.</p><formula xml:id="formula_92">♦ Proposition 9.1. (1) dCov 2 (X 1 , . . . , X d ) ≤ E[ d j=1 min{a j (X j ), a j (X j )}] with a j (x) = max{E|X j - X j |, |E|X j -X j | -2E|x -X j ||}. For any partition S 1 ∪ S 2 = {1, 2, . . . , d} and S 1 ∩ S 2 = ∅, we have dCov 2 (X 1 , . . . , X d ) ≤ E[ i∈S 1 a j (X j )] E[ i∈S 2 a j (X j )]. (2) dCov 2 (X 1 , . . . , X d ) ≤ d i=1 E[|U i (X i , X i )| d ] 1/d . In particular, when d is even, dCov 2 (X 1 , . . . , X d ) ≤ d i=1 dCov 2 (X i , . . . , X i d ) 1/d .</formula><p>(3) Denote by µ j the uniform probability measure on the unit sphere S p j -1 . Then</p><formula xml:id="formula_93">dCov 2 (X 1 , . . . , X d ) = C d j=1 S p j -1 dCov 2 ( u 1 , X 1 , . . . , u d , X d )dµ 1 (u 1 ) • • • dµ d (u d ),</formula><p>and</p><formula xml:id="formula_94">JdCov 2 (X 1 , . . . , X d ; c) =C d j=1 S p j -1 JdCov 2 ( u 1 , X 1 , . . . , u d , X d ; c)dµ 1 (u 1 ) • • • dµ d (u d ),</formula><p>for some positive constants C and C .</p><p>Proof of Proposition 9.1. To prove (1), we have by the triangle inequality</p><formula xml:id="formula_95">|E[|X j -x |] -|x -x || ≤ E[|x -X j |].</formula><p>Thus we have |U j (x, x )| ≤ min{a j (x), a j (x )}, which implies that</p><formula xml:id="formula_96">E d j=1 U j (X j , X j ) ≤ E d j=1 min{a j (X j ), a j (X j )} .</formula><p>For any partition S 1 ∪ S 2 = {1, 2, . . . , d} and S 1 ∩ S 2 = ∅, using the independence between X j and X j , we get</p><formula xml:id="formula_97">dCov 2 (X 1 , X 2 , . . . , X d ) ≤ E i∈S 1 a j (X j ) E i∈S 2 a j (X j ) .</formula><p>(2) follows from the Hölder's inequality directly. Finally, by the change of variables: t 1 = r i u i where r i ∈ (0, +∞) and u i ∈ S p i -1 , we have</p><formula xml:id="formula_98">dCov 2 (X 1 , X 2 , . . . , X d ) = R p 0 E d i=1 (f i (t i ) -e ı t i ,X i ) 2 dw =C 1 S p 1 + • • • S p d + +∞ -∞ • • • +∞ -∞ E d i=1 (Ee ır i u i ,X i -e ır i u i ,X i ) 2 d i=1 dµ i (u i )dr i =C 2 S p 1 + • • • S p d + JdCov 2 ( u 1 , X 1 , . . . , u d , X d ; c)dµ 1 (u 1 ) • • • dµ d (u d ) =C 3 S p 1 • • • S p d JdCov 2 ( u 1 , X 1 , . . . , u d , X d ; c)dµ 1 (u 1 ) • • • dµ d (u d ),</formula><p>where C 1 , C 2 , C 3 are some positive constants. ♦ Property (1) gives an upper bound for dCov 2 (X 1 , X 2 , . . . , X d ), which is motivated by Lemma 2.1 of <ref type="bibr" target="#b13">Lyons (2013)</ref>, whereas an alternative upper bound is given in Property (2) which follows directly from the Hölder's inequality. Property (3) allows us to represent dCov of random vectors of any dimensions as an integral of dCov of univariate random variables, which are the projections of the aforementioned random vectors.</p><p>Proof of Proposition 2.3. The "if" part is trivial. To prove the "only if" part, we proceed using induction. Clearly this is true if d = 2. Suppose the result holds for d = m. Note that dCov 2 (X 1 , X 2 , . . . , X m+1 ) = 0 implies that E m+1 i=1 (f i (t i ) -e ı t i X i ) = 0 almost everywhere. Thus we can write the higher order effect f 12•••(m+1) (t 1 , . . . , t m+1 ) -m+1 i=1 f i (t i ) as a linear combination of the lower order effects. By the assumption that (X i 1 , . . . , X im ) are mutually independent for any m-tuples in I d m with m &lt; d, we know</p><formula xml:id="formula_99">f 12•••(m+1) (t 1 , . . . , t m+1 ) -m+1 i=1 f i (t i ) = 0. ♦ Proof of Proposition 2.4. Notice that d i=1 (U i (X i , X i ) + c) =c d + c d-1 d i=1 U i (X i , X i ) + c d-2 (i 1 ,i 2 )∈I d 2 U i 1 (X i 1 , X i 1 )U i 2 (X i 2 , X i 2 ) + • • • + d i=1 U i (X i , X i ).</formula><p>The conclusion follows from the fact that E[U i (X i , X i )] = 0, equation ( <ref type="formula">4</ref>) and the definition of JdCov.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>♦</head><p>Proof of Proposition 2.7. We only prove the "if" part. If dcf (t 1 , . . . , t d ) can be factored, U (X i , X i ) are independent. Therefore, it is easy to see that JdCov 2 (X 1 , . . . , X d ; c) = 0, which implies that {X 1 , . . . , X d } are mutually independent by Proposition 2.3. ♦ Proof of Lemma 3.1. The RHS of (13) in the main paper is equal to</p><formula xml:id="formula_100">1 n 2 n k,l=1 m i=1 R ( fi (t i ) -e ı t i ,X ki )( fi (-t i ) -e -ı t i ,X li )w p i (t i )dt i .</formula><p>Thus it is enough to prove that</p><formula xml:id="formula_101">( fi (t i ) -e ı t i ,X ki )( fi (-t i ) -e -ı t i ,X li ) w p i (t i )dt i = U i (k, l).</formula><p>Direct calculation shows that For complex numbers x 1 , x 2 , . . . , x n with n ≥ 2, the CR inequality says that for any r &gt;</p><formula xml:id="formula_102">ξ(t 1 , t 2 , . . . , t d ) = d j=1 f j -d d j=1 f j + f 12 j =1,2 f j + f 13 j =1,3 f j + • • • -f 123 j =1,2,3 f j + f 124 j =1,2,4 f j + • • • + • • • + (-</formula><formula xml:id="formula_103">1 n i=1 x i r ≤ n r-1 n i=1 |x i | r .<label>(22)</label></formula><p>Using ( <ref type="formula" target="#formula_103">22</ref>), we get</p><formula xml:id="formula_104">|ξ n (t 1 , t 2 , . . . , t d )| 2 = 1 n n j=1 d i=1 ( fi (t i ) -e ı t i ,X ji ) 2 ≤ 1 n 2 n 2-1 n j=1 d i=1 fi (t i ) -e ı t i ,X ji 2 = 1 n n j=1 d i=1 4 = 4 d .</formula><p>For any δ &gt; 0, define </p><formula xml:id="formula_105">D(δ) = {(t 1 , t 2 , . . . , t d ) : δ ≤ |t i | p i ≤ 1/δ , i = 1, 2, . . . , d}. Notice that dCov 2 (X 1 , X 2 , . . . , X d ) = D(δ) | ξ n (t 1 , t 2 , . . . , t d ) | 2 dw + D c (δ) | ξ n (t 1 , t 2 , . . . , t d ) | 2 dw = D (1) n,δ + D (2) n,δ<label>(say)</label></formula><formula xml:id="formula_106">δ = | ξ(t 1 , t 2 , .. , t d ) | 2 dw = dCov 2 (X 1 , X 2 , .. , X d ).</formula><p>The proof will be complete if we can show almost surely</p><formula xml:id="formula_107">lim δ→0 lim n→∞ D<label>(2)</label></formula><p>n,δ = 0.</p><p>To this end, write</p><formula xml:id="formula_108">D c (δ) = d i=1 (A 1 i ∪ A 2 i ), where A 1 i = {|t i | p i &lt; δ} and A 2 i = {|t i | p i &gt; 1 δ } for i = 1, 2, . . . , d. Then we have D (2) n,δ = D c (δ) | ξ n (t 1 , t 2 , . . . , t d ) | 2 dw ≤ i=1,2,...,d k=1,2 A k i |ξ n (t 1 , t 2 , . . . , t d )| 2 dw.</formula><p>Define u i j = e ı t i ,X ji -f i (t i ) for 1 ≤ j ≤ n and 1 ≤ i ≤ d. Following the proof of Theorem 2 of <ref type="bibr" target="#b27">Székely et al. (2007)</ref>, we have for i = 1, 2, . . . , d,</p><formula xml:id="formula_109">R p i |u i j | 2 c p i |t i | 1+p i p i dt i ≤ 2 ( |X ji | + E|X i | ), (<label>23</label></formula><formula xml:id="formula_110">)</formula><formula xml:id="formula_111">|t i |p i &lt;δ |u i j | 2 c p i |t i | 1+p i p i dt i ≤ 2 E[|X ji -X i ||X ji ] G( |X ji -X i |δ ), (<label>24</label></formula><formula xml:id="formula_112">)</formula><formula xml:id="formula_113">|t i |p i &gt;1/δ |u i j | 2 c p i |t i | 1+p i p i dt i ≤ 4δ,<label>(25)</label></formula><p>where</p><formula xml:id="formula_114">G(y) = |z|&lt;y 1 -cos z 1 |z| 1+p dz,</formula><p>which satisfies that G(y) ≤ c p and lim y→0 G(y) = 0. Notice that</p><formula xml:id="formula_115">ξ n (t 1 , t 2 , . . . , t d ) = 1 n n j=1 d i=1 1 n n k=1 u i k -u i j .</formula><p>Some algebra yields that</p><formula xml:id="formula_116">ξ n (t 1 , t 2 , . . . , t d ) = d i=1 1 n n k=1 u i k -d d i=1 1 n n k=1 u i k + 1 n n k=1 u 1 k u 2 k i =1,2 1 n n k=1 u i k + 1 n n k=1 u 1 k u 3 k i =1,3 1 n n k=1 u i k + • • • + 1 n n k=1 u 1 k u 2 k u 3 k i =1,2,3 1 n n k=1 u i k + 1 n n k=1 u 1 k u 2 k u 4 k i =1,2,4 1 n n k=1 u i k + • • • + (-1) d 1 n n k=1 u 1 k u 2 k • • • u d k .</formula><p>By the CR-inequality, we get</p><formula xml:id="formula_117">|ξ n (t 1 , t 2 , . . . , t d )| 2 =C d i=1 1 n n k=1 |u i k | 2 + d 2 d i=1 1 n n k=1 |u i k | 2 +      1 n n k=1 u 1 k u 2 k 2   i =1,2 1 n n k=1 |u i k | 2 +   1 n n k=1 u 1 k u 3 k 2   i =1,3 1 n n k=1 u i k + • • •    +      1 n n k=1 u 1 k u 2 k u 3 k 2   i =1,2,3 1 n n k=1 |u i k | 2 +   1 n n k=1 u 1 k u 2 k u 4 k 2   i =1,2,4 1 n n k=1 |u i k | 2 + • • •    + (-1) d   1 n n k=1 u 1 k u 2 k • • • u d k 2   ,</formula><p>for some positive constant C &gt; 0. By the Cauchy-Schwarz inequality, we have for any 2 ≤ q ≤ d,</p><formula xml:id="formula_118">1 n n k=1 u 1 k u 2 k • • • u q k 2 ≤ 1 n n k=1 i∈Sq 1 |u i k | 2 . 1 n n k=1 i∈Sq 2 |u i k | 2 , (<label>26</label></formula><formula xml:id="formula_119">) A = d i=1 E 1 n 2 n a,b=1 e ı t i ,X ai -ı t i0 ,X bi - 1 n n b=1 e ı t i ,X ki -ı t i0 ,X bi - 1 n n a=1 e -ı t i0 ,X ki +ı t i ,X ai + e ı t i -t i0 ,X ki = d i=1 1 n 2 n f i (t i -t i0 ) + n(n -1)f i (t i )f i (-t i0 ) - 2 n f i (t i -t i0 ) + (n -1)f i (t i )f i (-t i0 ) + f i (t i -t i0 ) = n -1 n d d i=1 f i (t i -t i0 ) -f i (t i )f i (-t i0 ) ,</formula><p>and</p><formula xml:id="formula_120">B = d i=1 E 1 n 2 n a,b=1 e ı t i ,X ai -ı t i0 ,X bi - n b=1 e ı t i ,X ki -ı t i0 ,X bi - 1 n n a=1 e -ı t i0 ,X li +ı t i ,X ai + e ı t i ,X ki -ı t i0 ,X li = d i=1 1 n 2 n f i (t i -t i0 ) + n(n -1)f i (t i )f i (-t i0 ) - 2 n f i (t i -t i0 ) + (n -1)f i (t i )f i (-t i0 ) + f i (t i )f i (-t i0 ) = - 1 n d d i=1 f i (t i -t i0 ) -f i (t i )f i (-t i0 ) .</formula><p>Hence we obtain</p><formula xml:id="formula_121">E Γ n (t)Γ n (t 0 ) = c n d i=1 f i (t i -t i0 ) -f i (t i )f i (-t i0 ) ,<label>(27)</label></formula><p>where</p><formula xml:id="formula_122">c n = n-1 n d + (n -1) -1 n d . To prove Γ n 2 d -→ Γ 2 , we construct a sequence of random variables {Q n (δ)} such that 1. Q n (δ) d -→ Q(δ) as n → ∞, for any fixed δ &gt; 0; 2. lim sup n→∞ E| Q n (δ) -Γ n 2 | → 0 as δ → 0; 3. Q(δ) d -→ Γ 2 as δ → 0. Then Γ n 2 d</formula><p>-→ Γ 2 follows from Theorem 8.6.2 of <ref type="bibr" target="#b19">Resnick (1999)</ref>.</p><p>We first show (1). Define</p><formula xml:id="formula_123">Q n (δ) = D(δ) |Γ n (t)| 2 dw, Q(δ) = D(δ) |Γ(t)| 2 dw.</formula><p>Given &gt; 0, choose a partition {D k } N k=1 of D(δ) into N measurable sets with diameter at most . Then</p><formula xml:id="formula_124">Q n (δ) = N k=1 D k |Γ n (t)| 2 dw, Q(δ) = N k=1 D k |Γ(t)| 2 dw. Define Q n (δ) = N k=1 D k |Γ n (t k )| 2 dw, Q (δ) = N k=1 D k |Γ(t k )| 2 dw,</formula><p>where {t k } N k=1 are a set of distinct points such that t k ∈ D k . In view of Theorem 8.6.2 of <ref type="bibr" target="#b19">Resnick (1999)</ref>, it suffices to show that i) lim sup</p><formula xml:id="formula_125">→0 lim sup n→∞ E| Q n (δ) -Q n (δ) | = 0; ii) lim sup →0 E| Q (δ) -Q(δ) | = 0; iii) Q n (δ) d -→ Q (δ) as n → ∞, for any fixed δ &gt; 0.</formula><p>To this end, define</p><formula xml:id="formula_126">β n ( ) = sup t,t 0 E |Γ n (t)| 2 -|Γ n (t 0 )| 2 and β( ) = sup t,t 0 E |Γ(t)| 2 -|Γ(t 0 )| 2 ,</formula><p>where the supremum is taken over all all t = (t 1 , .. , t d ) and t 0 = (t 10 , .. , t d0 ) such that δ &lt; |t i |, |t i0 | &lt; 1/δ for i = 1, 2, . . . , d, and d i=1 |t i -t i0 | 2 p i &lt; 2 . Since the function inside the supremum is continuous in t and t 0 , and using the fact that a continuous function on a compact support is uniformly continuous, it follows that lim →0 β( ) = 0 and lim →0 β n ( ) = 0 for fixed δ &gt; 0 and fixed n. Thus (i) and (ii) hold. To </p><formula xml:id="formula_127">show (iii), it is enough to show         Γ n (t 1 ) Γ n (t 2 ) . . . Γ n (t N )         d -→         Γ(t 1 ) Γ(t 2 ) . . . Γ(t N )         , where (t 1 , . . . , t N ) ∈ R p 1 × R p 2 × • • • × R p d is</formula><formula xml:id="formula_128">(t) = 1 √ n n j=1 d i=1 fi (t i ) -f i (t i ) -e ı t i ,X ji - f i (t i )</formula><p>. By some algebra and the weak law of large number, we have</p><formula xml:id="formula_129">        Γ n (t 1 ) Γ n (t 2 ) . . . Γ n (t N )         = 1 √ n n j=1 Z j + o p (1),</formula><p>where</p><formula xml:id="formula_130">Z j = (Z j1 , . . . , Z jN ) with Z jk = d i=1 f i (t k i ) -e ı t k i ,X ji for 1 ≤ k ≤ N. By the independence assumption, E[X j ] = 0 and for 1 ≤ l, m ≤ N , E[Z jl Z jm ] = d i=1 E e ı t l i ,X ji -f i (t l i ) e -ı t m i ,X ji -f i (t m i ) = R(t l , t m ).</formula><p>By the Central Limit Theorem (CLT) and Stutsky's theorem, as n → ∞,</p><formula xml:id="formula_131">        Γ n (t 1 ) Γ n (t 2 ) . . . Γ n (t N )         d -→         Γ(t 1 ) Γ(t 2 ) . . . Γ(t N )        </formula><p>, which completes the proof of (1).</p><p>To prove (2), define</p><formula xml:id="formula_132">u i = e ı t i ,X i -f i (t i ). Then |u i | 2 = 1 + |f i (t i )| 2 -e ı t i ,X i f i (t i ) -e -ı t i ,X i f i (t i ),</formula><p>and hence</p><formula xml:id="formula_133">E|u i | 2 = 1 -|f i (t i )| 2 . (<label>28</label></formula><formula xml:id="formula_134">)</formula><p>Following the similar steps as in the proof of Theorem 5 in <ref type="bibr" target="#b27">Székely et al.(2007)</ref> and using the Fubini's</p><p>Theorem,</p><formula xml:id="formula_135">E | Q n (δ) -Γ n (t) 2 | = E D(δ) |Γ n (t)| 2 dw - |Γ n (t)| 2 dw ≤ |t 1 |p 1 &lt;δ E|Γ n (t)| 2 dw + |t 1 |p 1 &gt;1/δ E|Γ n (t)| 2 dw + • • • + |t d |p d &lt;δ E |Γ n (t)| 2 dw + |t d |p d &gt;1/δ E |Γ n (t)| 2 dw.<label>(29)</label></formula><p>Using ( <ref type="formula" target="#formula_121">27</ref>) and ( <ref type="formula" target="#formula_133">28</ref>), we have</p><formula xml:id="formula_136">E |Γ n (t)| 2 = c n d i=1 E|u i | 2 .</formula><p>Along with the independence assumption, we have</p><formula xml:id="formula_137">|t 1 |p 1 &lt;δ E |Γ n (t)| 2 dw = c n |t 1 |p 1 &lt;δ E|u 1 | 2 c p 1 |t 1 | 1+p 1 p 1 dt 1 d i=2 E|u i | 2 c p i |t i | 1+p i p i dt i ≤ 2c n E|X 1 -X 1 | p 1 G(|X 1 -X 1 | p 1 δ) d i=2 4E|X i | p i . Therefore lim δ→0 lim n→∞ |t 1 |p 1 &lt;δ E |Γ n (t)| 2 dw = 0 . Similarly |t 1 |p 1 &gt;1/δ E |Γ n (t)| 2 dw = c n |t 1 |p 1 &gt;1/δ E|u 1 | 2 c p 1 |t 1 | 1+p 1 p 1 dt 1 . d i=2 E|u i | 2 c p i |t i | 1+p i p i dt i ≤ 4δc n d i=2 4E|X i | p i . Therefore lim δ→0 lim n→∞ |t 1 |p 1 &gt;1/δ E |Γ n (t)| 2 dw = 0 .</formula><p>Applying similar argument to the remaining summands in (29), we get</p><formula xml:id="formula_138">lim δ→0 lim n→∞ E| Q n (δ) -Γ n (t) 2 | = 0 .</formula><p>To prove (3), we note that</p><formula xml:id="formula_139">Γ(t) 1 t ∈ D(δ) a.s -→ Γ(t) 1 t ∈ R p 1 × R p 2 × • • • × R p d ,</formula><p>as δ → 0. Again by the Fubini's Theorem and equation (2.5) of <ref type="bibr" target="#b27">Székely et al. (2007)</ref>,</p><formula xml:id="formula_140">E Γ 2 = d i=1 1 -|f i (t i )| 2 dw = d i=1 1 -|f i (t i )| 2 c p i |t i | 1+p i p i dt i = d i=1 E 1 -cos t i , X i -X i c p i |t i | 1+p i p i dt i = d i=1 E |X i -X i | p i &lt; ∞ . Hence Γ 2 &lt; ∞ almost surely. By DCT, Q(δ) a.s</formula><p>-→ Γ 2 as δ → 0, which completes the proof. ♦ Lemma 9.1. U i (k, l) can be composed as</p><formula xml:id="formula_141">U i (k, l) = n -3 (n -1)(n -2) u / ∈{k,l} U i (X ui , X li ) + n -3 (n -1)(n -2) v / ∈{k,l} U i (X ki , X vi ) - n -3 n -1 U i (X ki , X li ) + 2 (n -1)(n -2) u,v / ∈{k,l},u&lt;v U i (X ui , X vi ),</formula><p>where the four terms are uncorrelated with each other.</p><p>Proof of Lemma 9.1. The result follows from direct calculation.</p><formula xml:id="formula_142">♦ Proposition 9.2. E[ dCov 2 (X i , X j )] = dCov 2 (X i , X j ).</formula><p>Proof of Proposition 9.2. Using Lemma 9.1 and the fact that dCov 2 (X i , X</p><formula xml:id="formula_143">j ) = E[U i (X ki , X li )U j (X kj , X lj )] for k = l, we have for k = l, E[U i (X ki , X li )U j (X kj , X lj )] = (n -3) 2 (n -1) 2 + 2(n -3) 2 (n -1) 2 (n -2) + 2(n -3) (n -1) 2 (n -2) E[U i (X ki , X li )U j (X kj , X lj )] = n -3 n -1 dCov 2 (X i , X j ). It thus implies that E[ dCov 2 (X i , X j )] = n -1 n -3 E[U i (X ki , X li ; α)U j (X kj , X lj )] = dCov 2 (X i , X j ),</formula><p>which completes the proof. ♦</p><p>Proof of Proposition 4.1. Denote by X = {X 1 , . . . , X n }. By independence of the bootstrap samples,</p><p>we have E [Γ * n (t)| X] = 0. Proceeding in the similar way as in the proof of Proposition 3.2, it can be shown that</p><formula xml:id="formula_144">E Γ * n (t)Γ * n (t 0 ) | X = c n d i=1 fi (t i -t i0 ) -fi (t i ) fi (-t i0 ) ,<label>(30)</label></formula><p>where</p><formula xml:id="formula_145">c n = n-1 n d + (n -1) -1 n d . To prove Γ * n 2 d -→ Γ 2 almost surely, we construct a sequence of random variables {Q * n (δ)} such that 1. Q * n (δ) d -→ Q(δ) almost surely as n → ∞, for any fixed δ &gt; 0; 2. lim sup n→∞ E Q * n (δ) -Γ * n 2 | X → 0 almost surely as δ → 0; 3. Q(δ) d -→ Γ 2 as δ → 0. Then Γ * n 2 d</formula><p>-→ Γ 2 almost surely follows from Theorem 8.6.2 of <ref type="bibr" target="#b19">Resnick (1999)</ref>.</p><p>We first show (1). Define</p><formula xml:id="formula_146">Q * n (δ) = D(δ) |Γ * n (t)| 2 dw, Q(δ) = D(δ) |Γ(t)| 2 dw.</formula><p>Given &gt; 0, choose a partition {D k } N k=1 of D(δ) into N measurable sets with diameter at most . Then</p><formula xml:id="formula_147">Q * n (δ) = N k=1 D k |Γ * n (t)| 2 dw, Q(δ) = N k=1 D k |Γ(t)| 2 dw. Define Q * n (δ) = N k=1 D k |Γ * n (t k )| 2 dw, Q (δ) = N k=1 D k |Γ(t k )| 2 dw,</formula><p>where {t k } N k=1 are a set of distinct points such that t k ∈ D k . In view of Theorem 8.6.2 of <ref type="bibr" target="#b19">Resnick (1999)</ref>, it suffices to show that i) lim sup</p><formula xml:id="formula_148">→0 lim sup n→∞ E | Q * n (δ) -Q * n (δ) | X = 0 almost surely ; ii) lim sup →0 E [ | Q (δ) -Q(δ) | ] = 0; iii) Q * n (δ) d -→ Q (δ)</formula><p>almost surely as n → ∞, for any fixed δ &gt; 0.</p><p>To this end, define</p><formula xml:id="formula_149">β * n ( ) = sup t,t 0 E |Γ * n (t)| 2 -|Γ * n (t 0 )| 2 X ,</formula><p>and,</p><formula xml:id="formula_150">β( ) = sup t,t 0 E |Γ(t)| 2 -|Γ(t 0 )| 2 ,</formula><p>where the supremum is taken over all all t = (t 1 , .  f * i (t i ) -fi (t i ) -e ı t i ,X * jifi (t i ) . Using Markov's inequality and Triangle inequality, observe that</p><formula xml:id="formula_151">        Γ * n (t 1 ) Γ * n (t 2 ) . . . Γ * n (t N )         d -→         Γ(t 1 ) Γ(t 2 ) . . . Γ(t N )         almost surely, where (t 1 , . . . , t N ) ∈ R p 1 × R p 2 × • • • × R p d is</formula><formula xml:id="formula_152">∞ n=1 P 1 n n k=1 (e ı t i ,X * ki -fi (t i ) &gt; = ∞ n=1 P n k=1 Y ki &gt; n = ∞ n=1 P   n k=1 Y ki 2 &gt; n 2 2   = ∞ n=1 P n k,l=1 Y ki Y li &gt; n 2 2 ≤ ∞ n=1 1 (n ) 4 E ( n k,l=1 Y ki Y li ) 2 X = ∞ n=1 1 (n ) 4 E         n k 1 ,l 1 , k 2 ,l 2 =1 Y k 1 i Y l 1 i Y k 2 i Y l 2 i     X     ≤ ∞ n=1 1 (n ) 4 . Cn 2 &lt; ∞,</formula><p>where C &gt; 0, Y k = e ı t i ,X  The proof of part (3) is exactly the same as its counterpart in the proof of Proposition 3.2, which completes the proof.</p><formula xml:id="formula_153">E |u * 1 | 2 X c p 1 |t 1 | 1+p 1 p 1 dt 1 d i=2 E |u * i | 2 X c p i |t i | 1+p i p i dt i ≤ 2 c n E |X * 1 -X * 1 | p 1 G(|X * 1 -X * 1 | p 1 δ) | X d i=2 4 E |X * i | p i | X = 2 c n 1 n 2 n j,k=1 |X j1 -X k1 | p 1 G(|X j1 -X k1 | p 1 δ) d i=2 4 1 n n j=1 |X ji | p i a.s → 2 E |X 1 -X 1 | p 1 G(|X 1 -X 1 | p 1 δ) d i=2 4 E |X i | p i as n → ∞.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>♦</head><p>Let G n be the set of all functions from {1, 2, . . . , n} to {1, 2, . . . , n}. Define a map g: R n×d → R n×d as the following g(X 1 , . . . , For α ∈ (0, 1), we define the α-level bootstrap-assisted test for testing H 0 against H A as φ n (X 1 , . . . , X n ) := 1 { n JdCov 2 (ψ(X 1 , .. ,Xn)) &gt; ( Fn(X 1 ,...,Xn)) -1 (1-α) } .</p><formula xml:id="formula_154">X n ) =         X g 1 (1)</formula><p>Proof of Proposition 4.2. The proof is in similar lines of the proof of Theorem 3.7 in Pfister et al.</p><p>(2018). There exists a set A 0 with P (A 0 ) = 1 such that for all ω ∈ A 0 and ∀ t ∈ R, Since G is continuous, for all ω ∈ A 0 and ∀ t ∈ R, we have lim n→∞ F n (X 1 (ω), .. , X n (ω))</p><p>-1</p><p>(t) = G -1 (t) .</p><p>In particular, for all ω ∈ A 0 , we have lim n→∞ F n (X 1 (ω), .. , X n (ω))</p><p>-1</p><p>(1 -α) = G -1 (1 -α) .</p><p>(34)</p><p>When H 0 is true, using Proposition 3.2, equation ( <ref type="formula" target="#formula_155">33</ref>) and Corollary 11.2.3 in <ref type="bibr" target="#b12">Lehmann and Romano (2005)</ref>, we have lim sup n→∞ P ( φ n (X 1 , . . . , X n ) = 1 )</p><p>= lim sup n→∞ P n dcov 2 (X 1 , .. , X n ) &gt; F n (X 1 , .. , X n )</p><p>-1</p><p>(1 -α)</p><formula xml:id="formula_156">= 1 -lim inf n→∞ P n dcov 2 (X 1 , .. , X n ) ≤ F n (X 1 , .. , X n ) -1 (1 -α) =1 -G G -1 (1 -α) = 1 -(1 -α) = α .</formula><p>This completes the proof of the proposition. ♦</p><p>Proof of Proposition 4.3. The proof is in similar lines of the proof of Theorem 3.8 in Pfister et al.</p><p>(2018). In the proof of Proposition 4.2, we showed that there exists a set A 0 with P (A 0 ) = 1 such that for all ω ∈ A 0 , lim n→∞ F n (X 1 (ω), .. , X n (ω))</p><p>-1</p><p>(1 -α) = G -1 (1 -α) .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Define the set</head><p>A 1 = { ω : ∀ t ∈ R, lim n→∞ 1 { n dcov 2 (X 1 (ω), .. ,Xn(ω)) ≤t } = 0 } .</p><p>Clearly, P (A 1 ) = 1 and hence P (A 0 ∩ A 1 ) = 1. Fix ω ∈ A 0 ∩ A 1 . Then by ( <ref type="formula" target="#formula_155">33</ref>) and ( <ref type="formula">34</ref>), there exists a constant t * ∈ R such that ∀n ∈ N, lim n→∞ F n (X 1 (ω), .. , X n (ω))</p><p>-1</p><p>(1 -α) ≤ t * .</p><p>Therefore, lim n→∞ 1 { n dcov 2 (X 1 (ω), .. ,Xn(ω)) ≤ ( Fn(X 1 (ω), .. ,Xn(ω)) ) -1</p><p>(1-α) } ≤ lim n→∞ 1 { n dcov 2 (X 1 (ω), .. ,Xn(ω)) ≤ t * } = 0 , i.e., 1 { n dcov 2 (X 1 , .. ,Xn) ≤ ( Fn(X 1 , .. ,Xn) ) -1</p><p>(1-α) } a.s -→ 0 as n → ∞. It follows by dominated convergence theorem that lim n→∞ P n dcov 2 (X 1 , .. , X n ) ≤ F n (X 1 , .. , X n )</p><p>-1</p><p>(1 -α)</p><p>= lim n→∞ E 1 { n dcov 2 (X 1 , .. ,Xn) ≤ ( Fn(X 1 , .. ,Xn) ) -1</p><p>(1-α) } = 0 , which completes the proof of the proposition. ♦</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Section 5</head><label>5</label><figDesc>is devoted to numerical studies. The new metrics are employed to perform model selection in causal inference in Section 6. Section 7 discusses the efficient computation of distance metrics and future research directions. The technical details are gathered in the supplementary material. Notations. Consider d ≥ 2 random vectors X = {X 1 , . . . , X d }, where X i ∈ R p i . Set p 0 = d i=1 p i . Let {X 1 , . . . , X d } be an independent copy of X . Denote by ı = √ -1 the imaginary unit. Let | • | p be the Euclidean norm of R p with the subscript omitted later without ambiguity. For a, b ∈ R p , let a, b = a b.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Example 5 . 3 .</head><label>53</label><figDesc>The data X = (X, Y, Z) are generated as follows:1. X, Y i.i.d∼ N (0, 1), Z = sign(XY ) W , where W follows an exponential distribution with mean √ 2;2. X, Y are independent Bernoulli random variables with the success probability 0.5, and Z = 1{X = Y }.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>show that all the five tests perform more or less equally well in examples 5.1.1 and 5.1.2, and the rejection probabilities are quite close to the 10% or 5% nominal level. In Example 5.1.3, the tests based on our proposed statistics show greater conformation of the empirical size to the actual size of the test than T M T . In Example 5.2, the tests based on JdCov 2 , JdCov 2S and JdCov 2 R as well as T M T significantly outperform the dHSIC-based test. Note that the empirical power becomes higher when c increases to 2. From Figure2, we observe that in Example 5.3 all the tests perform very well in the second case. However, in the first case, our tests and the dHSIC-based test deliver higher power as compared to T M T . Finally, in Example 5.4, we allow X, Y, Z to be random vectors with dimension p = 5, 10. The rejection probabilities for each of the five tests increase with n, and the proposed tests provide better performances in comparison with the other two competitors. In particular, the test based on JdCov 2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>in the supplementary materials) shows the proportion of rejections (out of 1000 simulation runs) for the tests based on JdCov 2 , JdCov 2 S and JdCov 2 R , setting c = 1. The results indicate that use of the bias corrected estimators lead to greater conformation of the empirical size to the actual size of the test (in Example 5.1), and slightly better power in Example 5.3.Remark 5.2. In connection to the heuristic idea discussed in Remark 2.3 about choosing the tuning parameter c, we conduct tests for joint independence of the random variables in all the above examples, choosing c in that way. Table5(in the supplementary materials) presents the proportion of rejections for the proposed tests and the values of c for each example, averaged over the 1000 simulated datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Figures showing the empirical size and power for the different tests statistics in Examples 5.1 and 5.2. c * denotes the data-driven choice of c. The vertical height of a bar and a line on a bar stand for the empirical size or power at levels α = 0.1 or α = 0.05, respectively.</figDesc><graphic coords="22,46.80,528.61,228.09,180.47" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Figures showing the empirical power for the different tests statistics in Examples 5.3 and 5.4. c * denotes the data-driven choice of c. The vertical height of a bar and a line on a bar stand for the empirical power at levels α = 0.1 or α = 0.05, respectively.</figDesc><graphic coords="23,46.80,301.69,228.09,180.47" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>(a) JdCov 2 , JdCov 2 S , JdCov 2 R and T M T (b) dHSIC</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The DAG models corresponding to the largest p-values from the five tests.</figDesc><graphic coords="25,86.94,123.94,207.35,164.06" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>Figure 4: The DAG models selected (most frequently out of 100 times) by the five tests, without doing residual bootstrap to re-estimate f j,k .</figDesc><graphic coords="27,50.01,186.78,165.89,131.25" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>≤ D(δ) 4 d &lt; ∞. Using the Dominated Convergence Theorem (DCT), we have as n → ∞ , | ξ(t 1 , t 2 , .. , t d ) | 2 dw = D</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>fixed. The rest follows from the Continuous Mapping Theorem and the Cramer-Wold Device. Notice that Γ * n (</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>1 |p 1 &lt;δ E |Γ * n (t)| 2 X dw = 0 almost surely.Similarly|t 1 |p 1 &gt;1/δ E |Γ * n (t)| 2 X dw = c n |t 1 |p 1 &gt;1/δ E |u * 1 | 2 X c p 1 |t 1 | 1+p 1 1 |p 1 &gt;1/δ E |Γ * n (t)| 2 X dw = 0 almost surely.Applying similar argument to the remaining summands in (32), we getlim δ→0 lim n→∞ E | Q * n (δ) -Γ * n (t) 2 | X = 0 almost surely.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>F</head><label></label><figDesc>n (X 1 (ω), .. , X n (ω)) (t) = lim 2 g(X 1 (ω), .. ,Xn(ω)) ≤ t } = lim n→∞ E 1 { n JdCov 2 (g(X 1 (ω), .. ,Xn(ω)) ) ≤ t } = lim n→∞ P n JdCov 2 g X 1 (ω), .. , X n (ω)) ≤ t = G(t) ,where G(•) is the distribution function of +∞ j=1 λ j Z 2 j .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>1. Randomly sample * j = ( * 1j , . . . , * nj ) with replacement from the residuals {ˆ 1j , . . . , ˆ nj }, 1 ≤ j ≤ d. Construct the bootstrap sample X * ij = k∈Par(j) fj,k (X ik ) + * ij .</figDesc><table><row><cell>2. Based on the bootstrap sample {X  *  i } n i=1 with X  *  i = (X  *  i1 , . . . , X  *  id ), estimate f j,k for k ∈ Par(j),</cell></row><row><cell>and denote the corresponding residuals by ˆ  *  ij .</cell></row><row><cell>3. Calculate the bootstrap statistic T  *  n based on {ˆ  *  ij }.</cell></row><row><cell>4. Repeat the above steps B times and let {T  *  b,n } B b=1 be the corresponding values of the bootstrap</cell></row><row><cell>statistics. The p-value is given by B</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>1) d f 12,...,d , and ξ n (t 1 , t 2 , . . . , t d ) has the same expression by replacing the characteristic functions by their empirical counterparts in ξ(t 1 , t 2 , . . . , t d ). Then by the strong law of large numbers, we have for any fixed (t 1 , t 2 , . . . , t d ), ξ n (t 1 , t 2 , . . . , t d )</figDesc><table><row><cell>a.s -→ ξ(t 1 , t 2 , . . . , t d ).</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>fixed. The rest follows from the Continuous Mapping Theorem and the Cramer-Wold Device. Notice that Γ n</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>. , t d ) and t 0 = (t 10 , .. , t d0 ) such that δ &lt; |t i |, |t i0 | &lt; 1/δ for i = 1, 2, . . . , d, and d i=1 |t i -t i0 | 2 p i &lt; 2 . Then for fixed δ &gt; 0, lim Thus (i) and (ii) hold. To show (iii), it is enough to show</figDesc><table><row><cell>→0</cell><cell>β( ) = 0 and lim →0</cell><cell>β  *  n ( ) = 0</cell></row><row><cell>almost surely for fixed n.</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>* ki -fi (t i ), and|Y k | ≤ 2 for any 1 ≤ k ≤ n. By Borel-Cantelli Lemma, as n → ∞, f * i (t i ) -fi (t i )whereZ j = (Z j1 , . . . , Z jN ) with Z jk = d i=1 fi (t k i ) -e ı t kTo prove (2), defineu * i = e ı t i ,X * i -fi * (t i ). Then |u i | 2 = 1+| fi (t i )| 2 -e ı t i ,X * i fi (t i )-e -ı t i ,X * |X = 1 -| fi (t i )| 2 . (31)Following the similar steps as in the proof of Theorem 5 in<ref type="bibr" target="#b27">Székely et al.(2007)</ref> and using the</figDesc><table><row><cell cols="2">which, along with the fact R n assumption, we have</cell><cell cols="6">a.s -→ R and Slutsky's Theorem, implies</cell></row><row><cell cols="8">weak law of large number, we have          Γ  *  n (t 1 ) Γ  *  n (t 1 ) Γ  *  n (t 2 ) . . . Γ  *  n (t N )         d  -→ E |Γ  *  n (t)| 2 X dw |t 1 |p 1 &lt;δ = c n |t 1 |p 1 &lt;δ</cell><cell>       </cell><cell>Γ(t 1 ) Γ(t 2 ) . . . Γ(t N )        </cell><cell>a.s -→ 0 almost surely. By some algebra and the almost surely ,</cell></row><row><cell cols="6">      and thus completes the proof of (1).</cell><cell cols="2">Γ  *  n (t 2 ) . . .</cell><cell>     </cell><cell>=</cell><cell>1 √ n</cell><cell>n j=1</cell><cell>Z j + U ,</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2"></cell><cell cols="3">Γ  *  n (t N ) </cell><cell>i</cell><cell>fi (t i ),</cell></row><row><cell>and hence</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="8">E |u  *  i | 2 Fubini's Theorem,</cell></row><row><cell cols="8">E |Q  *  n (δ) -Γ  *  n (t) 2 | X</cell></row><row><cell>= E</cell><cell cols="6">|Γ  *  n (t)| 2 dw -</cell><cell>|Γ  *  n (t)| 2 dw| X</cell></row><row><cell></cell><cell>D(δ)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>≤</cell><cell cols="7">E |Γ  *  n (t)| 2 X dw +</cell><cell>E |Γ  *  n (t)| 2 X dw</cell><cell>(32)</cell></row><row><cell cols="2">|t 1 |p 1 &lt;δ + • • • +</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">d i=1</cell><cell>fi (t l i -t m i ) -fi (t l |t 1 |p 1 &gt;1/δ i ) fi (-t m i ) ,</cell></row><row><cell cols="8">and, Using (30) and (31), we have E |Γ  *  R(l, m) = n (t)| 2 X = c n d f i (t l i -t m i ) -f i (t l i )f i (-t m i ) . d i=1 E |u  *  i | 2 X . Along with the independence</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">i=1</cell></row><row><cell>By Multivariate CLT,</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Γ  *  n (t 1 )</cell><cell></cell></row><row><cell></cell><cell cols="2">R n -1 2</cell><cell>      </cell><cell cols="3">Γ  *  n (t 2 ) . . . Γ  *  n (t N )</cell><cell>      </cell><cell>d -→ N (0, I N ) almost surely ,</cell></row></table><note><p><p><p><p>i ,X * ji for 1 ≤ k ≤ N , and, U a.s -→ 0, almost surely. By the independence of Bootstrap samples, E[Z j | X] = 0 and for 1</p>≤ l, m ≤ N , E[Z jl Z jm ] = d i=1 E (e ı t l i ,X * ji -fi (t l i )) (e -ı t m i ,X * ji -fi (-t m i ))| X = d i=1 fi (t l i -t m i ) -fi (t l i ) fi (-t m i ) .</p>Let R n and R be N × N matrices with the (l, m) th element being</p>R n (l, m) = |t d |p d &lt;δ E |Γ * n (t)| 2 X dw + |t d |p d &gt;1/δ E |Γ * n (t)| 2 X dw.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>,1 X g 2 (1),2 . . . X g d (1),dX g 1 (2),1 X g 2 (2),2 . . . X g d (2),d g 1 (n),1 X g 2 (n),2 . . . X g d (n),dwhereg i ∈ G n for 1 ≤ i ≤ d.With some abuse of notation, we denote by JdCov 2 (g(X 1 , . . . , X n )) the sample (squared) JdCov computed based on the sample g(X 1 , . . . , X n ). Conditional on the sample, the resampling distribution function F n : [0, +∞) → [0, 1] of the bootstrap statistic is defined for all t ∈ R as F n (X 1 , . . . , X n )(t) :=</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>. . .</cell><cell>. . .</cell><cell>. . .</cell><cell>. . .</cell><cell>      </cell></row><row><cell>1</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>n nd</cell><cell>g∈G d</cell><cell></cell><cell></cell><cell></cell></row></table><note><p>X n 1 { n JdCov 2 (g(X 1 ,...,Xn)) ≤ t } .</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>Assumption 3.1. Suppose for any subset S of {1, 2, . . . , d} with |S| ≥ 2, there exists a partitionS = S 1 ∪ S 2 such that E i∈S 1 |X i | &lt; ∞ and E i∈S 2 |X i | &lt; ∞.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1"><p>JdCov in light of<ref type="bibr" target="#b32">Wang et al. (2015)</ref>, to test if the variables (X 1 , . . . , X d ) are jointly independent given another variable Z.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="8">Acknowledgments</head><p>We acknowledge partial funding from the <rs type="funder">NSF</rs> grant <rs type="grantNumber">DMS-1607320</rs>. We are grateful to the Editor and two very careful reviewers for their immensely helpful comments and suggestions which greatly improved the paper. It would also be our pleasure to thank <rs type="person">Niklas Pfister</rs> for sharing with us the R codes used in <ref type="bibr" target="#b17">Pfister et al. (2018)</ref>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_UyyemnG">
					<idno type="grant-number">DMS-1607320</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>To this end, we note that ( fi (t i ) -e ı t i ,X ki ) ( fi (-t i ) -e -ı t i ,X li ) = fi (t i ) fi (-t i ) -e ı t i ,X ki fi (-t i ) -fi (t i )e -ı t i ,X li + e ı t i ,(X ki -X li ) = 1 n 2 n k,l=1 e ı t i ,(X ki -X li ) -1 n n l=1 e ı t i ,(X ki -X li ) -1 n n k=1 e ı t i ,(X ki -X li ) + e ı t i ,(X ki -X li ) = 1 n n l=1</p><p>(1 -e ı t i ,(X ki -X li ) ) + 1 n n k=1</p><p>(1 -e ı t i ,(X ki -X li ) ) -(1 -e ı t i ,(X ki -X li ) )</p><p>(1 -e ı t i ,(X ki -X li ) ) .</p><p>Using (2.11) of <ref type="bibr" target="#b27">Székely et al. (2007)</ref>, we obtain ( fi (t i ) -e ı t i ,X ki )( fi (t i ) -e -ı t i ,X li )</p><p>Finally, (14) in the paper follows from (13) in the paper and the definition of JdCov 2 . ♦ Proof of Proposition 3.1. Define</p><p>( fi (t i ) -e ı t i ,X ji ), and note that</p><p>where S q 1 ∪ S q 2 = {1, 2, . . . , d}. By Assumption 3.1 and ( <ref type="formula">23</ref>)-( <ref type="formula">25</ref> </p><p>n,δ = 0 almost surely and thus completes the proof. ♦</p><p>Proof of Proposition 3.2. Define the empirical process   (2) 100 5 1.657 1.731 0.990 0.986 0.985 0.977 0.992 0.985 (2) 100 10 1.656 1.731 0.986 0.982 0.976 0.962 1.000 1.000 (3) 100 5 1.657 1.731 0.998 0.996 0.996 0.990 0.996 0.992 (3) 100 10 1.656 1.731 0.991 0.984 0.974 0.965 1.000 0.999 <ref type="bibr">Ex 5.3</ref> (1) 50 3 0.554 0.729 0.984 0.962 0.998 0.994 0.899 0.843 (2) 50 3 0.438 0.527 1.000 1.000 1.000 1.000 1.000 1.000 (1) 100 3 .439 0.530 1.000 1.000 1.000 1.000 1.000 1.000 (2) 100 3 0.438 0.527 1.000 1.000 1.000 1.000 1.000 1.000 </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Kernel independent component analysis</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">R</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1" to="48" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A consistent test of independence based on a sign covariance related to Kendall&apos;s tau</title>
		<author>
			<persName><forename type="first">W</forename><surname>Bergsma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dassios</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bernoulli</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1006" to="1028" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">CAM : Causal additive models, high-dimensional order search and penalized regression</title>
		<author>
			<persName><forename type="first">P</forename><surname>Bühlmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ernest</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2526" to="2556" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Transmission of Information</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Fano</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1961">1961</date>
			<publisher>M.I.T Press</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Cover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Thomas</surname></persName>
		</author>
		<title level="m">Elements of Information Theory</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Wiley</publisher>
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Matsui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mikosch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.05481</idno>
		<title level="m">Applications of distance correlation to time series</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The affinely invariant distance correlation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Dueck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Edelmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Gneiting</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Richards</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bernoulli</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="2305" to="2330" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Measuring statistical dependence with Hilbert-Schmidt norms. Algorithmic Learning Theory</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Bousquet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
			<publisher>Springer-Verlag</publisher>
			<biblScope unit="page" from="63" to="77" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A kernel statistical test of independence</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Fukumizu</surname></persName>
		</author>
		<author>
			<persName><surname>Teo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="585" to="592" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Huo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.06054</idno>
		<title level="m">A statistically and numerically efficient independence test based on random projections and distance covariance</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Fast computing for distance covariance</title>
		<author>
			<persName><forename type="first">X</forename><surname>Huo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Székely</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Technometrics</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="435" to="446" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">O</forename><surname>Lancaster</surname></persName>
		</author>
		<title level="m">The Chi-Squared Distribution</title>
		<meeting><address><addrLine>London</addrLine></address></meeting>
		<imprint>
			<publisher>Wiley</publisher>
			<date type="published" when="1969">1969</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">L</forename><surname>Lehmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Romano</surname></persName>
		</author>
		<title level="m">Testing Statistical Hypotheses</title>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Distance covariance in metric spaces</title>
		<author>
			<persName><forename type="first">R</forename><surname>Lyons</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Probability</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="3284" to="3305" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Independent component analysis via distance covariance</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Matteson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Tsay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">112</biblScope>
			<biblScope unit="issue">518</biblScope>
			<biblScope unit="page" from="623" to="637" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Multivariate information transmission</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Mcgill</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychometrika</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="97" to="116" />
			<date type="published" when="1954">1954</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Causal discovery with continuous additive noise models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Mooij</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Janzing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="page" from="15" to="2009" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Kernel-based tests for joint independence</title>
		<author>
			<persName><forename type="first">N</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bühlmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Peters</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society, Series B</title>
		<imprint>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="5" to="31" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Goodness-Of-Fit Statistics for Discrete Multivariate Analysis</title>
		<author>
			<persName><forename type="first">T</forename><surname>Read</surname></persName>
		</author>
		<author>
			<persName><forename type="middle">N</forename><surname>Cressie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1988">1988</date>
			<publisher>Springer-Verlag</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">I</forename><surname>Resnick</surname></persName>
		</author>
		<title level="m">A Probability Path</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A kernel test for three-variable interactions</title>
		<author>
			<persName><forename type="first">D</forename><surname>Sejdinovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Bergsma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS 26)</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1124" to="1132" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Equivalence of distance-based and RKHS-based statistics in hypothesis testing</title>
		<author>
			<persName><forename type="first">D</forename><surname>Sejdinovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Sriperumbudur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Fukumizu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2263" to="2291" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Testing independence and goodness-of-fit in linear models</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Sen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">927942</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">The mathematical theory of communication</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Shannon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Weaver</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1949">1949</date>
			<publisher>University of Illinois Press</publisher>
			<pubPlace>Urbana</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Lancaster interactions revisited</title>
		<author>
			<persName><forename type="first">B</forename><surname>Streitberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1878" to="1885" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Testing for equal distributions in high dimension</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Székely</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Rizzo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
	<note>InterStat</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Hierarchical clustering via joint between-within distances: Extending Ward&apos;s minimum variance method</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Székely</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Rizzo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Classification</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="151" to="183" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Measuring and testing independence by correlation of distances</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Székely</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Rizzo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">K</forename><surname>Bakirov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2769" to="2794" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Székely</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Rizzo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Brownian distance covariance</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1236" to="1265" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">On the uniqueness of distance covariance</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Székely</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Rizzo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistics and Probability Letters</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="page" from="2278" to="2282" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Energy statistics: A class of statistics based on distances</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Székely</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Rizzo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Statistical Planning and Inference</title>
		<imprint>
			<biblScope unit="volume">143</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1249" to="1272" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Partial distance correlation with methods for dissimilarities</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Székely</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Rizzo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2382" to="2412" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Conditional distance correlation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wenliang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">110</biblScope>
			<biblScope unit="issue">512</biblScope>
			<biblScope unit="page" from="1726" to="1734" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">GAMs with integrated model selection using penalized regression splines and applications to environmental modelling</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">N</forename><surname>Wood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">H</forename><surname>Augustin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ecological Modelling</title>
		<imprint>
			<biblScope unit="volume">157</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="157" to="177" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Testing mutual independence in high dimension via distance covariance</title>
		<author>
			<persName><forename type="first">S</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society, Series B</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>to appear</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
