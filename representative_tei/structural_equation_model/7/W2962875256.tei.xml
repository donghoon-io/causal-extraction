<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Empirical Likelihood for Linear Structural Equation Models with Dependent Errors</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2018-09-18">September 18, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Y</forename><forename type="middle">Samuel</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Statistics</orgName>
								<orgName type="institution">University of Washington</orgName>
								<address>
									<postCode>98103</postCode>
									<settlement>Seattle</settlement>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mathias</forename><surname>Drton</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Statistics</orgName>
								<orgName type="institution">University of Washington</orgName>
								<address>
									<postCode>98103</postCode>
									<settlement>Seattle</settlement>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Empirical Likelihood for Linear Structural Equation Models with Dependent Errors</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2018-09-18">September 18, 2018</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:1710.02588v1[stat.CO]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-14T17:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Empirical likelihood</term>
					<term>causal inference</term>
					<term>graphical model</term>
					<term>structural equation model</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We consider linear structural equation models that are associated with mixed graphs. The structural equations in these models only involve observed variables, but their idiosyncratic error terms are allowed to be correlated and non-Gaussian. We propose empirical likelihood (EL) procedures for inference, and suggest several modifications, including a profile likelihood, in order to improve tractability and performance of the resulting methods. Through simulations, we show that when the error distributions are non-Gaussian, the use of EL and the proposed modifications may increase statistical efficiency and improve assessment of significance.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Structural equation models (SEMs) are multivariate statistical models in which each considered variable is a function of other variables and a stochastic error term. Often, some of these other variables are latent <ref type="bibr" target="#b0">[Bollen, 1989]</ref>. This paper, however, focuses on SEMs in which the effects of latent variables are summarized. Adopting the dominant linear paradigm, we will thus be concerned with models in which linear functions relate only observed variables, but error terms may be dependent. Such models are sometimes referred to as semi-Markovian <ref type="bibr" target="#b23">[Shpitser and Pearl, 2006]</ref>. Avoiding any explicit specification of latent confounding, the models play an important role in exploration of causeeffect structures <ref type="bibr" target="#b5">[Colombo et al., 2012</ref><ref type="bibr" target="#b17">, Pearl, 2009</ref><ref type="bibr" target="#b20">, Richardson and Spirtes, 2002</ref><ref type="bibr" target="#b24">, Spirtes et al., 2000</ref><ref type="bibr" target="#b26">, Wermuth, 2011]</ref>. Much insight about the models can be gained from a natural graphical representation by mixed graphs/path diagrams that originates in work of <ref type="bibr" target="#b27">Wright [1921]</ref>.</p><p>Formally, let Y 1 , . . . , Y n be a multivariate sample with each observation indexed by a set V . So, Y i = (Y vi ) v∈V with each Y vi real-valued. Now consider the system of structural equations</p><formula xml:id="formula_0">Y vi = µ v + u∈V \v β vu Y ui + ǫ vi , v ∈ V, i = 1, . . . , n,<label>(1)</label></formula><p>where the µ v and β vu are unknown parameters and the ǫ vi are random errors. Define vectors ǫ i = (ǫ vi ) v∈V and µ = (µ v ) v∈V , and a matrix B = (β vu ) v,u∈V with β vu = 0 if v = u. We assume that the error vectors ǫ i are independent and identically distributed, have zero means, and have covariance matrix E (ǫ i ǫ t i ) = Ω = (ω vu ). However, we do not specify any parametric form for their distribution. For each i, the equations in (1) can be written as Y i = µ + BY i + ǫ i . If (I -B) is non-singular, then this system is solved uniquely by Y i = (I -B) -1 (µ + ǫ i ). This solution has mean vector (I -B) -1 µ and covariance matrix Σ(B, Ω) := (I -B) -1 Ω(I -B) -t .</p><p>(2) Specific models are now obtained by hypothesizing that a particular collection of coefficients β vu and error covariances ω vu is zero. An SEM can be represented conveniently by a path diagram/mixed graph G = (V, E → , E ↔ ). Here, the vertex set V yields a correspondence between the nodes of the graph and the observed variables. The set E → is a set of directed edges u → v, which encode that variable u may have a direct effect on variable v. The set E ↔ comprises bidirected edges u ↔ v that indicate that the errors ǫ ui and ǫ vi may be correlated. Define the set of parents of node v as pa(v) = {u ∈ V : u → v ∈ E → }. Similarly, define a set of siblings as sib(v) = {u ∈ V : u ↔ v ∈ E ↔ }. Bidirected edges have no orientation, and v ∈ sib(u) if and only if u ∈ sib(v). Now, the graph G induces a model through the requirement that</p><formula xml:id="formula_1">B ∈ B(G) := B ∈ R V ×V : det(I -B) = 0, β vu = 0 if u ∈ pa(v) ,<label>(3)</label></formula><formula xml:id="formula_2">Ω ∈ W(G) := Ω ∈ R V ×V : Ω pos. def., ω vu = ω uv = 0 if v ∈ sib(u) .<label>(4)</label></formula><p>We emphasize that our treatment allows the model to have feedback loops, that is, G may have directed cycles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Related Work</head><p>Frequently, the errors in a SEM, and consequently also the observations Y i , are assumed to be multivariate Gaussian which yield maximum likelihood estimates (MLEs). The Gaussian likelihood is often maximized using generic optimization methods; as done in the popular packages sem <ref type="bibr" target="#b9">[Fox et al., 2017]</ref> and lavaan <ref type="bibr" target="#b21">[Rosseel, 2012]</ref> for R [R Core <ref type="bibr" target="#b19">Team, 2017]</ref>. The coordinate-descent methods proposed by <ref type="bibr" target="#b7">Drton et al. [2009]</ref> and <ref type="bibr" target="#b8">Drton et al. [2017]</ref> can be a useful computational alternative that largely avoids convergence issues.</p><p>As a less parametric method, generalized least squares (GLS) minimizes a discrepancy between the sample covariance and the covariance implied by the parameters. Although the estimates are slightly more robust to misspecification, they are still asymptotically equivalent to the Gaussian MLEs <ref type="bibr" target="#b14">[Olsson et al., 2000]</ref>. When multivariate Gaussianity is inappropriate, MLEs and GLS generally lose statistical efficiency and yield incorrectly calibrated confidence intervals. Weighted least squares methods (WLS)-also called asymptotically distribution free-weight the discrepancy between the observed and hypothesized covariance structure by explicitly estimated fourth moments. Although WLS estimates are consistent and produce asymptotically correct confidence intervals even with non-Gaussian errors, the estimation of higher order moments may come at a loss of statistical efficiency and cause convergence issues, which has limited their use <ref type="bibr" target="#b13">[Muthen and Kaplan, 1992]</ref>. <ref type="bibr" target="#b2">Chaudhuri et al. [2007]</ref> propose using the empirical likelihood (EL) of <ref type="bibr" target="#b16">Owen [2001]</ref> to estimate a covariance matrix with structural zeros. In our setup, this corresponds to the special case of a mixed graph with no directed edges. <ref type="bibr" target="#b12">Kolenikov and Yuan [2009]</ref> use EL to estimate the parameters of a linear SEM. In contrast to the mixed graph formulation, <ref type="bibr" target="#b12">Kolenikov and Yuan [2009]</ref> consider the case where the latent variable structure is explicitly modeled and all errors are independent. The EL approach is appealing as it gives consistent estimates and asymptotically correct confidence intervals even when the errors are not multivariate Gaussian. However, EL can present numerous practical difficulties when the sample size is small relative to the number of parameters or estimating equations used. Moreover, standard implementation of EL methods is computationally feasible only for systems with a handful of variables. We believe that these issues have prevented application of EL to linear SEMs beyond what was done by <ref type="bibr" target="#b12">Kolenikov and Yuan [2009]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Contribution</head><p>In this article, we apply the empirical likelihood framework to SEMs represented by mixed graphs and propose several modifications to a naive approach which address the most salient practical concerns:</p><p>(i) We show that in the mixed graph setting, the covariance parameters Ω can be profiled out. This greatly reduces the computational burden by reducing the number of estimating equations imposed and parameters directly estimated. It also naturally encodes the positive definite constraint on Ω and yields a positive definite estimate of Ω for any point B with a well defined empirical likelihood.</p><p>(ii) When maximizing the empirical likelihood, we leverage a recent insight and directly incorporate gradient information in a quasi-Newton procedure instead of the typical derivative-free approaches to empirical likelihood optimization. This again yields substantial computational savings.</p><p>(iii) We use the adjusted empirical likelihood (AEL), first proposed by <ref type="bibr" target="#b4">Chen et al. [2008]</ref>. This adjustment ensures that an empirical likelihood and corresponding gradient is well defined for every value in the parameter space.</p><p>(iv) We apply the idea of extended empirical likelihood (EEL), which furnishes drastically improved coverage of confidence intervals at small sample sizes <ref type="bibr" target="#b25">[Tsao and Wu, 2014]</ref>.</p><p>Our simulations show that with these proposed modifications, empirical likelihood becomes an attractive alternative for practitioners concerned with non-Gaussianity in structural equation modeling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background on Empirical Likelihood</head><p>Let Y = (Y 1 , . . . , Y n ) be a sample from an m-variate distribution P belonging to a non-/semiparametric statistical model M. Let P n be the n -1 dimensional probability simplex. For p = (p 1 , . . . , p n ) ∈ P n , define the log-empirical likelihood ℓ(p; Y ) = n i=1 log(p i ). This is the log-likelihood of the sample under the discrete distribution with mass p i at each point Y i . Suppose we are interested in a parameter θ = θ(P ) taking values in Θ ⊆ R d such that for a map G : R m × R d → R q we have E P G(Y i , θ(P )) = 0 for all P ∈ M. The log-empirical likelihood at a given parameter value θ is then</p><formula xml:id="formula_3">ℓ(θ; Y ) = max p∈P θ ℓ(p; Y ) = max p∈P θ n i=1 log(p i ),<label>(5)</label></formula><p>where the feasible set</p><formula xml:id="formula_4">P θ = p ∈ P n : n i=1 p i G(Y i , θ) = 0 (6)</formula><p>reflects that the expectation of G(•; θ) vanishes for distributions compatible with θ.</p><p>The empirical likelihood (EL) from ( <ref type="formula" target="#formula_3">5</ref>) provides a basis for statistical inference. Maximizing it over θ ∈ Θ yields the maximum empirical likelihood estimator θ = arg max</p><formula xml:id="formula_5">θ ℓ(θ; Y )<label>(7)</label></formula><p>that we refer to as MELE. Ratios of the EL yield empirical likelihood ratio statistics. <ref type="bibr" target="#b15">Owen [1988]</ref> derives an EL analogue of Wilk's Theorem, and the result was expanded to the general estimating equation framework by <ref type="bibr" target="#b18">Qin and Lawless [1994]</ref>. The specific regularity conditions needed are discussed in Section 3.3, and the results imply under very general conditions that the MELE is consistent and asymptotically normal. In addition, EL ratio statistics have limiting χ 2 distributions that can be used to calibrate statistical tests and create confidence intervals or regions.</p><p>For a detailed exposition of these ideas, we refer readers to <ref type="bibr" target="#b16">Owen [2001]</ref>.</p><p>The nice theoretical properties for EL, however, come at a high practical cost. The practical issues become particularly pressing for applications to linear SEMs, for which the number of parameters and estimating equations generally grow on the order of m 2 , where m = |V | is the number of variables considered. We describe three difficulties that complicate the direct use of EL for SEMs:</p><p>(i) For some values θ, the origin may be outside the convex hull of {G(Y i , θ) : i = 1, . . . , n}, in which case the feasible set P θ from ( <ref type="formula">6</ref>) is empty and the EL at θ is zero. This "convex hull problem" occurs more often when the sample size is small relative to the number of estimating equations or when the data is skewed. As discussed by <ref type="bibr" target="#b11">Grendár and Judge [2009]</ref>, it is possible that P θ = ∅ for all parameter vectors θ, which is known as the "empty set problem". In addition, the log-EL is typically not a convex function <ref type="bibr" target="#b3">[Chaudhuri et al., 2017]</ref>, and finding an initial point that has well-defined EL and is in the basin of attraction of the MELE can be difficult.</p><p>(ii) The optimization problem defining the log-EL ℓ(θ; Y ) from ( <ref type="formula" target="#formula_3">5</ref>) is typically solved iteratively through its dual.</p><p>Although this problem is convex, it can be computationally burdensome when the number of estimating equations, which corresponds to the number of dual variables, is large.</p><p>(iii) Confidence intervals based on the asymptotic normal variance and χ 2 likelihood ratio calibration have been shown to often undercover at small sample sizes <ref type="bibr" target="#b25">[Tsao and Wu, 2014]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Empirical Likelihood for SEMs</head><p>We now turn to the application of EL to SEMs. For expository simplicity, we assume throughout that our observations are centered. In other words, the intercept parameter vector µ for (1) is zero, so that E(Y i ) = 0. However, our ideas extend straightforwardly to the case where we also make inference about µ = 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Profiled Formulation</head><p>Consider the linear SEM given by a mixed graph G = (V, E → , E ↔ ), as defined in the Introduction. The general framework laid out in Section 2 can be applied directly to such a model by taking the covariance matrix of the observations Y i as the general parameter θ. We may then define an EL at a pair of parameter matrices (B, Ω) as the EL at the covariance matrix Σ(B, Ω) from ( <ref type="formula">2</ref>). In such a direct application to the linear SEM, the log-EL function ℓ(B, Ω; Y ) is the maximum of the log-EL ℓ(p; Y ) over the set</p><formula xml:id="formula_6">P Σ(B,Ω) = p ∈ P n : n i=1 p i Y i = 0, n i=1 p i vech Y i Y T i -vech Σ(B, Ω) = 0 . (8)</formula><p>Here, vech is the half-vectorization operator for symmetric matrices. Under this formulation, there are m constraints for the mean and m(m + 1)/2 covariance constraints, and the MELE is computed by optimization with respect to the pair of m × m matrices (B, Ω), with Ω restricted to be positive definite. Inspection of the covariance constraints reveals that a great simplification is possible by profiling out Ω. Indeed, the covariance constraint yields an explicit solution for Ω given B, Y = (Y 1 , . . . Y n ), and p. Specifically, with Π = diag(p 1 , . . . , p n ), we have</p><formula xml:id="formula_7">Y ΠY T = Σ(B, Ω) = (I -B) -1 Ω(I -B) -t ⇐⇒ (I -B)Y ΠY (I -B) t = Ω. (<label>9</label></formula><formula xml:id="formula_8">)</formula><p>The entries of Ω are either constrained to be zero or freely varying. No constraints arise from the freely varying entries, and we may base estimation of B on only the structural zeros in Ω, that is,</p><formula xml:id="formula_9">(I -B)Y ΠY (I -B) t uv = 0 ∀{u, v} ∈ E ↔ .</formula><p>Once a solution for B is found, we may simply compute Ω = Ω(B) by setting</p><formula xml:id="formula_10">ω uv = {(I -B)Y ΠY (I -B) t } uv for u = v or {u, v} ∈ E ↔ .</formula><p>The profile log-EL in this approach is the function</p><formula xml:id="formula_11">ℓ(B; Y ) = max p∈PB ℓ(p; Y ) (10)</formula><p>obtained from the set of weight vectors</p><formula xml:id="formula_12">P B =    p ∈ P n : n i=1 p i Y i = 0, n i=1 p i   Y vi - s∈pa(v) β vs Y si     Y ui - t∈pa(u) β ut Y ti   = 0 ∀{v, u} ∈ E ↔    .</formula><p>(11) The MELE B is found by maximizing ℓ(B; Y ) over the set B(G) from (3), and then Ω = Ω( B). We emphasize that there are now only m(m -1)/2 -|E ↔ | covariance constraints, and only the matrix B needs to be optimized.</p><p>Following a standard strategy, we evaluate ℓ(B; Y ), that is, solve the "inner maximization" in (10) at a fixed B, through the dual problem. Strong duality holds because the constraints in (11) are linear in the weights</p><formula xml:id="formula_13">p i . Let G(Y i , B) be the map with coordinates G v (Y i , B) = Y iv for v ∈ V and G uv (Y i , B) = g u (Y i , B)g v (Y i , B) for each nonedge {u, v} / ∈ E ↔ , where g v (Y i , B) = Y vi -s∈pa(v) β vs Y si . With dual variables α ∈ R and λ ∈ R m+m(m-1)/2-|E↔|</formula><p>, the Lagrangian for the inner optimization over P B is</p><formula xml:id="formula_14">L B (p, α, λ) = - n i=1 log(p i ) + α n i=1 p i -1 + n n i=1 p i   v∈V λ v G v (Y i , B) + {u,v} / ∈E↔ λ uv G uv (Y i , B)   .</formula><p>(12) Maximizing over the weights, with α = n, we find</p><formula xml:id="formula_15">pi = 1 n 1 1 + v∈V λ v G v (Y i , B) + {u,v} / ∈E↔ λ uv G uv (Y i , B) ,<label>(13)</label></formula><p>and substitution into L B yields a convex dual function of λ. We optimize it via Newton-Raphson with a backtracking line search to ensure 0 ≤ p i ≤ 1.</p><p>In the "outer maximization", we optimize ℓ(B; Y ) with respect to B using a gradient based quasi-Newton method. Although we can only evaluate ℓ(B; Y ) numerically, once we have the optimal dual variables λ and the corresponding weights from (13), we can analytically compute the gradient of ℓ(B; Y ) as</p><formula xml:id="formula_16">∇ℓ(B; Y ) = -λ T n i=1 pi ∇G(Y i , B);<label>(14)</label></formula><p>see <ref type="bibr" target="#b3">Chaudhuri et al. [2017]</ref>. The Hessian, however, cannot be computed in closed form, so we use BFGS which builds an approximate Hessian via the gradient.</p><p>Although both formulations yield the same MELE, the profile approach from (10) and ( <ref type="formula">11</ref>) drastically eases difficulties (i) and (ii) discussed in Section 2 as the number of estimating equations for the covariance is reduced to m + m(m -1)/2 -|E ↔ |. This reduces the number of dual variables to optimize in the inner maximization. Moreover, when profiled, the outer maximization searches over only B ∈ B(G) while the naive direct formulation from (8) requires a search over both B ∈ B(G) and Ω ∈ W(G); in particular, positive definiteness of Ω needs to be respected in the naive optimization. Finally, satisfying the convex hull condition for the error covariances typically requires a simultaneous good choice of B and Ω. The directed edge weights can be easily initialized with regression estimates, but the covariance parameters are typically more difficult to specify. In Section 4, we show that the computational advantages produce substantial gains in computation time and converge to a valid stationary point at a much higher proportion of the time even when the sample size is small.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Small Sample Improvements</head><p>In addition to reformulating the optimization problem, we make two modifications to improve the performance of EL for SEMs. We apply adjusted empirical likelihood (AEL) to improve the search for a MELE and use extended empirical likelihood (EEL) to improve the coverage of confidence intervals.</p><p>Chen et al. <ref type="bibr">[2008]</ref> proposed AEL to alleviate the convex hull problem mentioned in difficulty (i) above. The adjustment amounts to adding a pseudo-observation whose contribution to the estimating equations is</p><formula xml:id="formula_17">G n+1 (B) = -a n Ḡ(B) = -a n 1 n n i=1 G(Y i , B</formula><p>) for a choice of a n &gt; 0. Adding this term ensures that no matter the value of B, the set of feasible weight vectors, now in P n+1 , is non-empty. Hence, the log-AEL ℓ a (B; Y ) and its gradient</p><formula xml:id="formula_18">∇ℓ a (B; Y ) = -λ T n i=1 pi + - a n n pn+1 ∇G(Y i , B)<label>(15)</label></formula><p>are well defined across the entire parameter space. Chen et al. <ref type="bibr">[2008]</ref> show that AEL retains the asymptotic properties of the original EL when a n = o(n 2/3 ), and suggest a n = log(n)/2. We adopt this choice. The terms in our covariance constraint are products,</p><formula xml:id="formula_19">G uv (Y i , B) = g v (Y i , B)g u (Y i , B</formula><p>). This is generally not true for the added term G n+1 (B) and is not clear how to define an appropriately sparse and positive definite matrix Ω(B) using AEL weights. Thus, we propose finding an estimate B that maximizes the AEL and computing Ω = Ω( B) based on weights from recalculating the original EL at B. As demonstrated in our numerical experiments, this approach alleviates some convergence issues but, of course, the original EL may be zero at the AEL maximizer B, in which case we do not have an estimate of Ω and say that the AEL procedure has not converged.</p><p>To address undercoverage of confidence regions for smaller samples, as described in difficulty (iii), we adopt the EEL of <ref type="bibr" target="#b25">Tsao and Wu [2014]</ref> who show that their χ 2 -calibrated EEL confidence regions outperform those from the original EL. Assuming the MELE B exists, a positive EEL may be defined for any matrix B ∈ B(G) by taking the original EL at a convex combination of B and B. Specifically, the log-EEL suggested by <ref type="bibr" target="#b25">Tsao and Wu [2014]</ref> is</p><formula xml:id="formula_20">ℓ e (B; Y ) = ℓ(h -1 (B, Y ), Y ) (16) for h(B, Y ) = B + γ(n, ℓ(θ; Y ))(B -B) with γ(n, ℓ(B; Y )) = 1 + 2(-n log(n)-ℓ(B;Y )) 2n</formula><p>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Asymptotic Distribution of Empirical Likelihood Estimators</head><p>It follows from <ref type="bibr">Qin and Lawless [1994, Theorem 1</ref>] that under the following assumptions, MELEs are asymptotically normal and empirical likelihood ratios converge to χ 2 limits. The same is true for the modifications from Section 3.2.</p><p>Proposition 1. Let G = (V, E → , E ↔ ) be a mixed graph, let B 0 ∈ B(G) and Ω 0 ∈ W(G). Let ǫ be a zero-mean random vector with covariance matrix Ω 0 . Assume that:</p><p>(a) The Jacobian of the parametrization (B, Ω) → Σ(B, Ω) defined on B(G) × W(G) has full rank at (B 0 , Ω 0 ).</p><p>(b) The joint distribution of ǫ and ǫ (2) = (ǫ v ǫ u : v, u ∈ V ) is non-degenerate and has finite third moments.</p><p>If Y 1 , . . . , Y n is an i.i.d. sample from the distribution determined by (B 0 , Ω 0 , ǫ), i.e., the distribution of (I -B 0 ) -1 ǫ, then the MELE θ = vech B , vech Ω( B) is asymptotically normal with</p><formula xml:id="formula_21">√ n θ -θ 0 → N (0, V ), V -1 = E ∂G(Y, θ 0 ) ∂θ t E[G(Y, θ 0 )G(Y, θ 0 ) t ] -1 E ∂G(Y, θ 0 ) ∂θ . (<label>17</label></formula><formula xml:id="formula_22">)</formula><p>Here, G is given by the estimating equations corresponding to the naive formulation in ( <ref type="formula">8</ref>). Furthermore, EL ratio statistics have χ 2 limits. In particular, for q = m + m(m + 1)/2 and</p><formula xml:id="formula_23">d = |E → | + |E ↔ | + m, we have 2 -n log(n) -ℓ( θ; Y ) → χ 2 (q-d) , 2 ℓ( θ; Y ) -ℓ(θ 0 ; Y ) → χ 2 d .<label>(18)</label></formula><p>We sketch the proof of the proposition in the appendix.</p><p>If the rank condition from (a) holds, then the rational map (B, Ω) → Σ(B, Ω) has full rank Jacobian at almost all choices of (B, Ω), and the map is generically finite-to-one. There is thus a connection to local/finite identifiability of (B, Ω) from the covariance matrix. For state-of-the-art methods for determining identifiability see <ref type="bibr" target="#b10">Foygel et al. [2012]</ref>, Chen [2016], <ref type="bibr" target="#b6">Drton and Weihs [2016]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Numerical Simulations</head><p>We now show a series of numerical experiments to evaluate the effectiveness of the proposed methods and compare the results to existing methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Convergence of Optimizers for Naive vs Profile Formulation</head><p>We first compare the naive/direct procedure which explicitly estimates B and Ω to the profiled procedure which only involves B. For both procedures, we use the original EL and adjusted EL. We also consider a hybrid method, which first finds the maximum AEL point to initialize a search which then uses original EL. We randomly generate acyclic mixed graphs with 8 nodes, 10 directed edges, and 6 bidirected edges. We randomly select directed edges u → v from all pairs such that u &lt; v and then select bidirected edges u ↔ v from the remaining unselected pairs. This setup ensures that (B, Ω) are generically identifiable from Σ(B, Ω) by the result of <ref type="bibr" target="#b1">Brito and Pearl [2002]</ref>.</p><p>We generate random true parameter matrices B = (β uv ) and Ω = (ω uv ) as follows. The coefficients β uv are drawn uniformly from (-1, . -.2) ∪ (.2, 1). For Ω, we draw off-diagonal elements ω uv = ω vu , u = v, uniformly from (-.8, . -.3) ∪ (.3, .8). We then use exponential draws to set ω vv = u =v |ω uv | + 1 + exp(1).</p><p>We consider errors from four distributions. First, we generate centered multivariate Gaussian errors with covariance matrix Ω. Second, we generate them from a multivariate T -distribution with 4 degrees of freedom, which we denote by T 4 , again with expectation zero and covariance matrix Ω. Third, we consider log-normal errors. In this case, we simulate a multivariate Gaussian vector Z, centered and with covariance matrix equal to the correlation matrix C that corresponds to Ω. We then set the error vector to ǫ = exp(Z)-√ e, which yields covariance matrix e(exp(C)-1). Finally, in order to draw a multivariate distribution with recentered gamma marginals and covariance Ω, we follow the steps:</p><formula xml:id="formula_24">1. Draw ǫ v ∼ gamma(shape = ω vv -v =u |ω uv |, scale = 1).</formula><p>2. For each {u, v} ∈ E ↔ , generate δ uvi ∼ gamma(shape = |ω uv |, scale = 1) and a random sign ξ uv ∈ {-1, 1}.</p><p>3. If ω uv &gt; 0, add ξ uv δ uvi to ǫ ui and ǫ vi . If ω uv &lt; 0, add ξ uv δ uvi to ǫ ui and -ξ uv δ uvi to ǫ vi . 4. Subtract the true mean from each error term so that it has mean 0.</p><p>All optimizations are initialized with a procedure from <ref type="bibr" target="#b8">Drton et al. [2017]</ref>, where the free elements of B are calculated via least squares. The resulting residuals are used to initialize the non-zero values ω uv . If a row is not diagonally dominant, the off-diagonal elements are scaled so that j =i |ω ij | &lt; .9 × ω ii to ensure Ω is positive definite.</p><p>Figure <ref type="figure">1</ref> shows that in all cases the profiled formulation converges at least as often as the naive formulation. AEL converges more often than original EL, and the hybrid procedure converges the most often. Even at a sample size of n = 100, the profiled problem converges nearly every single time, except in the case of log-normal errors. Figure <ref type="figure">2</ref> shows that the profiled form can be up to 40 times faster on average than the naive form.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Estimation Error</head><p>We now explore the estimation errors resulting from different approaches. We compare both original EL and AEL to the Gaussian MLE computed as in <ref type="bibr" target="#b7">Drton et al. [2009]</ref>, GLS, and WLS. The latter two estimates are computed using the R package lavaan <ref type="bibr" target="#b21">[Rosseel, 2012]</ref>. We also include a hybrid procedure that finds the Gaussian MLE B and then uses the resulting residuals and the maximum EL weights at B to form an estimate Ω = (I -B) t Y ΠY t (I -B) t . Note that the T 4 distribution does not have finite 6th moments, so the limiting distributions from Proposition 1 may not hold; however, all estimation procedures still appear to be consistent.</p><p>Proceeding as in Section 4.1, we generate 1000 graphs for each error distribution and sample size. To measure estimation accuracy, we average the relative error vech( Σ)vech(Σ) 2 / vech(Σ) 2 for Σ(B, Ω) across each of the simulation runs in which all methods converge; recall Figure <ref type="figure">1</ref>. The results are shown in Figure <ref type="figure" target="#fig_0">3</ref>.</p><p>In general, there is no substantial difference in accuracy between the adjusted and original empirical likelihood methods. For the Gaussian case, MLE and GLS perform better than the methods which do not assume Gaussianity, but the improvement is slight. In the T 4 and log-normal case, the EL procedures perform substantially better than the other methods. Finally, for the gamma case, the hybrid method seems to outperform the other methods, followed closely by the EL methods; however, the differences between the methods are not substantial. In Figure <ref type="figure">4</ref>, all methods converge more than 95% of the time in all distributions, except for the log-normal case. In this case, the WLS procedure still only converges roughly 90% at n = 1000.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Confidence Regions</head><p>We examine the coverage frequencies of joint confidence regions for the parameters β uv and ω uv . We construct Wald regions using the estimates of Var( θ) from the Gaussian MLE, GLS, and WLS. We also calculate a sandwich variance estimator using the Gaussian likelihood as the estimating equations and the asymptotic EL variance via <ref type="bibr" target="#b18">Qin and Lawless [1994]</ref>. Alternatively, we calculate the EL at (B 0 , Ω 0 ) using original EL, EEL, AEL. We then compare the resulting EL ratio to its asymptotic χ 2 distribution. If a method does not converge, we count this as a case in which the confidence region does not cover the true parameters.</p><p>At each sample size and error distribution, we construct 1000 graphs with 6 nodes, 8 directed edges and 4 bidirected edges from the procedure described in Section 4.1. For the T distribution, we increase the degrees of freedom to 7 to ensure Proposition 1 applies. The coverage rates for 90% confidence intervals are shown in Figure <ref type="figure">5</ref>. Based on the displayed results, regions obtained from the Gaussian MLE and GLS can only be recommended when the errors are (close to) Gaussian. The EEL method performs the best, staying close to the parametric methods in the Gaussian case and doing the best in most non-Gaussian scenarios. The sandwich method is another good choice. However, we also observe that in order to achieve nominal coverage levels very large sample sizes may be required. <ref type="bibr">Sachs et al. [2005, Figure 2</ref>] present a signaling network of 11 observed molecules and 13 unobserved molecules. The black edges in Figure <ref type="figure">6</ref> give a plausible mixed graph representation of that network and was also considered by <ref type="bibr" target="#b8">Drton et al. [2017]</ref>. A log-transformation of the available protein expression data improves Gaussianity but leaves the distribution of some of the variables skewed and/or multimodal. We consider two separate tests; each compares the SEM sub-model corresponding to the graph of black edges against a full model which adds one of the two red edges also shown in Figure <ref type="figure">6</ref>. Note that the added red edge from Mek → PKA induces a directed cycle. For the log-transformed data, we perform a Gaussian as well as an empirical likelihood ratio test. For the test involving the directed edge Mek → PKA, the Gaussian LR is .416 (p-value = .52) and the ELR is 4.379 (p-value = .04). For the test involving the bidirected edge Akt ↔ PIP2, the Gaussian LR is 15.216 (p-value ¡ .001) and the ELR is .782 (p-value = .37). While we do not have a certified gold standard network, and the implicit assumption of linearity may not be appropriate for all postulated relationships, these examples present situations in which the Gaussian assumption is particularly inappropriate and may cause concerns for a practitioner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Protein Signaling Network</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion</head><p>In this article, we showed that EL methods are an attractive alternative for estimation and testing of non-Gaussian linear SEMs. Our approach of profiling out the error covariance matrix Ω drastically reduces computational effort and creates a far more tractable and reliable estimation procedure. Furthermore, we showed that the use of AEL may further improve convergence of optimizers, particularly, when the sample size is small and the errors are skewed. EEL was seen to drastically improve the coverage rate of the joint confidence intervals.</p><p>Our EL methods are applicable under very few distributional assumptions, all the while allowing statistical inference in close to analogy to parametric modeling. When the data is non-Gaussian, the modified EL methods outperform the other methods we considered in almost all scenarios we explored. This concerns the proportion of times a valid estimate is returned, statistical efficiency, and also confidence region coverage. While there remains significant room for improvement in the design of confidence regions, we conclude that EL methods are a valuable tool for applications of linear SEMs to non-Gaussian data.    </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Log mean relative squared estimation error in Σ over 1000 simulations, plotted versus the sample size.Average is only taken on simulations in which all methods converged. A-adjusted EL; E-empirical likelihood; Ggeneralized least squares; H-hybrid Gauss/EL; M-Gaussian MLE; W-weighted least squares.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :Figure 5 :</head><label>45</label><figDesc>Figure 4: Proportion of times (over 1000 simulations) the method converged to a local maximum of the objective function, plotted versus the sample size. A-adjusted EL; E-empirical likelihood; G-generalized least squares; Hhybrid Gauss/EL; M-Gaussian MLE; W-weighted least squares.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Figure 2: The average run time in seconds among the simulations in which all methods converge to a valid stationary point, plotted versus the sample size. O-original EL; A-adjusted EL; H-hybrid EL. Red points indicate the profile formulation; blue points indicate the naive formulation.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">Gaussian errors Gaussian errors</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">T errors T errors</cell><cell></cell></row><row><cell>% converged time (s)</cell><cell>0.0 0.4 0.8 0 10 20 30 40</cell><cell>O O A A H H O O A A H H</cell><cell>O O A A H H O A H O A H</cell><cell>O O A A H H O A H O A H</cell><cell>O O A A H H O A H O A H</cell><cell>% converged time (s)</cell><cell>0.0 0.4 0.8 0 50 100 150</cell><cell>O O A A H H O O A A H H</cell><cell>O O A A H H O A H O A H</cell><cell>O O A A H H O A H O A H</cell><cell>O O A A H H O A H H O A</cell></row><row><cell></cell><cell></cell><cell>50 50</cell><cell>100 100</cell><cell>250 250</cell><cell>500 500</cell><cell></cell><cell></cell><cell>50 50</cell><cell>100 100</cell><cell>250 250</cell><cell>500 500</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">sample size sample size</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">sample size sample size</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Log-normal errors Log-normal errors</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Gamma errors Gamma errors</cell><cell></cell></row><row><cell>% converged time (s)</cell><cell>0.0 0.4 0.8 0 50 150</cell><cell>O O A A H H O O A A H H</cell><cell>O O A A H H O A A H O H</cell><cell>O O A A H H O A H O A H</cell><cell>O O A A H H H O A H O A</cell><cell>% converged time (s)</cell><cell>0.0 0.4 60 0.8 0 20 40</cell><cell>O O A A H H O O A A H H</cell><cell>O A A H H O O A H A O H</cell><cell>O O A A H H O A H O A H</cell><cell>O O A A H H H O A H O A</cell></row><row><cell></cell><cell></cell><cell>50 50</cell><cell>100 100</cell><cell>250 250</cell><cell>500 500</cell><cell></cell><cell></cell><cell>50 50</cell><cell>100 100</cell><cell>250 250</cell><cell>500 500</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">sample size sample size</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">sample size sample size</cell><cell></cell></row><row><cell cols="12">Figure 1: Proportion of 500 simulations which converge to a valid stationary point, plotted versus the sample size. O-</cell></row><row><cell cols="12">original EL; A-adjusted EL; H-hybrid EL. Red points indicate the profile formulation; blue points indicate the naive</cell></row><row><cell cols="3">formulation.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head><p>Proof of Proposition 1. We recall our notation θ = (B, Ω) and θ 0 = (B 0 , Ω 0 ). Based on the right-most expression in (9), the considered naive/direct estimating equations may be based on the function G(y, B) with coordinates</p><p>for v ∈ V and {u, v} ∈ V × V , respectively. Our claim follows from Theorem 1 of <ref type="bibr" target="#b18">Qin and Lawless [1994]</ref> under the following conditions:</p><p>(1) E(G(Y i , θ 0 )G(Y i , θ 0 ) t ) is positive definite.</p><p>(2) In a neighborhood of the d-dimensional parameter θ 0 , the derivative ∂G(y,θ) ∂θ is continuous, and ∂G(y,θ)   ∂θ and G(y, θ) 3 are bounded by an integrable function M 1 (y).</p><p>(3) E ∂G(Yi,θ0) ∂θ has rank d.</p><p>(4) ∂ 2 G(y, θ)/∂θθ T is continuous and ∂ 2 G(y, θ)/∂θθ T is bounded by an integrable function M 2 (y) in a neighborhood of the true parameter θ 0 .</p><p>Here, • denotes the Euclidean norm. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Structural equations with latent variables</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bollen</surname></persName>
		</author>
		<idno type="DOI">10.1002/9781118619179</idno>
		<ptr target="http://dx.doi.org/10.1002/9781118619179" />
	</analytic>
	<monogr>
		<title level="m">Wiley Series in Probability and Mathematical Statistics: Applied Probability and Statistics</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>A Wiley-Interscience Publication</publisher>
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A new identification condition for recursive models with correlated errors</title>
		<author>
			<persName><forename type="first">C</forename><surname>Brito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Struct. Equ. Model</title>
		<idno type="ISSN">1070-5511</idno>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="459" to="474" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Estimation of a covariance matrix with zeros</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Drton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Richardson</surname></persName>
		</author>
		<idno type="DOI">10.1093/biomet/asm007</idno>
		<ptr target="http://dx.doi.org/10.1093/biomet/asm007" />
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<idno type="ISSN">0006-3444</idno>
		<imprint>
			<biblScope unit="volume">94</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="199" to="216" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Hamiltonian Monte Carlo sampling in Bayesian empirical likelihood computation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mondal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yin</surname></persName>
		</author>
		<idno type="DOI">10.1111/rssb.12164</idno>
		<ptr target="http://dx.doi.org/10.1111/rssb.12164" />
	</analytic>
	<monogr>
		<title level="j">J. R. Stat. Soc. Ser. B. Stat. Methodol</title>
		<idno type="ISSN">1369-7412</idno>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="293" to="320" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Identification and overidentification of linear structural equation models</title>
		<author>
			<persName><forename type="first">B</forename><surname>Chen ; Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Variyath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Abraham</surname></persName>
		</author>
		<idno type="DOI">10.1198/106186008X321068</idno>
		<ptr target="http://dx.doi.org/10.1198/106186008X321068" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">U</forename><forename type="middle">V</forename><surname>Luxburg</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">I</forename><surname>Guyon</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2008">2016. 2008</date>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="426" to="443" />
		</imprint>
	</monogr>
	<note>Adjusted empirical likelihood and its properties</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning high-dimensional directed acyclic graphs with latent and selection variables</title>
		<author>
			<persName><forename type="first">D</forename><surname>Colombo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Maathuis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kalisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Richardson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Statist</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="294" to="321" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Generic identifiability of linear structural equation models by ancestor decomposition</title>
		<author>
			<persName><forename type="first">M</forename><surname>Drton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Weihs</surname></persName>
		</author>
		<idno type="DOI">10.1111/sjos.12227</idno>
		<ptr target="http://dx.doi.org/10.1111/sjos.12227" />
	</analytic>
	<monogr>
		<title level="j">Scand. J. Stat</title>
		<idno type="ISSN">0303-6898</idno>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1035" to="1045" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Computing maximum likelihood estimates in recursive linear models with correlated errors</title>
		<author>
			<persName><forename type="first">M</forename><surname>Drton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Eichler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Richardson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<idno type="ISSN">1532-4435</idno>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="2329" to="2348" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Computation of maximum likelihood estimates in cyclic structural equation models</title>
		<author>
			<persName><forename type="first">M</forename><surname>Drton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">S</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.03434</idno>
	</analytic>
	<monogr>
		<title level="j">Ann. Statist., to appear</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Byrnes</surname></persName>
		</author>
		<ptr target="https://CRAN.R-project.org/package=sem" />
		<title level="m">sem: Structural Equation Models</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>R package version 3.1-9</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Half-trek criterion for generic identifiability of linear structural equation models</title>
		<author>
			<persName><forename type="first">R</forename><surname>Foygel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Draisma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Drton</surname></persName>
		</author>
		<idno type="DOI">10.1214/12-AOS1012</idno>
		<ptr target="http://dx.doi.org/10.1214/12-AOS1012" />
	</analytic>
	<monogr>
		<title level="j">Ann. Statist</title>
		<idno type="ISSN">0090-5364</idno>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1682" to="1713" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Empty set problem of maximum empirical likelihood methods</title>
		<author>
			<persName><forename type="first">M</forename><surname>Grendár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Judge</surname></persName>
		</author>
		<idno type="DOI">10.1214/09-EJS528</idno>
		<ptr target="http://dx.doi.org/10.1214/09-EJS528" />
	</analytic>
	<monogr>
		<title level="j">Electron. J. Stat</title>
		<idno type="ISSN">1935-7524</idno>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1542" to="1555" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Empirical likelihood estimation and testing in covariance structure models</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kolenikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<ptr target="http://staskolenikov.net" />
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A comparison of some methodologies for the factor analysis of non-normal Likert variables: A note on the size of the model</title>
		<author>
			<persName><forename type="first">B</forename><surname>Muthen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kaplan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">British Journal of Mathematical and Statistical Psychology</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="19" to="30" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The performance of ML, GLS, and WLS estimation in structural equation modeling under conditions of misspecification and nonnormality</title>
		<author>
			<persName><forename type="first">U</forename><forename type="middle">H</forename><surname>Olsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Foss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">V</forename><surname>Troye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Howell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Structural Equation Modeling</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="557" to="595" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Empirical likelihood ratio confidence intervals for a single functional</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Owen</surname></persName>
		</author>
		<idno type="DOI">10.1093/biomet/75.2.237</idno>
		<ptr target="http://dx.doi.org/10.1093/biomet/75.2.237" />
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<idno type="ISSN">0006-3444</idno>
		<imprint>
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="237" to="249" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Empirical likelihood</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Owen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
			<publisher>CRC press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Causality</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Models, reasoning, and inference</title>
		<meeting><address><addrLine>Cambridge</addrLine></address></meeting>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
	<note>second edition</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Empirical likelihood and general estimating equations</title>
		<author>
			<persName><forename type="first">J</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lawless</surname></persName>
		</author>
		<idno type="DOI">10.1214/aos/1176325370</idno>
		<ptr target="http://dx.doi.org/10.1214/aos/1176325370" />
	</analytic>
	<monogr>
		<title level="j">Ann. Statist</title>
		<idno type="ISSN">0090-5364</idno>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="300" to="325" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">R: a language and environment for statistical computing</title>
		<author>
			<persName><forename type="first">Team</forename><surname>Core</surname></persName>
		</author>
		<ptr target="https://www.R-project.org/" />
	</analytic>
	<monogr>
		<title level="m">R Foundation for Statistical Computing</title>
		<meeting><address><addrLine>Vienna, Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Ancestral graph Markov models</title>
		<author>
			<persName><forename type="first">T</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Spirtes</surname></persName>
		</author>
		<idno type="DOI">10.1214/aos/1031689015</idno>
		<ptr target="http://dx.doi.org/10.1214/aos/1031689015" />
	</analytic>
	<monogr>
		<title level="j">Ann. Statist</title>
		<idno type="ISSN">0090-5364</idno>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="962" to="1030" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">lavaan: An R package for structural equation modeling</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Rosseel</surname></persName>
		</author>
		<ptr target="http://www.jstatsoft.org/v48/i02/" />
	</analytic>
	<monogr>
		<title level="j">Journal of Statistical Software</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="36" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Causal protein-signaling networks derived from multiparameter single-cell data</title>
		<author>
			<persName><forename type="first">K</forename><surname>Sachs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Pe'er</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Lauffenburger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">P</forename><surname>Nolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">308</biblScope>
			<biblScope unit="issue">5721</biblScope>
			<biblScope unit="page" from="523" to="529" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Identification of joint interventional distributions in recursive semi-Markovian causal models</title>
		<author>
			<persName><forename type="first">I</forename><surname>Shpitser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
		<ptr target="http://www.aaai.org/Library/AAAI/2006/aaai06-191.php" />
	</analytic>
	<monogr>
		<title level="m">Proceedings, The Twenty-First National Conference on Artificial Intelligence and the Eighteenth Innovative Applications of Artificial Intelligence Conference</title>
		<meeting>The Twenty-First National Conference on Artificial Intelligence and the Eighteenth Innovative Applications of Artificial Intelligence Conference<address><addrLine>Boston, Massachusetts, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">July 16-20, 2006. 2006</date>
			<biblScope unit="page" from="1219" to="1226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Causation, prediction, and search. Adaptive Computation and Machine Learning</title>
		<author>
			<persName><forename type="first">P</forename><surname>Spirtes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Glymour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Scheines ; David Heckerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Meek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><forename type="middle">F</forename><surname>Cooper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Richardson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">With additional material by</title>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>A Bradford Book</publisher>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
	<note>second edition</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Extended empirical likelihood for estimating equations</title>
		<author>
			<persName><forename type="first">M</forename><surname>Tsao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<idno type="DOI">10.1093/biomet/asu014</idno>
		<ptr target="http://dx.doi.org/10.1093/biomet/asu014" />
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<idno type="ISSN">0006-3444</idno>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="703" to="710" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Probability distributions with summary graph structure</title>
		<author>
			<persName><forename type="first">N</forename><surname>Wermuth</surname></persName>
		</author>
		<idno type="DOI">10.3150/10-BEJ309</idno>
		<ptr target="http://dx.doi.org/10.3150/10-BEJ309" />
	</analytic>
	<monogr>
		<title level="j">Bernoulli</title>
		<idno type="ISSN">1350- 7265</idno>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="845" to="879" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Correlation and causation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wright</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Agric. Res</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="557" to="585" />
			<date type="published" when="1921">1921</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
