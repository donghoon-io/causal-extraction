<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Identifiability and estimation of recursive max-linear models</title>
				<funder>
					<orgName type="full">Deutsche Forschungsgemeinschaft (DFG)</orgName>
				</funder>
				<funder>
					<orgName type="full">TUM International Graduate School of Science and Engineering</orgName>
					<orgName type="abbreviated">IGSSE</orgName>
				</funder>
				<funder>
					<orgName type="full">Alexander von Humboldt Stiftung</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2019-10-08">October 8, 2019</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Nadine</forename><surname>Gissibl</surname></persName>
							<email>n.gissibl@tum.de</email>
							<affiliation key="aff0">
								<orgName type="department">Center for Mathematical Sciences</orgName>
								<orgName type="institution">Technical University of Munich</orgName>
								<address>
									<addrLine>Boltzmannstrasse 3</addrLine>
									<postCode>85748</postCode>
									<settlement>Garching</settlement>
									<country>Germany;</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Claudia</forename><surname>Klüppelberg</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Mathematical Sciences</orgName>
								<orgName type="institution">Technical University of Munich</orgName>
								<address>
									<addrLine>Boltzmannstrasse 3</addrLine>
									<postCode>85748</postCode>
									<settlement>Garching</settlement>
									<country>Germany;</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Steffen</forename><surname>Lauritzen</surname></persName>
							<email>lauritzen@math.ku.dk</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Mathematical Sciences</orgName>
								<orgName type="institution">University of Copenhagen</orgName>
								<address>
									<addrLine>Universitetsparken 5</addrLine>
									<postCode>2100</postCode>
									<settlement>Copenhagen</settlement>
									<country>Denmark;</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Identifiability and estimation of recursive max-linear models</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2019-10-08">October 8, 2019</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1111/sjos.12446</idno>
					<idno type="arXiv">arXiv:1901.03556v2[math.ST]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-28T23:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>MSC 2010 subject classifications: Primary 60E15</term>
					<term>62H12; secondary 62G05</term>
					<term>60G70</term>
					<term>62-09 Causal inference</term>
					<term>Bayesian network</term>
					<term>directed acyclic graph</term>
					<term>extreme value theory</term>
					<term>generalized maximum likelihood estimation</term>
					<term>graphical model</term>
					<term>identifiability</term>
					<term>max-linear model</term>
					<term>structural equation model</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We address the identifiablity and estimation of recursive max-linear structural equation models represented by an edge weighted directed acyclic graph (DAG). Such models are generally unidentifiable and we identify the whole class of DAGs and edge weights corresponding to a given observational distribution. For estimation, standard likelihood theory cannot be applied because the corresponding families of distributions are not dominated. Given the underlying DAG, we present an estimator for the class of edge weights and show that it can be considered a generalized maximum likelihood estimator. In addition, we develop a simple method for identifying the structure of the DAG. With probability tending to one at an exponential rate with the number of observations, this method correctly identifies the class of DAGs and, similarly, exactly identifies the possible edge weights.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Establishing and understanding cause-effect relations is an omnipresent desire in science and daily life. It is especially important when dealing with extreme events, because they are mostly dangerous and very costly; knowing and understanding the causes of such events and their causal relations could help us to deal better with them. Examples include incidents at airplane landings (Gissibl et al. <ref type="bibr" target="#b12">[13]</ref>), flooding in river networks (Asadi et al. <ref type="bibr" target="#b0">[1]</ref>, Engelke and Hitz <ref type="bibr" target="#b8">[9]</ref>), financial risk (Einmahl et al. <ref type="bibr" target="#b7">[8]</ref>), and chemical pollution of rivers (Hoef et al. <ref type="bibr" target="#b14">[15]</ref>). Such applications, where extreme risks may propagate through a network, have been the motivation behind the definition of recursive max-linear (ML) models in Gissibl and Klüppelberg <ref type="bibr" target="#b11">[12]</ref>. Recursive ML models are structural equation models (SEMs) represented by a directed acyclic graph (DAG) and thereby obey the basic Markov properties associated with directed graphical models (Lauritzen <ref type="bibr" target="#b20">[21]</ref>, Lauritzen et al. <ref type="bibr" target="#b21">[22]</ref>). Both SEMs (see for example Bollen <ref type="bibr" target="#b2">[3]</ref>, Pearl <ref type="bibr" target="#b22">[23]</ref>) and directed graphical models (see for example Koller and Friedman <ref type="bibr" target="#b18">[19]</ref>, Lauritzen <ref type="bibr" target="#b19">[20]</ref>, Spirtes et al. <ref type="bibr" target="#b26">[27]</ref>) are well-established concepts for the understanding and quantification of causal inference from observational data. We note that Hitz and Evans <ref type="bibr" target="#b13">[14]</ref> and Engelke and Hitz <ref type="bibr" target="#b8">[9]</ref> discuss graphical models for extremes that are based on undirected graphs.</p><p>Recursive ML models are defined by a DAG, a collection of edge weights, and a vector of independent innovations. Important research problems that are addressed for recursive SEMs are the question of identifiability of the coefficients and the associated DAG from the observational distribution. Although the true DAG and edge weights for a recursive ML model are not identifiable from the observational distribution, the so-called max-linear coefficient matrix is identifiable and determines the possible class of DAGs and edge weights uniquely.</p><p>We shall show that estimation and structure learning of recursive ML models can be done in a simple and efficient fashion by exploiting properties of the ratios between observable components of the model. For a sufficiently large number of observations, these ratios identify the true ML coefficient matrix with a probability that converges exponentially fast to 1. For the situation where the DAG is known, we show that our estimator can be considered a maximum likelihood estimator in an extended sense, originally introduced by Kiefer and Wolfowitz <ref type="bibr" target="#b17">[18]</ref>.</p><p>Our paper is organized as follows. In Section 2 we introduce the model class of recursive ML models and the notation used throughout. In Section 3 we discuss the identifiability of a recursive ML model from its observational distribution. Here we show distributional properties of the ratio between two components. Based on these properties, we suggest an identification method. Section 4 is then devoted to the estimation of recursive ML models where we assume the DAG to be known. We show that the proposed estimates are generalized maximum likelihood estimates (GMLEs) in the sense of Kiefer-Wolfowitz. The main part is here the derivation of a specific Radon-Nikodym derivative. In Section 5 we complement the theoretical findings on the identifiability of recursive ML models with an efficient procedure to learn recursive ML models from observations only, even when the DAG itself is also unknown. Section 6 concludes and suggests further directions of research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Preliminaries -recursive max-linear models</head><p>In this section we introduce notation and summarize the most important properties of recursive ML models needed. A recursive ML model for a random vector X " pX 1 , . . . , X d q is specified by an underlying structure in terms of a DAG D with nodes V " t1, . . . , du, positive edge weights c ki for i P V and k P papiq, and independent positive random variables Z 1 , . . . , Z d with support R `:" p0, 8q and atom-free distributions:</p><formula xml:id="formula_0">X i " ł kPpapiq c ki X k _ Z i , i " 1, . . . , d,<label>(2.1)</label></formula><p>where papiq are the parents of node i in D. To highlight the DAG D, we say that X follows a recursive ML model on D. Note that this is a slight variation of the original definition in <ref type="bibr" target="#b11">[12]</ref>. We shall refer to Z " pZ 1 , . . . , Z d q as the vector of innovations.</p><p>In the context of risk analysis, natural candidates for distributions of the innovations are extreme value distributions or distributions in their domain of attraction, resulting in a corresponding multivariate distribution (for details and background on multivariate extreme value models, see for example Beirlant et al. <ref type="bibr" target="#b1">[2]</ref>, de Haan and Ferreira <ref type="bibr" target="#b6">[7]</ref>, Resnick <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25]</ref>).</p><p>Throughout the paper we use the following notation. The sets anpiq, papiq, and depiq contain the ancestors, parents, and descendants of node i in D. We set Anpiq " anpiq Y tiu and Papiq " papiq Y tiu. For U Ĺ V we write X U " pX ℓ , ℓ P U q and accordingly for x P R d `, x U " px ℓ , ℓ P U q. Instead of k P papiq we also write k Ñ i. Assigning the weight d ji ppq " ś n´1 ν"0 c kν k ν`1 to every path p " rj " k 0 Ñ k 1 Ñ ¨¨¨Ñ k n " is and denoting the set of all paths from j to i by P ji , the non-negative matrix B " pb ij q dˆd with entries b ji " ł pPP ji d ji ppq for j P anpiq, b ii " 1, and b ji " 0 for j P V zAnpiq,</p><p>is said to be the ML coefficient matrix of X. This means for distinct i, j P V , b ji is positive if and only if there is a path from j to i; in that case b ji is the maximum weight of all paths from j to i, where the weight of a path is the product of all edge weights c ki along this path. We say that a path from j to i whose weight equals b ji is max-weighted.</p><p>The components of X can also be expressed as max-linear functions of their ancestral innovations and an independent one; the corresponding ML coefficients are the entries of B: <ref type="bibr" target="#b11">[12]</ref>.</p><formula xml:id="formula_2">X i " d ł j"1 b ji Z j " ł jPAnpiq b ji Z j , i " 1, . . . , d; (2.3) see Theorem 2.2 of</formula><p>For two non-negative matrices F and G, where the number of columns in F is equal to the number of rows in G, we define the matrix product d : R</p><formula xml:id="formula_3">mˆn `ˆR nˆp `Ñ R mˆp `by pF " pf ij q mˆn , G " pg ij q nˆp q Þ Ñ F d G :" ´n ł k"1 f ik g kj ¯mˆp , (<label>2.4)</label></formula><p>where R `" r0, 8q. The triple pR `, _, ¨q, is an idempotent semiring with 0 as 0-element and 1 as 1-element and the operation d is therefore a matrix product over this semiring; see for example Butkovič <ref type="bibr" target="#b3">[4]</ref>. Denoting by M all d ˆd matrices with non-negative entries and by _ the componentwise maximum between two matrices, pM, _, dq is also a semiring with the null matrix as 0-element and the d ˆd identity matrix I d as 1-element. The matrix product d allows us to represent the ML coefficient matrix B of X in terms of the weighted adjacency matrix pc ij 1 papjq piqq dˆd of D since (2.2) and (2.3) simply become</p><formula xml:id="formula_4">B " pI d _ Cq dpd´1q " d´1 ł k"0 C dk , X " Z d B, (<label>2.5)</label></formula><p>where we have let A d0 " I d and A dk " A dpk´1q d A for A P R dˆd `and k P N; see Proposition 1.6.15 of Butkovič <ref type="bibr" target="#b3">[4]</ref> as well as Theorem 2.4 and Corollary 2.5 of <ref type="bibr" target="#b11">[12]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Identifiability of a recursive max-linear model</head><p>In this section we discuss the question of identifiability of the elements of a recursive ML model from the distribution LpXq of X. Indeed we shall show the following: Theorem 3.1. Let LpXq be the distribution of X following a recursive ML model. Then its ML coefficient matrix B and the distribution of its innovation vector Z are identifiable from LpXq. Furthermore, the class of all DAGs and edge weights that could have generated X by (2.1) can be obtained.</p><p>The remaining part of this section is devoted to proving Theorem 3.1, but first we shall consider a small example, illustrating the issues. According to (2.1), the components of X have the following representations</p><formula xml:id="formula_5">X 1 " Z 1 , X 2 " c 12 X 1 _ Z 2 , and X 3 " c 13 X 1 _ c 23 X 2 _ Z 3 .</formula><p>but also representations in terms of the innovations using (2.3) as Consequently, we can neither identify D nor the value c 13 from the distribution LpXq of X. However, note that the ML coefficient b 13 " c 12 c 23 _ c 13 is uniquely determined. If we however assume that c 13 ą c 12 c 23 , only D and the edge weights c 12 , c 23 , c 13 represent X in the sense of (2.1). Thus in this case the DAG and the edge weights are identifiable from the distribution LpXq. l</p><formula xml:id="formula_6">X 1 " Z 1 , X 2 " c 12 Z 1 _ Z 2 ,</formula><p>As conclusion of Example 3.2, it is generally not possible to identify the true DAG D and the edge weights c ki underlying X in representation (2.1) from LpXq, since several DAGs and edge weights may exist such that X has this representation. The smallest DAG of this kind is the DAG that has an edge k Ñ i if and only if k Ñ i is the only max-weighted path from k to i. We call this DAG D B the minimum ML DAG of X and note that this is uniquely determined from the ML coefficient matrix B. All other DAGs representing X are those that include the edges of D B and whose nodes have the same ancestors. The edge weights c ki in the representation (2.1) of X are only uniquely determined for edges contained in D B ; namely, by b ki ; otherwise, c ki may be any number in p0, b ki s. We summarize these findings in the following theorem which is paraphrasing Theorems 5.3 and 5.4 of <ref type="bibr" target="#b11">[12]</ref>. Based on the above observations, we investigate the identifiability of the whole class of DAGs and edge weights representing the max-linear structural equations (2.1) of X from LpXq. Since this class can be recovered from B, it suffices to clarify whether B is identifiable from LpXq. There are many ways to prove that this is indeed the case. The way we present in this section suggests a simple procedure to estimate B from independent realizations of X (see Algorithm 5.1 below). An alternative way can be found in Appendix 4.A.1 of <ref type="bibr" target="#b10">[11]</ref>.</p><p>The ratios Y " tY ij " X j {X i , i, j " 1 . . . , du between all pairs of components of X are the essential quantities used to identify B from LpXq. We first present distributional properties of these ratios, where we let pΩ, F, Pq denote the probability space of Z and, hence, of X. In what follows, we use the standard convention and write events such as tω P Ω : X i pωq ă X j pωqu as tX i ă X j u, etc. Unsurprisingly, because of the max-linear representation (2.3) of the components of X, the ratios inherit their distributional properties from the innovations. It plays an important role that the event Z i " xZ j ( for distinct i, j P V and x P R `has probability zero, (</p><p>which follows from the independence of the innovations and the fact that their distributions are atom-free.</p><p>Lemma 3.4. Let i, j P V be distinct.</p><p>(a) The ratio Y ji " X i {X j has an atom in x P R `if and only if Anpiq X Anpjq ‰ H and x " b ℓi {b ℓj for some ℓ P Anpiq X Anpjq.</p><p>(b) We have</p><formula xml:id="formula_8">supppY ji q " $ ' ' &amp; ' ' % rb ji , 8q if j P anpiq `0, 1{b ij ‰ if j P depiq R `otherwise,</formula><p>where supppY ji q denotes the support of Y ji .</p><p>Proof. To establish (a) note that (2.3) and (3.1) imply that the sets tX i " xX j u "</p><formula xml:id="formula_9">Ž ℓPAnpiq b ℓi Z ℓ " Ž ℓPAnpjq xb ℓj Z ℓ (<label>and</label></formula><formula xml:id="formula_10">! ł ℓPAnpiqXAnpjq: b ℓi "b ℓj x b ℓi Z ℓ ą ł ℓPAnpiqXAnpjq: b ℓi ‰b ℓj x pb ℓi _ xb ℓj qZ ℓ _ ł ℓPAnpiqzAnpjq b ℓi Z ℓ _ ł ℓPAnpjqzAnpiq xb ℓj Z ℓ )</formula><p>differ only by a set of probability zero. Since the innovations are independent and have support R `the conclusion follows.</p><p>To establish (b) note that the support R `of the innovations and the representation (2.3) yield</p><formula xml:id="formula_11">supppY ji q " # Ž ℓPAnpiq b ℓi z ℓ Ž ℓPAnpjq b ℓj z ℓ : z AnpiqYAnpjq P R |AnpiqYAnpjq| `+ .</formula><p>The continuity of the function</p><formula xml:id="formula_12">R |AnpiqYAnpjq| `Ñ R `, z AnpiqYAnpjq Þ Ñ Ž ℓPAnpiq b ℓi z ℓ Ž ℓPAnpjq b ℓj z ℓ</formula><p>implies that supppY ji q is an interval in R `. Since for j P anpiq by Corollary 3.13 of <ref type="bibr" target="#b11">[12]</ref> b ji ď Y ji and by (a) b ji is an atom of Y ji , it suffices to show that j P anpiq if supppY ji q has a positive lower bound. For this assume that j R anpiq. Because of the positive lower bound of supppY ji q, there exists some a P R `such that ł</p><formula xml:id="formula_13">ℓPAnpiqXAnpjq ab ℓj z ℓ _ ł ℓPAnpjqzAnpiq ab ℓj z ℓ ď ł ℓPAnpiq b ℓi z ℓ (3.2)</formula><p>for all z AnpiqYAnpjq P R |AnpiqYAnpjq| `. As AnpjqzAnpiq ‰ H, for fixed z Anpiq P R |Anpiq| `, we can choose z ℓ for some ℓ P AnpjqzAnpiq so large that ab ℓj z ℓ is greater than the maximum on the right-hand side of (3.2). This contradicts (3.2). Hence, j P anpiq.</p><p>In Table <ref type="table">3</ref>.1 we summarize the results of Lemma 3.4: depending on the relationship between i and j in D, the support and atoms of Y ji are shown. Relationship between i and j supppY ji q Atoms j P anpiq rb ji , 8q tb ℓi {b ℓj , ℓ P Anpjqu</p><formula xml:id="formula_14">i P anpjq p0, 1{b ij s tb ℓi {b ℓj , ℓ P Anpiqu otherwise: if anpiq X anpjq ‰ H R `tb ℓi {b ℓj , ℓ P anpiq X anpjqu if anpiq X anpjq " H R `H</formula><p>Table <ref type="table">3</ref>.1 and the fact that b ji " 0 for j R Anpiq (cf. (2.2)) suggest the following algorithm to find B from LpXq since we can identify the support of Y ji from LpXq. This proves the identifiability of B from LpXq. In fact, it is sufficient to know supppY ji q for all i, j P V with i ‰ j rather than the whole distribution LpXq.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 3.5. [Find B from LpXq]</head><p>1. For all i P V " t1, . . . , du, set b ii " 1.</p><p>2. For all i, j P V with i ‰ j, find supppY ji q: if supppY ji q " ra, 8q for some a P R `, then set b ji " a; else, set b ji " 0.</p><p>So far we have shown that the ML coefficient matrix B of X can be obtained from LpXq. Since all DAGs and edge weights that represent X in the sense of (2.1) can be determined from B, the only quantities we do not know about yet but appear in the definition of X are the innovations. In what follows we show that the distribution of the innovation vector Z is also identifiable from LpXq. For this, due to the identifiability of B from LpXq and the independence of the innovations, it suffices to provide an algorithm that determines the distributions of the innovations from LpXq and B. Note that B also determines the ancestral relationships between any pair of nodes in that j P Anpiq for any DAG representing X if and only if b ji ą 0.</p><p>We denote by F Z i the distribution function of the innovation Z i . For this algorithm, we do not have to know the whole distribution LpXq; it is enough to know the ML coefficient matrix B and the univariate marginal distribution functions of LpXq.</p><formula xml:id="formula_15">Algorithm 3.6. [Find F Z 1 pxq, . . . , F Z d pxq for x P R `from B and LpXq] For ν " 0, . . . , d ´1, for i P V such that |anpiq| " |tj P V ztiu : b ji ‰ 0u| " ν, set F Z i pxq " PpX i ď xq ś jPanpiq F Z j px{b ji q .</formula><p>Here we have used the convention that ś jPH a j " 1. The correctness of Algorithm 3.6 follows from the independence of the innovations and representation (2.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Estimation with known directed acyclic graph</head><p>In this section we consider independent realizations x ptq " `xptq 1 , . . . , x ptq d ˘, t " 1, . . . , n, of a random vector X " pX 1 , . . . , X d q following a recursive ML model with its DAG D given. Further, we consider the distribution of the innovation vector to be fixed; however, we emphasize that our estimates and their validity do not depend on this distribution as long as it prescribes independent, atom-free margins with support R `. Our aim is the estimation of the edge weights c ki and the ML coefficient matrix B. We recall from Theorem 3.3 that only the ML coefficient matrix B can be directly identified from LpXq and hence our focus will be on the estimation of B; subsequently all DAGs and systems of edge weights compatible with B can be obtained from Theorem 3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The ML coefficient matrix B</head><p>In the following we let BpDq denote the class of possible ML coefficient matrices of all recursive ML models on D. For B being a matrix with non-negative entries and diagonal elements b ii " 1 we define B 0 :" pb ij 1 papjq piqq dˆd . Then it holds that B P BpDq if and only if B satisfies the following rb ji ą 0 ðñ j P Anpiqs and B " </p><formula xml:id="formula_16">I d _ pB d B 0 q; (<label>4</label></formula><formula xml:id="formula_17">0 0 0 b 44 ‹ ‹ ‹ ‹ ‹ ‚ B 0 " ¨0 b 12 0 0 0 0 0 b 24 0 0 0 b 34 0 0 0 0 ‹ ‹ ‹ ‹ ‹ ‚</formula><p>, where we have used that 1 and 2 are not ancestors of 3 and 1 is not a parent of 4. We wish to check whether B P BpDq for this particular DAG so we further calculate </p><formula xml:id="formula_18">I 4 _ pB d B 0 q " I 4 _<label>¨0</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A simple estimate of B</head><p>Next we discuss a sensible estimate of B. Table <ref type="table">3</ref>.1 shows that for j P anpiq the minimal value that can be observed for the ratio Y ji " X i {X j is b ji , which is an atom of Y ji . This suggests the following estimate B of the ML coefficient matrix:</p><p>bii " 1, bji " 0 for j P V zAnpiq, and bji "</p><formula xml:id="formula_19">n ľ t"1 y ptq ji " n ľ t"1 x ptq i x ptq j</formula><p>for j P anpiq.</p><p>Davis and Resnick <ref type="bibr" target="#b5">[6]</ref> suggested such minimal observed ratios as estimates for parameters in max-ARMA processes. For n sufficiently large, we can expect to observe the atoms b ji for j P anpiq in the sample x p1q , . . . , x pnq and, hence, to estimate the ML coefficients exactly. However, if n is not large we may with positive probability have that B is not an ML coefficient matrix of any recursive ML model on D as the following simple example shows: </p><formula xml:id="formula_20">if and only if A " pI d _ B 0 q dpd´1q .</formula><p>Proof. We first show that A " pI d _ B 0 q dpd´1q satisfies (4.2). It is immediate that a ji ą 0 ðñ j P Anpiq. We have <ref type="bibr">([4]</ref>, Proposition 1.6.10) that</p><formula xml:id="formula_21">pI d _ B 0 q dpd´1q " d´1 ł k"0 B dk 0 " 8 ł k"0</formula><p>B dk 0 and hence</p><formula xml:id="formula_22">I d _ pA d B 0 q " I d _ tpI d _ B 0 q dpd´1q d B 0 u " I d _ t 8 ł k"1 B dk 0 u " 8 ł k"0 B dk 0 " A.</formula><p>It is easy to see directly that B dk 0 " 0 for k ě d and hence if Ǎ is a solution to (4.2) we get by iteration, using that pM _ N q d K " pM d Kq _ pN d Kq,</p><formula xml:id="formula_23">Ǎ " I d _ p Ǎ d B 0 q " I d _ rtI d _ p Ǎ d B 0 qu d B 0 s " I d _ B 0 _ p Ǎ d B d2 0 q " ¨¨" pI d _ B 0 q dpd´1q _ p Ǎ d B dd 0 q " pI d _ B 0 q dpd´1q " A</formula><p>and hence the solution to the equation is unique.</p><p>Thus we may define the estimate p B by first calculating the matrix B0 " p bij 1 papjq piqq dˆd and then iterating the d-matrix product as:</p><formula xml:id="formula_24">p B " pI d _ B0 q dpd´1q . (4.3)</formula><p>It then follows that p B 0 " B0 and Lemma 4.3 yields that p B is the unique element of BpDq satisfying (4.3). By Lemma 3.4(b), we also have b ji ď p b ji ď bji for j P anpiq. Consequently, when using p B or B as an estimate of B, we never underestimate a ML coefficient; furthermore, the matrix p B always estimates B more precisely than B and since we always have p B P BpDq, p B seems to be clearly preferable as an estimate of B.</p><p>The following example shows how effective the estimate p B can be; in particular, n does not necessarily need to be large. </p><formula xml:id="formula_25">X 2 " b 12 X 1 ( X X 3 " b 13 X 1 ( X X 4 " b 24 X 2 ( X X 4 " b 34 X 3 ( , then p</formula><p>B " B so we estimate all ML coefficients exactly. Note that this event has positive probability and occurs P-almost surely if and only if Z 1 realizes all node variables; i.e., if X 2 " b 12 Z 1 , X 3 " b 13 Z 1 , and X 4 " b 14 Z 1 . l</p><p>Since by Table <ref type="table">3</ref>.1 PpX i " b ki X k q ą 0 for k P papiq, it follows from the Borel-Cantelli lemma that p b ki P-almost surely equals the true value for n sufficiently large. Thus, if n is large, p B finds, with probability 1, the true B. In <ref type="bibr" target="#b5">[6]</ref> this is discussed in a time-series framework used there and in Davis and McCormick <ref type="bibr" target="#b4">[5]</ref> they show that under suitable assumptions in the timeseries framework, this estimator is asymptotically Fréchet distributed. Assuming the probability of tX i " b ki X k u is known, we show next how one has to choose n to observe this event with probability greater than 1´p for some p P p0, 1q. We also prove that the probability for estimating the true b ki converges exponentially fast to 1. Proof. First note that the events tX i " b ki X k u and tX i ą b ki X k u are complementary and both have positive probability. Further, using that X p1q , . . . , X pnq are independent and identically distributed yields</p><formula xml:id="formula_26">P ´n ľ t"1 Y ptq ki " b ki ¯" 1 ´P´n ľ t"1 Y ptq ki ą b ki ¯" 1 ´n ź t"1 PpY ptq ki ą b ki q " 1 ´PpY ki ą b ki q n .</formula><p>Altogether, the statements follow.</p><p>In conclusion, p B has the nice property to be 'geometrically consistent' in the sense that the probability of t p B " Bu converges exponentially fast to one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The matrix p B is a generalized maximum likelihood estimate</head><p>As we found in the previous section, the estimate p B is preferable to the direct estimate B as it will always be closer to the true value. In this section we further establish that p B is not just an ad hoc estimator, but can indeed be derived from likelihood considerations.</p><p>For B P BpDq and a fixed distribution of the innovation vector we let P B denote the probability measure induced by a recursive ML model on D with ML coefficient matrix B, i.e. the distribution of X where X " Z d B. We shall denote the family of these probability measures by PpDq.</p><p>We cannot use standard maximum likelihood methods to estimate B, since the family PpDq is not dominated (cf. Example 4.4.1 of <ref type="bibr" target="#b10">[11]</ref>) and hence the standard likelihood function is not well defined. However, there exist generalizations of maximum likelihood estimation (GMLE) that cover the undominated case as well; Kalbfleisch and Prentice <ref type="bibr" target="#b16">[17]</ref>, Kiefer and Wolfowitz <ref type="bibr" target="#b17">[18]</ref>, and Scholz <ref type="bibr" target="#b25">[26]</ref> suggested such extensions. We essentially follow the Kiefer-Wolfowitz definition of a GMLE as also done, for example, by Gill et al. <ref type="bibr" target="#b9">[10]</ref> and Johansen <ref type="bibr" target="#b15">[16]</ref>. In the following we shall show that p B can be seen as a maximum likelihood estimate of B in the extended sense introduced by Kiefer and Wolfowitz in <ref type="bibr" target="#b17">[18]</ref>.</p><p>Let P be a family of probability measures on pR Since P is absolutely continuous with respect to P `Q, the density dP {dpP `Qq always exists according to the Radon-Nikodym theorem. This means that the GMLE is well-defined, save for the usual ambiguity in the method of maximum likelihood that densities are only defined up to null sets and therefore a specific choice of densities must be made. The <ref type="bibr">Kiefer</ref> </p><p>(C): P B ˚pA 1 pB, B ˚qq " 0.</p><p>Then we choose as density the measurable function from R d `to t0, 1{2, 1u defined as</p><formula xml:id="formula_28">x Þ Ñ ρpx, B, B ˚q :" 1 2 ¨1A 1{2 pB,B ˚qpxq `1A 1 pB,B ˚qpxq " $ ' ' &amp; ' ' % 0, if x P A 0 pB, B ˚q, 1 2 , if x P A 1{2 pB, B ˚q, 1, if x P A 1 pB, B ˚q. (4.6)</formula><p>This is a valid density because, using the properties (A), (B), (C), we obtain for every A P BpR d `q, ż A ρpx, B, B ˚qpP B `PB ˚qpdxq " P B pA X A 1{2 pB, B ˚qq `PB pA X A 1 pB, B ˚qq " P B pAq.</p><p>We begin with an example that shall help to get an idea and provide insights into the concepts and arguments we shall use in the general case. It is deliberately very detailed and although it deals with a very special case, it illustrates the main issues also for the general case.</p><p>Example 4.6. [How to find a density and the associated GMLEs] For B, B ˚P BpDq where D " pt1, 2u, 1 Ñ 2q, we show that the partition ! A 0 pB, B ˚q :</p><formula xml:id="formula_29">" x P R 2 `: x 2 ă b 12 x 1 ( Y x P R 2 `: x 2 " b 12 x 1 ą b 12 x 1 ( , A 1{2 pB, B ˚q :" x P R 2 `: x 2 " b 12 x 1 " b 12 x 1 ( Y x P R 2 `: x 2 ą pb 12 _ b 12 qx 1 ( , A 1 pB, B ˚q :" x P R 2 `: b 12 x 1 ą x 2 ě b 12 x 1 ( Y x P R 2 `: x 2 " b 12 x 1 ą b 12 x 1 ( ) of R 2</formula><p>`satisfies properties (A), (B), (C) of (4.5). Figure <ref type="figure" target="#fig_17">4</ref>.1 shows the corresponding density ρp¨, B, B ˚q from (4.6) for the three possible order relations between b 12 and b 12 .  Since by Table <ref type="table">3</ref>.1, supppX 2 {X 1 q " rb 12 , 8q and b 12 is the only atom of X 2 {X 1 , property (A) is true. By reversing the roles of B and B ˚, (C) follows from (A). The condition (B) is obvious if b 12 " b 12 . Assume that b 12 ‰ b 12 . We then have by definition of X that tX P A 1{2 pB, B ˚qu " tX 2 ą pb 12 _ b 12 qX 1 u " tZ 2 ą pb 12 _ b 12 qZ 1 u and X 2 " Z 2 on tZ 2 ą pb 12 _ b 12 qZ 1 u. With this, using that A 1{2 pB ˚, Bq " A 1{2 pB, B ˚q, we obtain for A P BpR 2 `q,</p><formula xml:id="formula_30">x 2 " b 1 2 x 1 x 2 " b 1 2 x 1 x2 x1 x2 x1 b 12 b 12 x 2 " b 1 2 x 1 " b 12 x 1 x2 x1 x2 x1 b 12 " b 12 x 2 " b 1 2 x 1 x 2 " b 12</formula><formula xml:id="formula_31">P B pA X A 1{2 pB, B ˚qq " PptX P Au X tZ 2 ą pb 12 _ b 12 qZ 1 uq " PptpZ 1 , Z 2 q P Au X tZ 2 ą pb 12 _ b 12 qZ 1 uq " P B ˚pA X A 1{2 pB ˚, Bqq " P B ˚pA X A 1{2 pB, B ˚qq.</formula><p>We now use the density found to determine the GMLE of B. The only ML coefficient we have to estimate is b 12 . As before we let p b 12 " b12 be the minimal observed ratio of X 2 {X 1 and let p B be the corresponding ML coefficient matrix from (4.3). Defining npB, B ˚q " |tt : x ptq P A 1{2 pB, B ˚qu| and using that npB, B ˚q " npB ˚, Bq, we obtain   Bq. The two plots on the left-hand side correspond to the situation from above: in the upper plot there are realizations in the 0-area, namely those that are on the line x P R 2 `: x 2 " b 12 x 1 ( , but then there are also realizations in the 0-area of the lower plot (those that lie below this line). Hence, (4.8) holds. Since there is no realization in the 0-area of the middle and right plot in the top line, (4.8) is automatically satisfied if b 12 ď p b 12 .</p><formula xml:id="formula_32">n ź t"1 ρpx ptq , B, B ˚q " 2 ´npB,B ˚q n ź t"1 1 R d `zA 0 pB,B ˚q`x ptq ˘, n ź t"1 ρpx ptq , B ˚, Bq " 2 ´npB,B ˚q n ź t"1 1 R d `zA 0 pB ˚,</formula><p>l</p><p>In what follows we specify, for the general case, one density of P B with respect to P B `PB that has a representation as in (4.6) and leads to p B as a GMLE of B.</p><formula xml:id="formula_33">Our partition A 0 pB, B ˚q, A 1{2 pB, B ˚q, A 1 pB, B ˚q( of R d</formula><p>`is based on the following representation for the components of X:</p><formula xml:id="formula_34">X i " ł kPpapiq b ki X k _ Z i ; in particular, X i ě ł kPpapiq b ki X k , i P V. (4.9)</formula><p>We begin with the specification of A 1{2 pB, B ˚q and prove a property needed subsequently to verify property (B). Have in mind that if b ki ą b ki for all k P papiq or b ki ă b ki for all k P papiq then x P R d `: </p><formula xml:id="formula_35">x i " Ž kPpapiq b ki x k " Ž kPpapiq b ki x k ( " H. x 2 " b 1 2 x 1 " p b 1 2 x 1 x 2 " r b1 2 x 1 ρp¨, r B, Bq x2 x1 x 2 " b 1 2 x 1 " p b 1 2 x 1 ρp¨, B, r Bq x2 x1 x 2 " r b1 2 x 1 (a) r b12 ă p b12 is no GMLE. x 2 " r b 1 2 x 1 x 2 " b1 2 x 1 x2 x1 ρp¨, r B, Bq x 2 " r b 1 2 x 1 x 2 " b1 2 x 1 x2 x1 ρp¨, B, r Bq (b) r b12 ą p b12 is no GMLE. ρp¨, p B, Bq: x 2 " p b 1 2 x 1 x 2 " b 1 2 x 1 x2 x1 x 2 " p b 1 2 x 1 " b 1 2 x 1 x2 x1 x 2 " p b 1 2 x 1 x 2 " b1 2 x 1 x2 x1 ρp¨, B, p Bq: x 2 " p b 1 2 x 1 x 2 " b 1 2 x 1 x2 x1 x 2 " p b 1 2 x 1 " b 1 2 x 1 x2 x1 x 2 " p b 1 2 x 1 x 2 " b1 2 x 1 x2 x1 (c) r b12 " p b12 is a GMLE.</formula><formula xml:id="formula_36">d č i"1 " x P R d `: x i " ł kPpapiq b ki x k " ł kPpapiq b ki x k ( Y x P R d `: x i ą ł kPpapiq pb ki _ b ki qx k (‰ .</formula><p>Then for every F P F, </p><formula xml:id="formula_37">PpF X tX P A 1{2 pB, B ˚quq " PpF X ΩpB, B ˚qq. (<label>4</label></formula><formula xml:id="formula_38">A 0 pB, B ˚q " ď iPV " x P R d `: x i ă ł kPpapiq b ki x k ( Y x P R d `: x i " ł kPpapiq b ki x k ą ł kPpapiq b ki x k (‰ ,</formula><p>and</p><formula xml:id="formula_39">A 1 pB, B ˚q " R d `z`A 0 pB, B ˚q Y A 1{2 pB, B ˚q˘.</formula><p>With this partition we then have:</p><formula xml:id="formula_40">Theorem 4.8. Let B, B ˚P BpDq. Then the function ρ : R d `Ñ t0, 1{2, 1u x Þ Ñ ρpx, B, B ˚q " 1 2 ¨1A 1{2 pB,B ˚qpxq `1A 1 pB,B ˚qpxq " $ ' ' &amp; ' ' % 0, if x P A 0 pB, B ˚q, 1 2 , if x P A 1{2 pB, B ˚q, 1, if x P A 1 pB, B ˚q,<label>(4.11)</label></formula><p>is a density of P B with respect to P B `PB ˚.</p><p>Proof. See the appendix.</p><p>We observe an interesting relation between the density (4.11) for D and corresponding densities for subgraphs of D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Example 4.9. [Local densities ρ</head><formula xml:id="formula_41">i ] Consider the DAGs 1 2 3 D 1 2 D 2 2 3 D 3</formula><p>Let ρ, ρ 2 , and ρ 3 be the corresponding densities from <ref type="bibr">(4.11)</ref>. For the ML coefficient matrix B of a recursive ML model on D, let B 2 and B 3 be the ML coefficient matrices of recursive ML models on D 2 and D 3 with edge weight c 12 " b 12 and c 23 " b 23 , and let starred quantities denote the same for B ˚. We then find for x " px 1 , x 2 , x 3 q P R 3</p><formula xml:id="formula_42">`, ρpx, B, B ˚q " `ρ2 px Pap2q , B 2 , B 2 q _ ρ 3 px Pap3q , B 3 , B 3 q ˘1p0,8q `ρ2 px Pap2q , B 2 , B 2 q ^ρ3 px Pap3q , B 3 , B 3 q ˘.</formula><p>This can be observed from Figure <ref type="figure" target="#fig_17">4</ref>.3, where the densities are depicted as functions of x 2 {x 1 and/or x 3 {x 2 for all nine different orders between the ML coefficients in B and B ˚. Conversely, ρ 2 and ρ 3 can be derived from ρ as follows: We now extend the findings from Example 4.9 to the general case. Furthermore, we show that the densities ρ i are densities of regular conditional distributions.  is a regular conditional distribution of X i given X papiq and P i|papiq B ˚one of X i given X papiq .</p><formula xml:id="formula_43">ρ 2 px Pap2q , B 12 , B 12 q " min</formula><formula xml:id="formula_44">ρpx, B, B ˚q " `ł iPV ρ i px Papiq , B i , B i q ˘1p0,8q `ľ iPV ρ i px Papiq , B i , B i q ˘. (<label>4</label></formula><p>Proof. See the appendix.</p><p>Next, we show that p B is indeed a GMLE in the sense of <ref type="bibr" target="#b17">[18]</ref>. Note also that the GMLE is obtained by piecing together individual GMLEs corresponding to conditional distributions of any variable given its parents. Thus this is similar to what is obtained in cases where the distributions have densities with respect to a product measure, as the maximum of the likelihood function is then obtained by maximizing each conditional likelihood function for the density of a node given its parents. Proof. (a) First, recall that p B is indeed a ML coefficient matrix of a recursive ML model on D. The first condition in the definition of a GMLE in <ref type="bibr">(4.4)</ref>   <ref type="bibr">(4.4)</ref>, it suffices to show that there is some realization x pt 1 q P A 0 pB, p Bq whenever there is some realization x pt 2 q P A 0 p p B, Bq; cf. Example 4.6, in particular (4.8). So let x pt 2 q P A 0 p p B, Bq for some t 2 P t1, . . . , nu. We find, for some i P V , from the definition of A 0 p p B, Bq and the fact that</p><formula xml:id="formula_45">x ptq i ě Ž kPpapiq p b ki x ptq k , x pt 2 q P x P R d `: ł kPpapiq p b ki x k ă x i " ł kPpapiq b ki x k ( . Hence, x pt 2 q i " b ki x pt 2 q k</formula><p>for some k P papiq with p b ki ă b ki . Let now t 1 P t1, . . . , nu such that Ź n s"1 y psq ki " y pt 1 q ki . As p b ki "</p><p>Ź n s"1 y psq ki , we have x</p><formula xml:id="formula_46">pt 1 q i ă b ki x pt 1 q k</formula><p>implying that x pt 1 q P A 0 pB, p Bq. The statement in (b) is a consequence of (a), and (c) has already been shown in Example 4.6.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Learning the structure of a recursive max-linear model</head><p>In contrast to the assumptions in the previous section, we now assume independent realizations x p1q , . . . , x pnq of X following a recursive ML model but the underlying DAG D is unknown. We know from previous discussions that it is not possible to recover D and the true edge weights c ki , and we therefore again focus on the estimation of B. Following Algorithm 3.5, it suffices for any pair of distinct i, j P V to decide whether supppY ji q " supppX i {X j q has a positive lower bound, alternatively a finite upper bound, and if so, to estimate the bound. Recall from Table <ref type="table">3</ref>.1 that, if there is such a bound, then it is an atom of Y ji . Since we can expect to observe atoms more than twice for n sufficiently large, we propose the following estimation method. Algorithm 5.1. [Find an estimate q B of B from x p1q , . . . , x pnq ] 1. For all i P V " t1, . . . , du, set q b ii " 1.</p><p>2. For all i, j P V with i ‰ j , if #</p><formula xml:id="formula_47">! t : Ź n s"1 y psq ji " y ptq ji</formula><p>) ě 2, then conclude j P anpiq, set q b ji " Ź n t"1 y ptq ji ;</p><p>else, set q b ji " 0.</p><p>The second item summarizes two steps: the first is concerned with estimating the ancestors of the nodes, the second with estimating the ML coefficients.</p><p>Note that the estimate q B from Algorithm 5.1 is not necessarily a ML coefficient matrix of a recursive ML model. For example, the property that b ji ą 0 if b jk b ki ą 0 (see, for example, Corollary 3.12 of <ref type="bibr" target="#b11">[12]</ref>) is not guaranteed. Many modifications of q B are possible, and here we shall not discuss this in detail. Rather we notice that the probability that Algorithm 5.1 outputs the true ML coefficient matrix B tends to one as n Ñ 8. As in the case where the DAG is known -see Proposition 4.5 -this probability converges to one at an exponential rate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion and outlook</head><p>We studied the identifiability of the elements of a recursive ML model from the distribution LpXq of X. The associated DAG and the edge weights are not identifiable, however, the ML coefficient matrix B is. In other words, we can identify the representation (2.3) but not (2.1). The class of all DAGs and edge weights that could have generated X via (2.1) and the distribution of the innovation vector are identifiable from LpXq. As a consequence, we can recover B, the class of the DAGs and edge weights, and the innovation distributions from realizations of X.</p><p>We have shown that B is a generalized maximum likelihood estimate. This is primarily of theoretical interest as it shows the estimate is not purely based on an ad hoc procedure. However, it opens up the possibility of going further, using likelihood theory, for example to study issues of likelihood ratio testing of hypothesis for specific values of the coefficients, or even for the presence or absence of edges in the underlying graph.</p><p>Parameter estimation and structure learning for recursive ML models seem to be challenging tasks because assumptions usually made in standard methods are not met. However, in both cases, B can be estimated by a simple procedure. The key idea of our approach is to consider the observed ratios between any pair of components, i.e. to perform a transformation on the realizations. The transformed realizations or rather the distributional properties of the corresponding random variables make it possible to identify, with probability 1, the true B whenever the number of observations n is sufficiently large. It would be interesting to investigate the relationship between the performance of our procedures and the number n of observations. Here, one possible question is how many observations are at least necessary to estimate B exactly; see, Example 4.4. In addition it would be interesting to study estimation of the DAG structure for moderate sample sizes, where exact estimation is not guaranteed.</p><p>We emphasize again that, although our estimates are derived under the assumption that the distribution of the innovation vector Z is fixed, the estimates do not depend on what this distribution is and would therefore also be valid in the situation where the innovations are independent with unkown distributions that are atom-free and have support equal to R `. Algorithm 3.6 provides a recursive procedure to obtain the distribution functions F Z i from B and the marginal distribution functions F X i of X i . Estimating B by p B and the distributions F X i , for example, by their empirical versions, we can apply this procedure to find estimators of the distributions F Z i although it will formally violate the assumption of atom-freeness and thus it is both more efficient and formally correct to estimate these parametrically, or under suitable monotonicity restrictions.</p><p>An important goal for future work is to apply the procedures to real-world data. However, it is unreasonable to expect any non-simulated data to follow a recursive ML model exactly, and the model should then be modified by adding appropriate noise terms. In particular we should not expect that we observe a minimal observed ratio more than twice, as we exploit in Algorithm 5.1. It seems to be more reasonable to expect values close to each other. We therefore want to develop methods based on accumulation points. It is hard to imagine noise models that would lead to simple exact likelihood analysis. One should then rather study the asymptotic precision of reasonable estimates and their behaviour under appropriate scaling, for example along the lines of <ref type="bibr" target="#b4">[5]</ref>.</p><p>we obtain from (A.2) on</p><formula xml:id="formula_48">Ş d i"1 Ω i , ł kPpapd`1q b k,d`1 X k " ł kPpapd`1q b k,d`1 ł jPAnpkq b jk Z j " ł jPanpiq b j,d`1 Z j .</formula><p>Thus, again by (2.3),</p><formula xml:id="formula_49">d č i"1 Ω i X Ω 1,d`1 1{2 " d č i"1 Ω i X ł jPAnpd`1q b j,d`1 Z j " ł jPanpd`1q b j,d`1 Z j " ł jPanpd`1q b j,d`1 Z j ( , d č i"1 Ω i X Ω 2,d`1 1{2 " d č i"1 Ω i X ł jPAnpd`1q b j,d`1 Z j ą ł jPanpd`1q pb j,d`1 _ b j,d`1 qZ j ( " d č i"1 Ω i X b j,d`1 Z j ą ł jPanpd`1q pb j,d`1 _ b j,d`1 qZ j ( .</formula><p>From (3.1) we then finally observe that</p><formula xml:id="formula_50">Ş d i"1 Ω i X `Ω1,d`1 1{2 Y Ω 2,d`1 1{2 ˘and Ş d i"1 Ω i X Ω d`1</formula><p>only differ by a set of probability zero, and, hence, (4.10) follows from (A.1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proof of Theorem 4.8</head><p>Proof. We must verify properties (A)-(C) of (4.5). (A) Since V is finite, it suffices to show for every i P V ,</p><formula xml:id="formula_51">P B ` x P R d `: x i ă ł kPpapiq b ki x k (˘" P `Xi ă ł kPpapiq b ki X k ˘" 0, (A.3) P B `tx P R d `: x i " ł kPpapiq b ki x k ą ł kPpapiq b ki x k (˘" P `Xi " ł kPpapiq b ki X k ą ł kPpapiq b ki X k ˘" 0.</formula><p>The former is immediate by (4.9). By the same argument we have for the latter, Since A 0 pB ˚, Bq is a P B ˚-null set by (A), this holds for the subset A 1 pB, B ˚q as well.</p><formula xml:id="formula_52">0 ď P `ł kPpapiq b ki X k _ Z i " ł kPpapiq b ki X k ą ł</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proof of Proposition 4.10</head><p>Proof. Denoting by A i 0 pB i , B i q, A i 1{2 pB i , B i q, A i 1 pB i , B i q the sets defining ρ i p¨, B i , B i q, we have for the corresponding sets of ρ, </p><formula xml:id="formula_53">A</formula><formula xml:id="formula_54">x Papiq P A i 1{2 pB i , B i q Y A i 1 pB i , B i q ( X " R d `zA 1{2 pB i , B i q ‰ .</formula><p>From this we obtain (a) and (b). Now, to see (c) we reason as follows:</p><formula xml:id="formula_55">P i|papiq B `p0, x i s | x papiq ˘" F Z i px i q1 r Ž kPpapiq b ki x k ,8q px i q, x Papiq P R |Papiq| `,</formula><p>is a regular conditional distribution function of X i given X papiq . To see this, use (4.9) and the independence of the innovations to obtain</p><formula xml:id="formula_56">P i|papiq B</formula><p>`p0, x i s | x papiq ˘" PpX i ď x i | X papiq " x papiq q " P `ł kPpapiq b ki X k _ Z i ď x i | X papiq " x papiq "</p><p>F Z i px i q1 r Ž kPpapiq b ki x k ,8q px i q. Since X and X ˚share the same innovation vector, we have</p><formula xml:id="formula_57">P i|papiq B ˚`p0, x i s | x papiq ˘" F Z i px i q1 r Ž kPpapiq b ki x k ,8q px i q, x Papiq P R |Papiq| `,</formula><p>is a regular conditional distribution function of X i given X papiq . Since F Z i is atom-free, this can be read directly from </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Example 3 . 2 .</head><label>32</label><figDesc>[The DAG and the edge weights are not necessarily identifiable] Consider a recursive ML model on the DAG D depicted below with edge weights c 12 , c 23 , c 13 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>and X 3 " pc 12 c 23 _ c 13 qZ 1 _ c 23 Z 2 _ Z 3 , If c 13 ď c 12 c 23 we have for any c 13 P r0, c 12 c 23 s that b 13 " c 12 c 23 _ c 13 " c 12 c 23 _ c 13 " c 12 c 23 ; so we could also write X 3 " c 13 X 1 _ c 23 X 2 _ Z 3 without changing the distribution LpXq of X. This implies that if c 13 ď c 12 c 23 , X follows a recursive ML model on D with edge weights c 12 , c 23 , c 13 but it also follows a recursive model on the DAG D B depicted below with edge weights c 12 , c 23 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Theorem 3 . 3 .</head><label>33</label><figDesc>Suppose X follows a recursive ML model with edge weights C " tc ij u and ML coefficient matrix B. Let D B be the minimum ML DAG of X as described above. Then a DAG D ˚with associated weight matrix C ˚is a valid representation of X if and only if paq D B Ď D ˚; pbq D ˚and D B have the same reachability matrix; pcq c ij " c ij for i P pa B pjq; pdq c ij P p0, b ij s for i P pa ˚pjqzpa B pjq, where pa B pjq and pa ˚pjq denote the parents of j in D B and D ˚respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>. 1 ) 3 4</head><label>13</label><figDesc>see Theorem 4.2 or Corollary 4.3(a) of [12]. Example 4.1. [Illustration of (4.1)] To illustrate the above, consider the small network below 1 2 and a potential ML coefficient matrix B with reduction B 0 , as given below. B " ¨b11 b 12 0 b 14 0 b 22 0 b 24 0 0 b 33 b 34</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>b 11 b 12 0 b 12 b 24</head><label>24</label><figDesc>Now B " I 4 _ pB d B 0 q readily implies that b ii " 1, i " 1, . . . , 4 and b 14 " b 12 b 24 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Example 4 . 2 . 3 D</head><label>423</label><figDesc>[ B is not necessarily in BpDq] Consider the DAG 1 2 and assume we observe b31 ą b32 b21 . Then the matrix B fails to satisfy (4.1) and hence is not an element of BpDq. l However, if we only estimate the ML coefficients corresponding to edges in D and then compute an estimate based on Lemma 4.3 below this phenomenon cannot occur. Lemma 4.3. Let B 0 P R dˆd `be a matrix with b ji ą 0 ðñ j Ñ i. A matrix A P R dˆd `satisfies ra ji ą 0 ðñ j P Anpiqs and A " I d _ pA d B 0 q (4.2)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Example 4 . 4 . 4 D</head><label>444</label><figDesc>[One observation may be enough to estimate B exactly] and assume that the paths r1 Ñ 2 Ñ 4s and r1 Ñ 3 Ñ 4s are both max-weighted, which is equivalent to b 12 b 24 " b 13 b 34 . If we observe the event</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Proposition 4 . 5 .</head><label>45</label><figDesc>Let X ptq " `Xptq 1 , . . . , X ptq n ˘for t " 1, . . . , n be a sample from a recursive ML model on a DAG D with ML coefficient matrix B. Let i P V and k P papiq. It then holds that P ˜n ľ t"1 Y ptq ki " b ki ¸ě 1 ´p for some p P p0, 1q if and only if n ě lnppq lnpPpY ki ą b ki qq . Furthermore, the convergence P `Źn t"1 Y ptq ki " b ki ˘Ñ 1 as n Ñ 8 is exponentially fast.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 4 . 1 :</head><label>41</label><figDesc>Figure 4.1: The density ρp¨, B, B ˚q from Example 4.6 shown as a contour plot (top line) and as a function of y 12 " x 2 {x 1 (bottom line) for the three situations b 12 ă b 12 (left-hand side), b 12 " b 12 (middle), and b 12 ą b 12 (right-hand side). The area where it is 0{ 1 2 {1 is coloured in red/blue/green.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>Figure 4.2(a) we illustrate this situation. On the left-hand side a contour plot of the density ρp¨, r B, Bq is shown, on the right-hand side of ρp¨, B, r Bq. The crosses represent the realizations x p1q , . . . , x pnq . In the left plot crosses are in the 0-area coloured in red, namely, those that realize p b 12 , but in the right plot not. So r B cannot be a GMLE of B. (b) r b 12 ą p b 12 is no GMLE: This follows directly from (4.7).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 4 . 2 " b 12 x ptq 1 ą p b 12 x ptq 1 , 1 " x psq 2 , 2 ă b 12 x psq 1 .</head><label>4211221</label><figDesc>2(b) shows a situation that contradicts (4.8), similarly to Figure 4.2(a) in (1). (c) r b 12 " p b 12 is a GMLE: Condition (4.7) holds obviously. To prove (4.8), assume for some B P BpDq that some x ptq P A 0 p p B, Bq. By definition of A 0 p p B, Bq, x ptq which implies that b 12 ą p b 12 . For x psq such that p b 12 x psq we then find that x psq Hence, x psq P A 0 pB, p Bq, and p b 12 is a GMLE of b 12 . We learn this informally from Figure 4.2(c). The top line shows contour plots of ρp¨, p B, Bq for the three different orders between b 12 and p b 12 , and the bottom line shows the corresponding contour plots of ρp¨, B, p</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 4 . 2 : 6 . 4 . 7 .ł</head><label>42647</label><figDesc>Figure 4.2: Discussion of the GMLEs of b 12 with respect to the density from Figure 4.1.; see further explanation in (a), (b), and (c) of Example 4.6.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 4 . 3 :</head><label>43</label><figDesc>Figure 4.3:The densities ρpx, B, B ˚q, ρ 2 px Pap2q , B 2 , B 2 q, ρ 3 px Pap3q , B 3 , B 3 q from Example 4.9 as functions of x 2 {x 1 and/or x 3 {x 2 . The area where the density is 0{ 1 2 {1 is coloured in red/blue/green.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head></head><label></label><figDesc>tyPR `:ρppx Pap2q ,yq,B,B ˚qą0u ρppx Pap2q , yq, B, B ˚q, ρ 3 px Pap3q , B 23 , B 23 q " min tyPR `:ρppy,x Pap3q q,B,B ˚qą0u ρppy, x Pap3q q, B, B ˚q, which we learn from Figure 4.3 again. l</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Proposition 4 . 10 .</head><label>410</label><figDesc>Let B, B ˚P BpDq and let X " Z d B, X ˚" Z d B ˚fol low corresponding recursive ML models on D. For i P V , let ρ i be the density given in (4.11) with respect to the DAG D i " pPapiq, tpk, iq : k P papiquq as well as B i and B i the ML coefficient matrices of recursive ML models on D i with edge weights c ki " b ki and c ki " b ki , respectively.(a) We have for ρpx, B, B ˚q given in(4.11)    </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Theorem 4 . 11 .</head><label>411</label><figDesc>Let x ptq " `xptq 1 , . . . , x ptq n ˘for t " 1, . . . , n be a sample from a recursive ML model on a DAG D with ML coefficient matrix B P BpDq unknown.(a) The matrix p B from (4.3) is a GMLE of B. (b) For every i P V , p p b ki , k P papiqq is a GMLE of the ML coefficients pb ki , k P papiqq of a random vector following a recursive ML model on D i " pPapiq, tpk, iq : k P papiquq with edge weights c ki " b ki . (c) For every i P V and k P papiq, p b ki is the only GMLE of the ML coefficient b ki of a random vector following a recursive ML model on D ki " ptk, iu, tpk, iquq with edge weight c ki " b ki .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4.4 illustrates the DAGs D i in Theorem 4.11(b) or Proposition 4.10.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Figure 4 . 4 :</head><label>44</label><figDesc>Figure 4.4: The DAGs D i from Theorem 4.11(b) for a recursive ML model on the DAG D depicted on the left-hand side with ML coefficient matrix B. The edges are marked with the corresponding ML coefficients. Note that b 12 , b 14 , b 34 , b 24 can be arbitary positive numbers but b 24 ě b 23 b 34 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>b ki x k ą x i ě ł kPpapiq b ki x k ( Y x P R d `: x i " ł kPpapiq b ki x k ą ł kPpapiq b ki x k ( ‰ Ď</head><label>‰</label><figDesc>kPpapiq b ki X k ˘" P `Zi " ł kPpapiq b ki X k ą ł kPpapiq b ki X k ď P `Zi " ł kPpapiq b ki ł jPAnpkq b jk Z j ˘" 0,where we have used (2.3) and (3.1) for the last inequality and equality, respectively. Thus we have verified (A). (B) Recall that P B and P B ˚share the same innovation vector when represented by a recursive ML model. Furthermore, note that the set ΩpB, B ˚q from Lemma 4.7 is a subset ofŞ iPV X i " Ž jPAnpiq:b ji "b ji b ji Z j (. We have ΩpB, B ˚q " ΩpB ˚, Bq and hence we obtain from (4.10) forA P BpR d `q, P B pA X A 1{2 pB, B ˚qq " PptX P Au X ΩpB, B ˚qq " P ` `ł jPAnpiq:b ji "b ji b ji Z j , i P V ˘P A ( X ΩpB, B ˚q" P ` `ł jPAnpiq:b ji "b ji b ji Z j , i P V ˘P A ( X ΩpB ˚, Bq ˘" P B ˚pA X A 1{2 pB, B ˚qq.(C) We observe from the definition of A 0 pB, B ˚q and A 1{2 pB, B ˚q that A 1 pB, B ˚q " R d `z`A 0 pB, B ˚q Y A 1{2 pB, B A 0 pB ˚, Bq.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>x i s | x papiq ˘xi 0 F Z i 1 Px i s | x papiq ˘xi 0 F Z i 1 PFigure A. 1 :</head><label>01011</label><figDesc>Figure A.1: The conditional distribution functions from the proof of Proposition 4.10(c).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head></head><label></label><figDesc>Figure A.1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 3 . 1 :</head><label>31</label><figDesc>Distributional properties of Y ji for distinct i, j P V .</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>-Wolfowitz definition extends the definition of a MLE in a very natural way as it simply says that for any Q P P, p P is the MLE in the smaller family t P, Qu, consisting of only two distributions. In<ref type="bibr" target="#b17">[18]</ref> only the second condition in (4.4) is required, but the first condition is implicit. The first step in verifying that p B is a GMLE of B is to specify densities of P B with respect to P B `PB ˚P BpDq. For this purpose we determine a partition A 0 pB, B ˚q, A 1{2 pB, B ˚q, A 1 pB, B P B pA X A 1{2 pB, B ˚qq " P B ˚pA X A 1{2 pB, B ˚qq for every A P BpR d</figDesc><table><row><cell>˚q(</cell><cell>of R d `that satisfies the following three properties,</cell></row><row><cell cols="2">(A): P B pA 0 pB, B ˚qq " 0,</cell></row><row><cell cols="2">(B): `q,</cell></row></table><note><p>for any two B, B</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>BpDq is a GMLE of B if and only if (4.7) and (4.8) are satisfied. We discuss the possible GMLEs of b 12 in detail. (a) r b 12 ă p b 12 is no GMLE: Set b 12 " p b 12 , and let x ptq be such that p b 12 x ; consequently, r b 12 cannot be a GMLE of b 12 . In</figDesc><table><row><cell>`xptq</cell><cell>˘.</cell></row><row><cell>Bq</cell><cell></cell></row><row><cell cols="3">Let now r B be an arbitrary potential GMLE of B. Then P r B P PpDq satisfies the first condition</cell></row><row><cell>in (4.4) if and only if</cell><cell></cell></row><row><cell>r b 12 x ptq 1 ď x ptq 2 for all t, equivalently r b 12 ď p b 12</cell><cell></cell><cell>(4.7)</cell></row><row><cell>and the second condition if and only if</cell><cell></cell></row><row><cell cols="2">for all B P BpDq, if some x ptq P A 0 p r B, Bq, then some x psq P A 0 pB, r Bq.</cell><cell>(4.8)</cell></row><row><cell cols="3">In summary, some r B P ( . This contradicts</cell></row><row><cell>(4.8)</cell><cell></cell></row></table><note><p>ptq 1 " x ptq 2 . Then x ptq P x P R 2 `: x 2 " b 12 x 1 ą r b 12 x 2 ( Ď A 0 p r B, Bq but no x psq P A 0 pB, r Bq " x P R 2 `: x 2 ă b 12 x 1</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>.10)    Proof. The proof is deferred to the appendix.As a partition of R d`we now suggest A 0 pB, B ˚q, A 1{2 pB, B ˚q, A 1 pB, B</figDesc><table /><note><p><p>˚q(</p>, where A 1{2 pB, B ˚q is defined above in Lemma 4.7,</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>.12) (b) The function ρ i can be computed from ρ by ρ i px Papiq , B i , B i q " min tyPR d `:y Papiq "x Papiq ,ρpy,B,B ˚qą0u ρpy, B, B ˚q,where we set min yPH ρpy, B, B ˚q " 0.(c) The function ρ i : R d `Ñ t0, 1{2, 1u such that x Papiq Þ Ñ ρ i px Papiq , B i , B i q is a density of P</figDesc><table><row><cell>i|papiq B</cell><cell>with respect to P B i|papiq</cell><cell>`P i|papiq B ˚, where P B i|papiq</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>0 pB, B ˚q "ď Papiq P A i 0 pB i , B i q Papiq P A i 1{2 pB i , B i q</figDesc><table><row><cell></cell><cell></cell><cell>(</cell><cell>,</cell></row><row><cell>A 1{2 pB, B ˚q "</cell><cell>č</cell><cell cols="2">x P R d `: x ( ,</cell></row><row><cell>A 1 pB, B ˚q "</cell><cell>iPV č</cell><cell></cell></row></table><note><p>iPV x P R d `: x iPV x P R d `:</p></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>We thank <rs type="person">Justus Hartl</rs> for providing a first discussion about the different estimators suggested in this paper in his master's thesis. NG acknowledges support by <rs type="funder">Deutsche Forschungsgemeinschaft (DFG)</rs> through the <rs type="funder">TUM International Graduate School of Science and Engineering (IGSSE)</rs>. All authors benefited from financial support from the <rs type="funder">Alexander von Humboldt Stiftung</rs>.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendix: some technical proofs</head><p>Proof of Lemma 4.7</p><p>Proof. First, define for i P V Ω 1,i 1{2 :" X i "</p><p>The proof is by induction on the number of nodes of D. For d " 1 the statement is clear. Assume now that D " pV, Eq has d `1 nodes and that the assertion holds with respect to DAGs with at most d nodes. Furthermore, assume without loss of generality that d `1 is a terminal node (i.e., depd `1q " H). Since pX 1 , . . . , X d q follows a recursive ML model on the DAG pt1, . . . , du, E X pt1, . . . , du ˆt1, . . . , duqq with ML coefficient matrix B " pb ij q dˆd and B ˚" pb ij q dˆd is the ML coefficient matrix of a recursive ML model on this DAG as well, the induction hypothesis yields that</p><p>For every i P V we have by (2.3) on Ω i that</p><p>Noting from the proof of Theorem 4.2 of <ref type="bibr" target="#b11">[12]</ref> that ł </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Extremes on river networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Asadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Engelke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Applied Statistics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2023" to="2050" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Statistics of Extremes: Theory and Applications</title>
		<author>
			<persName><forename type="first">J</forename><surname>Beirlant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Goegebeur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Segers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Teugels</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
			<publisher>Wiley</publisher>
			<pubPlace>Chichester</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Structural Equations with Latent Variables</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Bollen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989">1989</date>
			<publisher>Wiley</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Max-linear Systems: Theory and Algorithms</title>
		<author>
			<persName><forename type="first">P</forename><surname>Butkovič</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>Springer</publisher>
			<pubPlace>London</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Estimation for first-order autoregressive processes with positive or bounded innovations</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">P</forename><surname>Mccormick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Stoch. Proc. Appl</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="237" to="250" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Basic properties and prediction of max-ARMA processes</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">I</forename><surname>Resnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Applied Probability</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="781" to="803" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Extreme Value Theory: An Introduction</title>
		<author>
			<persName><forename type="first">L</forename><surname>De Haan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ferreira</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>Springer</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A continuous updating weighted least squares estimator of tail dependence in high dimensions</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H J</forename><surname>Einmahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kiriliouk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Segers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Extremes</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="205" to="233" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Graphical models for extremes</title>
		<author>
			<persName><forename type="first">S</forename><surname>Engelke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hitz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.01734</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Non-and semi-parametric maximum likelihood estimators and the von-Mises method (part 1)</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Gill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Wellner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Praestgaard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scandinavian Journal of Statistics</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="97" to="128" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Graphical Modeling of Extremes: Max-linear Models on Directed Acyclic Graphs</title>
		<author>
			<persName><forename type="first">N</forename><surname>Gissibl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
		<respStmt>
			<orgName>Technical University of Munich</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Max-linear models on directed acyclic graphs</title>
		<author>
			<persName><forename type="first">N</forename><surname>Gissibl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Klüppelberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bernoulli</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="2693" to="2720" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Big data: progress in automating extreme risk analysis</title>
		<author>
			<persName><forename type="first">N</forename><surname>Gissibl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Klüppelberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mager</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Berechenbarkeit der Welt?</title>
		<editor>
			<persName><forename type="first">W</forename><surname>Pietsch</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Wernecke</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Ott</surname></persName>
		</editor>
		<meeting><address><addrLine>Wiesbaden</addrLine></address></meeting>
		<imprint>
			<publisher>Springer VS</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="171" to="189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">One-component regular variation and graphical modeling of extremes</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Evans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Appl. Prob</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="page" from="733" to="746" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Spatial statistical models that use flow and stream distance</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M V</forename><surname>Hoef</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Peterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Theobald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Environmental and Ecological Statistics</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="449" to="464" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The product limit estimator as maximum likelihood estimator</title>
		<author>
			<persName><forename type="first">S</forename><surname>Johansen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scandinavian Journal of Statistics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="195" to="199" />
			<date type="published" when="1978">1978</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Kalbfleisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Prentice</surname></persName>
		</author>
		<title level="m">The Statistical Analysis of Failure Time Data</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Wiley</publisher>
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Consistency of the maximum likelihood estimator in the presence of infinitely many incidental parameters</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kiefer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wolfowitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Mathematical Statistics</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="887" to="906" />
			<date type="published" when="1956">1956</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Probabilistic Graphical Models: Principles and Techniques</title>
		<author>
			<persName><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Friedman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Lauritzen</surname></persName>
		</author>
		<title level="m">Graphical Models</title>
		<meeting><address><addrLine>Oxford, United Kingdom</addrLine></address></meeting>
		<imprint>
			<publisher>Clarendon Press</publisher>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Causal inference from graphical models</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Lauritzen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Complex Stochastic Systems</title>
		<editor>
			<persName><forename type="first">O</forename><forename type="middle">E</forename><surname>Barndorff-Nielsen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Cox</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Klüppelberg</surname></persName>
		</editor>
		<meeting><address><addrLine>London/Boca Raton</addrLine></address></meeting>
		<imprint>
			<publisher>Chapman and Hall/CRC Press</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="63" to="107" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Independence properties of directed Markov fields</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Lauritzen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Dawid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">N</forename><surname>Larsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-G</forename><surname>Leimer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Networks</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="491" to="505" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Causality: Models, Reasoning, and Inference</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>Cambridge University Press</publisher>
			<pubPlace>Cambridge</pubPlace>
		</imprint>
	</monogr>
	<note>2nd edition</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Extreme Values, Regular Variation, and Point Processes</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">I</forename><surname>Resnick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1987">1987</date>
			<publisher>Springer</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Heavy-Tail Phenomena: Probabilistic and Statistical Modeling</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">I</forename><surname>Resnick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
			<publisher>Springer</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Towards a unified definition of maximum likelihood</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">W</forename><surname>Scholz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Canadian Journal of Statistics / La Revue Canadienne de Statistique</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="193" to="203" />
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Causation, Prediction, and Search</title>
		<author>
			<persName><forename type="first">P</forename><surname>Spirtes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Glymour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Scheines</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
	<note>2nd edition</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
