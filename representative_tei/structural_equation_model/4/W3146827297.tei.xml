<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">The sensitivity of structural equation modeling with ordinal data to underlying non-normality and observed distributional forms</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Njål</forename><surname>Foldnes</surname></persName>
							<email>njal.foldnes@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Norwegian Business School http://brage.bibsys.no/bi</orgName>
								<orgName type="institution" key="instit1">BI</orgName>
								<orgName type="institution" key="instit2">BI Norwegian Business School</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Steffen</forename><surname>Grønneberg</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Norwegian Business School http://brage.bibsys.no/bi</orgName>
								<orgName type="institution" key="instit1">BI</orgName>
								<orgName type="institution" key="instit2">BI Norwegian Business School</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">The sensitivity of structural equation modeling with ordinal data to underlying non-normality and observed distributional forms</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1037/met0000385</idno>
					<note type="submission">It contains the accepted and peer reviewed manuscript to the article cited below. It may contain minor differences from the journal&apos;s pdf version.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-14T17:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Ordinal data</term>
					<term>latent variable models</term>
					<term>underlying normality</term>
					<term>simulation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Authors of articles published in APA journalsthe authoritative document, i.e., peer reviewed publication of recordmay post a prepublication copy of the final manuscript, as accepted for publication as a word processing file, on their personal website, their employer's server, in their institution's repository, reference managers (e.g., Mendeley) and author social networks (e.g., Academia.edu and ResearchGate) after it is accepted for publication.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>tinuous methodology as cont-ML. One reason for using cont-ML with ordinal data is the relative ease of handling missing data and large complex models.</p><p>A popular approach of modeling the ordinal nature of the data is to add a discretization process based on thresholds to the standard SEM. This approach was first proposed by <ref type="bibr" target="#b10">Christoffersson (1977)</ref> for binary data, and subsequently developed to handle observed variables measured with more than two ordered categories <ref type="bibr" target="#b32">(Jöreskog, 1994;</ref><ref type="bibr" target="#b45">Muthén, 1984)</ref>. First, for any pair of ordinal variables, the so-called polychoric correlation is estimated. This is the correlation between two underlying unobserved variables that are posited to have created the observed categorical data through discretization. Since the time of <ref type="bibr" target="#b56">Pearson (1900)</ref> it has been a standard assumption that these underlying variables follow a normal distribution. In the second stage, the model parameters are estimated by fitting the model to the matrix of polychoric correlations using least-square principles, and we refer to this methodology as cat-LS. In cat-LS formulas for standard errors and model test statistics <ref type="bibr" target="#b48">(Muthén, 1993)</ref> are similar to those proposed for non-normal data in the continuous case <ref type="bibr" target="#b63">(Satorra &amp; Bentler, 1988)</ref>  <ref type="foot" target="#foot_0">1</ref> . Simulation stud-ies (e.g., <ref type="bibr" target="#b3">Beauducel &amp; Herzberg, 2006;</ref><ref type="bibr" target="#b38">Li, 2016b;</ref><ref type="bibr">Rhemtulla, Brosseau-Liard, &amp; Savalei, 2012)</ref> indicate that cat-LS in general outperforms cont-ML. However, the discrepancy between these two methodologies has been reported to decrease with five or more categories that are symmetrically distributed (e.g., <ref type="bibr">Rhemtulla et al., 2012)</ref>.</p><p>Both cont-ML and cat-LS are frequently used to estimate SEM models with ordinal data, and their properties have been extensively studied in many simulation studies. The consequences of underlying non-normality on estimation bias have been thoroughly investigated (e.g., <ref type="bibr" target="#b13">Flora &amp; Curran, 2004;</ref><ref type="bibr">Li, 2016a;</ref><ref type="bibr" target="#b58">Quiroga, 1994;</ref><ref type="bibr">Rhemtulla et al., 2012)</ref>, resulting in a general consensus that ordinal SEM is quite robust to moderate violation of underlying normality. However, these studies were conducted using simulation methodology which is equivalent to simulating exactly normal data <ref type="bibr">(Grønneberg &amp; Foldnes, 2019)</ref>. In the present study, we reassess the robustness issue using a newly developed simulation methodology <ref type="bibr">(Foldnes &amp; Grønneberg, 2019)</ref> which ensures proper violation of underlying normality. We also propose and evaluate an alternative to cont-ML which takes advantage of the discretization model employed by cat-LS. We refer to this new method as cont-ML-adj. Inference for cont-ML-adj utilizes the same robustification formulas for standard errors and model fit statistics as cont-ML and cat-LS.</p><p>Our research questions are as follows. First, how sensitive are cont-ML, cont-ML-adj, and cat-LS to variation in the observed ordinal distributions? That is, we will study the stability, or lack thereof, of correlation and ordinal SEM estimates under a large number of discretizations of the same underlying continuous distribution. Previous studies (e.g., <ref type="bibr" target="#b3">Beauducel &amp; Herzberg, 2006;</ref><ref type="bibr">Li, 2016a</ref><ref type="bibr" target="#b38">Li, , 2016b;;</ref><ref type="bibr" target="#b46">Muthén &amp; Kaplan, 1985;</ref><ref type="bibr">Rhemtulla et al., 2012)</ref> have included only a small set of threshold configurations to generate ordinal data, typically consisting of a symmetrical distribution and one or two asymmetrical observed ordinal distributions. The focus in these studies has been to compare the performance of different estimators, but using only a handful of threshold configurations. In the present study we utilize 150 threshold configurations in each design condition to assess the extent to which cont-ML, cont-ML-adj, and cat-LS estimation depend on the specific marginal ordinal distribution. To the best of our knowledge, this is a novel perspective leading to insights into the sensitivity of the three estimators to the distributional shapes of the ordinal univariate marginals.</p><p>The second research question is: How sensitive are cont-ML, cont-ML-adj, and cat-LS to departures from normality in the underlying dataset? In the case where the observed ordinal data are produced by the discretization of an underlying continuous vector that does not have the multivariate normal distribution, the normal theory polychoric estimator will typically be biased <ref type="bibr" target="#b18">(Foldnes &amp; Grønneberg, 2019b)</ref>. That is, the true correlation matrix among the underlying continuous variables will differ from the estimated polychoric correlations, even at the population level. This could have a profound impact on the second stage of cat-LS, with the SEM model estimated from a biased polychoric correlation matrix. Hence, in the case of underlying non-normality, a wellfitting SEM may be estimated with substantial bias in model parameters and fit indices, which invalidates inference. We apply newly developed simulation methodology for ordinal data in the context of a realistic SEM model to investigate the extent to which underlying non-normality produces bias in parameter estimation, confidence interval (CI) coverage rates, and in model fit assessment.</p><p>Our third research question relates to the proposed cont-ML-adj estimator. What are its theoretical properties, in relation to both cont-ML and cat-LS? And how does it empirically perform in the setting of a realistic SEM model?</p><p>Our fourth and final question is: How well can we detect underlying non-normality in an ordinal dataset, in the context of a medium-sized SEM model? In conditions where underlying non-normality is detrimental to SEM inference, we aim at detecting the underlying non-normality using a statistical test. A bootstrap test for this purpose has recently been proposed <ref type="bibr" target="#b18">(Foldnes &amp; Grønneberg, 2019b)</ref>, and found to outperform the test of <ref type="bibr" target="#b42">Maydeu-Olivares (2006)</ref> in a limited simulation study with small-and medium-sized factor models. In the present article we evaluate the performance of this bootstrap test in a medium-sized SEM setting.</p><p>The remainder of the article is structured as follows: We first provide a concise description of cont-ML, cont-ML-adj and cat-LS, and then describe a new simulation method for ordinal data that we employ in the present study. Next, a literature review provides the context of our research questions. This is followed by a discussion of the complexity of interpreting ordinal SEM simulation studies, which provides justification for employing the new simulation approach. Theoretical results on the consistency of our three estimators as the number of categories increase are provided. We report results from three empirical studies on the performance of cont-ML, cont-ML-adj and cat-LS in the context of a medium-sized SEM. We investigate how the shape of ordinal data distributions and underlying non-normality affect parameter bias, confidence interval coverage and model fit testing. The performance of the bootstrap test for underlying non-normality is also evaluated. We conclude with a discussion of the results with recommendations for applied researchers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Estimation methods</head><p>There are two popular limited-information approaches to ordinal SEM. The first approach is cont-ML, which calcu-tor=MLM/MLMV" and cat-LS by specifying the "ordered" option.</p><p>lates the correlation or covariance matrix directly on the ordinal observations, which is then provided as input to the standard normal theory maximum likelihood estimator. The second approach is cat-LS, where we first estimate the correlation matrix of an underlying continuous random vector X * and then uses least squares estimation methods to fit the proposed SEM to the estimated correlation matrix <ref type="bibr" target="#b45">(Muthén, 1984)</ref>. This approach is implemented in current software packages such as EQS <ref type="bibr" target="#b5">(Bentler, 2006)</ref>, Mplus <ref type="bibr" target="#b47">(Muthén &amp; Muthén, 2012)</ref>, LISREL <ref type="bibr" target="#b33">(Jöreskog &amp; Sörbom, 2015)</ref>, and lavaan <ref type="bibr" target="#b62">(Rosseel, 2012)</ref>, and is frequently employed by researchers.</p><p>cat-LS is based on the following discretization model. We have n independent and identically distributed observations of a random ordinal vector X = (X 1 , X 2 , . . . , X p ) . We assume that each X k may take on discrete values x k,1 , . . . , x k,K for some number K, and that the data stem from discretization of an unobserved continuous p-dimensional random vector X * = (X * 1 , X * 2 , . . . , X * p ) as follows:</p><formula xml:id="formula_0">X k =                  x k,1 , if X * k ≤ τ k,1 x k,2 , if τ k,1 &lt; X * k ≤ τ k,2 . . . x k,K , if τ k,K-1 &lt; X * k<label>(1)</label></formula><p>for k = 1, 2, . . . , p. We refer to the τ as thresholds which fulfills τ k, j-1 ≤ τ k, j and define τ k,0 = F -1 k (0), τ k,K = F -1 k (1), where F k (x) = P(X * k ≤ x) is the k-th marginal distribution of X * . For all distributions with infinite support, such as the normal distribution, we have τ k,0 = -∞, τ k,K = ∞. To identify the thresholds, the marginals of X * have to be known. Usual practice is to assume that X * has standardized normal marginals: X * k ∼ N(0, 1) for k = 1, 2, . . . , p. The SEM specified by the researcher relates to the continuous X * vector. That is, model estimation requires the correlation matrix of X * . This matrix is not identified, and the traditional assumption since <ref type="bibr" target="#b56">Pearson (1900)</ref> has therefore been to assume that X * is multivariate normal. Under this assumption the correlation matrix of X * may be consistently estimated by the polychoric correlation matrix, which is a limited information maximum likelihood estimator <ref type="bibr" target="#b54">(Olsson, 1979a)</ref>. The polychoric correlation matrix then forms the basis for estimating the parameters of the SEM, typically using diagonally weighted least squares estimation (DWLS). This means that the discrepancy between model-implied and estimated polychoric correlations is minimized, where the weights are obtained from the diagonal of the asymptotic covariance matrix of the estimated polychoric correlations. Robust standard errors and goodness-of-fit testing are also obtained based on the asymptotic covariance matrix (see <ref type="bibr" target="#b43">Monroe (2018)</ref> for further details).</p><p>We next propose a third limited-information approach to ordinal SEM, with complete technical descriptions given in a later section. Usually, the values of the coordinates of X are taken as a sequence of consecutive integers 1, 2, . . . , K, which is not in general the scale used by X * . This is the approach of cont-ML. We propose to adjust cont-ML as follows. Instead of integer values, we choose the values of X in such a way that the covariance matrix of X approximates that of X * . To estimate the parameters of the SEM, the usual normal theory ML procedure is then applied to the appropriately encoded observations. The resulting estimator is called cont-ML-adj, and it borrows from cat-LS the discretization model in Equation <ref type="bibr" target="#b9">(1)</ref> in order to adjust the cont-ML correlations. While polychoric correlations assume that X * is normal, cont-ML-adj requires only that the marginal distributions of X * are known, i.e., we know the functions P(X * k ≤ x) for k = 1, . . . , p. The marginals do not have to be normal. Since the marginals are known, we will show later in the article that the thresholds (τ k, j ) can be estimated from data by estimators, say, (τ k, j ). Consider the first coordinate X 1 . From Equation ( <ref type="formula" target="#formula_0">1</ref>), X 1 takes the value of x 1,1 when X * 1 ≤ τ 1,1 . We therefore choose x 1,1 somewhere in the interval &lt; -∞, τ 1,1 ]. In general, we adjust x k, j to a new value xk, j which is placed in the interval [τ k, j-1 , τ k, j ]. One way to achieve this is to use xk, j = m k (τ k, j-1 , τk, j ) where m k (x, y) = E X * k x ≤ X * k ≤ y is the conditional expectation of X * k when the value of X * k is compatible with the observed value of X in the original encoding. When X * k is assumed to be standard normal so that F k = Φ with density φ = Φ , we have that m k (x, y) = [φ(x) -φ(y)]/[Φ(y) -Φ(x)], so that xk, j = [φ(τ k, j-1 ) -φ(τ k, j )]/[Φ(τ k, j ) -Φ(τ k, j-1 )].</p><p>Figure <ref type="figure" target="#fig_0">1</ref> shows the probability plot for the original and adjusted values, for a K = 4 distribution with floor effects. While the original values are equidistant, the adjusted values are unevenly spaced. The original values range from X = 1 to X = 4, with probabilities 0.54, 0.36, 0.07, and 0.03, resulting in adjusted values X = -0.74, 0.60, 1.49, and 2.21, respectively. The following R-code implements the adjustment on the ordinal dataframe ordinal.df: adjust &lt;-function(x){ taus&lt;-c <ref type="bibr">(-Inf,qnorm(cumsum(prop.table(table(x)</ref>)</p><p>→ ))) newvalues &lt;-apply(embed(taus, 2)[, 2:1], <ref type="bibr">1, function(x) (dnorm(x[2]</ref>)-dnorm(x <ref type="bibr" target="#b9">[1]</ref>))/(pnorm(x → <ref type="bibr" target="#b9">[1]</ref>)-pnorm(x[2]))) plyr::mapvalues(x, sort(unique(x)), newvalues) → } adjusted.df &lt;-sapply(ordinal.df, adjust)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A new simulation technique for non-normal ordinal covariance models</head><p>The work of <ref type="bibr" target="#b24">Grønneberg and Foldnes (2019)</ref> provided a major impetus for the present study in pointing out that, surprisingly, a large portion of the literature on robustness of ordinal SEM against underlying non-normality has to be re-interpreted. These studies generated ordinal data by discretizing a non-normal continuous vector and the results showed that the effect of non-normality on ordinal SEM was moderate or minor. However, careful examination of the properties of the non-normal continuous vector reveals that the ordinal data produced from it are indistinguishable from ordinal data produced by discretizing a fully normal continuous vector. In brief, thresholds, and marginal distributions are not jointly identified, and the effect of marginals and threshold choice is confounded. These observations have complex implications for the interpretation of simulation studies for ordinal data methods, which we discuss in a separate and upcoming section, see p.7, as well as in the online supplementary material (p. <ref type="bibr" target="#b9">1)</ref>.</p><p>As a response to the shortcoming of the existing simulation methodology, <ref type="bibr" target="#b74">Foldnes and Grønneberg (2019)</ref> proposed a new simulation technique for ordinal data in the context of covariance structure analysis, where the continuous marginals are fixed to the standard normal distribution. That is, multivariate ordinal data for covariance structure models are generated by randomly drawing observations from the vector X * with standard normal marginals, and the ordinal data are obtained through discretizing these observations with user-specified thresholds. Importantly, since we are dealing with factor or structural equation models, the vector X * must have a user-specified correlation matrix. There are many possible non-normal multivariate distributions for X * where the univariate marginals are standard normal, and with a given correlation matrix. <ref type="bibr" target="#b74">Foldnes and Grønneberg (2019)</ref> proposed to investigate robustness to non-normality by varying the type and degree of non-normality embedded in X * . Since the marginals are always standard normal, keeping the thresholds fixed is equivalent to keeping the marginal distribution of the observed ordinal variables fixed. That is, we may vary the distribution of X * from the multivariate normal case to different types of non-normal distributions, without changing either the univariate observed ordinal variables or the correlation matrix of X * . This approach therefore allows us to disentangle effects of the observed ordinal distribution (level of symmetry, ceiling effects, etc) from effects of the underlying degree and type of non-normality.</p><p>To construct non-normal distributions X * with standard normal marginals and a given correlation matrix, <ref type="bibr" target="#b74">Foldnes and Grønneberg (2019)</ref> suggested to use the VITA (VIne-To-Anything) method <ref type="bibr">(Grønneberg &amp; Foldnes, 2017)</ref>. We here informally present VITA. A more technical introduction to copulas, regular vines and VITA is available in the online supplementary material (p. 6). VITA constructs a multivariate non-normal distribution, with given marginal distributions and correlation matrix, called a regular vine <ref type="bibr">(Bedford &amp; Cooke, 2002)</ref>. Regular vines is a flexible class of distributions, constructed by assembling bivariate copulas in a hierarchy of tree structures. There are many copulas to choose from, and many possible tree structures. We emphasize that the VITA vectors used in this study all have standard normal marginals, and all match the same fixed target population correlation matrix. To illustrate bivariate copulas, which serve as basic building blocks for VITA distributions, we paired three different bivariate copulas with standard normal marginals and plotted the contours in Figure <ref type="figure" target="#fig_2">2</ref>. All three distributions have correlation ρ = 0.56 and standard normal marginals. However, the distributions are constructed from different copulas. In Figure <ref type="figure" target="#fig_2">2</ref>(a) the copula is the normal copula, which when combined with normal marginals yields the bivariate normal distribution. In Figure <ref type="figure" target="#fig_2">2</ref> As can be seen in Figure <ref type="figure" target="#fig_2">2</ref>, relative to the normal copula, the Clayton copula has stronger lower tail dependency, while the Joe copula has stronger upper tail dependency. Also, to illustrate the discretization process, in Figure <ref type="figure" target="#fig_2">2</ref> we have drawn lines representing fixed thresholds of τ</p><formula xml:id="formula_1">1,1 = -1, τ 1,2 = 0 for X * 1 , and τ 2,1 = -1, τ 2,2 = 1 for X * 2 .</formula><p>The ordinal variables X 1 and X 2 will then have three levels: 1, 2 and 3, for example. The marginal distributions of X 1 and X 2 will be identical across the three distributions in Figure <ref type="figure" target="#fig_2">2</ref>. However, the bivariate distributions of the ordinal vector (X 1 , X 2 ) will not be identical. For instance, the lower left corner of each figure corresponds to the event X 1 = 1 ∩ X 2 = 1, and inspection of the contour lines suggests that this event is most likely in the Clayton case in Figure <ref type="figure" target="#fig_2">2</ref>(b), and least likely in the Joe case in Figure <ref type="figure" target="#fig_2">2(c</ref>). In fact, we may calculate these probabilities using the copula package <ref type="bibr">(Hofert, Kojadinovic, Maechler, &amp; Yan, 2013)</ref> for R (R Core Team, 2020). The probabilities for X 1 = 1 ∩ X 2 = 1 are 0.068, 0.095 and 0.046 for the distributions in Figures <ref type="figure" target="#fig_2">2(a)</ref>, 2(b) and 2(c), respectively. The normality-based maximum likelihood for the polychoric correlation is maximized at ρ = 0.56 only in the normal case in Figure <ref type="figure" target="#fig_2">2</ref>(a). For the Clayton distribution in Figure <ref type="figure" target="#fig_2">2</ref>(b) the value that maximizes the likelihood is 0.613, while for the Joe distribution in Figure <ref type="figure" target="#fig_2">2</ref>(c) the likelihood is maximized at 0.496. This simple illustration suggests that the polychoric estimator is sensitive to the underlying distribution. For the distributions in Figures <ref type="figure" target="#fig_2">2(b</ref>) and 2(c), the true correlation is 0.56, but the polychoric estimates are biased, with a relative bias of +9.5% and -11.4%, respectively.</p><p>The full VITA distribution is constructed by specifying bivariate copulas for every pair of variables, where the construction order is dictated by the sequence of trees. Each sequence of trees results in a different distribution, as does each choice of bivariate copulas for the variable pairs, lending the VITA class great flexibility. VITA construction involves numerically solving for a bivariate copula parameter for each pair of variables, rendering VITA calibration more time consuming than other, less flexible methods (e.g., <ref type="bibr" target="#b19">Foldnes &amp; Olsson, 2016;</ref><ref type="bibr" target="#b65">Vale &amp; Maurelli, 1983)</ref>. VITA calibration is implemented in the R package covsim <ref type="bibr">(Foldnes &amp; Grønneberg, 2020)</ref>. After VITA calibration, fast simulation is available using the rvinecopulib package <ref type="bibr">(Nagler &amp; Vatter, 2019)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Literature review and the present study</head><p>In previous simulation studies only a small number of threshold configurations have been used, i.e., the simulated ordinal data exhibited only a small number of distributional forms. For instance, <ref type="bibr">Rhemtulla et al. (2012)</ref> and <ref type="bibr" target="#b38">Li (2016b)</ref> employed only three specific threshold configurations (symmetrical, moderately skewed, and severely skewed). That is, for each generated continuous dataset only three marginal distributions of the ordinal datasets were produced. In the present article, our first empirical study reverses this perspective, by generating a large number of ordinal datasets from a limited number of underlying continuous large-sample datasets. We generate a dataset from each of three underlying distributions (multivariate normal and two non-normal distributions) and discretize it using several different thresholds, chosen from a large set of threshold configurations. Then, variation in parameter bias may be studied as a function of variation in threshold sets. Also, in previous studies, with the exception of Rhemtulla et al. ( <ref type="formula">2012</ref>), all ordinal variables in the observed ordinal vector shared the same distribution. In other words, ordinal data were generated by applying the same set of thresholds to each continuous variable. Such distributions are rarely encountered in a realworld dataset, where one variable is typically distributed differently than another variable. In the present study no two ordinal variables have the exact same distribution, and thereby better reflecting a real-world dataset. Foldnes and Grøn- neberg (2019b) demonstrated, under bivariate non-normality, that the specific choice of thresholds, i.e., of observed ordinal distributions, substantively impacts the bias of the estimated polychoric correlation. In the present study we extend this line of research from the bivariate case of a polychoric correlation to the multivariate case of model parameters, CI coverage, and model fit assessment in a medium-sized SEM.</p><formula xml:id="formula_2">x 1 x 2 -3 -2 -1 0 1 2 -3 -2 -1 0 1 2 3 (a) Normal copula x 1 x 2 -3 -2 -1 0 1 2 -3 -2 -1 0 1 2 3 (b) Clayton copula x 1 x 2 -3 -2 -1 0 1 2 -3 -2 -1 0 1 2 3 (c) Joe copula</formula><p>We expect that the variation in bias observed by <ref type="bibr" target="#b18">Foldnes and Grønneberg (2019b)</ref> for the polychoric case will propagate into ordinal SEM estimation.</p><p>Many simulation studies (e.g., <ref type="bibr" target="#b13">Flora &amp; Curran, 2004;</ref><ref type="bibr" target="#b28">Jin, Luo, &amp; Yang-Wallentin, 2016;</ref><ref type="bibr">Li, 2016a;</ref><ref type="bibr" target="#b44">Moshagen &amp; Musch, 2014;</ref><ref type="bibr" target="#b50">Natesan, 2015;</ref><ref type="bibr" target="#b53">Nestler, 2013;</ref><ref type="bibr" target="#b58">Quiroga, 1994;</ref><ref type="bibr">Rhemtulla et al., 2012;</ref><ref type="bibr" target="#b66">Yang &amp; Green, 2015)</ref> have investigated the robustness of ordinal SEM to non-normality. The general consensus formed from these studies is that "estimation of polychoric correlations is robust to modest violations of underlying normality" <ref type="bibr" target="#b13">(Flora &amp; Curran, 2004)</ref>. For instance, <ref type="bibr" target="#b39">Liu et al. (2017)</ref> refer to these studies and suggest that cat-LS factor loading bias may tend to be trivial when the underlying distribution is non-normal (p.501). Recent research <ref type="bibr">(Grønneberg &amp; Foldnes, 2019)</ref> identified a problem in the way data were generated in these studies, that weakens and severely complicates claims of robustness of the polychoric estimator. These studies relied on the popular data generation technique of <ref type="bibr" target="#b65">Vale and Maurelli (1983)</ref>, which is readily available in software packages. <ref type="bibr" target="#b24">Grønneberg and Foldnes (2019)</ref> pointed out that, surprisingly, discretizing a non-normal vector obtained using the approach of Vale and Maurelli (VM) is in most cases equivalent to discretizing a normal vector. This is the result of a fundamental lack of identifiability of the underlying continuous vector X * , combined with the fact that the VM approach in most cases results in data that have a normal copula <ref type="bibr" target="#b16">(Foldnes &amp; Grønneberg, 2015)</ref>. A careful and detailed discussion of this surprising equivalence is given in the next section.</p><p>There are few studies focusing on polychoric correlation estimation bias based on underlying bivariate distributions that are not VM transforms. <ref type="bibr" target="#b43">Monroe (2018)</ref> used the bivariate t-distribution, a skew-normal distribution, and a normal mixture and found substantial bias, especially in the two latter distributions. Jin and Yang-Wallentin (2017) used skewnormal and skewed t-distributions and found moderate bias. Likewise, to our knowledge, there is a scarcity of studies on the effect of underlying non-normality on ordinal factor analysis/structural equation modeling (CFA/SEM) based on underlying continuous distributions other than the VM transform. <ref type="bibr" target="#b43">Monroe (2018)</ref> employed outliers and the inclusion of a quadratic term to generate multivariate non-normality for a two-factor analytical model using 10 and 20 variables, and reported substantial bias in estimated factor loadings and test statistics when data were generated by using a quadratic term. <ref type="bibr" target="#b42">Maydeu-Olivares (2006)</ref> presented a small simulation study using a CFA with 12 variables, and a fixed symmetrical ordinal distribution with 3 levels, where the underlying distribution followed the multivariate t-distribution. Bias in parameter estimates and standard errors was low, with median absolute relative bias at 1% and 4%, respectively, for a sample size of n = 1000. Given the scarcity of studies not using the VM method that investigate how ordinal CFA/SEM estimation is affected by underlying non-normality, and the conflicting results among such studies, we deem it important to further investigate this issue.</p><p>A majority of simulation studies have employed confirmatory factor models <ref type="bibr" target="#b27">(Hoogland &amp; Boomsma, 1998)</ref>, and it has therefore been recommended to extend such studies to full SEM models <ref type="bibr" target="#b3">(Beauducel &amp; Herzberg, 2006;</ref><ref type="bibr" target="#b13">Flora &amp; Curran, 2004;</ref><ref type="bibr">Rhemtulla et al., 2012)</ref>. Hence, in the present study we adopt the full SEM model used by <ref type="bibr" target="#b38">Li (2016b)</ref> to assess the performance of various estimators for ordinal SEM under the assumption of underlying normality. This model was constructed from a review of empirical studies using structural equation modeling to be representative from a practical standpoint.</p><p>In non-normal conditions where cont-ML, cont-ML-adj, and cat-LS estimation may perform poorly due to estimation bias, there is a need for a test for underlying normality. Before conducting a SEM with ordinal data, a researcher could use such a test to assess whether there is evidence of discretized non-normality in the data. Should the test indicate that the ordinal dataset is unlikely to stem from discretizing a normal vector, cat-LS must be used with caution. Unfortunately, due to the lack of a statistical test for underlying nonnormality in popular software packages, the current practice in empirical studies utilizing ordinal SEM is to take the normality of X * for granted. A true multivariate test for underlying normality was proposed by <ref type="bibr" target="#b42">Maydeu-Olivares (2006)</ref>, but this test has been largely ignored in the literature and in software packages. <ref type="bibr" target="#b18">Foldnes and Grønneberg (2019b)</ref> conducted the first empirical investigation of this test, and also proposed a new parametric bootstrap test. The bootstrap test operates by estimating the polychoric correlation and the thresholds from the original sample. Then, many continuous samples are drawn from a multivariate normal vector whose correlation matrix equals the polychoric correlation matrix. Each continuous sample is further discretized using the estimated thresholds to produce an ordinal data sample. Then, the test statistic of Maydeu-Olivares ( <ref type="formula">2006</ref>) is calculated in each ordinal sample. The proportion of times the bootstrapped test statistics exceed the test statistic from the original sample serves as the p-value of the bootstrap test for underlying normality. The test of <ref type="bibr" target="#b42">Maydeu-Olivares (2006)</ref> and the bootstrap test were found to maintain Type I error well for dimensionalities less than ten, but only the bootstrap test maintained an adequate Type I error control for larger dimensionalities. The bootstrap test has hitherto been studied only by <ref type="bibr" target="#b18">Foldnes and Grønneberg (2019b)</ref> in a CFA context with at most 15 variables. An aim in the present study was therefore to evaluate the bootstrap test in the context of a medium-sized SEM with 20 variables. It is hoped that such a test may reliably detect conditions where underlying non-normality and the observed ordinal distributional forms render ordinal SEM inference severely biased.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The complications of interpreting ordinal SEM simulation studies</head><p>The problem of identification of aspects of the underlying distribution of X * based on observations from the ordinalcategorical X has recently received attention <ref type="bibr" target="#b0">(Almeida &amp; Mouchart, 2014;</ref><ref type="bibr">Foldnes &amp; Grønneberg, 2019</ref><ref type="bibr" target="#b18">, 2019b;</ref><ref type="bibr">Grønneberg &amp; Foldnes, 2019)</ref>. Here, we summarize some of the main conclusions from <ref type="bibr" target="#b24">Grønneberg and Foldnes (2019)</ref> and <ref type="bibr" target="#b74">Foldnes and Grønneberg (2019)</ref>, which show that utmost caution must be taken when interpreting simulation studies using the VM simulation method in the context of ordinal variables. We split our discussion into two subsections. First, we outline how the effect of the marginal distribution of X * and the threshold placement are confounded in simulation designs. Second, we discuss the interpretation of simulation results for cat-LS where the simulation method is to discretize a VM vector. We review the arguments of <ref type="bibr" target="#b24">Grønneberg and Foldnes (2019)</ref> which note that in most circumstances, simulating X by discretizing a VM vector X * is numerically identical to simulating X from an exactly multivariate normal X * . The only difference is that the thresholds are changed.</p><p>Since the ordinal vector X is a censored version of the underlying vector X * , there will in general be many other vectors, say X * , which when discretized produce exactly the same vector X, or at least a vector with the same distribution. Therefore, one cannot use X to pin-point the exact distribution of X * . To simulate by discretization of X * into X therefore also implies that we simulate by discretization of X * into X. This complicates the interpretation of all simulation studies where ordinal data stem from discretizing continuous variables. The concept of partial identification analysis is briefly introduced as a theoretical solution to this problem in the online supplementary material (p. 1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Marginal distributions and thresholds are confounded</head><p>The marginal distributions of X * and the thresholds (τ k, j ) cannot be separately identified from data. That is, when only observing X, we cannot simultaneously estimate the thresholds and marginal distributions if they are both unknown. Since the values of X * are not used in generating X except through the thresholds, the marginal scale of X * is not identified when observing only X. Let F k (x) = P(X * k ≤ x) be the k-th marginal distribution of X * . We assume that F k is invertible. Then</p><formula xml:id="formula_3">P(X k ≤ x k, j ) = P(X * k ≤ τ k, j ) = F k (τ k, j )<label>(2)</label></formula><p>Since P(X k ≤ x k, j ) is a feature of the distribution of X, it can be deduced from data. However, only F k (τ k, j ) is identified, and the marginal F k and the thresholds τ k, j are not separately identified. Once the marginal distributions of X * are specified, we can use this to solve Equation (2) for τ k, j , producing the equation</p><formula xml:id="formula_4">τ k, j = F -1 k P(X k ≤ x k, j ) .<label>(3)</label></formula><p>This equation shows that in an empirical setting the thresholds and the marginal distribution are inseparable, unless we know one or the other. Let us further consider the interaction between thresholds and marginal distributions and how this affects simulation when X * is non-normal. From Equation (1), we have the equivalence</p><formula xml:id="formula_5">X k = x k, j ⇐⇒ τ k, j-1 &lt; X * k ≤ τ k, j .<label>(4)</label></formula><p>However, as we will see in examples in the upcoming subsection, there are many random vectors X * which generate X via Equation (1) using a modified set of thresholds. Indeed, for any sequence H 1 , . . . , H p of strictly increasing transformations, we have from Equation (4) that</p><formula xml:id="formula_6">X k = x k, j ⇐⇒ H k (τ k, j-1 ) &lt; H k (X * k ) ≤ H k (τ k, j ). Let τk, j = H k (τ k, j-1 ) and X * k = H k (X * k )</formula><p>. Then this equation is</p><formula xml:id="formula_7">X k = x k, j ⇐⇒ τk, j-1 &lt; X * k ≤ τk, j .<label>(5)</label></formula><p>The distribution of X * = ( X1 , . . . , Xk ) will be different than the distribution of X * . Mathematically, the marginals of X * have changed compared to X * , but the rest of the distribution, what is formerly known as the copula, has stayed the same. By an appropriate choice of H k functions, one can show that the marginal distributions of X * can be transformed into whatever distribution one would like. For example, it is always possible to choose H k in such a way that X * is marginally standard normal <ref type="bibr">(Foldnes &amp; Grønneberg, 2019, Proposition 1)</ref>. Hence, in a simulated sample, there is no single and correct way to define the population thresholds and marginal distributions of X * . We have seen that simulating X through discretizing X * yields ordinal observations that are numerically equal to simulating X through discretizing X * . By numerically equal, we mean that the simulated samples of X from X * and X * are identical. The simulation of X can therefore be seen as having been generated simultaneously from X * as well as X * , though with different thresholds.</p><p>Let us now return to Equation (3). In our simulation design, outlined in the method section, we vary the marginal distribution of X across other factors, such as the type of nonnormality of X * . Since the marginal distributions of X * and the threshold values jointly produce the marginal distribution of X through Equation (3), we decided to fix the marginal distributions of X * to standard normal, and let the threshold values vary to produce different marginal X distributions. Then the thresholds uniquely define the marginal distribution of X, and vice versa, by</p><formula xml:id="formula_8">P(X k ≤ x k, j ) = Φ(τ k, j ), τ k, j = Φ -1 P(X k ≤ x k, j ) .</formula><p>Fixing the marginal distribution of X * allows us to study the effect of the shape of the ordinal observed marginal distribution of X on ordinal SEM. The choice of standard normal marginals for X * is natural because this is the scale of the standard methodology: The covariance of a random vector depends on the marginal distributions, yet the marginal distribution of X * is not identified based on observations from X. Therefore, consistent statistical estimators for Cov(X * ) must make assumptions on what the marginal distribution of X * is. The common assumption is that they are standard normal, hence dictating the scale at which the covariance matrix is to be estimated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The surprising equivalence between ordinal VM simulation and ordinal normal simulation</head><p>Here, we consider the consequences of the nonidentifiability of the distribution of X * from the distribution of X in the case of the well-known VM simulation method. Briefly, VM consists of applying third order polynomial transformations to the coordinates of a random vector Z which is multivariate normal with a covariance matrix Σ Z . In practice, the polynomial transformations are usually either strictly increasing or strictly decreasing. We will limit attention to the strictly increasing case, and refer the reader to <ref type="bibr" target="#b24">Grønneberg and Foldnes (2019)</ref> for a full discussion. For each k = 1, . . . , p, VM generates</p><formula xml:id="formula_9">X * k = H k (Z k ),</formula><p>where H K is strictly increasing.</p><p>While the function H k used in the original VM approach is a third degree polynomial, the upcoming argument rests only on the assumption that H k is a strictly increasing transformation. The ordinal observations are thereafter generated following Equation ( <ref type="formula" target="#formula_0">1</ref>):</p><formula xml:id="formula_10">X k = x k, j , when τ k, j-1 &lt; X * k ≤ τ k, j<label>(6)</label></formula><p>Since</p><formula xml:id="formula_11">X * k = H k (Z k ) we have that τ k, j-1 &lt; X * k ≤ τ k, j is equiva- lent to τ k, j-1 &lt; H k (Z k ) ≤ τ k, j .<label>(7)</label></formula><p>We next apply the strictly increasing inverse of H k to the above inequalities, and get</p><formula xml:id="formula_12">X k = x k, j , when τk, j-1 &lt; Z k ≤ τk, j<label>(8)</label></formula><p>where τk, j = H -1 k (τ k, j ). This argument can be reversed by starting with discretizing a multivariate normal random vector Z into X, and applying H k to the inequalities in Equation ( <ref type="formula" target="#formula_12">8</ref>) and arrive at Equation (6).</p><p>We conclude that simulating X by discretizing a VM generated X * is equivalent to simulating X by discretizing an exactly multivariate normal vector Z. The polynomial transformation that the VM method applies to Z is essentially lost under discretization. This has consequences when applying statistical methods to samples from X which assume that X * is multivariate normal. One such prominent method is the polychoric correlation of <ref type="bibr" target="#b57">Pearson and Pearson (1922)</ref> and <ref type="bibr" target="#b54">Olsson (1979a)</ref>, which is also central to cat-LS. Above we observed that when X * = Z is exactly multivariate normal, any strictly increasing transformation H k can be applied to the coordinates of X * , and the resulting vector, say X * , could have generated X when discretized. The only change is the numerical values of the thresholds. When the transformations are strictly increasing third-order polynomials, the resulting vector X * has the same distribution as a VM simulated vector. Yet, the polychoric estimator will estimate the correlation matrix of X * and not X * which here could have been generated from the VM method. Why is this? The answer lies in the consistency of the polychoric correlation when X * is normal, which is the case for the fully normal X * and not for the VM vector X * . The polychoric correlation estimator will estimate the covariance matrix of the version of the underlying vector which is fully normal. Should X be a discretization of a VM vector X * , the polychoric correlation estimator will still implicitly work with the transformed vector Z, because it is exactly normal. Therefore, the polychoric correlation will not estimate the correlation matrix of the simulated X * , but will instead estimate the correlation matrix of the underlying Z. A more mathematical explanation of this correspondence is given in <ref type="bibr" target="#b24">Grønneberg and Foldnes (2019)</ref>. Several numerical illustrations of this correspondence, where it is shown that the above observations may be used to exactly predict the results reported in simulation studies on the effect of non-normality on polychoric estimation (e.g., <ref type="bibr" target="#b13">Flora &amp; Curran, 2004)</ref>, is given in the online appendix of <ref type="bibr" target="#b24">Grønneberg and Foldnes (2019)</ref>. A particularly unfortunate feature of the VM method is that the covariance matrix of Z and X * are often very similar, therefore making it more difficult to detect this problem in numerical investigations.</p><p>We illustrate the above discussion by considering a typical simulation condition used in ordinal SEM simulation articles. <ref type="bibr" target="#b13">Flora and Curran (2004)</ref> included a condition with moderate skewness and moderate kurtosis, where skewness=1.25 and excess kurtosis = 3.75 in all marginal distributions. For the K = 5 condition they adopted thresholds from <ref type="bibr" target="#b46">Muthén and Kaplan (1985)</ref>: τ 1 = -1.645, τ 2 = -0.643, τ 3 = 0.643, and τ 4 = 1.645. The VM method first determines the polyno- mial coefficients so that X * k = H(Z k ) has the required skewness and kurtosis. Such a polynomial is depicted in Figure <ref type="figure" target="#fig_3">3</ref>. Note that the polynomial is strictly increasing, so we may solve for the thresholds, as indicated by dashed arrows. The new thresholds τ1 = -2.122, τ2 = -0.657, τ3 = 0.817, and τ4 = 1.53 are located along the Z axis. This means that discretizing a standard normal variable according to the τi , i = 1, . . . , 4 leads to the exact same univariate ordinal distribution as discretizing the Fleishman variable according to the original thresholds, that is, a five-category distribution with proportions 0.017, 0.239, 0.537, 0.144, and 0.063.</p><p>In the simulation design of <ref type="bibr" target="#b13">Flora and Curran (2004)</ref> it was required that the correlation between X * 1 = H(Z 1 ) and X * 2 = H(Z 2 ) be equal to ρ X * = 0.49. A correlation between Z 1 and Z 2 which makes this possible is ρ Z = 0.5084. We may therefore simulate a VM pair of variables by first simulating from a bivariate normal distribution with correlation ρ Z = 0.5084 and standard marginals, and then apply the polynomial H(•) to both margins. This defines a bivariate distribution whose population (univariate) skewness and kurtosis equals 1.25 and 3.75, respectively, and whose correlation equals ρ = 0.49. The crucial insight now is to observe that to discretize a VM dataset using thresholds τ, is exactly the same as discretizing the original data set drawn from (Z 1 , Z 2 ) using the transformed thresholds τ. We exemplify this by simulating a dataset with n = 500 using the VM method. This means first drawing a sample with n = 500 from a bivariate normal distribution with standard normal marginals and with correlation ρ Z = 0.5084 and then discretizing X * = (H 1 (Z 1 ), H 2 (Z 2 )) into X. Figure <ref type="figure" target="#fig_5">4(b)</ref> shows a scatter plot of the simulated sample from X * together with its thresholds. Similarly, Figure <ref type="figure" target="#fig_5">4</ref>(a) shows a scatter plot of the simulated sample from the normal data Z, together with the transformed thresholds.</p><p>In both plots, we use a color coding for the cells in the resulting 5 × 5 contingency table and show that the corresponding contingency tables deriving from the data in Fig-q q q q q q q q q q q q q q q q q qq q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q qq q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q -2 0</p><formula xml:id="formula_13">2 -2 0 2 Z 1 Z 2</formula><p>(a) Sample drawn from bivariate normal distribution, with thresholds τ1 = -2.122, τ2 = -0.657, τ3 = 0.817, and τ4 = 1.53.</p><p>q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q qq q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q -2 0  </p><formula xml:id="formula_14">(Z) = -0.16 + 0.82Z + 0.16Z 2 + 0.05Z 3 .</formula><p>ures 4(a) and 4(b) are identical. The monotonic VM transformation is such that the discretized data are actually produced by discretizing data drawn from the normal distribution. In large samples drawn from the VM distribution the polychoric estimator will tend to approach 0.5084; refer to the R supplementary material for more details on the computations. The reason for the polychoric correlation not approaching the specified ρ X = 0.49 therefore is not to be found in any underlying non-normality, but is due to the demonstrated equiv-alence of discretized VM data and discretized normal data. Such an equivalence does not happen when using the VITA simulation method, except in extraordinary cases which may be detected by applying a test for underlying normality to a large VITA sample.</p><p>To sum up, studies using the VM method for ordinal data do not, in this sense, violate the normality assumption, but instead do something exactly equivalent to simulating from multivariate normal random vector X * = Z whose covariance matrix departs slightly from the intended covariance matrix. In many studies, the latent variable model employed was not correctly specified for X * = Z even when it holds for the VM random vector X * . This means that several influential simulation studies must be re-interpreted in light of this observation.</p><p>Theoretical investigations: What happens when the number of categories increases?</p><p>The behavior of ordinal SEM methodology crucially depends on the way correlations are calculated. Here, we develop several fundamental theoretical and mathematical insights into the behavior, as the number of categories increases, of three approaches to calculating correlations in ordinal data. The first two approaches are well-known, namely the Pearson correlation and the polychoric correlation. Our analysis leads us to propose an adjusted version of the Pearson correlation, as well as extending the scope of valid estimation for the polychoric correlation. To the best of our knowledge, all the results presented in this section are new. We give brief numerical illustrations of our results.</p><p>We first provide conditions under which the observed vector X is similar to X * in the sense that Cov(X) is a good approximation to Cov(X * ) as K → ∞. This motivates an empirical adjustment to integer-encoded data which approximates the values that lead to consistency. The crucial assumption to achieve this is that the marginal distributions of X * are known. Since no aspect of the marginal distribution can be estimated through statistical means, this requirement is absolute when aiming at estimating Cov(X * ) unless we have knowledge of the values of the thresholds.</p><p>To our knowledge, our empirical adjustment and its theoretical justification are new, but we emphasize that it is closely related to other methods that convert ordinal variables into continuous variables. These methods use different assumptions and theoretical frameworks, e.g., variants of optimal scaling (De Leeuw, Young, &amp; Takane, 1976), item response theory prediction of summary scores <ref type="bibr" target="#b25">(Harwell &amp; Gatti, 2001)</ref>, anchoring vignettes <ref type="bibr" target="#b35">(King, Murray, Salomon, &amp; Tandon, 2003)</ref>, and the Markov Chain Monte Carlo Scaling method of <ref type="bibr" target="#b22">Granberg-Rademacker (2010)</ref>. The adjustment we suggest has the advantage of being simple and computationally cheap. It does not require numerical optimization and rests only on univariate assumptions on X * . This is in contrast to more complicated approaches, such as Markov Chain Monte Carlo Scaling, which rests on an assumption of multivariate normality for an underlying variable and is computationally demanding.</p><p>Initial numerical experiments indicated that the empirical adjustment method we propose is usually inferior to the polychoric correlation, even when the assumption of full normality of X * is not true. We found a theoretical answer to why this is the case: the polychoric correlation is consistent as K → ∞, even when the distribution of X * is non-normal. Again, we must assume that the marginals of X * are known, though they need not be normal. This observation significantly increases the scope of the polychoric correlation.</p><p>Our final contribution in this section is an expression for the population value of the polychoric correlations as K → ∞ if the marginal distributions are supposed to be known, but are misspecified. A consequence of this analysis is the fact that we can consistently estimate the Spearman correlation of X * as K → ∞, by assuming uniform marginals. We also show that simply applying the regular Spearman correlation to the ordinal data is also consistent as K → ∞.</p><p>To make clear that X depends on K, we write X (K) , and indicate this relationship in the same manner also for other quantities that depend on K, such as the thresholds.</p><p>When is the Pearson correlation consistent with a large number of categories?</p><p>Our observations X (K) are not on the interval scale. Their numerical values are therefore strictly speaking arbitrary, as long as the order is properly encoded. The common practice of using cont-ML with integer encoded data is therefore problematic, as the encoding will influence the analysis in an arbitrary manner. As discussed in <ref type="bibr">Bollen (1989, Chapter 4.</ref>3), the assumptions underlying SEM are not fulfilled for integer encoded data, and therefore cont-ML need not be consistent, even when the underlying vector X * is exactly normal.</p><p>Instead of encoding data by a sequence of consecutive integers, we propose to encode in such a way that the covariance matrix of X (K) approximates that of X * as well as possible. We here identify encoding conditions making this possible, which leads us to understand when integer-encoded data can be used as continuous when the number of categories increases to infinity. Also, our analysis suggests a data-driven adjusted encoding which places the ordinal observations on the same scale as the underlying variable X *</p><p>Let us consider why it could be worth studying what happens as K → ∞, although K in practice is typically less than ten. An asymptotic analysis has the advantage of identifying assumptions that have to be fulfilled if consistency is to be reached, even in the best case scenario of having very large K. From the perspective of infinity, it is of little consequence whether K is 7 or 7 100 , since both numbers are very far from infinity. The hope in asymptotic analysis is that the resulting approximations are relevant for small K. Whether or not this is the case may be determined by simulations.</p><p>Assumption 1.</p><p>1. We assume there is an &gt; 0 such that</p><formula xml:id="formula_15">E |X * k | 2+ &lt; ∞ for k = 1, . . . , p. 2. We have τ (K) k, j-1 ≤ x (K) k, j ≤ τ (K)</formula><p>k, j for k = 1, . . . , p and j = 1, . . . , K -1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">We have that lim</head><formula xml:id="formula_16">K→∞ sup 1≤k≤p,2≤ j≤K-1 τ (K) k, j -τ (K) k, j-1 = 0, and that τ (K) k,1 ----→ K→∞ F -1 k (0) and τ (K) k,K-1 ----→ K→∞ F -1 k (1) for k = 1, . . . , p. 4. We have x (K) k,1 F k (τ (K) k,1 ) ----→ K→∞ 0 and x (K) k,K [1 - F k (τ (K) k,K-1 )] ----→ K→∞ 0 for k = 1, . . . , p.</formula><p>A technical discussion of these assumptions, a mathematical proof of the following result, and the intuition that led to its formulation is given in the online supplementary material, p. 2.</p><formula xml:id="formula_17">Proposition 1. Under Assumption 1 we have lim K→∞ Σ K = Σ, where Σ K = Cov(X (K) ) and Σ = Cov(X * ).</formula><p>This proposition gives insight into why cont-ML sometimes produces reasonable results. As seen above, the marginals of X * cannot be deduced from X, but as X * is usually assumed to be standardized, the scale of the covariance matrix of X is entirely a product of the integer encoding together with the placement of the thresholds, both of which should not affect a statistical analysis. Further, it is clear that as</p><formula xml:id="formula_18">K → ∞, both | E X (K) k | → ∞ and Var X (K)</formula><p>k → ∞, hence the covariance matrix of X (K) cannot approximate Cov(X). But the correlation matrix can approximate Cov(X), and since it stays invariant under affine transformations (i.e., transformations of the form x → ax + b), cont-ML based on the correlation matrix will be consistent as K → ∞ when the thresholds are evenly spaced. If this is the case, Equation (2) (p. 7) implies that for each k, the CDF of X k matches that of X * k . This means that if the thresholds are evenly spaced, and X * k is normal, the bar plot of the ordinal observations will appear normal in shape. The fact that ordinal observations can successfully be treated as continuous when the marginal observations of X are shaped like the normal distribution and X * in fact is normal, has been observed on several occasions <ref type="bibr" target="#b2">(Babakus, Ferguson, &amp; Jöreskog, 1987;</ref><ref type="bibr" target="#b12">DiStefano, 2002;</ref><ref type="bibr" target="#b30">Johnson &amp; Creech, 1983;</ref><ref type="bibr" target="#b46">Muthén &amp; Kaplan, 1985;</ref><ref type="bibr" target="#b55">Olsson, 1979b;</ref><ref type="bibr">Rhemtulla et al., 2012)</ref>, but to date, to our knowledge, this seems to have not been explained theoretically. We formalize this into the following corollary.</p><p>Assumption 2. Suppose that for each 1 ≤ k ≤ p, the thresholds τ (K)  k, j for j = 1, 2, . . . , K -1 are evenly spaced and of the form</p><formula xml:id="formula_19">τ (K) k, j = z (K) m + (z (K) M -z (K) m )( j -1)/(K -2)</formula><p>where z (K) m and z (K) M are such that Assumption 1 (4) are fulfilled for x (K)  k,1 = z (K) m and x (K) k,K = z (K) M . The proof of the following corollary is given in the online supplementary material (p.4). <ref type="table">Assumption 1 (1), (2)</ref> and<ref type="table" target="#tab_2">(3)</ref> and<ref type="table">Assumption 2 hold.</ref> Then, for integer encoded observations, i.e., when x k, j = j, we have lim K→∞ Cor(X (K)  k ,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Corollary 1. Suppose</head><formula xml:id="formula_20">X (K) l ) = Cor(X * k , X * l )</formula><p>for each 1 ≤ k, l ≤ p. Corollary 1 can in many cases be used to deduce what cont-ML estimates outside Assumption 2. Here, we give an illustration. Suppose there is an invertible function</p><formula xml:id="formula_21">H k such that τ k, j = H k (τ (K) k, j</formula><p>), where (τ (K)  k, j ) fulfill Assumption 2. Since</p><formula xml:id="formula_22">τ k, j-1 ≤ X * k ≤ τ k, j ⇐⇒ H k (τ k, j-1 ) ≤ X * k ≤ H k (τ k, j ) ⇐⇒ τk, j-1 ≤ H -1 k (X * k ) ≤ τk, j the variable X * = (H -1 k (X * 1 , . . . , H -1 k (X * p )</formula><p>) is discretize equivalent to X * , using the thresholds (τ k, j ). Therefore, Corollary 1 implies that for integer encoded data, we have</p><formula xml:id="formula_23">lim K→∞ Cor(X (K) k , X (K) l ) = Cor(H -1 k (X * k ), H -1 l (X * l ))<label>(9)</label></formula><p>for each 1 ≤ k, l ≤ p. With the exception of rare cases, we have that Cor(H</p><formula xml:id="formula_24">-1 k (X * k ), H -1 l (X * l )) Cor(X * k , X * l )</formula><p>, showing that cont-ML will be invalid, even in the best case scenario that K → ∞.</p><p>What then, should be done if evenly spaced thresholds are incompatible with the marginal distributions of X * ? We may then use the threshold values to assign an encoding which is not evenly spaced. This leads us to an adjusted Pearson correlation. In practice, the thresholds are unknown, and therefore have to be estimated. A full theoretical analysis of the consequences of empirical threshold estimates is beyond the scope of the present article, but we briefly mention that one may extend Proposition 1 into a generally consistent method based on adjusted observations using an empirical version of Assumption 1 (2) given by τk</p><formula xml:id="formula_25">, j-1 ≤ xk, j ≤ τk, j<label>(10)</label></formula><p>where τk,0 = F -1 k (0) which equals -∞ in the normal case, and where τk,0 = F -1 k (1) which equals ∞ in the normal case, and where for j = 1, . . . , K -1 we have</p><formula xml:id="formula_26">τk, j = F -1 k (π k,1: j ), πk,1: j = 1 n n i=1 I{X (K) k,i ≤ j}.<label>(11)</label></formula><p>Here, X (K) k,i is the k'th marginal of the i'th person in a sample. The estimators (τ k, j ) are the standard estimator for thresholds, see <ref type="bibr" target="#b57">Pearson and Pearson (1922)</ref> and <ref type="bibr" target="#b54">Olsson (1979a)</ref>.</p><p>Ordinal SEM based on such empirically adjusted observations is referred to as cont-ML-adj. The requirement of Equation (10) leaves the concrete choice of ( xk, j ) open. A reasonable suggestion is for k = 1, . . . , p and j = 1, . . . , K to let</p><formula xml:id="formula_27">xk, j = m k (τ k, j-1 , τk, j ) where m k (x, y) = E X * k x ≤ X * k ≤ y .</formula><p>Note that m k is a known function since the marginal distributions (F k ) are assumed known. Ignoring sampling error, the value xk, j is the conditional expectation of X * k when we know that τk, j-1 ≤ X * k ≤ τk, j . When X * k is standard normal so that F k = Φ with density φ = Φ , we have that</p><formula xml:id="formula_28">m k (x, y) = [φ(x) -φ(y)]/[Φ(y) -Φ(x)]</formula><p>(Kotz, Balakrishnan, &amp; Johnson, 2004, Chapter 13, section 10.1). The suggested empirical adjustment procedure is therefore to encode the ordinal observations according to</p><formula xml:id="formula_29">xk, j = [φ(τ k, j-1 ) -φ(τ k, j )]/[Φ(τ k, j ) -Φ(τ k, j-1 )].</formula><p>cat-LS is consistent when the number of categories increases and marginal distributions are correctly specified</p><p>In our simulations (Study 1, p.16), it is seen that cont-MLadj performs overall better than the unadjusted cont-ML, although the improvement is generally moderate. In some of our simulation conditions, cont-ML-adj outperforms cat-LS, and this motivates a more general comparison as a topic for further research. However, in most simulation conditions, cat-LS based on the polychoric estimator of <ref type="bibr" target="#b54">Olsson (1979a)</ref>; <ref type="bibr" target="#b57">Pearson and Pearson (1922)</ref> is the best approach, despite cont-ML-adj being valid even when X * is not multivariate normal, as K → ∞.</p><p>A theoretical explanation for the relatively high performance of the polychoric correlation is that as K → ∞, cat-LS is actually also a consistent method as long as the marginals of X * are known and standardized. This is suprising, since the polychoric estimator was developed under the assumption that X * is fully normal. This new observation significantly increases the scope of the polychoric correlation, and may justify its use in empirical studies where full underlying normality is unknown. The following argument can be made mathematically rigorous, but the approximations which underly its formal proof are involved, and we consider it outside the scope of the present article. We will however numerically illustrate its validity.</p><p>We consider the thresholds known, as they can always be consistently estimated. This can be seen from Equation (11): Since πk,1: j consistently estimates π k,1:</p><formula xml:id="formula_30">j = P(X (K) k ≤ j) = P(X * k ≤ τ k, j ) = F k (τ k, j</formula><p>) the continuous mapping theorem implies that τk, j is a consistent estimator for any set of (continuous) marginals F k . Therefore,</p><formula xml:id="formula_31">τk, j = F -1 k (π k,1: j ) P ----→ n→∞ F -1 k (π k,1: j ) = F -1 k [F k (τ k, j )] = τ k, j<label>(12)</label></formula><p>where P ----→ n→∞ means convergence in probability as the sample size n increases to infinity. Now with a finer and finer net of thresholds, the polychoric correlation matrix, which is the normal theory MLE (maximum likelihood estimator) based on progressively less censored versions of X * as K → ∞, will approach the normal theory pairwise MLE of the underlying</p><formula xml:id="formula_32">X * as K → ∞.</formula><p>The normal theory pairwise MLE based on samples from the underlying X * when we only estimate the correlation (and use the knowledge that X * has standardized marginals) will, as the sample size increases, converge towards</p><formula xml:id="formula_33">ρ • (F k,l ) = arg max -1≤r≤1 E F k,l log φ 2 (X * k , X * l ; r),</formula><p>where s • = arg max s H(s) means the arguments that maximize H(s) are contained in s • , i.e., the set (or singleton when the maximum is unique) such that s ∈ s • implies H(s) ≥ H( s) for all s. Also, F k,l is the bivariate CDF of (X * k , X * l ) , and φ 2 (•, •; r) is the density of a bivariate normal Z with standardized marginals and correlation r. The proof of the following lemma is given in the online supplementary material (p.4).</p><formula xml:id="formula_34">Lemma 1. If F k , F l are standardized, then ρ • (F k,l ) = Cor(X * k , X * l )</formula><p>. Therefore, the polychoric correlation is consistent when K → ∞ as long as the standardized marginals are correctly specified.</p><p>Suppose now that the true marginal distributions are not known, but are erroneously assumed to equal F k for k = 1, . . . p, when in fact they are equal to G k for k = 1, . . . , p. Suppose F k , G k are continuous and strictly increasing for k = 1, 2, . . . , d, and that F k is standardized. Following the argument that leads to Equation ( <ref type="formula" target="#formula_31">12</ref>), we see that we then have τk,</p><formula xml:id="formula_35">j P ----→ n→∞ F -1 k G k (τ k, j</formula><p>). Since the marginals are arbitrary, we can choose them in a manner explained in the online supplementary material (p. 5) so that the thresholds in fact are consistent. We are then estimating a covariance matrix with elements σk,l = Cor(</p><formula xml:id="formula_36">F -1 k G k (X * k ), F -1 l G l (X * l )) as K → ∞.</formula><p>As an example application, let us consider now how to use this to estimate the Spearman correlation ρ S k,l of X * k , X * l , which is the Pearson correlation of the copula of X * k , X * l <ref type="bibr">(Nelsen, 2007, Theorem 5.1.6)</ref></p><formula xml:id="formula_37">. That is, ρ S k,l = Cor(F k (X * k ), F l (X * l )</formula><p>). In order to reach ρ S k,l by using polychoric correlations, the following considerations are necessary. Since</p><formula xml:id="formula_38">(U k , U l ) = (F k (X * k ), F l (X * k )</formula><p>) has uniform marginals on <ref type="bibr">[0,</ref><ref type="bibr" target="#b9">1]</ref>, which are not standardized, we deduce the standardized versions of these distributions. Since E U k = 1/2 and</p><formula xml:id="formula_39">√ Var U k = 1/ √ 12 = 1/(2 √ 3), we have that Ũk = (U k -1/2)(2 √ 3) is standardized, and uniform on [- √ 3, √ 3].</formula><p>Therefore, in order to reach the Spearman correlation, we estimate the thresholds using the assumption that the marginal distributions of X * k are uniform on [-√ 3,</p><p>√ 3], so that their CDF is</p><formula xml:id="formula_40">F k (x) = x -a b -a I{a ≤ x ≤ b} + I{x &gt; b}, a = -b, b = √ 3.</formula><p>Numerical illustrations of this technique are found after the next sub-section. Note that a simple extension of cont-MLadj is to use standardized uniform marginals, which leads to another estimator of Spearman's correlation.</p><p>The Spearman correlation is consistent as the number of categories increases</p><p>The empirical Spearman correlation based on the ordinal observations is also consistently estimating the Spearman correlation of X * as K → ∞. Since this is far simpler than computing the polychoric correlation with standardized uniform marginals, the above discussion is merely an illustration of our arguments. Note that applying both the empirical Spearman correlation and the polychoric correlation based on standardized uniform marginals will consistently reach the Spearman correlation of X * as the sample size and K increase, and also note that this is achieved without assumptions on the marginal distribution or any other feature of the distribution of X * . This is because the Spearman correlation is defined as the correlation of the distribution of X * when the marginals are transformed to a specific distribution, namely the uniform distribution. See the online supplementary material (p. 4) for a proof of the following proposition.</p><p>Proposition 2. Suppose Assumption 1 holds. Assume in addition that X * has continuous marginal distributions. Then the population Spearman's correlation based on X (K) consistently estimates the population Spearman's correlation based on X * .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Numerical illustrations</head><p>Here, we provide four illustrations of consistency, or lack thereof, as the number of categories increases. Reproduction files are found in the supplementary material of this article.</p><p>Illustration 1 demonstrates the result in Corollary 1 that the Pearson correlation is consistent as K increases, provided the thresholds are evenly distributed. A large sample with n = 10 7 was generated from a distribution with a Clayton copula (C) and normal marginals, and from a fully normal distribution (N), both with a correlation of ρ = 0.7. The samples are then discretized according to Equation <ref type="bibr" target="#b9">(1)</ref> with evenly spaced thresholds varying between -3.2 and 3.2, the number of which is equal to K -1 (producing K categories). For K = 1 we used τ k,1 = 0. For K = 2 we used</p><formula xml:id="formula_41">τ k,1 = -3.2/2, τ k,2 = 3.2/2. For the remaining values of K, we used τ k, j = z (K) m + (z (K) M -z (K) m )( j -1)/(K -2) with z m = -3.2, z M = 3.2.</formula><p>Based on the discretized datasets, we calculated the Pearson correlation for X (K) for each K = 2, . . . , 19. Since n is large, the computed values are close to the population values. The resulting values are plotted in Figure <ref type="figure" target="#fig_6">5</ref>. We clearly see the consistency as K increases, and the approximation is very good around K = 16. Whether X * has a normal or Clayton copula is of little consequence in this illustration. q q q q q q q q q q q q q q q q q q 0.4 Illustration 2 demonstrates Equation ( <ref type="formula" target="#formula_23">9</ref>), for the same two large datasets N and C, this time discretized only once, with unevenly spaced thresholds. We let K = 51 and use the following thresholds: 1/3 . In Table <ref type="table">1</ref> (p.13), we see that the Pearson correlation does not reach ρ = 0.7 even for K = 51. As dictated from Equation (9), it instead reaches Cor((X * 1 ) 1/3 , (X * 2 ) 1/3 ). In contrast, both the Pearson-adjusted correlation and the polychoric correlation are consistent as K → ∞.</p><formula xml:id="formula_42">τ k, j = τ3 k, j where τk, j = x (K) m + (x (K) M - x (K) m )( j-1)/(K -2) for x m = -(3.2) 1/3 , x m = (3.2)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 1</head><p>The inconsistency of the Pearson correlation with unevenly spaced thresholds. Illustration 3 also demonstrates the consistency of the adjusted Pearson and the polychoric correlations with unevenly spaced threshold generated in a similar way as was done in Illustration 2. This time we investigate the rate of convergence, as K increases, see Figure <ref type="figure" target="#fig_8">6</ref>. Under normality, it is seen that the polychoric correlation is unbiased for all K. Under the Clayton copula, the polychoric correlation is biased for low values of K, but approaches the true underlying correlation rather quickly, being virtually unbiased for K = 10. We also see the consistency of the adjusted Pearson correlation for both underlying distributions, as K increases. In contrast, the standard Pearson correlation converges at a fast rate to a value not close to ρ = 0.7, but converges instead to Cor (X * 1 ) 1/3 , (X * 2 ) 1/3 as dictated by theory. q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q C N 4 Illustration 4 demonstrates techniques developed for the polychoric correlation as outlined after Lemma 1. We identify a setting where the Pearson correlation and Spearman correlation differ considerably: X * 1 = Z, X * 2 = Z 3 + where Z, are independent, and both are standard normal. We simulated a sample from (X * 1 , X * 2 ) of size n = 10 7 . Since this is a large sample, the estimates based on this sample are close to population quantities. We calculated</p><formula xml:id="formula_43">(U 1 , U 2 ) = (F 1 (X * 1 ), F 2 (X * 2 ))</formula><p>where F 1 , F 2 are the CDFs of X * 1 , X * 2 respectively. The correlation of (U 1 , U 2 ) is the Spearman correlation of (X * 1 , X * 2 ) and equals 0.714. We then discretized (U 1 , U 2 ) with 60 evenly spaced thresholds on the interval [0.001, 0.999] and calculated the standard polychoric correlation, assuming normal marginals, which equaled 0.804. This polychoric correlation does not estimate Cor(U 1 , U 2 ) nor Cor(X * 1 , X * 2 ) but instead it estimates Cor(Φ -1 (U 1 ), Φ -1 (U 2 )) which we calculated as equal to 0.807. We then calculated the polychoric correlation using standardized uniform marginals, which equaled 0.714, which consistently approximates Cor(U 1 , U 2 ). Finally, we calculated the Pearson correlation directly on the integer-encoded data, which according to theory is consistent. Indeed, we get that this estimate equals 0.713, close to the true 0.714. Note that Cor(X * 1 , X * 2 ), which equaled 0.750, is not estimated by any of the procedures we considered.</p><p>Finally we mention that in the supplementary material we also provide R code that illustrates the general consistency of the Spearman correlation. For both the evenly and nonevenly spaced threshold configurations, Spearman's correlation based on the ordinal observations is demonstrated to be close to the true population correlation of X * .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Each of the studies presented here is based on the same SEM model and the same underlying multivariate continuous distributions. The threshold sets are of three types, according to whether they produce distributions with floor effects, ceiling effects or with symmetry. In each type, fifty threshold sets were generated. The large amount of threshold sets guarantees that our findings are not overly dependent upon specific ordinal distributions, and therefore allows us to interpret results as smoothed across fifty threholds all sharing the same basic type, e.g., across fifty distributions all having floor effects.</p><p>Study 1 is conducted at the population level, where we discretize the three distributions using a large number of threshold sets. The outcome variables are the relative bias of Pearson, Pearson adjusted, and polychoric correlations and cont-ML, cont-ML-adj, and cat-LS SEM model estimates across threshold sets and underlying distributions.</p><p>Study 2 is a simulation study where we draw and discretize medium-sized samples from the three underlying distributions, using the same threshold sets as in Study 1. The outcome variables are CI coverage rates for model parameters and rejection rates for a test of correct model specification.</p><p>Study 3 evaluates the performance of the bootstrap test at small to medium sample sizes, and is limited to nine threshold sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Population model</head><p>We follow <ref type="bibr" target="#b38">Li (2016b)</ref>, who conducted a thorough analysis of 36 empirical studies that used ordinal structural equation modeling, and found that the median number of factors was five and that the median number of variables was 18. Li (2016b) therefore emulated a realistic empirical setting by constructing a five-factor structural equation model. This model was deemed representative of medium-sized SEM model specifications encountered in applications, and we present its structural part in Figure <ref type="figure" target="#fig_9">7</ref>. Not depicted in the path diagram are the observed ordinal variables, four for each factor, so that the simulated data had p = 20 dimensions. The factor loadings for the continuous X * variables were set to 0.8, 0.7, 0.6 and 0.5 within each factor, and the corresponding residual variances were chosen so that each X * k , k = 1, . . . , 20, had unit variance. The interfactor correlation was φ = 0.3 for the two exogeneous factors, each of which had unit variance. The residuals ζ 1 , ζ 2 and ζ 3 had variances so that the path values in Figure <ref type="figure" target="#fig_9">7</ref> were standardized regression coefficients. The model-implied correlation matrix contains mostly moderate correlations, see Table <ref type="table">2</ref>.  Distributions for the underlying continuous vector</p><p>We considered three types of underlying continuous distributions: the multivariate normal (N) and two non-normal VITA distributions. In the first VITA distribution (C) we used Clayton bivariate copulas to construct the regular vine, while in the second distribution (J) we used Joe bivariate copulas. The tree structure was identical in both VITA distributions. The N, C, and J distributions were constructed in a way to have the correlation structure given in Table <ref type="table">2</ref>. That is, the SEM model is correctly specified with respect to all three distributions. Also, each of the N, C, and J distributions has standard normal univariate marginal distributions. Hence, it is the multivariate dependence structures inherent in the distributions that differ from one another.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sample size</head><p>In Study 1 we generated three large (n = 10 5 ) samples from distributions N, J, and C. The large sample size reduces sampling fluctuations so that this study is conducted close to the population level. Understanding parameter bias at the population level is a prerequisite before investigating sampling properties. Given that for fixed K, cont-ML and cont-ML-adj are generally not consistent, and cat-LS is not consistent under the VITA distributions C and J, populationlevel parameter bias does not disappear with increasing sample size. Using a large dataset allows us to calculate the bias with virtually no confounding due to sampling fluctuation.</p><p>In Study 2 we included medium to large sample sizes of n = 500 and n = 1000, while in Study 3 a range from small to medium sample sizes, n = 100, 300, 500, was employed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Distributions of the observed variables</head><p>Li (2016b) reported that in empirical research the most common number of levels in ordinal data are 4, 5 and 7, and we included these levels in our study. For each K, we considered three general types of distributions for the ordinal marginal distributions in X, which we refer to as Symmetrical, Floor, and Ceiling. Symmetrical implies that the probability distribution of X is symmetric with highest values in the middle of the range 1, . . . , K. Floor refers to a floor effect, that is, the highest probability occurs for the lowest levels of X and decreases with increasing levels. Ceiling refers to a ceiling effect, that is, the lowest probability occurs for the lowest level of X and increases with increasing levels. We note that distributions with floor or ceiling effects are routinely encountered in the social sciences.</p><p>A single univariate ordinal distribution is defined by a set of thresholds. For each of the twenty variables, we employed a different set of thresholds, so that the observed ordinal vector had marginals with different distributions, as we believe this is representative of real-world datasets. We refer to the collection of twenty different threshold sets as a threshold configuration. To avoid our results being contingent upon a small number of specific threshold values, we randomly generated fifty threshold configurations in each class of symmetrical, floor, and ceiling distributional forms. Our results concerning these three distributional forms may therefore be seen as smoothed over fifty specific distributions belonging to each form. The generation of threshold configurations using R is available in the supplementary material. In Figure <ref type="figure" target="#fig_11">8</ref> we have plotted, for K = 4, the ordinal distributions associated with three threshold configurations. The upper, middle, and lower panels in Figure <ref type="figure" target="#fig_11">8</ref> refer to ceiling, floor, and symmetrical configuration, respectively. The columns in the figure correspond to the twenty ordinal variables.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 2</head><p>Correlation matrix Σ implied by the population model. In Studies 1 and 2, datasets drawn from continuous distributions N, C, and J were each discretized using a total of 150 threshold configurations: 50 of each of type Floor, Symmetrical, and Ceiling. In Study 3, due to the considerable computational burden of the bootstrap, the number of threshold configurations was restricted to three for each of the types Floor, Symmetrical, and Ceiling. Also, to reduce computation time, we excluded the condition K = 5 from Study 3.</p><formula xml:id="formula_44">X * 1 X * 2 X * 3 X * 4 X * 5 X * 6 X * 7 X * 8 Y * 1 Y * 2 Y * 3 Y * 4 Y * 5 Y * 6 Y * 7 Y * 8 Y * 9 Y * 10 Y * 11 Y * 12 X * 1 1.00 X * 2 0.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data generation and model estimation</head><p>In each of the finite sample conditions in Studies 2 and 3 we generated 1000 datasets. In Study 3, we used 1000 bootstrap replications for each sample. The simulations in Studies 2 and 3 were performed on resources provided by UNINETT Sigma2 -the National Infrastructure for High Performance Computing and Data Storage in Norway. The samples were generated in the R computing environment (R Core Team, 2020), with the help of the VineCopula package <ref type="bibr">(Schepsmeier et al., 2018)</ref>. cont-ML and cont-ML-adj were computed based on the covariance matrix using fully standardized solutions. Polychoric correlation estimation was performed with the sirt package <ref type="bibr" target="#b61">(Robitzsch, 2019)</ref>. Model estimation and testing was achieved with lavaan <ref type="bibr" target="#b62">(Rosseel, 2012)</ref>, as detailed in the supplementary material. The models were identified by setting the variance of the latent variables to unity. The model fit statistic for all three estimators was the scaled-and-shifted statistic of <ref type="bibr" target="#b1">Asparouhov and Muthén (2010)</ref>. The bootstrap test of underlying normality was implemented using the R package discnorm <ref type="bibr" target="#b15">(Foldnes &amp; Grønneberg, 2020b)</ref>. Computer code is provided in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation criteria</head><p>In Studies 1 and 2 the outcome variable for a statistic θ was relative bias (RB), defined as the mean bias across all replications in the design condition: RB = 100 • ( θθ 0 )/θ 0 , where θ 0 denotes the true population value of the parameter. Note that while in Study 2 the replications are random samples from multivariate distributions, the replications in Study 1 consists of 50 configuration sets. To avoid canceling of negative and positive bias within a condition, the mean absolute relative bias (MARB) was also employed. CI coverage is defined as the proportion of times the 95% CI contained the true population value. For the statistical tests in Study 2 and Study 3 the evaluation criterion was the rejection rate of the test at the 5% level of significance.</p><p>Performance criteria for estimation and inference outcomes are as follows. We deem acceptable relative bias smaller than 5%, 95 % CI coverage rates between 92% and 98%, and rejection rates between 2.5% and 7.5% <ref type="bibr" target="#b7">(Bradley, 1978)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Study 1: Bias in parameter estimates at the population level</head><p>Relative bias at the population level was calculated for the 190 correlations. Our design crosses three underlying distributions with three levels of K and three ordinal distributional forms, yielding 27 conditions. In each of these conditions, MARB was calculated from 190 relative bias values across 50 threshold configurations, yielding Table <ref type="table" target="#tab_2">3</ref>.</p><p>Compared to the standard Pearson correlation, the adjusted Pearson correlation managed to reduce bias in 25 of a total of 27 conditions. The overall mean MARB reduction of the adjusted Pearson correlation relative to the original  Pearson correlation was 14%. The improvement offered by the adjusted Pearson correlation was most notable for K = 7:</p><p>The overall percentage improvement for K = 4, 5, and 7 was, respectively, 0%, 12%, and 31%. Given that the adjustment to the Pearson correlation was found overall to reduce bias relative to the original Pearson correlation, we do not comment further on the latter.</p><p>The adjusted Pearson and the polychoric correlation achieved acceptable bias in 6 and 14 of the 27 conditions, respectively. The overall MARB for the adjusted Pearson and the polychoric correlation was 14.1% and 7.8%, respectively. Overall, the polychoric correlation was therefore less biased at the population level than the adjusted Pearson correlation. However, in conditions with underlying non-normality and asymmetrical ordinal distributions, the polychoric correlation did not always outperform the adjusted correlation. Specifically, for Joe VITA discretized into floor distributions, and for Clayton VITA discretized into ceiling distributions, the adjusted Pearson correlation was less biased than the polychoric correlation.</p><p>Let us now turn to parameter bias in the population for the SEM model depicted in Figure <ref type="figure" target="#fig_9">7</ref>, which we first illustrate for two parameters in the model. Similar analyses for all model parameters are found in the online supplementary material. Figure <ref type="figure" target="#fig_12">9</ref> contains boxplots for bias in the structural regression coefficient γ 21 , relating ξ 1 to η 2 . With underlying normality, cont-ML relative bias was acceptable in 61% of the 450 (three levels of K, three distributional forms and 50 threshold configurations) ordinal distributions. For cont-ML-adj and cat-LS the corresponding rate of acceptable bias were 63% and 99%, respectively. Under the Joe VITA the acceptance rates were 55%, 65%, and 60% for cont-ML, cont-ML-adj, and cat-LS, respectively, while the respective per-centages for the Clayton VITA were 60%, 67%, and 71%. Under non-normality, the distributional shape of the ordinal data greatly affects the estimators. Categorization of the Joe VITA into ceiling effects produces severe negative bias in all three estimators. The Joe VITA discretized into floor effect distributions on the other hand does not produce bias in the majority of threshold configurations. For the Clayton VITA; it is seen that floor effect distributions lead to pronounced negative bias for all three estimators. There is also considerable bias variation within each distributional form, especially at K = 4. For instance, for the 50 K = 4 threshold configurations with floor effects, cat-LS relative bias ranged from +10.4% to -7.4%. Variation across threshold configurations decreased with increasing K, with the mean range in cont-ML relative bias decreasing from 35% to 27% for K = 4 to K = 7, respectively. For cont-ML-adj and cat-LS, relative bias range for K = 4 were 31% and 33%, respectively, decreasing to 17% and 16% for K = 7, respectively. Also, as K increases, relative bias tends to decrease. Overall, for K = 4 and K = 7, MARB values for cont-ML, cont-ML-adj and cat-LS were (6.3%, 4.6%), (5.8%, 3.3%), and (4.6%, 2.8%), respectively.</p><p>Figure <ref type="figure" target="#fig_13">10</ref> presents relative bias for the factor loading λ 11 , relating X * 1 to ξ 1 . As expected, cat-LS under the ideal condition of multivariate normality is unbiased. However, in striking contrast to the above findings for structural coefficient γ 21 , cont-ML and cont-ML-adj are biased under normality, even with symmetrical ordinal distributions. The bias in λ 11 was acceptable in only 43% of the 450 discretizations obtained by aggregating over K and ordinal distributions. In comparison, for cont-ML-adj and cat-LS the percentage of acceptable relative bias was 46% and 100%, respectively. In other words, under normality only cat-LS was unbiased. q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q cont-ML cont-ML-adj cat-LS With underlying non-normality all three estimators exhibited bias. Under the Joe VITA, cont-ML, cont-ML-adj and cat-LS bias was acceptable in 54%, 63%, and 66%, respectively, of the 450 threshold configurations. For the Clayton VITA, the corresponding acceptance rates were 55%, 59%, and 68%. We note that there is one specific symmetrical configuration for K = 5 where the bias is about -34% for cont-ML and cont-ML-adj. Similar to findings for γ 21 , we observed that the extent and direction of bias was dictated by the interaction of underlying distribution and ordinal distributional form. cat-LS with underlying Joe VITA distributions is positively biased, often above +10%, when the ordinal variables exhibit floor effects. In the same condition, bias become negative for ceiling effects. For underlying Clayton VITA, the situation is reversed, and ordinal distributions with ceiling effects yield positive bias, while ordinal distributions with floor effects is associated with negative bias. Overall, bias is reduced with increasing number of categories.</p><p>We now summarize bias across groups of model parameters. It is natural to assemble the SEM parameters into four groups: The first group consists of the 8 factor loadings for the exogenous variables ξ 1 and ξ 2 , and the second group consists of the twelve loadings for the endogenous variables η 1 , η 2 , and η 3 . The third group consists of the six structural coefficients γ i j relating the η-variables with the ξ-variables. Finally, the fourth group consists of the three structural coefficients relating the η variables. We refer to these four groups respectively as λ ex , λ en , γ, and β. We next report the mean absolute relative bias calculated across parameters in each of the four parameter groups. First, let us investigate the effect of K, the number of categories. Table <ref type="table" target="#tab_3">4</ref> shows that estimation bias consistently decreases with increasing number q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q cont-ML cont-ML-adj cat-LS Relative bias for factor loading associated with X * 1 and ξ 1 . Each boxplot is based on 50 threshold configurations. Floor, Ceiling, and Symmetrical refer to ordinal distributions with floor effect, ceiling effect, and symmetry, respectively. cont-ML, cont-ML-adj, and cat-LS refer to estimation based on Pearson, Pearson adjusted, and polychoric correlations, respectively. K is the number of categories. N, J, and C refer to normal, Joe VITA, and Clayton VITA underlying distributions.</p><p>of categories. This holds across model estimation methods and parameter groups. Also, it is seen that across all conditions, cat-LS (MARB 3.1% ) is less biased than cont-ML-adj (MARB 4.7% ), which in turn is less biased than cont-ML (MARB 5.5%). In none of the 12 conditions (4 parameter groups vs. 3 levels of K) did cont-ML or cont-ML-adj outperform cat-LS. However, in all nine conditions cont-MLadj outperformed cont-ML, so the proposed adjustment to Pearson correlation represents an improvement, although not enough to outperform cat-LS.</p><p>Further study of Table <ref type="table" target="#tab_3">4</ref> reveals that the distribution of bias among the parameter groups differs across estimation methods. For cont-ML and cont-ML-adj the most biased parameters are the factor loadings for exogenous variables, while for cat-LS the most biased parameters are the struc-tural regression coefficients from the exogenous to the endogenous latent variables. Nevertheless, in this group of parameters cat-LS still has less bias than cont-ML and cont-ML-adj.</p><p>We next investigate another characteristic that determines ordinal distributions, namely the distributional shapes, and how they interact with the underlying continuous distribution, see Table <ref type="table" target="#tab_4">5</ref>. The underlying continuous distribution affects bias for all three estimation methods. Inspection of the last column in Table <ref type="table" target="#tab_4">5</ref> reveals that cont-ML and cont-ML-adj are particularly biased when the underlying Joe VITA distribution is discretized so as to produce ceiling effects. With this combination, bias is above 13% for both estimators for parameter groups λ en and γ, that is, for factor loadings and structural paths emanating from the endogenous latent vari-ables. The other condition where cont-ML and cont-MLadj are particularly biased is when the Clayton VITA is discretized to produce floor effects, with bias at or well above 10% for groups λ en and γ.</p><p>In contrast to cont-ML and cont-ML-adj, cat-LS expectedly exhibits negligible bias with underlying normality. Even with underlying non-normality, cat-LS bias is still small, provided the ordinal distribution is symmetrical. Floor and ceiling effects in the ordinal distributions inflate cat-LS bias under non-normality to around 5%-6%. In particular, there are two such conditions (of a total of nine conditions obtained from crossing the three underlying and the three ordinal distributions) where cont-ML-adj is less biased than cat-LS. In the first condition the Joe VITA is discretized to produce floor effects (cat-LS: 5.9% vs. cont-ML-adj: 2.1%), and the second condition is discretizing the Clayton VITA to produce ceiling effects (cat-LS: 5.1% vs. cont-ML-adj: 1.8%). Detailed inspection of Table <ref type="table" target="#tab_4">5</ref> reveals that in these two conditions bias in cat-LS is located mainly in the factor loadings. Given that structural parameters, being estimates of association among latent constructs, in many cases are of more substantive interest than factor loadings, we calculated bias for the combined (γ and β) group of structural coefficients. The results indicate that in the two conditions where cat-LS is more biased than cont-ML-adj, the structural biases were comparable. More precisely, underlying Joe VITA combined with floor effects yielded a combined structural parameter bias of 3.2% and 3.1% for cat-LS and cont-ML-adj, respectively. Similarly, for underlying Clayton VITA combined with ceiling effects, combined structural parameter bias was 2.6% and 2.9% for cat-LS and cont-MLadj, respectively. Therefore, in terms of structural parameter bias, cont-ML-adj did not clearly outperform cat-LS in any condition. On the other hand, for factor loading bias cont-ML-adj outperformed cat-LS in 2 of 9 conditions.</p><p>To sum up, at the population level we found that cat-LS overall was less biased than cont-ML-adj, which again was less biased than cont-ML. Hence it is worthwhile to replace the Pearson correlations with adjusted Pearson correlations before doing the SEM analysis based on Pearson correlations, although this methodology was found to perform considerably worse than cat-LS. We found a strong interaction between the underlying continuous distribution and the ordinal observed distribution on parameter bias. Especially two conditions, namely Joe VITA combined with floor effects, and Clayton VITA combined with ceiling effects, produced poor performance of cat-LS relative to cont-ML-adj. In these conditions the bias was mostly related to factor loadings, and structural parameter bias in cat-LS was comparable to cont-ML-adj. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Study 2: CI coverage and model fit rejection rates</head><p>Across all conditions, cont-ML, cont-ML-adj and cat-LS always produced properly converged solutions in model estimation. Reported here are results for sample size n = 500. Findings for n = 1000 were not qualitatively different, and are presented in the online supplementary material.</p><p>Coverage rates at the 95% confidence level for the regression coefficient γ 21 and for the factor loading coefficient λ 11 are presented in Figures <ref type="figure" target="#fig_18">11</ref> and<ref type="figure" target="#fig_15">12</ref>. With underlying normality, coverage rates for the structural parameter γ 11 are deemed acceptable for all estimators. With underlying Joe or Clayton VITA, performance deteriorates, especially for cat-LS. Overall, for the Joe VITA, cont-ML, cont-MLadj and cat-LS have acceptable coverage in 96.5%, 96.7%, and 51.1% of the conditions, respectively. Similarly, for the Clayton VITA, acceptable coverage was achieved in 100%, 100%, and 75.1% of the conditions. Therefore, with underlying non-normality, cat-LS coverage for this structural parameter was far less acceptable than the coverage attained by cont-ML and cont-ML-adj. Let us next investigate coverage for the factor loading coefficient λ 11 which relates X * 1 with ξ 1 , see Figure <ref type="figure" target="#fig_15">12</ref>. Under normality cat-LS performs well, while cont-ML and cont-ML-adj perform poorly. In fact, cont-ML and cont-ML-adj had acceptable coverage in 0.7% and in 1.1% of the conditions with underlying normality, respectively, while cat-LS had acceptable coverage in 100% of the conditions. With underlying non-normality, performance deteriorates markedly for all three estimators. Overall, for the Joe VITA, cont-ML, cont-ML-adj, and cat-LS have acceptable coverage in 15.1%, 30.9%, and 9.3% of the conditions, respectively. With the Clayton VITA, acceptable coverage was achieved in 17.8%, 38.7%, and 25.1% of the conditions. Therefore, cat-LS did not perform as well as cont-ML and cont-ML-adj with underlying non-normality. The results are highly dependent upon the specific thresholds. Consider, for instance, cont-ML with five categories and underlying Joe VITA distribution. Across the fifty ceiling distributions coverage then ranges from 0% to 83.6%, with median coverage 36.8%.</p><p>There is a notable difference between the coverage of γ 21 and λ 11 . The former parameter is well estimated, while the latter has very poor coverage for all estimators in the majority of conditions. It is only cat-LS with underlying normality that yields valid inference for the factor loading λ 11 . To sys- Note. cont-ML, cont-ML-adj, and cat-LS refer to model estimator. λ ex , λ en , γ, and β refer to groups of model parameters, containing, respectively, factor loadings for exogenous and endogenous latent variables, regression coefficients among exogenous and endogenous latent variables, and among endogenous latent variables. N, J, and C refer to underlying distributions of type normal, Joe VITA, and Clayton VITA, respectively.</p><p>tematically inquire into the differential coverage among different types of model parameters, aggregated coverage rates for parameter groups λ ex , λ en , γ, and β, are presented in Table 6. For the factor loading parameters in λ ex and λ en , coverage is generally lower than coverage associated with the structural parameters γ and β. It is especially under cont-ML and cont-ML-adj that factor loadings have unacceptably low coverage rates. Coverage is slightly, but consistently, higher for the loadings of exogenous factors ξ 1 and ξ 2 , compared to coverage associated with endogenous factors η 1 , η 2 , and η 3 . Coverage for structural parameters is acceptable for all estimation methods. Overall, we see that coverage improves with the number of categories. The overall coverage rate is highest for cat-LS and lowest for cont-ML.</p><p>To further inquire into the effect of observed ordinal distributions, we next look into how the observed distributional forms interact with the type of underlying distribution, see Table <ref type="table" target="#tab_6">7</ref>. cat-LS achieves acceptable coverage under multi-variate normality regardless of ordinal distributional shape, and under Clayton VITA with symmetrical ordinal distributions. cont-ML does not achieve acceptable coverage in any of the nine conditions. cont-ML-adj has acceptable coverage when the Joe and Clayton VITA distributions are discretized into ordinal data with floor and ceiling effects, respectively. With underlying normality, cont-ML and cont-ML-adj coverage is only slightly better under symmetrical ordinal distributions compared to ordinal distributions with floor or ceiling effects. cat-LS is expectedly performing very well in terms of coverage under multivariate normality, for all distributional forms. With underlying non-normal distributions, all three estimators have coverage that is dependent upon the ordinal distributional form. For cont-ML and cont-ML-adj, coverage is particularly poor when the Joe VITA is discretized into ordinal data with ceiling effects, and when the Clayton VITA is discretized into data with floor effects. For cat-LS poorest coverage appears when the Joe and Clayton VITA distributions are discretized into floor and ceiling effects, respectively. To sum up, in those conditions where cont-ML and cont-ML-adj coverage is poorest, cat-LS offers improved coverage, and vice versa, in poor coverage conditions for cat-LS, cont-ML, and cont-ML-adj offer improved coverage. The scaled-and-shifted test of correct model specification was evaluated by calculating rejection rates at the 5% level of significance. In Figure <ref type="figure" target="#fig_16">13</ref> are depicted boxplots of rejection rates. Each of the 81 boxplots represent 50 threshold configurations in which rejection rate was calculated. cont-ML-adj attains acceptable performance in all 4050 conditions, while cont-ML has acceptable performance in 99.3%, Note. cont-ML, cont-ML-adj, and cat-LS refer to model estimator. λ ex , λ en , γ, and β refer to groups of model parameters, containing, respectively, factor loadings for exogenous and endogenous latent variables, regression coefficients among exogenous and endogenous latent variables, and among endogenous latent variables. N, J, and C refer to the underlying continuous distribution being normal, Joe VITA, and Clayton VITA, respectively. Floor, Ceiling, and Symmetrical refer to the ordinal distributional shape.</p><p>98% and 100% of conditions, when underlying distribution is normal, Joe VITA, and Clayton VITA, respectively. For cat-LS, 100% of the conditions involving underlying normality had acceptable rejection rates. However, under nonnormality cat-LS performs poorly in terms of Type I error control: Only 3.8% and 12% of conditions involving Joe and Clayton underlying VITA distributions, respectively, produced acceptable rejection rates. In these conditions it is seen that the type of underlying distribution interacts with ordinal observed distributional form when it comes to cat-LS rejection rate. Also, with an increasing number of categories, it is more likely to reject the null hypothesis of correct model specification.</p><p>It is surprising that cont-ML and cont-ML-adj model fit testing achieved acceptable Type I error control across a large majority of conditions, and utmost caution must be exercised q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q cont-ML cont-ML-adj cat-LS when interpreting these rejection rates, as cont-ML, cont-ML-adj and cat-LS are all inconsistent, with the exception of cat-LS under normality. Study 1 demonstrated that the Pearson (adjusted) correlations are biased (see Table <ref type="table" target="#tab_2">3</ref>) in the population, so that the SEM model would seem misspecified at the population level. Study 1 indeed showed that model parameters obtained with cont-ML and cont-ML-adj were biased (Tables <ref type="table" target="#tab_3">4</ref> and<ref type="table" target="#tab_4">5</ref>). However, the specific SEM model utilized in this study encompasses a large set of correlation structures, including those calculated in cont-ML and cont-ML-adj estimation. In the upcoming discussion section we discuss this topic further, and demonstrate that imposing a few correctly specified constraints in the original SEM model produces unacceptable Type I error control for the cont-ML and cont-ML-adj model test.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Study 3: The bootstrap test for underlying non-normality</head><p>The bootstrap test for underlying normality was evaluated using nine of the 150 threshold configurations employed in Studies 1 and 2. Type I error control (underlying distribution N) was acceptable in 17%, 78%, and 89% of the conditions for sample sizes n = 100, 300, and 500, respectively. At sample size n = 500, Type I error control was acceptable in all conditions, except under two of the three symmetrical configurations.</p><p>In Table <ref type="table" target="#tab_7">8</ref> are given the rejection rates at the 5% level of significance. Within each distributional form (floor effects, ceiling effects and symmetrical) there was little variation in q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q cont-ML cont-ML-adj cat-LS Each boxplot is based on 50 threshold configurations. Floor, Ceiling and Symmetrical refer to ordinal distributions with floor effect, ceiling effect and symmetry, respectively. cont-ML, cont-ML-adj and cat-LS refer to estimation based on Pearson, adjusted Pearson and polychoric correlations, respectively K is the number of categories. N, J, and C refer to normal, Joe VITA, and Clayton VITA underlying distributions. rejection rates across the three threshold configurations, so we decided to collapse results across these configurations. The full table with rejection rates for each threshold configuration is presented in the supplementary material.</p><p>The power of the bootstrap test (underlying distribution J and C) is generally high. For sample sizes n = 300 and n = 500 the bootstrap test always rejected the null hypothesis of underlying normality, for both types of underlying normality, both levels of K, and all ordinal distributional forms. At the smallest sample size, n = 100, violation of the normality assumption was not always detected, especially under the distribution C. However, the bootstrap test exhibited power in the ranges 0.6 -0.8 and 0.7 -0.9 for K = 4 and K = 7, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion</head><p>In the present article we carefully analyzed the discretization model which is frequently used to analyze ordinal data in SEM. Identification issues were discussed, and it was argued that to investigate the effect of underlying non-normality, a new simulation method based on VITA distributions is wellsuited. Also, we presented theoretical results on the consistency of three kinds of correlation matrices as the number of categories increases. First, we proposed a new encoding of ordinal data that ensures that the Pearson correlation matrix of the adjusted data will approach the true underlying correlation matrix as K increases. Second, we showed that this convergence also holds for the polychoric correlation matrix, provided correct marginal distributions are specified for the underlying vector. Thirdly, consistency of the Spearman cor-q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q cont-ML cont-ML-adj cat-LS Next, we investigated in the context of a specific mediumsized SEM model, the extent to which underlying nonnormality and variation in ordinal distributional forms affects ordinal SEM estimation and inference. We also evaluated for the first time the performance of a bootstrap test for underlying non-normality in the context of a medium-sized SEM model.</p><p>Our first research question asked whether variation in ordinal distributional forms affects ordinal SEM estimation and inference. Study 1 showed that population-level correlations and SEM model parameters for all three estimators to a large degree depended on the specific distributional form.</p><p>We found systematic variation in estimates as a function of whether the ordinal variables had floor effects, were symmetrical, or had ceiling effects. There was also significant variation within these classes of distributional forms. That is, discretizing a continuous vector in two ways, both of which produce floor effects in all twenty variables, may lead to very different model parameter estimates.</p><p>Our second research question asked whether cont-ML, cont-ML-adj, and cat-LS are robust to underlying nonnormality. We found that all three estimators are substantially biased when the underlying normality assumption is violated. In the present study cat-LS with underlying nonnormality performed much worse than previously reported (e.g., <ref type="bibr" target="#b13">Flora &amp; Curran, 2004;</ref><ref type="bibr">Li, 2016a;</ref><ref type="bibr">Rhemtulla et al., 2012)</ref>. The sensitivity of cat-LS to underlying non-normality should be taken into account by researchers. For instance, with underlying non-normality model fit assessment in terms of the chi-square statistic will tend to indicate that the model has poorer fit than is really the case. We also found that for all three estimators the effect of underlying non-normality was strongly moderated by the distributional form of the ordinal variables. For instance, in Figure <ref type="figure" target="#fig_12">9</ref>, cont-ML mean relative bias for the structural coefficient γ 21 across 50 ordinal ceiling distributions with K = 4 was -20% under the Joe VITA. The same ceiling ordinal distributions produced a mean relative bias of only -3% under the Clayton VITA. For the 50 ordinal K = 4 distributions with floor effects, the situation is reversed, with a mean relative bias of -1% for the Joe VITA, compared to -11% for the Clayton VITA. This pattern may allow us to speculate that upper (lower) tail dependency combined with ordinal ceiling (floor) effects makes it especially hard to recover the true underlying correlation.</p><p>The third topic on our research agenda was the theoretical and empirical assessment of the new cont-ML-adj estimator, which may be seen as a hybrid of cont-ML and cat-LS, since it incorporates the discretization model but still treats data as continuous in the sense that Pearson correlations are used.</p><p>In contrast to cont-ML, we proved that cont-ML-adj is consistent as K increases. Across all conditions in our simulation design cont-ML-adj outperformed cont-ML on all evaluation criteria. However, cat-LS outperformed cont-ML-adj in most conditions. Interestingly, the structural regression coefficients did not suffer from poor CI coverage in any of the three estimators, while the factor loading parameters had unacceptable coverage, especially under cont-ML and cont-ML-adj. Similar results were also reported in <ref type="bibr">Li (2016a)</ref> and <ref type="bibr" target="#b38">Li (2016b)</ref>. The acceptable performance of structural parameter coverage may well be, however, a by-product of the specified model in this and related studies.</p><p>We urge caution on this point because, as seen in Study 1, the parameter estimates are biased for all three estimators under non-normality. We have observed in numerical experiments that the bias is concentrated mainly in either the factor loadings or in the regression coefficients, depending upon how we identify the SEM. If the model is identified by setting unit variance for the latent variables, which we do in the present study, we impose a correct assumption on the latent variables, which reduces bias in the structural part of the model. However, when identifying the model by fixing one factor loading per latent variable to its true value, bias is reduced in the factor loadings, but increases in the structural part of the model. This means that the high quality of structural coefficient inference observed in Study 2 may be of limited external validity.</p><p>Similarly, caution is also warranted when interpreting the even more striking Type I error control of the test of model fit demonstrated by cont-ML and cont-ML-adj in Study 2. We know from Study 1 that these estimators are biased at the population level, so we would not in general expect the model fit test to perform well. We discuss this issue further at the end of this section.</p><p>We have seen that cont-ML, cont-ML-adj, and cat-LS need not give valid results when underlying normality is violated. It is therefore important to evaluate methods of detecting underlying non-normality. To answer our fourth research question, we evaluated in Study 3 the performance of the newly proposed bootstrap test <ref type="bibr" target="#b18">(Foldnes &amp; Grønneberg, 2019b)</ref> of underlying normality in ordinal data. The test was found to adequately maintain Type I error control for sample sizes of n = 300 or more. Also, the bootstrap test exhibited high power to detect underlying non-normality.</p><p>We next discuss some limitations in the present study, which may lead to follow-up studies extending the present work. Our results were obtained in the context of a single medium-sized SEM whose population values were chosen to be representative of real-world applications <ref type="bibr" target="#b38">(Li, 2016b)</ref>. The correlations implied by this model are moderate, with a maximum value of 0.56. We conjecture that bias becomes even more pronounced under stronger correlations up to a certain point, as observed by <ref type="bibr" target="#b18">Foldnes and Grønneberg (2019b)</ref>. Robustness of ordinal SEM to underlying nonnormality, and its dependence on threshold configurations in other application settings (e.g., multilevel or multi-group analyses) should be studied in future work. Our implementation of cat-LS was based on DWLS estimation. An alternative would be unweighted least squares (ULS) estimation. We believe however that our results will replicate under ULS estimation, given that ULS and DWLS have been reported to yield similar results <ref type="bibr" target="#b20">(Forero, Maydeu-Olivares, &amp; Gallardo-Pujol, 2009;</ref><ref type="bibr" target="#b38">Li, 2016b)</ref>. As for the proposed new cont-MLadj estimator, we considered only one simple way of obtaining adjusted values. There may be additional improvements to be gained from identifying better placements of the numerical values of the ordinal observations. The external validity of our simulation study is also limited by our choice of underlying and ordinal observed distributional forms. First, cat-LS assumes that the underlying variable is normal, and while we have studied its performance under non-normality, we have kept the underlying marginals exactly normal. This means that we have studied a situation where the marginals are correctly specified. Second, we have studied only two non-normal multivariate distributions. Using the VITA simulation technique of <ref type="bibr">Grønneberg and Foldnes (2017)</ref> and implemented in the Rpackage covsim <ref type="bibr">(Foldnes &amp; Grønneberg, 2020)</ref>, one may easily extend our study to a much broader range of dependencies. Third, although we considered many threshold configurations, the ordinal variables in each condition all belonged to the same type (ceiling, floor or symmetrical) of distributions. In real-world settings some observed variables may have symmetrical distributions, some may have ceiling effects, and others may have floor effects. An extension of our work would therefore be to discretize the continuous vector X * into variables whose distributions are mixed among ceiling, floor, and symmetrical observed distributions. Fourth, we have simulated data only with a correctly specified underlying moment model. Therefore, our discussion of the performance of model fit tests only concerns Type I error control.</p><p>Our simulation study was based on the VITA simulation methodology of <ref type="bibr">Grønneberg and Foldnes (2017)</ref>. The two advantages of using VITA to simulate X * in contrast to using the VM method are: First, VITA will in most cases not be discretized equivalent to an exactly multivariate normal random vector. Second, we are able to fix marginals and control other aspects of the distribution of X * including Cov(X * ), which means that we can isolate the effect of the marginal distribution of X as explained above. The VITA method is still just one possible member of the space of distributions that when discretized into an ordinal vector X has exactly the same distribution as X. Unless a partial identification analysis is conducted, as discussed in the online supplementary material (p. 1), simulation studies have to be interpreted with caution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Interpreting cont-ML and cont-ML-adj model fit rejection rates</head><p>In Study 2 (see Figure <ref type="figure" target="#fig_16">13</ref>) we reported that cont-ML and cont-ML-adj maintained Type I error rates surprisingly well for the the test of correct model specification. When dealing with consistent estimation methodology, observing a rejection rate which is close to nominal in a simulation study is in favor of the estimation methodology. However, Study 1 demonstrated that cont-ML and cont-ML-adj estimates are biased in the population (see Table <ref type="table" target="#tab_2">3</ref>). This should be kept in mind when interpreting the rejection rates in Study 2. The model fit test assesses whether the population covariance matrix of X is expressible by the considered covariance model: Σ(θ 0 ) = Cov(X) for some vector θ 0 of model parameters. The alternative hypothesis is that no such θ 0 exists. The model fit test is consistent, in other words, the rejection rate will converge to 100% under the alternative hypothesis. Our simulations therefore indicate that the null hypothesis is true when using cont-ML and cont-ML-adj. The null hypothesis only specifies that the covariance model is fulfilled, and it may therefore be true also when the population parameters of the underlying model for X * are not reached. Our design ensures that the model holds for X * , and there is therefore a θ</p><formula xml:id="formula_45">• such that Σ(θ • ) = Cov(X * ). From Study 1 we know that θ • θ 0 .</formula><p>This means that the covariance of the ordinal observations X are compatible with the covariance model θ → Σ(θ). The specific SEM model utilized in this study encompasses a large set of correlation structures, coincidentally also including those calculated in cont-ML and cont-MLadj estimation. The almost nominal rejection rates associated with cont-ML and cont-ML-adj therefore occur as a byproduct of the model we set up in Figure <ref type="figure" target="#fig_9">7</ref>, and should count in favor of cont-ML and cont-ML-adj only with these important caveats in mind. Why then, do not cat-LS rejection rates approximate the nominal 5% level under non-normality? It so happens that the parameters towards which cat-LS converges are not included in the covariance model θ → Σ(θ). If cat-LS estimates were inconsistent under non-normality yet converged to a parameter configuration compatible with the covariance model, this would not by itself be in favor of cat-LS. This argument is not a defense of the high rejection rate of the goodness of fit test with cat-LS. Instead, it is a cautionary note on interpreting rejection rates for the goodness of fit test with inconsistent estimators in general.</p><p>We confirm this observation by restricting the covariance model through imposing four correctly specified constraints on our model. We impose equality constraints on all five factor loadings that take the value 0.7 in the population. We simulated one thousand datasets, each of size n = 1000 from a multivariate normally distributed X * , which were discretized into ordinal datasets with K = 4 and ceiling effects. Three model fit statistics were computed: cont-ML without and with constraints, and cat-LS with constraints. The rejection rates were 4.9%, 99.9%, and 4.7% respectively. Compatible with the simulations from Study 2, cont-ML for the original unrestricted model has a rejection rate close to nominal. This is the same for cat-LS with the constraints, which is expected since we simulate under normality. In contrast, cont-ML with the (true) constraints has a rejection rate close to 1, illustrating that cont-ML does not in general reach the covariance model of X * . This supports our claim that the approximately nominal rejection rates observed for cont-ML and cont-ML-adj observed in Study 2 were due to the fact that the unconstrained SEM model considered encompasses the covariance matrices of X. Details of this numerical illustration are given in an R-script in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusions</head><p>Several well-cited simulation studies have concluded that ordinal SEM estimation (e.g., the WLSMV option in Mplus) is fairly robust to violation of underlying normality (e.g., <ref type="bibr" target="#b13">Flora &amp; Curran, 2004;</ref><ref type="bibr" target="#b58">Quiroga, 1994;</ref><ref type="bibr">Rhemtulla et al., 2012)</ref>, and consequently researchers rarely test for underlying normality in ordinal data. The present article describes the first comprehensive simulation study where the simulated data are not compatible with underlying normality. The results show that ordinal SEM is sensitive to underlying nonnormality, resulting in parameter bias at the population level and unacceptable CI coverage rates at the finite-sample level. We also introduced and analyzed cont-ML-adj, which theoretically and empirically outperforms the classical cont-ML approach to treating ordinal data as continuous.</p><p>The conclusion based on our simulation study is that standard methods, as well as the new cont-ML-adj, are all highly sensitive to underlying non-normality. It is therefore important for researchers to carefully consider whether underlying normality is a reasonable assumption, and to empirically assess whether the data contain evidence to the contrary. A bootstrap test to this purpose is available in the R package discnorm <ref type="bibr" target="#b15">(Foldnes &amp; Grønneberg, 2020b)</ref>. In the present study the bootstrap test adequately controlled the Type I error for a sample size of n = 300. Also, the test was able to reliably detect underlying non-normality at this sample size. We recommend that applied researchers use this test in applied work.</p><p>From our theoretical study of the problem, we also conclude that all three estimators will work better as the number of categories K increases. That is, when K increases, statistical analysis of ordinal data becomes more well behaved. Therefore, applied researchers ought to aim at increasing K when designing studies. Whether or not this is feasible in a practical situation involves many considerations that are outside the scope of the present work.</p><p>So which estimator do we recommend? We must be care-ful to point out that the external validity of our study is limited, as it is for all simulation studies. However, based on our study we recommend cat-LS over cont-ML-adj. If underlying normality is seen as reasonable, and the bootstrap test of underlying normality is not rejected, cat-LS estimation and inference are valid procedures. Should the bootstrap test indicate that we are handling data stemming from underlying non-normality, cat-LS still is preferable to cont-ML-adj.</p><p>In terms of population bias, Study 1 found that overall, the mean relative model parameter bias under non-normality was 0% and -4% for cat-LS and cont-ML-adj, respectively, and the corresponding values for the mean absolute relative bias were 4.4% and 4.9%, respectively. In addition, Study 2 revealed that CI coverage rates under non-normality are overall higher with cat-LS (84%), compared to cont-ML-adj (82%) (see Table <ref type="table" target="#tab_6">7</ref>). However, with underlying non-normality, we must interpret cat-LS results with caution. Estimates may be biased, and model fit assessment may be biased towards poor fit. In brief, applied researchers should be aware that cat-LS in the presence of underlying non-normality can not be considered a robust methodology.</p><p>Online Supplementary Material to Article The sensitivity of structural equation modeling with ordinal data to underlying non-normality and observed distributional forms</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Njål Foldnes &amp; Steffen Grønneberg</head><p>On Partial identification</p><p>Here, we consider the interpretation of simulation studies with discretized ordinal vectors when considering more general rather than normal theory methods such as the polychoric estimator. Interpretation then becomes more complex. For normal theory methods, we were able to pin-point an underlying single random vector X * which the polychoric estimator targeted. In general, such a simple answer will not hold, as the distribution of X * is not identified from the distribution of X. If we are to study the effect of non-normality of X * without further strong distributional restrictions that lead to the identification of the distribution of X * based on X, we must exercise utmost caution, since we are dealing with a non-identified statistical model where unexpected and unusual phenomena may happen.</p><p>We here review and extend some of the discussion in <ref type="bibr" target="#b74">Foldnes and Grønneberg (2019)</ref>. Let us consider the interpretation of using the VM simulation technique to generate X * that is then discretized into X, which for the sake of argument is encoded using integers from 1 to K. That is, in Equation ( <ref type="formula" target="#formula_0">1</ref>) we have x k, j = j for 1 ≤ k ≤ p and 1 ≤ j ≤ K. Based on a simulated sample from X, we compare the known Pearson correlation matrix of X * to that of the empirical correlation matrix of the sample from X. For large samples, this will estimate Cov(X). What the interpretation and justification of this comparison?</p><p>As we have seen, when simulating X * which is then discretized into X using Equation ( <ref type="formula" target="#formula_0">1</ref>) and some threshold set, there is a large class of random vectors X * that when discretized according to Equation ( <ref type="formula" target="#formula_0">1</ref>) with adjusted thresholds, yields the same distribution for X. Formally, Foldnes and Grønneberg (2019) calls X * and X * discretize equivalent if they can be discretized into vectors X and X where X and X have the same distribution. Recall that if two vectors have the same distribution, they are statistically identical. As discussed in <ref type="bibr" target="#b74">Foldnes and Grønneberg (2019)</ref>, it is not just the marginals of X * that may vary, but also features of the copula of X * . When simulating X through discretizing X * via Equation ( <ref type="formula" target="#formula_0">1</ref>), we are therefore implicitly simulating X also through discretizing X * for all X * which are discretize equivalent to X * . In general, the covariance matrix of discretize equivalent vectors X * and X * will differ, and may even differ greatly. The comparison of Cov(X) and Cov(X * ) is therefore somewhat arbitrary: Why compare Cov(X) with Cov(X * ) and not to Cov( X * ). After all, X * can be said to generate X just as much as X * . This observation rests on an ontological question: When we have instructed the computer to first generate X * , which is then discretized into X, why is it not clear that our X * is the "true" underlying vector? The reason that this may be a problematic interpretation is that simulation methods only deals with distribution functions. In a real data-set, assuming we are taking the discretization procedure described by the model in Equation ( <ref type="formula" target="#formula_0">1</ref>) very seriously, it seems unproblematic to think that there exists a real X * , and refer to its distribution and properties in the singular. But in simulation studies, we are only dealing with vectors fulfilling certain distributional laws. Everything is essentially a matter of combining the same source of pseudo random variables into something with a pre-defined distribution, and generating numbers with the pre-defined distribution is the only success criteria.</p><p>From this perspective, when simulating X by discretizing X * using some thresholds, we claim that we are simultaneously also simulating X by discretizing any X * (with possibly different thresholds) where X * is discretize equivalent to X * . Since this point may strike the reader as controversial, let us consider in more detail what makes a simulation method valid. Let us suppose that we have two independent random variables U 1 , U 2 , both uniformly distributed on <ref type="bibr">[0,</ref><ref type="bibr" target="#b9">1]</ref>. A standard way to simulate two independent standard normal random variables is then</p><formula xml:id="formula_46">Z 1 = Φ -1 (U 1 ), Z 2 = Φ -1 (U 2 ).</formula><p>This method is valid, since the joint distribution of Z 1 , Z 2 is as claimed. Similarly, the Box-Muller method of generating two independent normal random variables is given by</p><formula xml:id="formula_47">Z 3 = -2 log U 1 cos(2πU 2 ), Z 4 = -2 log U 2 cos(2πU 1 ).</formula><p>This method is equally valid, as one can show that the joint distribution of Z 3 , Z 4 is also that of a bivariate independent standard normal distribution. From a simulation point of view, it is irrelevant how the simulated random variables came about. Only the resulting distribution is relevant. Therefore, when simulating X by discretizing X * using thresholds (τ k, j ) is also a perfectly valid method for simulating X by discretizing a discretize equivalent X * using thresholds (τ k, j ) as long as the resulting variable X has the same distribution.</p><p>In order to quantify the types of variables X * that we are implicitly also simulating from when simulating X through discretizing the simulated X * , let us write X * ≡ X * as a short hand for the statement that X * is discretize equivalent to X * . Let us write the distribution of a random vector Y as P Y , and consider a set of distributions P which contains P X * . This set may for example be the set of all distributions whose marginal distributions are all standard normal. The space of possible covariance matrices compatible with generating X is the set</p><formula xml:id="formula_48">S K = {Cov( X * ) : X * ≡ X * , P X * ∈ P}.</formula><p>The calculation of such sets is called partial identification analysis, see e.g., <ref type="bibr" target="#b80">Manski (2003)</ref>. Partial identification analysis is an important theme in modern econometrics, but to our knowledge has hitherto played a minor role in psychometrics. A recent article <ref type="bibr" target="#b76">(Grønneberg, Moss, &amp; Foldnes, 2020)</ref> studies S K when K = 2, and further limits attention to each element of the covariance matrix separately. <ref type="bibr">Grønneberg et al. (2020)</ref> is able to quantify the amount of knowledge of the distribution of X * that is needed to get partial identification sets that are small. The required knowledge is considerable, and even when considering marginal distributions of X * as known, the set S is a large set that is too large to be useful.</p><p>The calculation or even approximation of S is non-trivial. Further, in terms of psychometric models, the covariance matrix of X * is really just a stepping-stone to the estimation of parameters of underlying factor or structural equation models that contain the parameters of fundamental interest. A non-trivial task is therefore to develop a partial identification machinery for the actual covariance model for X * based on available information in the distribution of X. Simulation studies therefore appear to be necessary, even though they have the unfortunate arbitrariness of comparing statistical methodology based on X to distributional aspects of X * which is just one out of many representations of underlying random variables that may generate X. The quantification of whether the resulting comparison is typical or not in the class of distributions which may generate X cannot be answered unless a partial identification analysis is performed, either exactly or approximately.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proof and a technical discussion of Proposition 1</head><p>We first provide some insights into the intuition leading up to Proposition 1. Our core observation is that the covariance matrices Cov(X (K) ) and Cov(X * ) are integrals where the integrand is the distribution of X (K) and X * respectively. For these to be close, we may construct the distribution of X (K) so that the resulting integral defining Cov(X) is a Riemann sum approximation to Cov(X * ). To see how this may be achieved, consider the simpler case of the expectation of X (K)  k , where 1 ≤ k ≤ p. Let F k (x) = P(X * k ≤ x) be the k-th marginal cumulative distribution function (CDF) of X * . Assuming F k to be continuous with density f k , and letting I{A} denote the indicator function of A, which is one if A is true, and zero otherwise, we have</p><formula xml:id="formula_49">E X (K) k = E K j=1 x (K) k, j I{τ (K) k, j-1 &lt; X * k ≤ τ (K) k, j } = K j=1 x (K) k, j E I{τ (K) k, j-1 &lt; X * k ≤ τ (K) k, j } = K j=1 x (K) k, j P(τ (K) k, j-1 &lt; X * k ≤ τ (K) k, j ) = K j=1 x (K) k, j F k (τ (K) k, j ) -F k (τ (K) k, j-1 )</formula><p>.</p><p>When choosing the values (x (K) k, j ) so that τ</p><formula xml:id="formula_50">(K) k, j-1 ≤ x (K) k, j ≤ τ (K) k, j</formula><p>for 1 ≤ k ≤ p and 1 ≤ j ≤ K, we recognize the above sum as what is known as a Riemann-Stieltjes sum, which approximates a Riemann-Stieltjes integral <ref type="bibr">(Rudin, 1976, Chapter 6)</ref>. Under further assumptions, we get lim K→∞ E X</p><formula xml:id="formula_51">(K) k = ∞ -∞ x dF k (x) = ∞ -∞ x f k (x)dx = E X *</formula><p>k , as desired. We now extend this analysis to the full covariance matrix of X (K) , and identify conditions on the values of (x (K)  k, j ), the thresholds and the marginal distributions of X * so that Cov(X (K) ) approximates Cov(X * ).</p><p>We next discuss Assumption 1. Assumption 1 ( <ref type="formula" target="#formula_0">1</ref>) is a moment condition required for our proof method. The assumption is close to minimal, in the sense that Cov(X * ) exists only when E |X * k | 2 &lt; ∞ for k = 1, . . . , p. Recall that if the k-th marginal is normal, which is the most common assumption, we have E |X * k | q &lt; ∞ for any q &gt; 0. Assumption 1 (2) specifies that the values X (K) are are getting increasingly closer to the values attained by X * . Assumption 1 (3) requires that resolution of the threshold configuration increases indefinitely, and that the range of the thresholds will eventually cover the full support of X * . Recall that for normal marginals, and any distribution with unbounded range, we have F -1 k (0) = -∞ and F -1 k (1) = ∞. Assumption 1 (4) places a restriction on the rate of growth of the values (x (K)  k, j ) compared to the threshold values (τ (K)  k, j ). Note that from Assumption 1 (2), the values of (x (K)  k, j ) are between the thresholds, whose range is expanding to the support of X * k and whose resolution increases indefinitely. These properties are therefore also transferred to the numbers (x (K)  k, j ). Suppose X * has standard normal marginals, and let us consider Assumption 1 (4) in detail. Since the standard normal distribution Φ is symmetric around zero, we restrict attention to the upper tail requirement lim K→∞ x (K)  k,K [1 -Φ(τ (K) k,K-1 )] = 0. We recall <ref type="bibr" target="#b71">(Feller, 1968</ref>, Lemma 2, Chapter VII) that for x &gt; 0 we have 1</p><formula xml:id="formula_52">-Φ(x) ≤ x -1 φ(x) where φ(x) = Φ (x) = (2π) -1/2 exp(-x 2 /2). From Assumption 1 (3), we have lim K→∞ τ (K) k,K-1 = Φ -1 (1) = ∞. The value x (K) k,K is then to fulfill lim K→∞ x (K) k,K (τ (K) k,K-1 ) -1 exp(-(τ (K) k,K-1 ) 2 /2) = 0, which is implied by the weaker lim K→∞ x (K) k,K exp(-(τ (K) k,K-1 ) 2 /2) = 0.</formula><p>For an illustration of this assumption, suppose τ (K)  k,K-1 grows at the very slow sub-logarithmic rate τ (K)  k,K-1 = 2(log K) 1/2 . Then, exp(-(τ (K)  k,K-1 ) 2 /2) = 1/K, so that for this slow growth we require that x (K)  k,K has a sub-linear growth to infinity as K → ∞.</p><p>Proof of Proposition 1. The result follows from standard convergence results in probability. We give all technical details for completeness.</p><p>We show convergence of the relevant expectations using the following argument, proved as a corollary to Theorem 25.12 in <ref type="bibr">(Billingsley, 1995, p. 348)</ref>. For completeness, we reproduce the result here (using obvious modifications to the notations in that book). In its statement, Y K =⇒ Y as K → ∞ denotes that Y K convergences in distribution to Y as K → ∞. From Theorem 25.2 in <ref type="bibr">(Billingsley, 1995, p. 340)</ref>, the required convergence in distribution is implied if Y K converges in probability to Y. We will write this as Y K -Y = o P (1) or equivalently Y K = Y + o P <ref type="bibr" target="#b9">(1)</ref>, where o P (1) is used to denote a quantity converging to zero in probability. We recall that Z K = o P (1) means that for any &gt; 0 we have that lim</p><formula xml:id="formula_53">K→∞ P(|Z K | &gt; ) = 0. Corollary 2. Let r be a positive integer. If Y K =⇒ Y and sup K≥1 E[|Y K | r+ ] &lt; ∞ where &gt; 0, then E[|Y| r ] &lt; ∞ and lim K→∞ E[Y r K ] = E[Y r ].</formula><p>Step 1: Proof that lim K→∞ E X (K) k = E X * k for each 1 ≤ k ≤ p. Lemma 3 shows that X (K) = X * + o P (1), which implies that for each 1 ≤ k ≤ p we have that</p><formula xml:id="formula_54">X (K) k = X * k + o P (1). The conclusion lim K→∞ E X (K) k = E X k therefore follows from Corollary 2 if we show sup K≥1 E |X (K) k | 1+ &lt; ∞. By assumption, E |X * k | 2+ &lt; ∞, which gives E |X * k | 1+˜ &lt; ∞ where ˜ = 1 + &gt; 0.</formula><p>The required result sup K≥1 E |X (K)  k | 1+ &lt; ∞ for some &gt; 0 therefore follows from Lemma 4.</p><p>Step 2: Proof that lim K→∞ E(X</p><formula xml:id="formula_55">(K) k X (K) l ) = E X * k X * l for all 1 ≤ k, l ≤ p. Let 1 ≤ k, l ≤ p. Since Cov(X (K) k , X (K) l ) = E(X (K) k X (K) l ) -(E X (K) k )(E X (K) l ) and we have already shown that E X (K) k → E X * k for each 1 ≤ k ≤ p, we only need to show that E(X (K) k X (K) l ) → E X * k X * l .</formula><p>Recall that a sequence of variables (Y K ) is said to be bounded in probability, denoted by O P (1), if for any &gt; 0 there exists an M &gt; 0 such that sup K≥1 P(Y K &gt; M) &lt; . Also recall that O P <ref type="bibr" target="#b9">(1)</ref> </p><formula xml:id="formula_56">≤ k ≤ p, we have X k = X * k + o P (1) = O P (1) from Lemma 3. Therefore, X (K) k X (K) l = (X * k + o P (1))(X * l + o P (1)) = X * k X * l + X * k o P (1) + X * l o P (1) + o P (1) = X * k X * l + o P (1).</formula><p>For Corollary 2 to give us the desired conclusion, we are therefore left with showing that for an &gt; 0 we have sup</p><formula xml:id="formula_57">K≥1 E |X (K) k X (K) l | 1+ /2 &lt; ∞.</formula><p>From the Cauchy-Schwarz inequality, we have</p><formula xml:id="formula_58">E |X (K) k X (K) l | 1+ /2 = E |X (K) k | 1+ /2 |X (K) l | 1+ /2 ≤ E |X (K) k | 2+ E |X (K) l | 2+ . (<label>13</label></formula><formula xml:id="formula_59">)</formula><p>From Lemma 4, we have that sup</p><formula xml:id="formula_60">K≥1 E |X (K) k | 2+ &lt; ∞ and that sup K≥1 E |X (K) l | 2+ &lt; ∞.</formula><p>Therefore, considering Equation (13), we also have that sup K≥1 E |X (K)  k X (K) l | 1+ /2 &lt; ∞. We have therefore verified the conditions for using Corollary 2, and we conclude that E X</p><formula xml:id="formula_61">(K) k X (K) l → E X * k X * l .</formula><p>We now provide the lemmas called upon in the first proof. We start by a preliminary lemma, which re-writes Assumption 1 into the forms which will be directly applied in the upcoming proofs.</p><p>Lemma 2.</p><p>1. Under Assumption 1 (4), we have that lim</p><formula xml:id="formula_62">K→∞ x (K) k,1 P(X (K) k = x (K) k,1 ) = 0 and lim K→∞ x (K) k,K P(X (K) k = x (K) k,K ) = 0. 2. Under Assumption 1 (3) we have X * k (I{X * k ≤ τ (K) k,1 or X * k &gt; τ (K) k,K-1 }) = o P (1)</formula><p>Proof.</p><p>1. We have P(X</p><formula xml:id="formula_63">(K) k = x (K) k,1 ) = P(X * k ≤ τ (K) k,1 ) = F k (τ (K) k,1 ) and P(X (K) k = x (K) k,K ) = P(X * k &gt; τ (K) k,K-1 ) = 1 -F k (τ (K) k,K-1 ). 2. We have 0 ≤ I{X * k ≤ τ (K) k,1 or X * k &gt; τ (K) k,K-1 } ≤ I{X * k ≤ τ (K) k,1 }+I{X * k &gt; τ (K) k,K-1 }. Since τ (K) k,1 ----→ K→∞ F -1 k (0) we have that for any E |I{X * k ≤ τ (K) k,1 }| = E I{X * k ≤ τ (K) k,1 } = P(X * k ≤ τ (K) k,1 ) ----→ K→∞ P(X * k ≤ F -1 k (0)) = F k (F -1 k (0)) = 0.</formula><p>Since convergence in absolute value, i.e., L 1 convergence, implies convergence in probability, we have</p><formula xml:id="formula_64">I{X * k ≤ τ (K) k,1 } = o P (1). Similarly, I{X * k &gt; τ (K) k,K-1 } = o P (1). Therefore, I{X * k ≤ τ (K) k,1 or X * k &gt; τ (K) k,K-1 } = o P (1). Now, X * k = O P (1) since X * k is a random variable. Therefore, X * k I{X * k ≤ τ (K) k,1 or X * k &gt; τ (K) k,K-1 } = O P (1)o P (1) = o P (1).</formula><p>Lemma 3. Suppose given Assumption 1. We then have that X</p><formula xml:id="formula_65">(K) = X * + o P (1). Proof. Let δ i (k) := I{τ (K) k,i-1 &lt; X * k ≤ τ (K) k,i }. We have X (K) k = K i=1 (x (K) k,i -X * k + X * k )δ i (k) = K i=1 (x (K) k,i -X * k )δ i (k) + X * k K i=1 δ i (k) =1 = K i=1 (x (K) k,i -X * k )δ i (k) + X * k = X * k + K-1 i=2 (x (K) k,i -X * k )δ i (k) + x (K) k,1 I{X * k ≤ τ (K) k,1 } + x (K) k,K I{X * k &gt; τ (K) k,K-1 } -X * k (I{X * k ≤ τ (K) k,1 } + I{X * k &gt; τ (K) k,K-1 }) = X * k + x (K) k,1 I{X * k ≤ τ (K) k,1 } + x (K) k,K I{X * k &gt; τ (K) k,K-1 } -X * k (I{X * k ≤ τ (K) k,1 or X * k &gt; τ (K) k,K-1 }) + K-1 i=2 (x (K) k,i -X * k )δ i (k). (<label>14</label></formula><formula xml:id="formula_66">)</formula><p>We have <ref type="bibr" target="#b9">1)</ref> since by Markov's inequality, we have, for &gt; 0, that</p><formula xml:id="formula_67">x (K) k,1 I{X * k ≤ τ (K) k,1 } = o P<label>(</label></formula><formula xml:id="formula_68">P(x (K) k,1 I{X * k ≤ τ (K) k,1 } &gt; ) ≤ -1 E x (K) k,1 I{X * k ≤ τ (K) k,1 } = -1 x (K) k,1 P(X * k ≤ τ (K) k,1 ) ----→ K→∞<label>0</label></formula><p>by Lemma 2 (1). Also by Lemma 2 (1),</p><formula xml:id="formula_69">x (K) k,K I{X * k &gt; τ (K) k,K-1 } = o P (1). We also have X * k (I{X * k ≤ τ (K) k,1 or X * k &gt; τ (K) k,K-1 }) = o P (1) by Lemma 2 (2). Finally, letting M K = sup 1≤k≤p,2≤ j≤K-1 τ (K)</formula><p>k, j -τ (K) k, j-1 , and recalling that by Assumption 1 we have lim K→∞ M K = 0, we see that</p><formula xml:id="formula_70">| K-1 i=2 (x (K) k,i -X * k )δ i (k)| ≤ K-1 i=2 |x (K) k,i -X * k |δ i (k) (a) ≤ M K K-1 i=2 δ i (k) (b) ≤ M K<label>(15)</label></formula><p>(a) When δ i (k) = 1, we have that both x (K) k,i and X * k are contained within the interval (τ k, j-1 , τ k, j ]. The absolute difference between two numbers contained in an interval is bounded by the length of the interval. Therefore, |x (K)  k,i -</p><formula xml:id="formula_71">X * k |δ i (k) ≤ (τ k, j -τ k, j-1 )δ i (k) ≤ M K δ i (k). When δ i (k) = 0, this inequality is also true, giving the conclusion. (b) We have K i=1 δ i (k) = 1.</formula><p>In conclusion, we have <ref type="bibr" target="#b9">1)</ref>. for each k.</p><formula xml:id="formula_72">X (K) k = X * k + o P<label>(</label></formula><p>Lemma 4. Suppose given Assumptions 1 (2), (3), and (4). If for some &gt; 0 we have E |X * | m+ &lt; ∞, we also have</p><formula xml:id="formula_73">sup K≥1 E |X (K) k | m+ &lt; ∞.</formula><p>Proof. Recalling Equation ( <ref type="formula" target="#formula_65">14</ref>), and writing the L p norm of a random variable Y as</p><formula xml:id="formula_74">Y p = (E |Y| p ) 1/p</formula><p>we use Minkowski's inequality (the triangle inequality for L p norms) with p = m + to see that</p><formula xml:id="formula_75">(E |X (K) k | m+ ) 1/(m+ ) = X (K) k m+ = X * k + x (K) k,1 I{X * k ≤ τ (K) k,1 } + x (K) k,K I{X * k &gt; τ (K) k,K-1 } -X * k (I{X * k ≤ τ (K) k,1 or X * k &gt; τ (K) k,K-1 }) + K-1 i=2 (x (K) k,i -X * k )δ i (k) m+ = X * k m+ + x (K) k,1 I{X * k ≤ τ (K) k,1 } m+ + x (K) k,K I{X * k &gt; τ (K) k,K-1 } m+ + X * k (I{X * k ≤ τ (K) k,1 or X * k &gt; τ (K) k,K-1 }) m+ + K-1 i=2 (x (K) k,i -X * k )δ i (k) m+ . Now X * k m+ &lt; ∞ since X * is assumed to have n + mo- ments.</formula><p>Recall that a sequence of numbers (</p><formula xml:id="formula_76">x K ) is o(1) if lim K→∞ x K = 0. Further, x (K) k,1 I{X * k ≤ τ (K) k,1 } m+ = (E |x (K) k,1 I{X * k ≤ τ (K) k,1 | m+ ) 1/(m+ ) = |x (K) k,1 |P(X * k ≤ τ (K) k,1 ) = o(1)</formula><p>by Lemma 2 (1), recalling that if for some numbers c K we have lim K→∞ c K = 0, then also lim</p><formula xml:id="formula_77">K→∞ |c K | = 0. Similarly, x (K) k,K I{X * k &gt; τ (K) k,K-1 } m+ = o(1). Since the indicator function is bounded by 1, we have X * k (I{X * k ≤ τ (K) k,1 or X * k &gt; τ (K) k,K-1 }) m+ ≤ X * k m+ &lt;</formula><p>∞ by the assumption of the lemma. Finally, from Equation (15), we have</p><formula xml:id="formula_78">K-1 i=2 (x (K) k,i -X * k )δ i (k) m+ ≤ M K m+ = M K = o(1).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proofs of Corollary 1, Lemma 1, and Proposition 2</head><p>Proof of Corollary 1.</p><formula xml:id="formula_79">For each 1 ≤ k ≤ p, let γ (K) k, j = τ k, j for j = 1, 2, . . . , K -1 and let γ k,K = z (K)</formula><p>M . By Assumption 2, we have that Assumption 1 (4) holds for x</p><formula xml:id="formula_80">(K) k, j = γ (K) k, j . Let X(K) = ( X(K) 1 , . . . , X(K) p ) where for each 1 ≤ k ≤ p we let X(K) k = K j=1 γ (K) k, j δ j (k) where δ j (k) := I{τ (K) k, j-1 &lt; X * k ≤ τ (K) k, j }. Propo- sition 1 therefore implies that Cov( X(K) ) ----→ K→∞ Cov(X * ). Therefore, lim K→∞ Cor( X(K) k , X(K) l ) = Cor(X * k , X * l ). We show that in fact Cor( X(K) k , X(K) l ) = Cor(X (K) k , X (K) l )</formula><p>, which finishes the proof, since this implies the required convergence. Using that K j=1 δ j (k) = 1 we get that</p><formula xml:id="formula_81">X(K) k = K j=1 z (K) m + (z (K) M -z (K) m )( j -1)/(K -2) δ j (k) = z (K) m + (z (K) M -z (K) m )         K j=1 jδ j (k) -1         /(K -2) = z (K) m + (z (K) M -z (K) m ) X (K) k -1 /(K -2) = a (K) k X (K) k + b (K) k for non-random numbers a (K) k , b (K) k .</formula><p>Since correlation is unchanged under affine transformations, the result follows.</p><p>Proof of Lemma 1. Recall that φ 2 (x 1 , x 2 ; r)</p><formula xml:id="formula_82">= 1 2π √ 1-r 2 exp -1 2(1-r 2 ) x 2 1 + x 2 2 -2rx 1 x 2 . Let H(r) = E F k,l log φ 2 (X * k , X * l ; r).</formula><p>Using that X * k , X * l are standardized, and denoting Cor(X * k , X * l ) by ρ, we have</p><formula xml:id="formula_83">H(r) = -log(2π) - 1 2 log(1 -r 2 ) - E(X * k ) 2 + E(X * l ) 2 -2r E X * k X * l 2(1 -r 2 ) = -log(2π) - 1 2 log(1 -r 2 ) - 2 -2rρ 2(1 -r 2 ) = -log(2π) - 1 2 log(1 -r 2 ) - 1 -rρ (1 -r 2 ) .</formula><p>We have</p><formula xml:id="formula_84">H (r) = - 1 2 2r 1 -r 2 - -ρ(1 -r 2 ) -(1 -rρ)(-2r) (1 -r 2 ) 2 = r(1 -r 2 ) + ρ(1 -r 2 ) -(1 -rρ)2r (1 -r 2 ) 2 = r -r 3 + ρ -ρr 2 -2r + 2ρr 2 (1 -r 2 ) 2 = -r -r 3 + ρ + ρr 2 (1 -r 2 ) 2 = (r 2 + 1)(ρ -r) (1 -r 2 ) 2 , so that H (r) = 0 if and only if r = ρ. We have H (r) = -(1 + 6r 2 + r 4 -6rρ -2r 3 ρ)/(1 -r 2 ) 3 and H (ρ) = -(r 2 + 1)/[(r 2 -1) 2 (r + 1) 2 ] &lt; 0, showing that r = ρ is the global maximum. Proof of Proposition 2. Let 1 ≤ k, l ≤ p be given. Let U (K) k = F (K) k (X (K) k ) and U k = F k (X * k ) for 1 ≤ k ≤ p. Recall ρ S ,(K) k,l = Cor(U (K) k , U (K) l ) and ρ S k,l = Cor(U k , U l ). We have ρ S ,(K) k,l = Cor(U (K) k , U (K) l ) = E U (K) k U (K) l -(E U (K) k )(E U (K) l ) E(U (K) k ) 2 -(EU (K) k ) 2 E(U (K) l ) 2 -(EU (K) l ) 2 . It therefore suffices to prove E U (K) k = E U k + o(1) and E U (K) k U (K) l = E U k U l + o(1) for every 1 ≤ k, l ≤ p. Let F (K) k (x) = P(X (K) k ≤ x). Since by Lemma 4 (p.4) we have X (K) k = X * k + o P (1) as K → ∞, meaning X (K) k P ----→ K→∞ X *</formula><p>k , and since (as reviewed at the start of the proof of Proposition 1) convergence in probability implies convergence in distribution, we have that also</p><formula xml:id="formula_85">X (K) k d ----→ K→∞ X * k .</formula><p>By the definition of convergence in distribution, this means that for all x where F k is continuous, which by the assumption that F k is continuous means for all x, we have lim K→∞ P(X</p><formula xml:id="formula_86">(K) k ≤ x) = lim K→∞ F (K) k (x) = F k (x) = P(X * k ≤ x).</formula><p>Since F k is continuous, this convergence is in fact uniform in x. Indeed, by e.g., Lemma 2.11 in <ref type="bibr" target="#b88">Van der Vaart (2000)</ref>, we have that lim</p><formula xml:id="formula_87">K→∞ sup x |F (K) k (x) -F k (x)| = 0. Since U (K) k = F (K) k (X (K) k ) = F k (X (K) k ) -[F (K) k (X (K) k ) -F k (X (K) k )] and |F (K) k (X (K) k ) -F k (X (K) k )| ≤ sup x |F (K) k (x) -F k (x)| = o(1), we get U (K) k = F k (X (K) k ) + o P<label>(1)</label></formula><p>. Finally, since F k is continuous, and X (K) k = X k + o P (1), the continuous mapping theorem gives</p><formula xml:id="formula_88">U (K) k = F k (X k +o P (1))+o P (1) = F k (X k )+o P (1) = U k +o P (1). Since U (K) k ∈ [0, 1]</formula><p>, its expectation is bounded by 1, as is its 1 + moment. Similarly, E |U (K)  k U (K) l | 1+ ≤ 1 for any &gt; 0. Therefore, by Corollary 2, the statement follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>On polychoric correlations with misspecified marginals</head><p>For simplicity, we assume that both G k and F k are strictly increasing, for k = 1, . . . , p. Recall that we assume that F k for k = 1, . . . , p are standardized, so Lemma 1 will give consistency of the polychoric estimator if the marginals are chosen in such a manner that the thresholds are consistent. Recall that the true marginals of X * are G k for k = 1, . . . , d. Since the marginals are not identified, they can be made to equal F k with a change of threshold values.</p><formula xml:id="formula_89">Let U k = G k (X * k ) so that U k is uniform on [0, 1], see Equation (16) (p.6.) Re- call that if given a random variable U k that is uniform on [0, 1], then Xk = F -1 k (U k ) is F k distributed.</formula><p>This follows as in Equation ( <ref type="formula" target="#formula_95">19</ref>), i.e., we have</p><formula xml:id="formula_90">P( Xk ≤ x) = P(F -1 k (U K ) ≤ x) = P(U k ≤ F k (x)) = F k (x).</formula><p>Recall that since F k is strictly increasing, also F -1 k is strictly increasing. Since G k is strictly increasing, also the composite map</p><formula xml:id="formula_91">F -1 k G k (x) = F -1 k (G k (x)</formula><p>) is strictly increasing. We have</p><formula xml:id="formula_92">X k = x k, j , when τ k, j-1 ≤ X * k ≤ τ k, j</formula><p>Emulating the argument from Equation (7) to Equation (8) on p. 8 shows that</p><formula xml:id="formula_93">X k = x k, j , when τk, j-1 ≤ X * k ≤ τk, j where X * k = F -1 k G k (X * k ), τk, j = F -1 k G k (τ k, j ).</formula><p>Therefore, X = ( X1 , . . . , Xp ) has marginals F 1 , . . . , F p and is discretize equivalent to X * . Since the threshold estimators built on the assumption that X * has marginals F 1 , . . . , F p are consistent estimators when this is correct, and we see that this in fact is correct for a discretize equivalent vector X * , they are consistently estimating the thresholds τk, j .</p><p>An introduction to copulas, vines and the VITA method</p><p>Vine distributions present a general method for combining a series of bivariate distributions into a valid full multivariate distribution, and was developed in <ref type="bibr" target="#b78">Joe (1996)</ref> and <ref type="bibr">Bedford and Cooke (2002)</ref>, see <ref type="bibr" target="#b79">Joe (2014)</ref>. Vines are built around the concept of copulas, which we will introduce in the following. See <ref type="bibr">Nelsen (2007)</ref> for a general introduction to copulas. Briefly, copulas allow the specification of a multivariate distribution with uniform marginals that are then connected with univariate marginal distributions, giving a full distribution with the attached marginals. The construction of multivariate distributions is therefore split into first constructing univariate marginal distributions, which is a well-studied problem, and then constructing a multivariate distribution with uniform marginals, i.e., copulas. The construction of copulas is in general difficult, although the vine technique allows a large class of multivariate copula distributions to be constructed in a simple manner.</p><p>We will not review all the technical details of the general vine construction. Our aim is instead to provide the necessary overview to gain an understanding of what the VITA simulation method of <ref type="bibr">Grønneberg and Foldnes (2017)</ref> does. The VITA method is based on simulating from a particularly chosen vine distribution whose Pearson covariance matrix is fixed. We also explain enough technical details in some introductory cases for the reader to appreciate the main limitation of the vine construction which is given by a property called the simplifying assumption. This limitation is on the other hand the reason for the ease of use and flexibility of vines.</p><p>Since VITA is based on vines, which in turn are based on connecting bivariate copula distributions in a certain manner, we start by reviewing copulas, we then present the bivariate copulas we use in our applications, and then we briefly present the core idea of vines. Finally, a brief introduction to the core idea of the VITA simulation method is summarized.</p><p>Copulas are distribution functions with marginal distributions that are uniform on <ref type="bibr">[0,</ref><ref type="bibr" target="#b9">1]</ref>. Let us therefore review these concepts first. Consider a random vector Y = (Y 1 , . . . , Y p ) . Its distribution is fully characterized by its CDF given by F(y 1 , . . . , y p ) = P(Y 1 ≤ y 1 , . . . , Y p ≤ y p ) = P(∩ p j=1 {Y j ≤ y j }).</p><p>For any 1 ≤ j ≤ p, let [a j , A j ] be the shortest interval (possibly infinite) where Y j is always contained (i.e., the support of Y j ). That is, we always have a j ≤ Y j ≤ A j . If Y j is normal, we have a j = -∞ and A j = ∞. Since {Y j ≤ A j } is always the case, we have that for any 1 ≤ k ≤ p that The importance of copulas comes from Sklar's theorem <ref type="bibr" target="#b87">(Sklar, 1959)</ref>, see also <ref type="bibr">Nelsen (2007)</ref>, which informally states that we may always express any CDF in terms of its copula and its marginal distributions, and that if we start with a copula and some marginal distributions, we may combine them in a manner that always lead to a valid full distribution having the chosen marginal distributions.</p><p>For simplicity we assume that for each 1 ≤ k ≤ p the function F k is continuous and strictly increasing, and hence has a continuous and strictly increasing inverse F -1 k . Let Y be a random vector with CDF F, which we will write as Y ∼ F. Then we define the random vector U = (F 1 (Y 1 ), . . . , F p (Y p )) .</p><p>Denote the CDF of U by C. That is, U ∼ C. The variable U has uniform marginals, which is seen by</p><formula xml:id="formula_94">P(U 1 ≤ u 1 ) = P(F 1 (Y 1 ) ≤ u 1 ) = P(Y 1 ≤ F -1 1 (u 1 )) = F 1 F -1</formula><p>1 (u 1 ) = u 1 (16) when 0 ≤ u 1 ≤ 1. Hence C is a copula, and is in fact the copula of F. We calculate the fundamental relations = C(F 1 (y 1 ), . . . , F p (y p )). ( <ref type="formula">18</ref>)</p><p>From Equation ( <ref type="formula">18</ref>), we gain a useful method to simulate Y in two stages. We may first simulate U ∼ C, and then perform the simple transformation Ỹ = (F -1 1 (U 1 ), . . . , F -1 p (U p )) .</p><p>The resulting vector, here tentatively called Ỹ, has the distribution of F, and is therefore a valid method to simulate from Y. This follows from Equation (18) since P(F -1 1 (U 1 ) ≤ y 1 , . . . , F -1 p (U p ) ≤ y p ) = P(U 1 ≤ F 1 (y 1 ), . . . , U p ≤ F p (y p ))</p><p>= F(y 1 , . . . , y p ).</p><p>(</p><formula xml:id="formula_95">)<label>19</label></formula><p>In our applications, U will be a vine copula, whose simulation methods are developed in <ref type="bibr" target="#b78">Joe (1996)</ref>, <ref type="bibr">Bedford and</ref><ref type="bibr" target="#b67">Cooke (2001), and</ref><ref type="bibr">presented, e.g., in Dissmann, Brechmann, Czado, and</ref><ref type="bibr" target="#b70">Kurowicka (2013)</ref>. The simulation methods are also implemented in several packages in the R system (R Core Team, 2020), such as the vinecopula <ref type="bibr">(Schepsmeier et al., 2018)</ref> and rvinecopulib <ref type="bibr">(Nagler &amp; Vatter, 2019)</ref> packages.</p><p>By the above observation, it is therefore easy (once U is simulated) to set the marginals to what the application requires. In our applications the marginal distributions will be standard normal. When we now discuss vines, we only focus on vine copulas, which are vine distributions with uniform marginals. The full vine distribution will be a vine copula connected to marginal distributions as in the above.</p><p>As vines are constructed from a sequence of bivariate copulas in a manner that produces a full multivariate distribution, let us first review the three central bivariate copulas used in the present study. The Gaussian, or normal copula is defined as follows. Let Y = (Y 1 , Y 2 ) ∼ N(µ, Σ) where µ = (µ 1 , µ 2 ) and Σ is a covariance matrix. We may identify the copula of Y using Equation ( <ref type="formula">17</ref>). The marginal distributions of Y are</p><formula xml:id="formula_96">F k (y k ) = P(Y k ≤ y k ) = P Y k -µ k σ k ≤ y k -µ k σ k = Φ y k -µ k σ k .</formula><p>Suppose F k (y k ) = x, which means Φ y k -µ k σ k = x. Therefore, Φ -1 (x) = y k -µ k σ k which means that F -1 k (x) = σ k Φ -1 (x) + µ k . Let us write the density F Y of Y via its correlation ρ and its standard deviations σ 1 , σ 2 , so that</p><formula xml:id="formula_97">f Y (y 1 , y 2 ) = 1 2πσ 1 σ 2 1 -ρ 2 exp - 1 2(1 -ρ 2 ) (y 1 -µ 1 ) 2 σ 2 1 + (y 2 -µ 2 ) 2 σ 2 2 - 2ρ(y 1 -µ 1 )(y 2 -µ 2 ) σ 1 σ 2</formula><p>Note that f Y (y 1 , y 2 ) = φ 2 ([y 1 -µ 1 ]/σ 1 , [y 2 -µ 2 ]/σ 2 ; ρ) where φ 2 (•, •; ρ) is the density of a bivariate normal Z with standardized marginals and correlation ρ. The copula of Y is found using Equation ( <ref type="formula">17</ref>) and the formula for the inverse of F 1 , F 2 , and is given by</p><formula xml:id="formula_98">C N (u 1 , u 2 ; ρ) = F(F -1 1 (u 1 ), F -1 2 (u 2 )) = F -1 1 (u 1 ) -∞ F -1 2 (u 2 ) -∞ f Y (y 1 , y 2 ) dy 1 dy 2 = σ 1 Φ -1 (u 1 )+µ 1 -∞ σ 2 Φ -1 (u 2 )+µ 2 -∞ φ 2 y 1 -µ 1 σ 1 , y 2 -µ 2 σ 2 ; ρ dy 1 dy 2 = Φ -1 (u 1 ) -∞ Φ -1 (u 2 )</formula><p>-∞ φ 2 (x 1 , x 2 ; ρ) dx 1 dx 2 using the change of variables x 1 = [y 1 -µ 1 ]/σ 1 and x 2 = [y 2 -µ 2 ]/σ 2 . We therefore see that the copula of Y is the same as the copula of Z, and the normal copula is therefore parameterized by the Pearson correlation ρ. There is no simple formula for C N . In contrast, the Joe and Clayton copulas, denoted by C J and C C , are defined directly in terms of their CDFs:</p><formula xml:id="formula_99">C J (u 1 , u 2 ; θ) = 1 -(1 -u) θ + (1 -v) θ -(1 -u) θ (1 -v) θ 1/θ , C C (u 1 , u 2 ; θ) = max u -θ + v -θ -1; 0 -1/θ ,</formula><p>where the dependence parameter θ ∈ [1, ∞) for the Joe copula, and θ ∈ [-1, ∞) \ {0} for the Clayton copula. Both copulas express positive dependence, but can be rotated to express also negative dependence, see e.g. the documentation of the R package copula <ref type="bibr">(Hofert et al., 2013)</ref>. These copulas produce different types of dependencies. The density of the distribution of Y = (Φ -1 (U 1 ), Φ -1 (U 2 )) when (U 1 , U 2 ) come from a normal, a Joe or a Clayton copula is plotted in Figure <ref type="figure" target="#fig_2">2</ref> (p.5) when the dependency parameters are all chosen such that the Pearson correlation of the final Y is fixed to 0.56. For the normal copula, the dependence parameter is precisely the correlation of the copula when joined with normal marginals, which is exactly what we do here, so that the dependence parameter of the copula is simply ρ = 0.56. In contrast, for the Joe and the Clayton copulas, the dependence parameters θ are found via a numerical search, computing for each candidate value θ the resulting Pearson correlation of Y until a match is found.</p><p>We next introduce vines. Our technical discussion is limited to the three-dimensional case, which illustrates the main features of the construction. The general case is later outlined, but without several complicated rules and definitions. A full presentation is given e.g. in <ref type="bibr" target="#b79">Joe (2014)</ref>, and is summarized in <ref type="bibr" target="#b70">Dissmann et al. (2013)</ref>, and <ref type="bibr">Grønneberg and Foldnes (2017)</ref>.</p><p>In psychometrics, we are used to summarizing dependencies using a covariance matrix. For a multivariate normal random vector Z = (Z 1 , Z 2 , Z 3 ) , the covariance matrix does indeed summarize the multivariate dependence properties of Z perfectly. Since we will be interested in copulas, we assume that Z is standardized. Therefore, we assume Z ∼ N(0, Σ), where Σ is a correlation matrix.</p><p>The fact that the correlation matrix suffices to describe the multivariate dependence structure of Z is somewhat surprising, since it only contains numbers that quantify bivariate features of the distribution. Indeed, Σ consists of the numbers ρ 1,2 = Cov(Z 1 , Z 2 ), ρ 1,3 = Cov(Z 1 , Z 3 ) and ρ 2,3 = Cov(Z 2 , Z 3 ). From these bivariate numbers, we may deduce for example genuine multivariate features of the distribution, such as the conditional distribution of Z 1 , Z 2 given Z 3 . Since the formula for this conditional distribution will motivate the construction of vines in general, we review its derivation. Using general results for the normal distribution (e.g., <ref type="bibr" target="#b81">Mardia, Kent, &amp; Bibby, 1979)</ref>, and writing</p><formula xml:id="formula_100">Σ =           1 ρ 1,2 ρ 1,3 ρ 1,2</formula><p>1 ρ 2,3 ρ 1,3 ρ 2,3 1 we have that (Z 1 , Z 2 |Z 3 ) ∼ N(Σ (1,2),3 Σ -1 3,3 Z 3 , Σ (1,2),(1,2) -Σ (1,2),3 Σ -1 3,3 Σ (1,2),3 )</p><formula xml:id="formula_101">          = Σ<label>(</label></formula><formula xml:id="formula_102">∼ N ρ 1,3 Z 3 ρ 2,3 Z 3 , 1 ρ 1,2 ρ 1,2 1 - ρ 1,3 ρ 2,3 (ρ 1,3 , ρ 2,3 ) ∼ N ρ 1,3 Z 3 ρ 2,3 Z 3 , 1 -ρ 2 1,3 ρ 1,2 -ρ 1,3 ρ 2,3 ρ 1,2 -ρ 1,3 ρ 2,3 1 -ρ 2 2,3</formula><p>.</p><p>Conditional on Z 3 , we therefore have that Z 1 , Z 2 is again normal. Therefore, it has a normal copula, with correlation equal to ρ 1,2|3 = ρ 1,2 -ρ 1,3 ρ 2,3</p><p>1 -ρ 2 1,3 1 -ρ 2 2,3</p><p>.</p><p>The number ρ 1,2|3 is known as the partial correlation, and under normality will exactly specify the dependency properties of Z 1 , Z 2 given Z 3 . We have also shown that</p><formula xml:id="formula_103">µ 1|3 = E[Z 1 |Z 3 ] = ρ 1,3 Z 3 , µ 2|3 = E[Z 2 |Z 3 ] = ρ 2,3 Z 3 , σ 1|3 = Var(Z 1 |Z 3 ) = 1 -ρ 2 1,3 , σ 2|3 = Var(Z 2 |Z 3 ) = 1 -ρ 2</formula><p>2,3 . From our earlier derivations on the normal copula, we have that</p><formula xml:id="formula_104">P(Z 1 ≤ z 1 , Z 2 ≤ z 2 |Z 3 ) = C N (σ 1|3 Φ -1 (z 1 ) -µ 1|3 , σ 2|3 Φ -1 (z 2 ) -µ 2|3 ; ρ 1,2|3 ) = C N (σ 1|3 Φ -1 (z 1 ) -ρ 1,3 Z 3 , σ 2|3 Φ -1 (z 2 ) -ρ 2,3 Z 3 ; ρ 1,2|3 ),</formula><p>where the dependence on Z 3 is only present through µ 1|3 and µ 2|3 . Importantly, the copula C N and its parameter ρ 1,2|3 do not depend on the value attained by Z 3 . The fact that the copula C N stays the same for every value of Z 3 need not be the case for general non-normal distributions, and we will call this assumption "the simplifying assumption" <ref type="bibr" target="#b79">(Joe, 2014)</ref>. <ref type="bibr" target="#b78">Joe (1996)</ref> defined (a subset of) the distributions later called vine copulas as a generalization of this simplifying assumption. Instead of the normal copula, we could replace C N by another copula, say C, and we would still have a valid probability distribution. From knowing the marginal distributions of Z 1 , Z 2 , Z 3 when seen separately, the bivariate distributions of (Z 1 , Z 3 ) and (Z 2 , Z 3 ), and finally the bivariate conditional distribution of (Z 1 , Z 2 ) conditioned on Z 3 , we have enough information to perfectly reconstruct the full distribution of (Z 1 , Z 2 , Z 3 ). To see this, we use the rule of iterated expectations together with the rule E[h(X)Y|X] = h(X) E[Y|X] so that</p><formula xml:id="formula_105">P(Z 1 ≤ z 1 , Z 2 ≤ z 2 , Z 3 ≤ z 3 ) = E I{Z 1 ≤ z 1 , Z 2 ≤ z 2 , Z 3 ≤ z 3 } = E I{Z 3 ≤ z 3 }I{Z 1 ≤ z 1 , Z 2 ≤ z 2 } = E (E(I{Z 3 ≤ z 3 }I{Z 1 ≤ z 1 , Z 2 ≤ z 2 }|Z 3 )) = E (I{Z 3 ≤ z 3 } E(I{Z 1 ≤ z 1 , Z 2 ≤ z 2 }|Z 3 )) = E (I{Z 3 ≤ z 3 }P(Z 1 ≤ z 1 , Z 2 ≤ z 2 |Z 3 )) = E I{Z 3 ≤ z 3 }C N (σ 1|3 Φ -1 (z 1 ) -ρ 1,3 Z 3 , σ 2|3 Φ -1 (z 2 ) -ρ 2,3 Z 3 ; ρ 1,2|3 ) = ∞ -∞ I{x 3 ≤ z 3 }C N (σ 1|3 Φ -1 (z 1 ) -ρ 1,3 x 3 , σ 2|3 Φ -1 (z 2 ) -ρ 2,3 x 3 ; ρ 1,2|3 )φ(x 3 ) dx 3 .</formula><p>This is therefore a type of expansion of the full distribution in terms of unconditional and conditional bivariate distributions, as well as the marginal distributions. Note that the bivariate distributions of (Z 1 , Z 3 ) and (Z 2 , Z 3 ) and the marginal distributions of Z 1 , Z 2 , Z 3 , as well as the conditional distribution of (Z 1 , Z 2 ) conditioned on Z 3 were used in forming the above expression. If instead C N was replaced by e.g., a Joe copula C J , say with dependence parameter θ, the resulting function</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Cont-ML-adj: Adjustment of original values X = 1, . . . , 4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>(b) the copula belongs to the class of Clayton copulas, and the figure presents contours for the distribution obtained by combining this copula with standard normal marginals. Similarly, Figure 2(c) depicts a Joe-type copula paired with standard normal marginals. The two latter distributions differ from the bivariate normal distribution by allowing tail dependencies.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Three bivariate distributions with correlation 0.56 and standard normal marginals.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Fleishman polynomial H(Z) = -0.16 + 0.82Z + 0.16Z 2 + 0.05Z 3 that yields skewness=1.25 and kurtosis = 3.75. The dashed arrows represent threshold transformations from the Fleishman metric to the standard normal metric.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>VM transformed sample, with thresholds τ 1 = -1.645, τ 2 = -0.643, τ 3 = 0.643, and τ 4 = 1.645.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. A n = 500 sample drawn from a bivariate normal distribution and then transformed with polynomial H(Z) = -0.16 + 0.82Z + 0.16Z 2 + 0.05Z 3 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Population values for the Pearson correlation of integer-coded data from evenly spaced thresholds. Dashed horizontal line at 0.7 represents true correlation of X * . K= number of categories. Normal=underlying bivariate normal distribution. Clayton=Underlying distribution with normal marginals and Clayton copula.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>True ρ Pearson Pearson-adj polychoric Cor (X * 1 ) 1/3 , (X * 2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Population correlation values for data stemming from unevenly spaced thresholds. Dashed horizontal line at 0.7 represents true correlation of X * . Pearson = Pearson correlation. Pearson-adj= The Pearson correlation of adjusted values. polychoric= polychoric correlation. K= number of categories. N and C refer to underlying distributions with normal and clayton copula, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. Population model. Only the structural model among five latent variables is shown, and for clarity indicator variables are not depicted.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 8 .</head><label>8</label><figDesc>Figure 8. Three configurations of thresholds for K = 4. The upper, middle, and lower panels contain distributions of twenty ordinal variables with ceiling, floor, and symmetrical distributions, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 9 .</head><label>9</label><figDesc>Figure 9. Study 1. Relative bias for regression parameter γ 21 . Each boxplot is based on 50 threshold configurations. Floor, Ceiling, and Symmetrical refer to ordinal distributions with floor effect, ceiling effect, and symmetry, respectively. cont-ML, cont-ML-adj, and cat-LS refer to estimation based on Pearson, Pearson adjusted, and polychoric correlations, respectively. K is the number of categories. N, J, and C, refer to normal, Joe VITA, and Clayton VITA underlying distributions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 10 .</head><label>10</label><figDesc>Figure10. Study 1. Relative bias for factor loading associated with X * 1 and ξ 1 . Each boxplot is based on 50 threshold configurations. Floor, Ceiling, and Symmetrical refer to ordinal distributions with floor effect, ceiling effect, and symmetry, respectively. cont-ML, cont-ML-adj, and cat-LS refer to estimation based on Pearson, Pearson adjusted, and polychoric correlations, respectively. K is the number of categories. N, J, and C refer to normal, Joe VITA, and Clayton VITA underlying distributions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 11 .</head><label>11</label><figDesc>Figure11. Study 2: Coverage rates for regression parameter γ 21 at sample size n = 500. Each boxplot is based on 50 threshold configurations. Floor, Ceiling and Symmetrical refer to ordinal distributions with floor effect, ceiling effect and symmetry, respectively. cont-ML, cont-ML-adj and cat-LS refer to estimation based on Pearson, adjusted Pearson and polychoric correlations, respectively K is the number of categories. N, J, and C refer to normal, Joe VITA, and Clayton VITA underlying distributions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 12 .</head><label>12</label><figDesc>Figure12. Study 2: Coverage rates for factor loading relating X * 1 to ξ 1 . Each boxplot is based on 50 threshold configurations. Floor, Ceiling and Symmetrical refer to ordinal distributions with floor effect, ceiling effect and symmetry, respectively. cont-ML, cont-ML-adj and cat-LS refer to estimation based on Pearson, adjusted Pearson and polychoric correlations, respectively K is the number of categories. N, J, and C refer to normal, Joe VITA, and Clayton VITA underlying distributions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 13 .</head><label>13</label><figDesc>Figure13. Study 2: Rejection rates at the 5% level of significance of the scaled-and-shifted test statistic. Each boxplot is based on 50 threshold configurations. Floor, Ceiling, and Symmetrical refer to ordinal distributions with floor effect, ceiling effect, and symmetry, respectively. K is the number of categories. N, J, and C refer to normal, Joe VITA, and Clayton VITA underlying distributions. The horizontal line corresponds to 5% rejection rate.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>F</head><label></label><figDesc>k (y k ) = F(A 1 , . . . , A k-1 , y k , A k+1 , . . . , A p ) = P({Y k ≤ y k } ∩ ∩ 1≤ j≤p, j k {Y j ≤ A j }) = P(Y k ≤ y k ). Therefore, F k is the k-th marginal distribution of Y, i.e., the CDF of Y k . Recall that a distribution F k is uniform on [0, 1] if F k (x) = xI{0 ≤ x ≤ 1} + I{x &gt; 1} where I{A} is 1 if A is true, and zero otherwise. For 0 ≤ x ≤ 1 we have simply F k (x) = x. The support of F k is then [a k , A k ] = [0, 1].Therefore, a copula is a CDF C with the property that for each 1 ≤ k ≤ p and each 0 ≤ u k ≤ 1 we have C(1, . . . , 1, u k , 1, . . . , 1) = u k .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>C(u 1</head><label>1</label><figDesc>, . . . , u p ) = P(U 1 ≤ u 1 , . . . , U d ≤ u p ) = P(F 1 (Y 1 ) ≤ u 1 , . . . , F d (Y d ) ≤ u p ) = P(Y 1 ≤ F -1 1 (u 1 ), . . . , Y d ≤ F -1 d (u p )) = F F -1 1 (u 1 ), . . . , F -1 d (u p )(17)andF(y 1 , . . . , y p ) = P(Y 1 ≤ y 1 , . . . , Y p ≤ y p ) = P(Y 1 ≤ y 1 , . . . , Y p ≤ y p ) = P(F 1 (Y 1 )≤ F 1 (y 1 ), . . . , F p (Y p ) ≤ F p (y p ))</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>Figure 14 .</head><label>14</label><figDesc>Figure 14. A four-dimensional regular vine.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3</head><label>3</label><figDesc>Study 1: Mean absolute relative bias of estimated correlations.</figDesc><table><row><cell>Underlying</cell><cell>Ordinal</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>distribution</cell><cell cols="5">distribution K Pearson Pearson-adj polychoric</cell></row><row><cell></cell><cell></cell><cell>4</cell><cell>17.8</cell><cell>17.4</cell><cell>0.7</cell></row><row><cell></cell><cell>Floor</cell><cell>5</cell><cell>13.7</cell><cell>12.9</cell><cell>0.6</cell></row><row><cell></cell><cell></cell><cell>7</cell><cell>9.6</cell><cell>8.1</cell><cell>0.4</cell></row><row><cell></cell><cell></cell><cell>4</cell><cell>18.0</cell><cell>17.5</cell><cell>0.7</cell></row><row><cell>N</cell><cell>Ceiling</cell><cell>5</cell><cell>13.8</cell><cell>12.9</cell><cell>0.6</cell></row><row><cell></cell><cell></cell><cell>7</cell><cell>9.8</cell><cell>8.1</cell><cell>0.4</cell></row><row><cell></cell><cell></cell><cell>4</cell><cell>14.1</cell><cell>13.9</cell><cell>0.6</cell></row><row><cell></cell><cell>Symm.</cell><cell>5</cell><cell>11.0</cell><cell>10.7</cell><cell>0.5</cell></row><row><cell></cell><cell></cell><cell>7</cell><cell>6.1</cell><cell>5.6</cell><cell>0.4</cell></row><row><cell></cell><cell></cell><cell>4</cell><cell>5.5</cell><cell>5.5</cell><cell>16.1</cell></row><row><cell></cell><cell>Floor</cell><cell>5</cell><cell>5.6</cell><cell>3.3</cell><cell>13.1</cell></row><row><cell></cell><cell></cell><cell>7</cell><cell>7.4</cell><cell>1.8</cell><cell>9.6</cell></row><row><cell></cell><cell></cell><cell>4</cell><cell>43.1</cell><cell>39.6</cell><cell>26.3</cell></row><row><cell>J</cell><cell>Ceiling</cell><cell>5</cell><cell>38.6</cell><cell>32.9</cell><cell>22.4</cell></row><row><cell></cell><cell></cell><cell>7</cell><cell>33.9</cell><cell>24.6</cell><cell>17.3</cell></row><row><cell></cell><cell></cell><cell>4</cell><cell>19.3</cell><cell>18.7</cell><cell>5.1</cell></row><row><cell></cell><cell>Symm.</cell><cell>5</cell><cell>14.3</cell><cell>13.4</cell><cell>3.3</cell></row><row><cell></cell><cell></cell><cell>7</cell><cell>9.5</cell><cell>7.9</cell><cell>2.1</cell></row><row><cell></cell><cell></cell><cell>4</cell><cell>37.0</cell><cell>33.8</cell><cell>19.4</cell></row><row><cell></cell><cell>Floor</cell><cell>5</cell><cell>32.5</cell><cell>27.6</cell><cell>16.4</cell></row><row><cell></cell><cell></cell><cell>7</cell><cell>28.0</cell><cell>20.0</cell><cell>12.5</cell></row><row><cell></cell><cell></cell><cell>4</cell><cell>4.1</cell><cell>5.7</cell><cell>14.6</cell></row><row><cell>C</cell><cell>Ceiling</cell><cell>5</cell><cell>3.4</cell><cell>2.9</cell><cell>12.1</cell></row><row><cell></cell><cell></cell><cell>7</cell><cell>5.3</cell><cell>1.2</cell><cell>9.0</cell></row><row><cell></cell><cell></cell><cell>4</cell><cell>16.6</cell><cell>16.3</cell><cell>2.5</cell></row><row><cell></cell><cell>Symm.</cell><cell>5</cell><cell>12.7</cell><cell>12.1</cell><cell>1.7</cell></row><row><cell></cell><cell></cell><cell>7</cell><cell>7.8</cell><cell>6.8</cell><cell>1.1</cell></row><row><cell>Total</cell><cell></cell><cell></cell><cell>16.2</cell><cell>14.1</cell><cell>7.8</cell></row><row><cell cols="6">Note. Floor, Ceiling, and Symmetrical refer to ordinal distri-</cell></row><row><cell cols="6">butions with floor effect, ceiling effect, and symmetry, respec-</cell></row><row><cell cols="6">tively. Pearson, Pearson-adj, and polychoric refer to estima-</cell></row><row><cell cols="6">tion based on Pearson, adjusted Pearson, and polychoric corre-</cell></row><row><cell cols="6">lations, respectively. N, J, and C refer to underlying distribu-</cell></row><row><cell cols="6">tions of type normal, Joe VITA, and Clayton VITA, respectively.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4</head><label>4</label><figDesc>Study 1: Mean absolute relative bias of model parameters.</figDesc><table><row><cell></cell><cell>Parameter</cell><cell></cell><cell></cell><cell></cell><cell>Group</cell><cell>Overall</cell></row><row><cell>Estimator</cell><cell>group</cell><cell></cell><cell>K</cell><cell></cell><cell>mean</cell><cell>mean</cell></row><row><cell></cell><cell></cell><cell>4</cell><cell>5</cell><cell>7</cell></row><row><cell></cell><cell>λ ex</cell><cell cols="3">6.0 4.9 3.5</cell><cell>4.8</cell></row><row><cell>cont-ML</cell><cell>λ en γ</cell><cell cols="3">9.0 7.3 5.6 6.9 5.9 5.1</cell><cell>7.3 5.9</cell><cell>     5.5</cell></row><row><cell></cell><cell>β</cell><cell cols="3">3.4 2.9 2.4</cell><cell>2.9</cell></row><row><cell></cell><cell>λ ex</cell><cell cols="3">5.6 4.2 2.4</cell><cell>4.1</cell></row><row><cell>cont-ML-adj</cell><cell>λ en γ</cell><cell cols="3">8.5 6.3 3.9 6.3 5.1 3.7</cell><cell>6.3 5.0</cell><cell>     4.7</cell></row><row><cell></cell><cell>β</cell><cell cols="3">3.0 2.3 1.5</cell><cell>2.3</cell></row><row><cell></cell><cell>λ ex</cell><cell cols="3">3.5 2.9 2.3</cell><cell>2.9</cell></row><row><cell>cat-LS</cell><cell>λ en γ</cell><cell cols="3">3.8 3.1 2.4 4.9 4.1 3.1</cell><cell>3.1 4.0</cell><cell>     3.1</cell></row><row><cell></cell><cell>β</cell><cell cols="3">2.6 2.1 1.5</cell><cell>2.0</cell></row><row><cell>Overall mean</cell><cell></cell><cell cols="3">5.6 4.5 3.2</cell></row><row><cell cols="6">Note. cont-ML, cont-ML-adj, and cat-LS refer to model es-</cell></row><row><cell cols="6">timator. λ ex , λ en , γ, and β refer to groups of model param-</cell></row><row><cell cols="6">eters, containing, respectively, factor loadings for exogenous</cell></row><row><cell cols="6">and endogenous latent variables, regression coefficients among</cell></row><row><cell cols="6">exogenous and endogenous latent variables, and among en-</cell></row><row><cell cols="2">dogenous latent variables.</cell><cell cols="4">K= number of categories.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5</head><label>5</label><figDesc>Study 1: Mean absolute relative bias of model parameters.</figDesc><table><row><cell></cell><cell>Underlying</cell><cell>Ordinal</cell><cell></cell><cell></cell></row><row><cell>Estimator</cell><cell>distribution</cell><cell>distribution</cell><cell></cell><cell cols="2">Parameter group</cell><cell>Overall</cell></row><row><cell></cell><cell></cell><cell></cell><cell>λ ex</cell><cell>λ en</cell><cell>γ</cell><cell>β</cell></row><row><cell></cell><cell></cell><cell>Floor</cell><cell cols="2">5.2 6.6</cell><cell>3.6 2.2</cell><cell>5.0</cell></row><row><cell></cell><cell>N</cell><cell>Ceiling</cell><cell cols="2">5.2 6.6</cell><cell>2.9 1.4</cell><cell>4.7</cell></row><row><cell></cell><cell></cell><cell cols="3">Symmetrical 5.0 5.3</cell><cell>1.3 1.2</cell><cell>3.9</cell></row><row><cell>cont-ML</cell><cell>J</cell><cell>Floor Ceiling</cell><cell cols="3">3.4 3.1 7.8 16.3 15.8 4.4 4.7 3.7</cell><cell>3.6 11.5</cell></row><row><cell></cell><cell></cell><cell cols="3">Symmetrical 2.9 5.8</cell><cell>6.4 3.0</cell><cell>4.4</cell></row><row><cell></cell><cell></cell><cell>Floor</cell><cell cols="3">7.9 14.6 10.7 4.1</cell><cell>9.9</cell></row><row><cell></cell><cell>C</cell><cell>Ceiling</cell><cell cols="2">2.6 2.3</cell><cell>3.6 3.9</cell><cell>2.8</cell></row><row><cell></cell><cell></cell><cell cols="3">Symmetrical 3.3 5.5</cell><cell>4.3 2.3</cell><cell>4.0</cell></row><row><cell></cell><cell></cell><cell>Floor</cell><cell cols="2">5.3 6.2</cell><cell>2.8 1.7</cell><cell>4.7</cell></row><row><cell></cell><cell>N</cell><cell>Ceiling</cell><cell cols="2">5.2 6.2</cell><cell>2.3 1.2</cell><cell>4.5</cell></row><row><cell></cell><cell></cell><cell cols="3">Symmetrical 4.9 5.2</cell><cell>1.2 1.1</cell><cell>3.8</cell></row><row><cell>cont-ML-adj</cell><cell>J</cell><cell>Floor Ceiling</cell><cell cols="3">1.7 1.5 5.9 13.0 14.0 3.9 3.6 2.2</cell><cell>2.1 9.3</cell></row><row><cell></cell><cell></cell><cell cols="3">Symmetrical 3.0 5.5</cell><cell>5.3 2.5</cell><cell>4.1</cell></row><row><cell></cell><cell></cell><cell>Floor</cell><cell cols="3">6.0 11.8 9.6 3.4</cell><cell>8.1</cell></row><row><cell></cell><cell>C</cell><cell>Ceiling</cell><cell cols="2">1.2 1.4</cell><cell>3.0 2.5</cell><cell>1.8</cell></row><row><cell></cell><cell></cell><cell cols="3">Symmetrical 3.5 5.3</cell><cell>3.6 1.9</cell><cell>3.9</cell></row><row><cell></cell><cell></cell><cell>Floor</cell><cell cols="2">0.3 0.3</cell><cell>1.1 1.1</cell><cell>0.5</cell></row><row><cell></cell><cell>N</cell><cell>Ceiling</cell><cell cols="2">0.3 0.3</cell><cell>1.3 1.1</cell><cell>0.6</cell></row><row><cell></cell><cell></cell><cell cols="3">Symmetrical 0.3 0.2</cell><cell>1.2 1.2</cell><cell>0.6</cell></row><row><cell>cat-LS</cell><cell>J</cell><cell>Floor Ceiling</cell><cell cols="3">7.5 6.7 4.4 7.0 11.8 3.4 3.5 2.5</cell><cell>5.9 6.6</cell></row><row><cell></cell><cell></cell><cell cols="3">Symmetrical 2.4 1.0</cell><cell>4.7 2.3</cell><cell>2.5</cell></row><row><cell></cell><cell></cell><cell>Floor</cell><cell cols="2">2.8 5.8</cell><cell>7.2 2.5</cell><cell>4.5</cell></row><row><cell></cell><cell>C</cell><cell>Ceiling</cell><cell cols="2">6.4 5.9</cell><cell>2.5 2.6</cell><cell>5.1</cell></row><row><cell></cell><cell></cell><cell cols="3">Symmetrical 1.5 0.6</cell><cell>3.0 1.6</cell><cell>1.6</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6</head><label>6</label><figDesc>Study 2: 95% CI coverage rates.</figDesc><table><row><cell></cell><cell>Parameter</cell><cell></cell><cell></cell><cell>Group</cell><cell>Overall</cell></row><row><cell>Estimator</cell><cell>group</cell><cell></cell><cell>K</cell><cell>mean</cell><cell>coverage</cell></row><row><cell></cell><cell></cell><cell>4</cell><cell>5</cell><cell>7</cell></row><row><cell></cell><cell>λ ex</cell><cell cols="3">68.2 74.5 80.6 74.5</cell></row><row><cell>cont-ML</cell><cell>λ en γ</cell><cell cols="3">65.5 72.3 78.2 72.0 94.6 94.6 94.6 94.6</cell><cell>     79.7</cell></row><row><cell></cell><cell>β</cell><cell cols="3">94.9 94.9 94.9 94.9</cell></row><row><cell></cell><cell>λ ex</cell><cell cols="3">70.0 78.6 87.6 78.7</cell></row><row><cell>cont-ML-adj</cell><cell>λ en γ</cell><cell cols="3">67.3 76.5 86.1 76.6 94.7 94.7 94.8 94.7</cell><cell>     82.8</cell></row><row><cell></cell><cell>β</cell><cell cols="3">95.0 94.9 95.0 95.0</cell></row><row><cell></cell><cell>λ ex</cell><cell cols="3">84.8 85.8 87.3 86.0</cell></row><row><cell>cat-LS</cell><cell>λ en γ</cell><cell cols="3">83.0 84.1 86.0 84.4 94.0 93.6 93.1 93.6</cell><cell>     87.7</cell></row><row><cell></cell><cell>β</cell><cell cols="3">94.4 93.9 93.4 93.9</cell></row><row><cell>Overall coverage</cell><cell></cell><cell cols="3">79.6 83.4 87.3</cell></row><row><cell cols="6">Note. cont-ML, cont-ML-adj, and cat-LS refer to model es-</cell></row><row><cell cols="6">timator. λ ex , λ en , γ, and β refer to groups of model param-</cell></row><row><cell cols="6">eters, containing, respectively, factor loadings for exogenous</cell></row><row><cell cols="6">and endogenous latent variables, regression coefficients among</cell></row><row><cell cols="6">exogenous and endogenous latent variables, and among en-</cell></row><row><cell cols="2">dogenous latent variables.</cell><cell cols="4">K= number of categories.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7</head><label>7</label><figDesc>Study 2: 95% CI coverage rates according to ordinal distributional form.</figDesc><table><row><cell></cell><cell>Underlying</cell><cell>Ordinal</cell><cell></cell><cell></cell></row><row><cell>Estimator</cell><cell>distribution</cell><cell>distribution</cell><cell></cell><cell cols="2">Parameter group</cell><cell>Overall</cell></row><row><cell></cell><cell></cell><cell></cell><cell>λex</cell><cell>λen</cell><cell>γ</cell><cell>β</cell></row><row><cell></cell><cell></cell><cell>Floor</cell><cell cols="3">79.0 76.9 94.6 95.1</cell><cell>83.0</cell></row><row><cell></cell><cell>N</cell><cell>Ceiling</cell><cell cols="3">79.6 76.4 95.0 95.5</cell><cell>83.1</cell></row><row><cell></cell><cell></cell><cell cols="4">Symmetrical 82.5 80.7 94.8 95.4</cell><cell>85.6</cell></row><row><cell>cont-ML</cell><cell>J</cell><cell>Floor Ceiling</cell><cell cols="3">86.1 85.0 94.2 94.3 39.7 34.8 93.8 94.2</cell><cell>88.2 54.5</cell></row><row><cell></cell><cell></cell><cell cols="4">Symmetrical 84.1 83.2 94.5 94.7</cell><cell>87.0</cell></row><row><cell></cell><cell></cell><cell>Floor</cell><cell cols="3">44.8 39.2 94.5 94.9</cell><cell>58.0</cell></row><row><cell></cell><cell>C</cell><cell>Ceiling</cell><cell cols="3">90.1 89.4 94.9 94.6</cell><cell>91.3</cell></row><row><cell></cell><cell></cell><cell cols="4">Symmetrical 84.1 82.4 95.0 95.0</cell><cell>86.8</cell></row><row><cell></cell><cell></cell><cell>Floor</cell><cell cols="3">79.6 77.5 94.7 95.3</cell><cell>83.5</cell></row><row><cell></cell><cell>N</cell><cell>Ceiling</cell><cell cols="3">80.0 77.1 94.9 95.5</cell><cell>83.5</cell></row><row><cell></cell><cell></cell><cell cols="4">Symmetrical 82.7 81.0 94.8 95.4</cell><cell>85.8</cell></row><row><cell>cont-ML-adj</cell><cell>J</cell><cell>Floor Ceiling</cell><cell cols="3">93.1 93.0 94.5 94.5 53.3 48.8 94.0 94.3</cell><cell>93.5 64.1</cell></row><row><cell></cell><cell></cell><cell cols="4">Symmetrical 84.8 84.2 94.6 94.8</cell><cell>87.6</cell></row><row><cell></cell><cell></cell><cell>Floor</cell><cell cols="3">56.7 51.4 94.6 94.9</cell><cell>66.3</cell></row><row><cell></cell><cell>C</cell><cell>Ceiling</cell><cell cols="3">93.8 93.5 95.1 94.8</cell><cell>94.1</cell></row><row><cell></cell><cell></cell><cell cols="4">Symmetrical 84.6 83.0 95.1 95.1</cell><cell>87.2</cell></row><row><cell></cell><cell></cell><cell>Floor</cell><cell cols="3">94.3 94.0 94.4 95.2</cell><cell>94.3</cell></row><row><cell></cell><cell>N</cell><cell>Ceiling</cell><cell cols="3">94.1 94.0 94.7 95.4</cell><cell>94.3</cell></row><row><cell></cell><cell></cell><cell cols="4">Symmetrical 94.2 94.0 94.6 95.3</cell><cell>94.3</cell></row><row><cell>cat-LS</cell><cell>J</cell><cell>Floor Ceiling</cell><cell cols="3">66.4 62.5 91.6 91.8 82.9 79.6 93.6 93.7</cell><cell>72.6 84.9</cell></row><row><cell></cell><cell></cell><cell cols="4">Symmetrical 89.8 89.9 91.8 92.2</cell><cell>90.5</cell></row><row><cell></cell><cell></cell><cell>Floor</cell><cell cols="3">86.8 83.7 94.3 94.6</cell><cell>87.9</cell></row><row><cell></cell><cell>C</cell><cell>Ceiling</cell><cell cols="3">73.0 69.7 93.6 93.4</cell><cell>78.0</cell></row><row><cell></cell><cell></cell><cell cols="4">Symmetrical 92.2 91.7 93.5 93.6</cell><cell>92.4</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8</head><label>8</label><figDesc>Study 3: Rejection rates at the α = 0.05 level of significance of the bootstrap test for underlying normality.</figDesc><table><row><cell>Underlying</cell><cell></cell><cell></cell><cell></cell></row><row><cell>distribution K</cell><cell>n</cell><cell cols="3">Floor Ceiling Symm.</cell></row><row><cell></cell><cell cols="2">100 0.08</cell><cell>0.08</cell><cell>0.11</cell></row><row><cell>4</cell><cell cols="2">300 0.05</cell><cell>0.05</cell><cell>0.07</cell></row><row><cell>N</cell><cell cols="2">500 0.05 100 0.18</cell><cell>0.05 0.13</cell><cell>0.06 0.29</cell></row><row><cell>7</cell><cell cols="2">300 0.07</cell><cell>0.06</cell><cell>0.08</cell></row><row><cell></cell><cell cols="2">500 0.05</cell><cell>0.05</cell><cell>0.08</cell></row><row><cell></cell><cell cols="2">100 0.92</cell><cell>0.89</cell><cell>0.96</cell></row><row><cell>4</cell><cell cols="2">300 1.00</cell><cell>1.00</cell><cell>1.00</cell></row><row><cell>J</cell><cell cols="2">500 1.00 100 0.85</cell><cell>1.00 0.99</cell><cell>1.00 0.98</cell></row><row><cell>7</cell><cell cols="2">300 1.00</cell><cell>1.00</cell><cell>1.00</cell></row><row><cell></cell><cell cols="2">500 1.00</cell><cell>1.00</cell><cell>1.00</cell></row><row><cell></cell><cell cols="2">100 0.76</cell><cell>0.66</cell><cell>0.80</cell></row><row><cell>4</cell><cell cols="2">300 1.00</cell><cell>1.00</cell><cell>1.00</cell></row><row><cell>C</cell><cell cols="2">500 1.00 100 0.96</cell><cell>1.00 0.68</cell><cell>1.00 0.88</cell></row><row><cell>7</cell><cell cols="2">300 1.00</cell><cell>1.00</cell><cell>1.00</cell></row><row><cell></cell><cell cols="2">500 1.00</cell><cell>1.00</cell><cell>1.00</cell></row></table><note><p>Note. N, J, and C refer to the underlying continuous distribution being normal, Joe VITA, and Clayton VITA, respectively. Floor, Ceiling, and Symmetrical refer to the ordinal distributional shape. K= number of categories. n= sample size.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>o P (1) = o P (1), that o P (1) + o P (1) = o P (1), and that for any random variable Y, we have Y + o P (1) = O P (1) (Van der Vaart, 2000, see e.g., Chapter 2.2). For any 1</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>In Mplus(Muthén &amp; Muthén,  </p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1"><p>2012) cont-ML and cat-LS are available as MLR and WLSMV estimation, respectively. In lavaan<ref type="bibr" target="#b62">(Rosseel, 2012)</ref> the cont-ML is obtained with "estima-</p></note>
		</body>
		<back>

			
			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank the associate editor and the reviewers for thoughtful and constructive feedback.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>∞ -∞</head><p>I{x 3 ≤ z 3 }C J (σ 1|3 Φ -1 (z 1 ) -ρ 1,3 x 3 , σ 2|3 Φ -1 (z 2 ) -ρ 2,3 x 3 ; θ)φ(x 3 ) dx 3 would be a valid distribution function. The remaining components of the distribution would still be normal, but the copula of the conditional distribution of (Z 1 , Z 2 ) conditioned on Z 3 would be a highly non-normal Joe copula. By the same types of steps, we could have changed the marginals, as well as the bivariate distributions of (Z 1 , Z 3 ) and (Z 2 , Z 3 ). The resulting distribution would be highly non-normal, even though it still would fulfill the algebraic similarity to the normal distribution by fulfilling the simplifying assumption, i.e., that the conditional distribution of Z 1 , Z 2 given Z 3 has a copula that does not change with the value of Z 3 .</p><p>In general, vine copulas can be formed by emulating the formulas for the expanded versions of the distribution of a multivariate normal distribution in terms of conditional and unconditional bivariate distributions, and then, while preserving the simplifying assumption, changing the normal copulas to copulas of our choosing. The only normal characteristic then remaining is the simplifying assumption. Without this assumption, the connecting of sequences of the conditional distributions would be much more complicated. In the tri-variate case, we could for example have expanded the distribution in terms of (Z 1 , Z 2 ), (Z 1 , Z 3 ) and then the conditional distribution of (Z 1 , Z 3 ) conditional on Z 2 . In higher dimensions, there are many options for such expansions.</p><p>The expansion of a copula distribution into conditional and unconditional bivariate distributions can be represented by a sequence of trees, called a vine. A four-dimensional example of such an expansion is visualized in Figure <ref type="figure">14</ref>. Let the resulting copula distribution be C, and let (U 1 , U 2 , U 3 , U 4 ) ∼ C. Figure <ref type="figure">14</ref> shows a vine comprised by three trees.</p><p>The lowest tree specifies which unconditional bivariate distributions are to be specified by bivariate copulas. The figure shows that we are required to directly specify the bivariate distributions of (U 1 , U 2 ), of (U 2 , U 3 ) and of (U 3 , U 4 ). The next tree knits together the copulas from the first tree, through saying that we are further to specify the copula of the conditional distribution of (U 1 , U 3 ) given U 2 , as well as the copula of the conditional distribution of (U 2 , U 3 ) given U 3 . These copulas are not to depend on the conditioning variables, thereby fulfilling the same type of simplifying assumption which we saw was fulfilled by the normal distribution. As in that case, the conditional marginal distributions will in general depend on the conditioning variables, but these conditional marginal distributions are consequences of the choices at the first level, and have therefore already been indirectly specified. The final level of the tree completes the linkage of a complete multivariate distribution, and requires the specification of a bivariate copula for the conditional distribution of (U 1 , U 4 ) when we condition on the remaining variables, i.e., (U 2 , U 3 ).</p><p>There are other four-dimensional vines, and with higher dimensions there is a great number of possible vine configurations. Each bivariate copula distribution represented in the figure is a copula distribution that we can choose freely, and the resulting distribution will be valid. In our applications for simplicity we considered only cases where every copula is of the same class, and fix the dependence parameters of the copulas separately in order to control the covariance matrix of the resulting full distribution once we attach marginal distributions to the copula. Since there are as many copulas as there are elements in a correlation matrix, this matching is often possible. This idea is the core idea of the VITA method of <ref type="bibr" target="#b73">Foldnes and Grønneberg (2017)</ref>, where the practical and computational details for achieving this match is discussed. Through certain simplifications originating from the simplifying assumption, simulation is easily implemented without the need for numerical integration, at least when using common copulas such as the Joe or Clayton copula, as shown in <ref type="bibr" target="#b78">Joe (1996)</ref> and discussed in <ref type="bibr" target="#b70">Dissmann et al. (2013)</ref>; <ref type="bibr">Schepsmeier et al. (2018)</ref>. The R package covsim <ref type="bibr">(Foldnes &amp; Grønneberg, 2020</ref>) can be used for identifying a vine with dependency parameters so that for given marginal distributions and for a given sequence of bivariate copulas, their combination into a full vine distribution results in a distribution that has a pre-specified covariance matrix.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Testing normality of latent variables in the polychoric correlation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mouchart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistica</title>
		<imprint>
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="25" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Simple second order chisquare correction</title>
		<author>
			<persName><forename type="first">T</forename><surname>Asparouhov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Muthén</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mplus technical appendix</title>
		<imprint>
			<biblScope unit="page" from="1" to="8" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The sensitivity of confirmatory maximum likelihood factor analysis to violations of measurement scale and distributional assumptions</title>
		<author>
			<persName><forename type="first">E</forename><surname>Babakus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Ferguson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">G</forename><surname>Jöreskog</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of marketing research</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="222" to="228" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">On the performance of maximum likelihood versus means and variance adjusted weighted least squares estimation in cfa</title>
		<author>
			<persName><forename type="first">A</forename><surname>Beauducel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">Y</forename><surname>Herzberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Structural Equation Modeling: A Multidisciplinary Journal</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="186" to="203" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Vines-a new graphical model for dependent random variables</title>
		<author>
			<persName><forename type="first">T</forename><surname>Bedford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Cooke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1031" to="1068" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Eqs 6 structural equations program manual</title>
		<author>
			<persName><forename type="first">P</forename><surname>Bentler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>Multivariate Software, Inc</publisher>
			<pubPlace>Encino, CA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Structural equations with latent variables</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Bollen</surname></persName>
		</author>
		<idno type="DOI">10.1002/9781118619179</idno>
		<imprint>
			<date type="published" when="1989">1989</date>
			<publisher>Wiley</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">V</forename><surname>Bradley</surname></persName>
		</author>
		<title level="m">Robustness? British Journal of Mathematical and Statistical Psychology</title>
		<imprint>
			<date type="published" when="1978">1978</date>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="144" to="152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Development and validation of seer (seeking, engaging with and evaluating research): a measure of policymakers&apos; capacity to engage with and use research</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Brennan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Mckenzie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Redman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Makkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Williamson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">.</forename><forename type="middle">.</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Health research policy and systems</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Asymptotically distribution-free methods for the analysis of covariance structures</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Browne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">British Journal of Mathematical and Statistical Psychology</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="62" to="83" />
			<date type="published" when="1984">1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Two-step weighted least squares factor analysis of dichotomized variables</title>
		<author>
			<persName><forename type="first">A</forename><surname>Christoffersson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychometrika</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="433" to="438" />
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Additive structure in qualitative data: An alternating least squares method with optimal scaling features</title>
		<author>
			<persName><forename type="first">J</forename><surname>De Leeuw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">W</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Takane</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychometrika</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="471" to="503" />
			<date type="published" when="1976">1976</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The impact of categorization with confirmatory factor analysis</title>
		<author>
			<persName><forename type="first">C</forename><surname>Distefano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Structural equation modeling</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="327" to="346" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">An empirical evaluation of alternative methods of estimation for confirmatory factor analysis with ordinal data</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Flora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Curran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological methods</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="466" to="491" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">covsim: Simulate from distributions with given covariance matrix and marginal information</title>
		<author>
			<persName><forename type="first">N</forename><surname>Foldnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Grønneberg</surname></persName>
		</author>
		<ptr target="https://CRAN.R-project.org/package=covsim" />
		<imprint>
			<date type="published" when="2020">2020a</date>
		</imprint>
	</monogr>
	<note>Computer software manual. R package version 0.1</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">). discnorm: Test for discretized normality in ordinal data</title>
		<author>
			<persName><forename type="first">N</forename><surname>Foldnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Grønneberg</surname></persName>
		</author>
		<ptr target="https://CRAN.R-project.org/package=discnorm" />
		<imprint>
			<date type="published" when="2020">2020b</date>
		</imprint>
	</monogr>
	<note>Computer software manual. R package version 0.1.0</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">How general is the Vale-Maurelli simulation approach?</title>
		<author>
			<persName><forename type="first">N</forename><surname>Foldnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Grønneberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychometrika</title>
		<imprint>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1066" to="1083" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">On identification and nonnormal simulation in ordinal covariance and item response models</title>
		<author>
			<persName><forename type="first">N</forename><surname>Foldnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Grønneberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychometrika</title>
		<imprint>
			<biblScope unit="volume">84</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1000" to="1017" />
			<date type="published" when="2019">2019a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Pernicious polychorics: The impact and detection of underlying non-normality</title>
		<author>
			<persName><forename type="first">N</forename><surname>Foldnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Grønneberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Structural Equation Modeling: A Multidisciplinary Journal</title>
		<imprint>
			<biblScope unit="page" from="1" to="19" />
			<date type="published" when="2019">2019b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A simple simulation technique for nonnormal data with prespecified skewness, kurtosis, and covariance matrix</title>
		<author>
			<persName><forename type="first">N</forename><surname>Foldnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><forename type="middle">H</forename><surname>Olsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multivariate behavioral research</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="207" to="219" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Factor analysis with ordinal indicators: A monte carlo study comparing dwls and uls estimation</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">G</forename><surname>Forero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Maydeu-Olivares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gallardo-Pujol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Structural Equation Modeling: A Multidisciplinary Journal</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="625" to="641" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Dimensional comparisons: How academic track students&apos; achievements are related to their expectancy and value beliefs across multiple domains</title>
		<author>
			<persName><forename type="first">H</forename><surname>Gaspard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Wigfield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Nagengast</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Trautwein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">W</forename><surname>Marsh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Contemporary Educational Psychology</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">An algorithm for converting ordinal scale measurement data to interval/ratio scale</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Granberg-Rademacker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Educational and Psychological Measurement</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="74" to="90" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Covariance model simulation using regular vines</title>
		<author>
			<persName><forename type="first">S</forename><surname>Grønneberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Foldnes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychometrika</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1035" to="1051" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A problem with discretizing Vale-Maurelli in simulation studies</title>
		<author>
			<persName><forename type="first">S</forename><surname>Grønneberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Foldnes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychometrika</title>
		<imprint>
			<biblScope unit="volume">84</biblScope>
			<biblScope unit="page" from="554" to="561" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Rescaling ordinal data to interval data in educational research</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Harwell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">G</forename><surname>Gatti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Review of Educational Research</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="105" to="131" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">copula: Multivariate dependence with copulas</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hofert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Kojadinovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Maechler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<ptr target="http://CRAN.R-project.org/package=copula" />
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note>Computer software manual. R package version 0.999-7</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Hoogland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Boomsma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Robustness studies in covariance structure modeling: An overview and a metaanalysis</title>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="329" to="367" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A simulation study of polychoric instrumental variable estimation in structural equation models</title>
		<author>
			<persName><forename type="first">S</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Yang-Wallentin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Structural Equation Modeling: A Multidisciplinary Journal</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="680" to="694" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Asymptotic robustness study of the polychoric correlation estimation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Yang-Wallentin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychometrika</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="67" to="85" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Ordinal measures in multiple indicator models: A simulation study of categorization error</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Creech</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American Sociological Review</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="398" to="407" />
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Some contributions to maximum likelihood factor analysis</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">G</forename><surname>Jöreskog</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychometrika</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="443" to="482" />
			<date type="published" when="1967">1967</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Structural equation modeling with ordinal variables</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">G</forename><surname>Jöreskog</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Lecture Notes-Monograph Series</title>
		<imprint>
			<biblScope unit="page" from="297" to="310" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">G</forename><surname>Jöreskog</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sörbom</surname></persName>
		</author>
		<title level="m">Lisrel 9.20 for windows</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note>computer software</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">IL: Scientific Software International</title>
		<author>
			<persName><surname>Skokie</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Inc.</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Enhancing the validity and cross-cultural comparability of measurement in survey research</title>
		<author>
			<persName><forename type="first">G</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Salomon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tandon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American political science review</title>
		<imprint>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="567" to="583" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Continuous multivariate distributions</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kotz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Balakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">L</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Models and applications</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<date type="published" when="2004">2004</date>
			<publisher>John Wiley &amp; Sons</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Confirmatory factor analysis with ordinal data: Comparing robust maximum likelihood and diagonally weighted least squares</title>
		<author>
			<persName><forename type="first">C.-H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behavior Research Methods</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="936" to="949" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">The performance of ml, dwls, and uls estimation with robust corrections in structural equation models with ordinal variables</title>
		<author>
			<persName><forename type="first">C.-H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological methods</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">369</biblScope>
			<date type="published" when="2016">2016b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Testing measurement invariance in longitudinal data with ordered-categorical measures</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Millsap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">G</forename><surname>West</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-Y</forename><surname>Tein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tanaka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J</forename><surname>Grimm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological methods</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">486</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Factorial, convergent, and discriminant validity of timss math and science motivation measures: A comparison of arab and anglo-saxon countries</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">W</forename><surname>Marsh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Abduljabbar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Abu-Hilal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Morin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Abdelfattah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">C</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">.</forename><forename type="middle">.</forename><surname>Parker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Educational Psychology</title>
		<imprint>
			<biblScope unit="volume">105</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">108</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Passion: Does one scale fit all? construct validity of two-factor passion scale and psychometric invariance over different activities and languages</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">W</forename><surname>Marsh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Vallerand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-A</forename><forename type="middle">K</forename><surname>Lafrenière</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Parker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Morin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Carbonneau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">.</forename><forename type="middle">.</forename><surname>Others</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Assessment</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">796</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Limited information estimation and testing of discretized multivariate normal structural models</title>
		<author>
			<persName><forename type="first">A</forename><surname>Maydeu-Olivares</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychometrika</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="57" to="77" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Contributions to estimation of polychoric correlations</title>
		<author>
			<persName><forename type="first">S</forename><surname>Monroe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multivariate behavioral research</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="247" to="266" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Sample size requirements of the robust weighted least squares estimator</title>
		<author>
			<persName><forename type="first">M</forename><surname>Moshagen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Musch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Methodology</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="60" to="70" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">A general structural equation model with dichotomous, ordered categorical, and continuous latent variable indicators</title>
		<author>
			<persName><forename type="first">B</forename><surname>Muthén</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychometrika</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="115" to="132" />
			<date type="published" when="1984">1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">A comparison of some methodologies for the factor analysis of non-normal likert variables</title>
		<author>
			<persName><forename type="first">B</forename><surname>Muthén</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kaplan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">British Journal of Mathematical and Statistical Psychology</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="171" to="189" />
			<date type="published" when="1985">1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Mplus version 7: User&apos;s guide</title>
		<author>
			<persName><forename type="first">B</forename><surname>Muthén</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Muthén</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>Muthén &amp; Muthén</publisher>
			<pubPlace>Los Angeles, CA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Goodness of fit with categorical and other nonnormal variables</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">O</forename><surname>Muthén</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SAGE Focus Editions</title>
		<imprint>
			<biblScope unit="volume">154</biblScope>
			<biblScope unit="page" from="205" to="205" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">rvinecopulib: High performance algorithms for vine copula modeling</title>
		<author>
			<persName><forename type="first">T</forename><surname>Nagler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Vatter</surname></persName>
		</author>
		<ptr target="https://CRAN.R-project.org/package=rvinecopulib" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>Computer software manual. R package version 0.5.1.1.0</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Comparing interval estimates for small sample ordinal CFA models</title>
		<author>
			<persName><forename type="first">P</forename><surname>Natesan</surname></persName>
		</author>
		<idno type="DOI">10.3389/fpsyg.2015.01599</idno>
	</analytic>
	<monogr>
		<title level="j">Frontiers in psychology</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">1599</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Examining the factor structure of the self-compassion scale in four distinct populations: Is the use of a total scale score justified?</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">D</forename><surname>Neff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">A</forename><surname>Whittaker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Karl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>Journal of Personality Assessment</publisher>
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="page" from="596" to="607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">An introduction to copulas</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Nelsen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">A Monte Carlo study comparing PIV, ULS and DWLS in the estimation of dichotomous confirmatory factor analysis</title>
		<author>
			<persName><forename type="first">S</forename><surname>Nestler</surname></persName>
		</author>
		<idno type="DOI">10.1111/j.2044-8317.2012.02044.x</idno>
	</analytic>
	<monogr>
		<title level="j">British Journal of Mathematical and Statistical Psychology</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="127" to="143" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Maximum likelihood estimation of the polychoric correlation coefficient</title>
		<author>
			<persName><forename type="first">U</forename><surname>Olsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychometrika</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="443" to="460" />
			<date type="published" when="1979">1979a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">On the robustness of factor analysis against crude classification of the observations</title>
		<author>
			<persName><forename type="first">U</forename><surname>Olsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multivariate behavioral research</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="485" to="500" />
			<date type="published" when="1979">1979b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Mathematical contributions to the theory of evolution. vii. on the correlation of characters not quantitatively measurable</title>
		<author>
			<persName><forename type="first">K</forename><surname>Pearson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Philos. Trans. R. Soc. SA</title>
		<imprint>
			<biblScope unit="volume">196</biblScope>
			<biblScope unit="page" from="1" to="47" />
			<date type="published" when="1900">1900</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">On polychoric coefficients of correlation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Pearson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">S</forename><surname>Pearson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="127" to="156" />
			<date type="published" when="1922">1922</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Studies of the polychoric correlation and other correlation measures for ordinal variables. (Unpublished doctoral dissertation)</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Quiroga</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994">1994</date>
			<pubPlace>Uppsala University</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">R: A language and environment for statistical computing</title>
		<author>
			<orgName type="collaboration">R Core Team.</orgName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>Computer software manual</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">When can categorical variables be treated as continuous? a comparison of robust continuous and categorical SEM estimation methods under suboptimal conditions</title>
		<author>
			<persName><forename type="first">M</forename><surname>Brosseau-Liard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">É</forename><surname>Savalei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename></persName>
		</author>
		<ptr target="https://www.R-project.org/Rhemtulla," />
	</analytic>
	<monogr>
		<title level="j">Psychological methods</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">354</biblScope>
			<date type="published" when="2012">2012</date>
			<pubPlace>Vienna, Austria</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">sirt: Supplementary item response theory models</title>
		<author>
			<persName><forename type="first">A</forename><surname>Robitzsch</surname></persName>
		</author>
		<ptr target="https://CRAN.R-project.org/package=sirt" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>Computer software manual. R package version 3.4-64</note>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">lavaan: An R package for structural equation modeling</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Rosseel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Statistical Software</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="36" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Scaling corrections for statistics in covariance structure analysis (UCLA statistics series 2)</title>
		<author>
			<persName><forename type="first">A</forename><surname>Satorra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bentler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1988">1988</date>
			<pubPlace>Los Angeles</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of California at Los Angeles, Department of Psychology</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Vinecopula: Statistical inference of vine copulas</title>
		<author>
			<persName><forename type="first">U</forename><surname>Schepsmeier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Stoeber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">C</forename><surname>Brechmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Graeler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Nagler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Erhardt</surname></persName>
		</author>
		<ptr target="https://CRAN.R-project.org/package=VineCopula" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>Computer software manual. R package version 2.1.8</note>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Simulating multivariate nonnormal distributions</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Vale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">A</forename><surname>Maurelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychometrika</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="465" to="471" />
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Evaluation of structural equation modeling estimates of reliability for scales with ordered categorical items</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">B</forename><surname>Green</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Methodology</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="23" to="34" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note>References</note>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Probability density decomposition for conditionally dependent random variables modeled by vines</title>
		<author>
			<persName><forename type="first">T</forename><surname>Bedford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Cooke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Mathematics and Artificial intelligence</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">1-4</biblScope>
			<biblScope unit="page" from="245" to="268" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Vines-a new graphical model for dependent random variables</title>
		<author>
			<persName><forename type="first">T</forename><surname>Bedford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Cooke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1031" to="1068" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">Probability and measure</title>
		<author>
			<persName><forename type="first">P</forename><surname>Billingsley</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995">1995</date>
			<publisher>John Wiley &amp; Sons</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Selecting and estimating regular vine copulae and application to financial returns</title>
		<author>
			<persName><forename type="first">J</forename><surname>Dissmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">C</forename><surname>Brechmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Czado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kurowicka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Statistics &amp; Data Analysis</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="page" from="52" to="69" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<author>
			<persName><forename type="first">W</forename><surname>Feller</surname></persName>
		</author>
		<title level="m">An introduction to probability theory and its applications</title>
		<imprint>
			<publisher>John Wiley &amp; Sons</publisher>
			<date type="published" when="1968">1968</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note>rd ed.</note>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title level="m" type="main">covsim: Simulate from distributions with given covariance matrix and marginal information</title>
		<author>
			<persName><forename type="first">N</forename><surname>Foldnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Grønneberg</surname></persName>
		</author>
		<ptr target="https://CRAN.R-project.org/package=covsim" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>Computer software manual. R package version 0.1</note>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">The asymptotic covariance matrix and its use in simulation studies</title>
		<author>
			<persName><forename type="first">N</forename><surname>Foldnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Grønneberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Structural Equation Modeling: A Multidisciplinary Journal</title>
		<imprint>
			<biblScope unit="page" from="1" to="16" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">On identification and nonnormal simulation in ordinal covariance and item response models</title>
		<author>
			<persName><forename type="first">N</forename><surname>Foldnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Grønneberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychometrika</title>
		<imprint>
			<biblScope unit="volume">84</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1000" to="1017" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Covariance model simulation using regular vines</title>
		<author>
			<persName><forename type="first">S</forename><surname>Grønneberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Foldnes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychometrika</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1035" to="1051" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Partial identification of latent correlations with binary data</title>
		<author>
			<persName><forename type="first">S</forename><surname>Grønneberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Moss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Foldnes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychometrika</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>submitted</note>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
		<title level="m" type="main">copula: Multivariate dependence with copulas</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hofert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Kojadinovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Maechler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<ptr target="http://CRAN.R-project.org/package=copula" />
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note>Computer software manual. R package version 0.999-7</note>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Families of m-variate distributions with given mar-gins and m (m-1)/2 bivariate dependence parameters</title>
		<author>
			<persName><forename type="first">H</forename><surname>Joe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Lecture Notes-Monograph Series</title>
		<imprint>
			<biblScope unit="page" from="120" to="141" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
		<title level="m" type="main">Dependence modeling with copulas</title>
		<author>
			<persName><forename type="first">H</forename><surname>Joe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>CRC Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<monogr>
		<title level="m" type="main">Partial identification of probability distributions</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">F</forename><surname>Manski</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">V</forename><surname>Mardia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Kent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Bibby</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1979">1979</date>
			<publisher>Academic Press</publisher>
		</imprint>
	</monogr>
	<note>Multivariate analysis</note>
</biblStruct>

<biblStruct xml:id="b82">
	<monogr>
		<title level="m" type="main">rvinecopulib: High performance algorithms for vine copula modeling</title>
		<author>
			<persName><forename type="first">T</forename><surname>Nagler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Vatter</surname></persName>
		</author>
		<ptr target="https://CRAN.R-project.org/package=rvinecopulib" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>Computer software manual. R package version 0.5.1.1.0</note>
</biblStruct>

<biblStruct xml:id="b83">
	<monogr>
		<title level="m" type="main">An introduction to copulas</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Nelsen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<monogr>
		<title level="m" type="main">R: A language and environment for statistical computing</title>
		<author>
			<orgName type="collaboration">R Core Team.</orgName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>Computer software manual</note>
</biblStruct>

<biblStruct xml:id="b85">
	<monogr>
		<ptr target="https://www.R-project.org/Rudin" />
		<title level="m">Principles of mathematical analysis</title>
		<meeting><address><addrLine>Vienna, Austria; New York</addrLine></address></meeting>
		<imprint>
			<publisher>McGraw-Hill</publisher>
			<date type="published" when="1976">1976</date>
		</imprint>
	</monogr>
	<note>rd ed.</note>
</biblStruct>

<biblStruct xml:id="b86">
	<monogr>
		<title level="m" type="main">Vinecopula: Statistical inference of vine copulas</title>
		<author>
			<persName><forename type="first">U</forename><surname>Schepsmeier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Stoeber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">C</forename><surname>Brechmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Graeler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Nagler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Erhardt</surname></persName>
		</author>
		<ptr target="https://CRAN.R-project.org/package=VineCopula" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>Computer software manual. R package version 2.1.8</note>
</biblStruct>

<biblStruct xml:id="b87">
	<monogr>
		<title level="m" type="main">Fonctions de repartition a n dimensions et leurs marges</title>
		<author>
			<persName><forename type="first">M</forename><surname>Sklar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1959">1959</date>
			<biblScope unit="volume">8</biblScope>
		</imprint>
		<respStmt>
			<orgName>Université Paris</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Van Der Vaart</surname></persName>
		</author>
		<title level="m">Asymptotic statistics</title>
		<imprint>
			<publisher>Cambridge university press</publisher>
			<date type="published" when="2000">2000</date>
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
