<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Path Models of Vocal Emotion Communication</title>
				<funder ref="#_EKEyR7j">
					<orgName type="full">Swiss National Science Foundation</orgName>
				</funder>
				<funder ref="#_wdEUswa">
					<orgName type="full">European Research Council</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2015-09-01">September 1, 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Tanja</forename><surname>Bänziger</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Psychology</orgName>
								<orgName type="institution">Mid Sweden University</orgName>
								<address>
									<settlement>Östersund</settlement>
									<country key="SE">Sweden</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Georg</forename><surname>Hosoya</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Educational Science and Psychology</orgName>
								<orgName type="institution">Freie Universität</orgName>
								<address>
									<settlement>Berlin</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Klaus</forename><forename type="middle">R</forename><surname>Scherer</surname></persName>
							<email>klaus.scherer@unige.ch</email>
							<affiliation key="aff2">
								<orgName type="department">Swiss Centre for Affective Sciences</orgName>
								<orgName type="institution">University of Geneva</orgName>
								<address>
									<settlement>Geneva</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">University of Sussex</orgName>
								<address>
									<country key="GB">UNITED KINGDOM</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Path Models of Vocal Emotion Communication</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2015-09-01">September 1, 2015</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1371/journal.pone.0136675</idno>
					<note type="submission">Received: March 9, 2015 Accepted: August 6, 2015</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-21T21:39+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose to use a comprehensive path model of vocal emotion communication, encompassing encoding, transmission, and decoding processes, to empirically model data sets on emotion expression and recognition. The utility of the approach is demonstrated for two data sets from two different cultures and languages, based on corpora of vocal emotion enactment by professional actors and emotion inference by naïve listeners. Lens model equations, hierarchical regression, and multivariate path analysis are used to compare the relative contributions of objectively measured acoustic cues in the enacted expressions and subjective voice cues as perceived by listeners to the variance in emotion inference from vocal expressions for four emotion families (fear, anger, happiness, and sadness). While the results confirm the central role of arousal in vocal emotion communication, the utility of applying an extended path modeling framework is demonstrated by the identification of unique combinations of distal cues and proximal percepts carrying information about specific emotion families, independent of arousal. The statistical models generated show that more sophisticated acoustic parameters need to be developed to explain the distal underpinnings of subjective voice quality percepts that account for much of the variance in emotion inference, in particular voice instability and roughness. The general approach advocated here, as well as the specific results, open up new research strategies for work in psychology (specifically emotion and social perception research) and engineering and computer science (specifically research and development in the domain of affective computing, particularly on automatic emotion detection and synthetic emotion expression in avatars).</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Accurately inferring the emotions of others in social interactions is extremely important, as it permits an understanding of the expresser's reaction to preceding events or behaviors and a prediction of the expresser's action tendencies and thus facilitates communication and interpersonal adjustment <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>. In consequence, the study of emotion expression and perception has become a major research area over the last 60 years and has played an important part in the development of emotion psychology as an interdisciplinary research area.</p><p>Emotions can be successfully communicated through vocal expressions alone (see reviews in <ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref>), but we still know little about the processes and mechanisms that allow humans to communicate emotions through vocal expressions <ref type="bibr" target="#b5">[6]</ref>. In particular, the nature of voice characteristics (also referred to as vocal cues or vocal features) responsible for successfully expressing and recognizing emotions in vocal utterances are not yet well understood.</p><p>The study of nonverbal communication of emotion through voice and speech has been examined in past decades by focusing on either acoustic descriptions (e.g. <ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref>; and Table A in S1 File-Appendix) or recognition of emotions by listeners (e.g. <ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref>). Reviews of the field <ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref> often refer to these two approaches as encoding studies, focusing on the acoustic description of emotional vocalizations, and decoding studies, focusing on emotion recognition or discrimination by listeners.</p><p>A recent, comprehensive review of studies on facial, vocal, gestural, and multimodal emotion communication <ref type="bibr" target="#b4">[5]</ref> calls attention to the following concerns: 1) Emotion expression (encoding) and emotion perception (decoding) are rarely studied in combination (and recognition studies are far more numerous than studies on the production of emotional expressions). As a consequence of the separation of these two central aspects of the communication process the underlying mechanisms, especially the nature of the cues used in emotion perception and inference, cannot be appropriately assessed. 2) Both encoding and decoding studies tend to focus on highly prototypical expressions of a handful of basic emotions. This raises important concerns: Prototypical expressions tend to increase the risk of stereotypical use of major emotion dimensions-especially valence (e.g., pleasantness-unpleasantness; <ref type="bibr" target="#b16">[17]</ref>) in the case of facial expression and arousal in the case of vocal expression (see Table A in S1 File-Appendix). Arousal refers primarily to the physiological activation associated with emotional reactions and can be considered as a dimension ranging from intense activation to calmness or even sleep <ref type="bibr" target="#b17">[18]</ref>. This bias, in addition to the few emotion alternatives generally provided for judgment in recognition studies, may lead to guessing and classification by exclusion in recognition studies ( <ref type="bibr" target="#b4">[5]</ref>; p. 415). Thus, the state of the art of research on vocal communication can be briefly characterized as follows: A handful of encoding studies shows that actors vocally enacting a relatively small number of basic emotions produce differentiated patterns of vocal parameters for different emotions (with a preponderance of arousal-related parameters). A rather large number of decoding or recognition studies shows that naïve judges recognize portrayals of a relatively small number of basic emotions with better than chance accuracy (although effects of guessing and classification by exclusion, based on arousal cues, cannot be excluded). Therefore a more comprehensive, integrative approach is needed to advance research on the mechanisms underlying the vocal communication process.</p><p>Here we examine the utility and feasibility of studying the encoding, transmission, and decoding phases of the vocal emotion communication process by using a Brunswikian lens model approach which is particularly well suited for this purpose as it allows combining encoding and decoding processes. In particular, we show that comprehensive models and their quantitative testing provide an important impetus for future research in this area, not only by providing a more theoretically adequate framework that allows hypothesis testing and accumulation of results, but also by pointing to areas where further method development is urgently required (e.g. the development of reliable measurement for new acoustic parameters that can be expected to correlate with voice quality perception).</p><p>We first describe the general framework provided by the lens model (with a focus on the variants of the model that are used for the analysis presented in this article). We then outline the statistical models that can be used for empirical model testing.</p><p>Theoretical models-from Brunswik's lens model to the TEEP Brunswik <ref type="bibr" target="#b18">[19]</ref> proposed that successful adjustment to an uncertain, constantly changing world requires the organism to rely on probabilistic inference mechanisms using multiple pieces of uncertain evidence (proximal cues) about the world (the distal object). He illustrated this process by a lens-shaped model in which a fan-shaped array of probabilistic sensory cues for a distal object are utilized to form a singular judgment about the object. The fit of this subjective judgment with world reality he called ecological validity. Brunswik originally focused on visual perception, but already proposed several variants of his lens model applied to the study of interpersonal perception. The simplest version of the lens model in this domain includes (a) a distal "object" (when applied to vocal communication of emotion, the emotion experienced by the speaker), (b) a number of observable and measurable cues (the vocal features affected by the emotion and used by the listener to infer the emotion), and (c) a perception or perceptual judgment from a human observer (the emotional attributions made by the listener). Examples of studies of interpersonal perception that make explicit reference to the lens model include analysis of the nonverbal communication of interpersonal dispositions <ref type="bibr" target="#b19">[20]</ref>, perception of intelligence from audio-video recording <ref type="bibr" target="#b20">[21]</ref>, perceived "quality of relationship" <ref type="bibr" target="#b21">[22]</ref>, and several recent studies on personality expression and inference, such as self-other agreement at zero acquaintance <ref type="bibr" target="#b22">[23]</ref>, hindsight effects and knowledge updating <ref type="bibr" target="#b23">[24]</ref>, the perception of trustworthiness from faces <ref type="bibr" target="#b24">[25]</ref>, behavioral cues of overconfidence <ref type="bibr" target="#b25">[26]</ref>, and vocal cues of hierarchical rank <ref type="bibr" target="#b26">[27]</ref>. Juslin and his collaborators have used this functional approach to cue utilization in studying emotion communication in music performances <ref type="bibr" target="#b27">[28]</ref><ref type="bibr" target="#b28">[29]</ref><ref type="bibr" target="#b29">[30]</ref><ref type="bibr" target="#b30">[31]</ref> and for the encoding and decoding of vocal emotions <ref type="bibr" target="#b31">[32]</ref>.</p><p>In an early study on the expression and perception of personality in the speaking voice, Scherer <ref type="bibr" target="#b32">[33]</ref> proposed and tested an extension of the lens model in which the cue domain is separated into (a) distal, objectively measurable cues (such as acoustic voice parameters for the speaker) and (b) subjective, proximal percepts of these cues (such as voice quality impressions formed by the listener). The major justification for this extension is that in perception and communication, the objectively measurable cues in nonverbal behavior are subject to a transmission process from sender to receiver (often adding noise) and need to be processed and adequately transformed by the sensorium of the receiver. A comprehensive model of the communication process requires conceptualization and empirical measurement of this transmission process ( <ref type="bibr" target="#b3">[4]</ref>; see also <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b34">35]</ref>).</p><p>More recently, Scherer <ref type="bibr" target="#b35">[36]</ref> has formalized the earlier suggestion for an extension of the lens model as a tripartite emotion expression and perception (TEEP) model (see <ref type="bibr">Fig 1)</ref>. The communication process is represented by four elements (emoter/sender, distal cues, proximal percepts and observer) and three phases (externalization driven by external models and internal changes, transmission, cue utilization driven by inference rules and schematic recognition). Applying this model to our specific research questions, the internal state of the speaker (e.g. the emotion process) is encoded via distal vocal cues (measured by acoustic analysis); the listener perceives the vocal utterance and extracts a number of proximal cues (measured by subjective voice quality ratings obtained from naive observers); and, finally, some of these proximal cues are used by the listener to infer the internal state of the speaker based on schematic recognition or explicit inference rules (measured by naive observers asked to recognize the underlying emotion). The first step in this process is called the externalization of the internal emotional state, the second step the transmission of the acoustic information and the forming of a perceptual representation of the physical speech/voice signal, and the third and last step the inferential utilization and the emergence of an emotional attribution.</p><p>Next we describe the statistical models that have been used in earlier studies for Brunswikian lens modeling, with a focus on the two models that are used in the empirical part of the present article.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Statistical Paradigms for Lens Modeling</head><p>The dominant statistical paradigm in work informed by a Brunswikian approach is the lens model equation (LME <ref type="bibr" target="#b36">[37]</ref>), originally developed by Hammond, Hursch, and Todd <ref type="bibr" target="#b37">[38]</ref> and Tucker <ref type="bibr" target="#b38">[39]</ref>. The LME is essentially based on two regression equations and two correlations.  correlation coefficient is used to assess the correspondence between the two regressions (G in Fig <ref type="figure" target="#fig_0">2</ref>). Juslin and collaborators <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b30">31]</ref> have adopted this paradigm in their work on emotional communication in music performances. In Scherer's <ref type="bibr" target="#b32">[33]</ref> extension of the Brunswikian lens model (in his work on the expression and perception of personality in vocal communication), an early version of path analysis (as proposed by Duncan <ref type="bibr" target="#b39">[40]</ref>) was used (see <ref type="bibr">Fig 3)</ref> in which the accuracy coefficient (i.e., the correlation between expressed and perceived emotion) can be split into (a) the contributions of the postulated central indirect paths, (b) peripheral indirect paths (either distally based, bypassing the percept component, or proximally based, bypassing the distal cue component), and (c) the remaining direct path (i.e., the variance explained that is not accounted for by the mediation).</p><p>The present article describes a first attempt to demonstrate the plausibility of the model assumptions by examining how well the model can account for empirical data on the emotion expression in the voice (externalization) and the corresponding inferences made by naive observers (utilization). An ancillary question that has hardly been addressed in the literature concerns cue transmission-the degree to which the proximal cues appropriately reflect emotion-differentiating distal cues and what the nature of the mapping is. For this purpose, we examined the respective contributions of the LME (Fig <ref type="figure" target="#fig_0">2</ref>) and the statistical model derived from the TEEP (path analysis illustrated in Fig <ref type="figure" target="#fig_2">3</ref>) in a re-analysis of two corpora of vocal emotion portrayals, using an exploratory approach. Specifically, we attempt to empirically determine the relative importance of different variables and their associations rather than testing specific hypotheses.</p><p>The data used for the re-analyses have been collected in two consecutive research programs with different corpora of vocal expressions enacted by professional actors using Stanislavski techniques (reconstituting vivid feelings by recalling past emotional experiences; <ref type="bibr" target="#b40">[41]</ref>). The results reported here are the product of studies conducted over a period of 15 years, during which the two corpora were recorded with professional actors (the "Munich" corpus [MUC] doi:10.1371/journal.pone.0136675.g003 <ref type="bibr" target="#b8">[9]</ref>; and the "Geneva" corpus [GVA], Geneva Multimodal Emotion Portrayals-GEMEP <ref type="bibr" target="#b41">[42]</ref>); appropriate stimuli were selected for ground truth and authenticity <ref type="bibr" target="#b42">[43]</ref>; acoustic analyses were developed, applied, and validated <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref>; and a new subjective voice rating scale was developed and validated <ref type="bibr" target="#b43">[44]</ref>. It was only after this preliminary work that all the necessary elements were available to proceed to an overall modeling of the TEEP model applied to vocal emotion communication. Although some of the raw data here have been used for earlier reports on development and validation, so far there has been no attempt to link the expression, or encoding, side to the inferential, or decoding, side using both distal acoustic parameters and subjective proximal ratings as mediators. In consequence, the analyses and results presented here are original to the current article.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Description of the Corpora Used in the Analyses</head><p>Detailed descriptions of speech recording, analysis and ratings are described in <ref type="bibr" target="#b41">[42]</ref> and <ref type="bibr" target="#b43">[44]</ref>. In consequence, we limit the description of the methods to an overview of the procedures that are essential for understanding the main features of the data used for the Brunswikian re-analysis (further details can be found in the original papers or in the supplementary information in S1 File-Appendix).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Selection of Emotion Portrayals</head><p>The recordings of emotion portrayals used from the MUC corpus were produced by Germanspeaking professional actors who enacted all emotions while articulating two meaningless pseudo-speech sentences (without any semantic content): (a) "hät san dig prong nju ven tsi" and (b) "fi gött laich jean kill gos terr" <ref type="bibr" target="#b8">[9]</ref>. For the current analyses, 144 expressions from this corpus have been used, corresponding to 16 portrayals produced by nine actors (four men and five women) for eight emotions (hot and cold anger, elation and calm joy, despair and sadness, panic fear and anxiety). Each pair of emotions listed corresponds to one family (anger, happiness, sadness, and fear). The first member of the pair is defined as involving high emotional arousal, whereas the second member of each pair involves low emotional arousal.</p><p>The recordings used from the GVA corpus were produced by French-speaking actors who enacted all emotions, coached by a professional director, while articulating two meaningless pseudo-speech sentences: (a) "ne kal ibam soud molen" and (b) "koun se mina lod belam" <ref type="bibr" target="#b43">[44]</ref>. The eight emotions with the closest possible match to those in the MUC corpus were chosen. The GVA corpus was recorded to include emotions equivalent to those that were used in the MUC corpus. Different labels were used because the actors/encoders producing the portrayals in both corpora spoke different languages (German for MUC and French for GVA), but essentially the definitions of emotions used were similar, with the exception of "pleasure" ("plaisir" in French) and "calm joy" ("Stille Freude" in German), which were not defined as corresponding to identical states, but which were nevertheless both intended to be positive emotions with low arousal. From the GVA corpus, 160 expressions were used, corresponding to 10 actors (five women) who portrayed the eight emotions by using the two pseudo-speech sentences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Objective Acoustic Measures (Distal Cues)</head><p>Distal voice cues are general assessed via objective acoustic measurement of vocalizations. Given the complexity of this domain we cannot describe the measures and procedures in detail (see <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10]</ref> for more extensive discussions). Parameter extraction for both corpora was performed with the open source speech analysis program PRAAT <ref type="bibr" target="#b44">[45]</ref>. The extraction procedures are described in <ref type="bibr" target="#b43">[44]</ref> (methodological details on the acoustic extractions are also provided in S1 File-Appendix). Acoustic parameters to be extracted for the MUC corpus were chosen from the extensive list in Banse and Scherer <ref type="bibr" target="#b8">[9]</ref>. Two of these measures (spectral slope and jitter) were excluded after extraction based on the assessment of reliability carried out for all measures. As the 44 extracted parameters extracted showed a high degree of collinearity (in the MUC corpus) a principal component analysis was calculated in order to select a reduced number of acoustic parameters. This analysis showed that nine factors allowed accounting for 80% of the variance present in these data. The full results of the PCA are available in Tables B-D in S1 File-Appendix. Nine parameters were selected (one for each factor in the analysis; see Table D in S1 File-Appendix). Acoustic intensity did not constitute an independent factor in the analysis, but given its importance for emotion expression and communication, the parameter "mean intensity" was added to this list. Two initially selected parameters did not differentiate the expressed emotions and were therefore discarded. The acoustic parameters included in the present analyses are shown in Table <ref type="table">1</ref>, indicators of fundamental frequency (F0), acoustic intensity, duration of speech segments (tempo) and measures of spectral energy distribution. As formant analyses were not carried out on the recordings of the MUC corpus, articulatory effects could not be assessed.</p><p>As there is sizeable and systematic variation in acoustic features across speakers (e.g. female voices having much higher fundamental frequency than male voices) all acoustic parameters were standardized within speaker (for both corpora) to control for these extraneous sources of variance.</p><p>The parameters listed in Table <ref type="table">1</ref> were used for the LME analyses. The same set of parameters was used for the computation of the path analyses, except for two parameters which were excluded to reduce the number of variables to be included in the models: the relative duration of voiced segments and the proportion of energy between 600 and 800 Hz (chosen because these rarely used parameters partly overlapped with other parameters and thus did not provide incremental contributions to the variance in the LME analyses).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Subjective Voice Quality Ratings (Proximal Percepts)</head><p>The procedures used to collect ratings of proximal voice cues have been described and justified in detail in Bänziger et al. <ref type="bibr" target="#b43">[44]</ref> (see also S1 File-Appendix). Here we describe only the essential aspects needed to understand and interpret the models we present in the current article.</p><p>Several groups of participants were recruited to assess the proximal voice cues in successive rating studies for both corpora. All ratings were collected in small laboratories dedicated to perception/judgment studies at the University of Geneva. Individual computers and closed-ear headphones were used to present the vocal portrayals, and computer interfaces were created to record the raters' answers. The raters were all students at the University of Geneva and were compensated for their services, either in the form of course credit or financial remuneration. Average ratings are used for the analyses presented in the current paper. The averages were obtained from 61 raters (48 women, average age 21 years) for the MUC corpus, but with only 15 or 16 ratings for each stimulus. Different raters provided ratings for various subsets of the corpus. For the GVA corpus, 19 participants (10 women, average age 22 years) provided ratings for all scales. Further details on the rating procedures have been published in <ref type="bibr" target="#b43">[44]</ref>. Table E in S1 File-Appendix provides the details of the composition of the various groups of raters involved in rating proximal voice cues in both corpora and displays the estimates of inter-rater reliabilities obtained for the various ratings. The level of reliability of the ratings ranged from very good to satisfactory (all Cronbach's alpha values were larger than .80). The proximal voice scales to be rated were selected in a series of pilot studies designed to identify vocal dimensions that could be assessed by untrained raters with acceptable consistency. Eight vocal dimensions were chosen to be included in the Geneva Voice Perception Scale (GVPS; see <ref type="bibr" target="#b43">[44]</ref>) and were used for both corpora described in this article. The eight scales are shown in Table <ref type="table" target="#tab_0">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Assessment of Perceived (Attributed) Emotions</head><p>For the MUC corpus, the perceived (attributed) emotions for each stimulus were assessed by asking groups of raters to judge the perceived intensity of fear, anger, happiness, and sadness by using the recursive stimulus ranking procedure described earlier for the ratings of perceived voice cues. The ratings were provided by 56 participants (45 women, average age 22 years). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acoustic duration</head><p>Relative duration of voiced segments on speech segments (duration of voiced divided by the sum of the duration of voiced and unvoiced segments, i.e. excluding phonetic interruptions) represents the relative duration of voiced speech segments (i.e. a variable related to accentuation; prolonged vocals reflect more accentuated speech).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Relative duration</head><p>Distribution of energy in the long-term averaged spectrum (voiced segments only) 0-1000 Hz (relative to 0-8000 Hz) represents the proportion of spectral energy in "low" frequency regions (i.e. a variable that reflects voice quality-a sharp voice is characterized by increased energy in the higher frequency regions).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Relative energy &lt;1000</head><p>600-800 Hz (relative to 0-8000 Hz) represents voice quality changes; this variable was selected because it was only mildly correlated with LTSv &lt; 1000, and it loaded on an independent factor in the PCA computed on all acoustic variables extracted on the MUC corpus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Relative energy &lt;800</head><p>LME = lens model equation; LTSv = long-term averaged spectrum (voiced segments); PCA = principal component analysis. a For the MUC corpus, the F0 contours were manually corrected (for extraction mistakes, such as octave jumps and detection of periodicity in unvoiced segments). For the GVA corpus, no such corrections were made. Consequently, the absolute minimum of F0 detected in each utterance was used for the MUC corpus, whereas the 5 th percentile of the automatically extracted F0 was used for the GVA corpus.</p><p>doi:10.1371/journal.pone.0136675.t001</p><p>Different raters provided ratings for various subsets of the corpus; 14 ratings were collected for each stimulus. The ratings were made on visual analogue scales and the answers were rescaled to vary between 0 and 100 (0 = the emotion is absent, 100 = extreme emotional intensity).</p><p>For the GVA corpus, emotion recognition accuracy was assessed by asking 23 raters (13 women, average age 29 years) to listen to all emotional expressions included in the larger database (in random order but grouped for each speaker) and to produce a categorical rating (selection of one emotional category among 18 alternatives, or no emotion present) and an intensity rating for each portrayal (the procedure and detailed results have been reported in <ref type="bibr" target="#b43">[44]</ref>; see also S1 File-Appendix). Recognition accuracy estimates were computed as the proportion of raters providing an accurate answer (i.e. selecting the emotion category matching the expressive intention of the actor). Arcsine transformations were then applied to the proportional emotion recognition scores (resulting in a score between 0 and 100). None of the raters assessing emotions participated in the assessment of the GVPS (i.e. ratings on emotions and GVPS are obtained independently for both corpora). Table E in S1 File-Appendix provides information on the groups of raters involved and the estimated inter-rater reliabilities. Reliabilities ranged from very good to satisfactory (all alpha values larger than .80).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Assessment of Perceived Emotional Arousal</head><p>One of the aims of the present analyses was to examine the role of arousal in the vocal communication process. In consequence, we obtained ratings of the arousal manifested by the speakers. For the MUC corpus, a separate group of 24 raters (all women, 22 years old on average; not involved in the ratings of emotions or GVPS) assessed the degree of perceived emotional arousal in all portrayals, using the recursive stimulus ranking procedure described earlier for the GVPS.</p><p>For the GVA corpus, the proximal voice ratings (GVPS) and the arousal ratings were obtained from the same 19 raters. After providing the ratings for the eight voice scales, the emotional expressions were presented again (in a new random order for each rater), and the The GVPS was used for the ratings in both corpora, but the procedures involved in collecting the ratings differed slightly. For the MUC corpus, the perceived voice ratings were collected by a stimulus ranking procedure of the emotion portrayals, separately for each speaker. On a computer screen, raters arranged icons representing the audio stimuli (which they could listen to repeatedly) recursively on a continuum that was consequently recoded to a score ranging from 0 to 100. For the corpus, a more conventional rating procedure was involved, with raters using a visual analogue scale on the screen immediately after listening to each portrayal (later recoded to a score ranging from 0 to 100). All participants provided ratings for all voice scales sequentially and in random order (stimuli were also presented in random order for assessment within each scale).</p><p>doi:10.1371/journal.pone.0136675.t002</p><p>raters had to assess arousal on a visual analogue scale presented on the screen. Table E in S1 File-Appendix provides the information on the raters involved and the estimated interrater reliability which was equally large in both corpora (alpha = .98).</p><p>The results showed that the ratings obtained for the two different data sets with the different rating procedures were remarkably similar <ref type="bibr" target="#b43">[44]</ref>. In the analyses reported in the following sections, we used aggregated scores in the form of the mean values reported for each portrayal on each rating scale (GVPS; emotional intensity/accuracy and arousal ratings). For the path analyses, we standardized the average ratings obtained on the GVPS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods of Statistical Modeling</head><p>As described in the introduction, we used two different approaches for modeling: (a) the Brunswikian LME and (b) path analysis with both distal and proximal cues into a single model (as shown in the TEEP model; <ref type="bibr">Fig 1)</ref>. For ease of comprehension, we provide the details on the modeling paradigms and the statistical operations in the Results section before the description of the results, separately for each of the two approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ethical statement</head><p>This work has been performed under strict observance of the ethical guidelines elaborated by the Ethics Committee of the Department of Psychology at the University of Geneva. Specifically, the Ethics Committee requested that we submit a detailed description of all studies to be conducted in the research program "Production and perception of emotion" funded by the European Research Council (ERC). We described all procedures in detail and confirmed that we would follow the instructions of the Ethics Committee concerning the procedure to obtain informed consent. Based on this declaration, the procedures in the series of studies were approved. For the professional actors we obtained informed consent to produce the required emotion enactments and that we could use these for research purposes. The remaining participants produced only ratings of the actor-expressed emotions. In consequence, they were not subject to any experimental manipulation. Raters were recruited from the student body of the University of Geneva via posted announcements describing the aim of the study and the procedures used for the ratings. They recorded their agreement to produce the ratings against payment or course credit on enrollment sheets which provided a detailed description of the rating procedure. All raters were informed of their right to abandon their rating activity at any time. Raters choosing to be paid recorded their consent to have their data used for research purposes by their signature on a form sheet that also served to document payment received for the ratings. Raters choosing to obtain course credit signed a consent form that stipulated that the data would be stored anonymously and course credit was granted based on the enrollment sheets specifying their choice of compensation (names were registered separately of the data recorded during the study).</p><p>In all cases, age, gender and native language of the raters were recorded along with the data collected during the rating sessions. The students were also required to report if they had any form of diagnosed deficit in auditory perception (without having to provide any further details; their reply to this question was recorded along with their ratings, anonymously).</p><p>It should be noted that some actor recordings and some rating studies for the MUC corpus were performed before the existence of an ethics committee in the Department of Psychology at the University of Geneva. However, the procedures used were identical to those later approved by the current ethics committee.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LME Modeling</head><p>The LME ( <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b45">46]</ref>; see Eq 1 and </p><formula xml:id="formula_0">r a ¼ G Â R e Â R s þ C Â ffiffiffiffiffiffiffiffiffiffiffiffiffiffi ffi 1 À R e 2 p Â ffiffiffiffiffiffiffiffiffiffiffiffiffiffi 1 À R s 2 p<label>ð1Þ</label></formula><p>The unmodeled component is the product of three parameters represented in the second term of the addition in Eq 1. Parameter C corresponds to the correlation between the residuals of the two multiple regressions, it can be derived from the values of the other parameters of the equation that are reported in the result Tables <ref type="table">for</ref>  This model provides indices that are essentially descriptive. All indices are correlations or multiple correlations and therefore represent effect sizes (proportion of variance shared/ explained between the respective variables). In the result tables, we include the ratio of (G × R e × R s )/r a . This ratio represents the proportion of the relationship between the expressed and perceived emotion that is accounted for by the voice features included in the model. All regression coefficients for the LME analyses including levels of significance are provided in Table A of S2 File-Data.</p><p>We computed separate LMEs for each corpus, for each of the four emotion families, and for distal cues and proximal percepts. The results for both corpora and all emotion families and arousal are shown in Table <ref type="table" target="#tab_1">3</ref> (for both distal cues and proximal percepts). These tables include the parameters that compose the linear component of the LME: achievement (r a , the correlation between emotion enacted and emotion perceived), ecological validity (R e , the multiple correlation between the acoustic parameters or the perceived vocal features, and the emotion enacted), functional validity (R s , the multiple correlation between the acoustic parameters or the perceived vocal features, and the emotion perceived), matching (G, the correlation between the variables predicted derived from the regression of the acoustic parameters or the perceived vocal features on the emotion enacted and on the emotion perceived), and the ratio of (G × R e × R s )/r a , which represents the proportion of the relationship between the expressed and perceived emotion that is accounted for by the respective voice features included in the model. The data shown in Table <ref type="table" target="#tab_1">3</ref> allow several types of comparison. The most important information is the proportion of the relationship between the emotion family expressed and the emotion family inferred ("achievement," shown as the correlation between these two variables in the first column) that is accounted for by the mediating variables in the model (this proportion is displayed in the last column of Table <ref type="table" target="#tab_1">3</ref>). While the values for achievement are the same, the values for the proportion accounted for show major differences for the distal and the proximal models. In all cases except for arousal, where both models perform about evenly, the proximal model explains more variance than the distal model, in some cases accounting for almost double the variance. This discrepancy cannot be accounted for by a lower level of matching between expression and inference, as index G is quite comparable for both models. In other words, the respective cues, distal or proximal, are appropriately used in inference, in line with the information they contain. Rather, the comparison of the models shows, on average, both lower ecological validity (i.e. captures less distinctiveness among the emotions expressed) and lower functional validity (i.e. contributes less to the variance in the inference) for distal cues than for proximal cues. This discrepancy seems somewhat more pronounced in the case of GVA corpus for anger, fear, and sadness. It is unlikely that the expressions in this corpus carry less acoustic information, since there are no such differences in the proportion accounted for between the corpora for the proximal model (except in the case of sadness), which suggests that the information is available and is correctly interpreted. Methodological issues can be excluded, as the same acoustic parameters were used, calculated with the same software. However as a relatively high amount of statistical error variance cannot be ruled out, any further attempts at interpretation seem moot.</p><p>The results observed for the arousal ratings shown in Table <ref type="table" target="#tab_1">3</ref> are highly similar in both corpora and strongly corroborate the dominant role of arousal in vocal emotion communication. For both the distal and proximal models, the parameters show almost complete explanation of the achievement by the respective variables. In other words, arousal differences in vocal emotion expressions are well captured by acoustic variables and voice ratings and play a powerful role in the inference by listeners.</p><p>Apart from some level differences, the values for the two corpora were highly comparable (profile correlations on the LME parameters are r = .55 for the models based on distal cues and r = .72 for models based on proximal cues). Kolmogorov-Smirnov-Tests were computed for all variables over the two datasets to test the equality of the probability distributions. All of the tests yielded statistically non-significant results. In consequence, it was decided to combine the two data sets for the following analyses, which include both distal and proximal cues, as the statistical tests of the TEEP model with path analyses requires more observations to obtain sufficient statistical power, given the larger number of variables and covariates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Path Analysis Based on the TEEP Model</head><p>We adopted the path analysis approach described by Scherer <ref type="bibr" target="#b32">[33]</ref> (based on <ref type="bibr" target="#b39">[40]</ref>) to model the vocal communication of emotion for the merged corpora with a total of 304 vocal emotion portrayals. It should be noted that even the pooled sample size is still low with respect to the number of parameters to be estimated in the path model (df = 204). Lei and Wu <ref type="bibr" target="#b46">[47]</ref> recommend a minimum of 5 cases per estimated parameter. In the path model described below, we used expressed happiness as a reference category (a separate analysis for happiness, compared with the other expressed emotions, can be found in Tables D and E in S2 File-Data).</p><p>Fig <ref type="figure" target="#fig_7">4</ref> illustrates the conceptualization of the TEEP path model for the current analysis. The leftmost box, labeled "expressed emotions," represents the binary coded emotions enacted by the actors (as well as the operationally defined expressed level of arousal).</p><p>The second box, labeled "acoustic measures", represents the extracted acoustic characteristics (i.e. the distal cues in the TEEP). The z-standardized acoustic cues used are mean intensity, intensity range, F0 floor 5 th percentile, F0 range, acoustic duration, and relative energy &lt; 1000 (see Table <ref type="table">1</ref>). The third box represents the perceived characteristics of the vocal portrayals (i.e. the proximal percepts in the TEEP model), consisting of the z-standardized voice quality ratings: intonation, loudness, pitch, roughness, speech rate, and instability. The rightmost box represents the perceived emotion(s) and the perceived arousal level. An arcus-sinus-square root transformation was applied to these variables, which were originally bound between 0 and 1.</p><p>The arrows in Fig <ref type="figure" target="#fig_7">4</ref> show the effects that were included in the model: (a) the direct path from the expressed to the perceived emotion (D); (b) paths from the expressed emotions to the proximal percepts bypassing the acoustic measures (EP); (c) paths from the acoustic measures to the perceived emotions bypassing the proximal percepts (AP); and (d) all paths via both the distal and the proximal cues (M1 to M3). The path group M1 allows one to assess how a certain emotional state of the sender is encoded into objectively measurable acoustic parameters, the path group M2 allows assessment of how the physical characteristics of the voice signal are translated into proximal percepts (transmission), and the last path group M3 is indicative of how the proximal percepts are used to infer an emotional state of the sender from the signal (decoding). Clearly, perfect mediation in the TEEP model would be indicated by all effects passing from M1 to M3 with no effects for the paths AP, EP, and particularly for the direct path D. In addition to analyzing all direct and indirect paths, we estimated covariances between all dependent variables belonging to the same variable group (acoustic measures, proximal percepts, perceived emotions) with the software Mplus <ref type="bibr" target="#b47">[48]</ref>, using the estimation procedure with robust standard errors. The input instructions for the model are documented in Table <ref type="table">B</ref> and excerpts of the output in Table C in S2 File-Data.</p><p>As the model is fairly large, the results are presented in three separate tables. Only paths reaching a significance level of p &lt; .02 are reported to guard against overinterpretation. These significant paths are also illustrated by a series of separate figures (one for each negative emotion and one for arousal), in order to facilitate interpretation. Table <ref type="table">4</ref> shows the path groups M3, AP, and D (as illustrated in <ref type="bibr">Fig 4)</ref> with the perceived emotions as dependent variables. The path group M3 allows assessment of cue utilization in the Brunswikian sense. For example, the detection of anger is predicted by perceived loudness (b = .296), low perceived instability (b = -.257), and high roughness (b = .177). The path group AP allows assessment of the contribution of the acoustic measures to the detection of anger. The results indicate that the acoustic measures included in the model contribute only to the prediction of perceived arousal. Finally, the path group D indicates the direct effects from expressed emotions to perceived emotions. For example, perceived anger is predicted by expressed anger (b = .566) and expressed fear (b = .145). The high path coefficients for expressed anger as a predictor indicates that not all information regarding the expressed emotion is mediated through the acoustic measures and the proximal percepts. The high path coefficients for expressed fear indicates that it may sometimes mistakenly be identified as anger.</p><p>Table <ref type="table" target="#tab_2">5</ref> shows the results for the path groups M2 and EP (defined in Table <ref type="table" target="#tab_4">6</ref> shows the relationship between the expressed emotions and the acoustic measures as the dependent variable. The path group M1 describes the externalization process in terms of the TEEP model. For example, mean intensity (loudness) is positively associated with anger (b = .340), negatively associated with sadness (b = -.135), and highly positively associated with arousal (b = .766).</p><p>In Tables <ref type="table" target="#tab_4">4 to 6</ref>, we computed for each dependent variable the amount of variance that is explained by the respective set of predictors in a stepwise regression. For example, Table <ref type="table">4</ref> indicates that the amount of variance in perceived anger that is explained by the proximal percepts is R 2 = .638. If the distal cues are added, this amount increases to R 2 = .655. Adding the expressed emotions increases the R 2 to .762. Although anger and arousal (R 2 = .905) are relatively well explained by the model, this is only moderately the case for happiness (R 2 = .265). For the proximal percepts, Table <ref type="table" target="#tab_2">5</ref> shows that roughness is accounted for only marginally by the predictors (R 2 = .160), and for the acoustic measures, Table <ref type="table" target="#tab_4">6</ref> indicates that mean intensity (acoustic loudness) is well explained by the expressed emotions (R 2 = .748), whereas this is not the case for acoustic duration (R 2 = .068).</p><p>The incremental R 2 values in Table <ref type="table">4</ref> are especially interesting in judging the importance of the distal cues and the expressed emotions once the proximal percepts are taken into account to explain the perceived emotions. As Table <ref type="table">4</ref> shows, adding the distal cues does not improve the prediction of the perceived emotions and arousal substantially.</p><p>Figs <ref type="figure" target="#fig_10">5</ref> and<ref type="figure" target="#fig_11">6</ref> show the specific models for anger and arousal. From the graph for anger, it is evident that the most dominant path chain from expressed anger to perceived anger runs from high acoustic intensity to high perceived loudness and from there to the inference of perceived anger. However, the direct path from expressed anger to perceived anger is relatively strong, indicating that the acoustic measures and the proximal percepts do not carry all the information that is used to infer the emotion. Fig <ref type="figure" target="#fig_11">6</ref> for arousal shows that high arousal is reflected in specific changes in almost all acoustic measures except for relative energy and duration. On the proximal side of the model, it is mostly loudness that is used to infer perceived arousal. Figs <ref type="figure" target="#fig_12">7</ref> and<ref type="figure" target="#fig_13">8</ref> show the results for fear and sadness. Fig <ref type="figure" target="#fig_12">7</ref> shows that fear portrayals differ from happiness portrayals by a lower F0 range, a higher F0 floor and lower duration. Expressed fear is negatively associated with duration, suggesting higher tempo. Correspondingly, duration is negatively associated with perceived speech rate and positively with perceived instability. Finally, high perceived instability and high perceived speech rate are associated with perceived fear. Fig <ref type="figure" target="#fig_13">8</ref> shows that the acoustic measures included in the model are only weakly associated with expressed sadness. The strongest paths between the acoustic measures and proximal percepts run from mean intensity to intonation and loudness. Perceived sadness is negatively associated with intonation modulation and speech rate, and positively with perceived instability.</p><p>Table <ref type="table" target="#tab_5">7</ref> shows the direct, total indirect, and total effects for the emotion families (the effects are estimated with Mplus using robust standard errors, which are shown in parentheses in  Table <ref type="table" target="#tab_5">7</ref>; total effect = total indirect effects + direct effect). The total indirect effects reflect mediation via the acoustic and the proximal percepts to detect an emotion. These coefficients are relatively low except for arousal. In addition, the relatively high direct effects indicate that the communication process is not sufficiently captured by the variables used. Mediation for happiness could not be assessed, as happiness was used as a reference category. An analysis for happiness (contrasted with all other emotions) is provided in the Supplementary Information (Tables D and E in S2 File-Data). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion</head><p>Overall, the results show that (a) the selected vocal parameters (distal/acoustic and proximal/ perceived) are differentially related different emotions and (b) they allow a more comprehensive account of the communication of the arousal dimension than of the differential quality of the emotion families (with the exception of anger). In what follows, we briefly discuss the main findings, first for the LME and then for the path analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LME Modeling</head><p>The results obtained for the analyses with the classical LME indicate that the vocal communication of emotion can be largely accounted for within the framework of the Brunswikian lens model. In both data sets, the receiver's emotional attributions could be partly accounted for either by the objectively measured acoustic parameters, or by the proximal percepts (as indicated by large R s coefficients). Likewise, the sender's expressed emotion could be partly accounted for by the acoustic parameters or by the proximal percepts (as indicated by large R e coefficients). The large coefficients of communication achievement (r a ) indicate that the intended emotional expressions of the speakers were to a large extent recognized by the decoders. The matching coefficients (G) were overall also very large, indicating that the use of the voice cues in emotion externalization (expression) is symmetric to the use of the voice cues on the receiver side (for emotional attributions). This observation supports hypotheses postulating symmetrical processes in expressive behavior on the one hand and in perception on the other. High coefficients for speaker and rater consistency (R e , R s ) show that the externalized emotions and the emotional attributions are relatively strongly related to the eight acoustic parameters or the perceived voice cues, respectively. In both data sets, the perceived voice cues appeared to be better predictors for the expressed emotions and the emotional attributions than the distal acoustic parameters. This observation is consistent with the results of van Bezooijen <ref type="bibr" target="#b48">[49]</ref>, who reported that ratings of vocal cues could better discriminate emotional expressions than could ratings of acoustic cues. This observation is interesting, given that subjective ratings are a priori less reliable measures (due to inter-individual differences, biases in ratings, and intrapersonal inconsistency issues) than objectively measured acoustic parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Path Analysis Based on the TEEP Model</head><p>One of the central questions in this research concerns the degree to which the voice cues (distal or proximal) can account for the variance in perceived emotions. The results (see <ref type="bibr">Table 5)</ref> show that overall, the R 2 are large: For all emotion families, about 60% to 70% of the variance (more than 90% in the case of arousal) can be explained by the complete predictor set, which includes also expressed emotions and expressed arousal. In contrast, there are marked differences between emotions in the degree to which the mediating predictors, distal and proximal variables, account for the explanatory power. The results for the hierarchical regression analysis allow partitioning of the total R 2 into the relative contribution of the respective predictor sets (see Table <ref type="table" target="#tab_5">7</ref>). The proximal percepts were entered into the hierarchical regressions first, accounting for 91% of variance for arousal and 64% for anger, followed by 49% for fear and 41% for sadness and 27% only for happiness.</p><p>Compared with the LME approach (where separate analyses had to be computed for proximal and distal variables), the TEEP model allows a more integrative approach. The results generally show that, for all emotions, the distal variables do not explain of the variance once the proximal variables are entered into the model. This corresponds to theoretical expectations, as one can assume that the proximal cues, the voice percepts, are in fact directly based on the acoustic information carried by the wave form. Most of the valid information provided by the acoustic cues ought to be available in the proximal percepts, provided that they reflect the same vocal features. Tables <ref type="table" target="#tab_4">4 to 6</ref> show the details of these relationships. The results show three important patterns:</p><p>1. There are only a few one-to-one relationships (i.e. a particular acoustic parameter exclusively corresponding to a parallel dimension in voice perception). With the exception of acoustic intensity or energy (intensity mean) accounting for most of the loudness judgments, in all other cases, the subjective voice perception dimensions are determined by several acoustic parameters, suggesting that the perceptual dimensions important for emotion recognition in the voice are defined by interactions between different acoustic cues.</p><p>2. The combination of acoustic parameters that best predicts a proximal percept varies over different proximal scales.</p><p>3. Except in the case of loudness, the distal cues account only for 60% or less of the variance in the proximal percepts (only about 30% for instability and a very low 8% for roughness). This means that additional acoustic parameters need to be identified and measured in order to understand how exactly distal acoustic cues determine the subjective impression of voice and speech quality. This is particularly true for voice quality dimensions such as instability and roughness that are not easily on the acoustic level in running speech. Most likely, the subjective judgments on these scales depend on complex combinations of acoustic cues, including parameters that have not been included in the current analysis such as perturbation measures, indices of glottal functioning <ref type="bibr" target="#b10">[11]</ref>, or articulatory phenomena like mouth shape variations that change radiation. Part of the variance not explained by the measured acoustic cues carries important information concerning the type of expressed emotion, as shown by the fact that additional variance in the proximal percepts is explained by direct paths from one or more expressed emotions after the effect of the acoustic cues has been partialed out (see Table <ref type="table" target="#tab_2">5</ref>).  The graphic representation of the results for negative emotions and arousal in Figs 5-8 demonstrates the advantages of the TEEP model as outlined in the introduction. It charts the complete process of expression, transmission, and inference and allows one to determine where the model supports earlier predictions and where further improvements need to be made.</p><p>For anger (Fig <ref type="figure" target="#fig_10">5</ref>), the strongest complete mediation path (M1+M2+M3 in Fig <ref type="figure" target="#fig_7">4</ref>) is found for the expression of anger through high acoustic intensity, leading to the impression of a loud voice giving rise to the inference of anger. A second such path is constituted by flattening of the spectral slope (i.e. a decrease in relative energy in the frequencies below 1000 Hz with a corresponding increase of the higher frequencies), which might, together with other changes, lead to an increase in the perception of "roughness" of the voice and then also be interpreted as anger. Both of these paths have been theoretically postulated. Thus, Scherer (see Table <ref type="table" target="#tab_4">6</ref> in <ref type="bibr" target="#b49">[50]</ref>) presented a complete set of theory-based acoustic predictions for major emotions, in which an increase in intensity and high frequency energy is predicted for anger (although the mediation through proximal percepts was not yet specified). An additional prediction suggested an increase in F0 range and variability. This is also confirmed by the present data for the distal path, anger expression leading to an increase in the range of F0. All of these predictions have also been empirically confirmed in the literature (see for example the review by Juslin &amp; Laukka, <ref type="bibr" target="#b2">[3]</ref>; see also Table A in S1 File-Appendix). In the present case, these acoustic parameters may contribute to the proximal impression of instability in the voice, which is interpreted as a counter indication of the presence of anger (negative path coefficient).</p><p>This apparently discrepant result demonstrates another advantage of the path analytic TEEP model-the generation of new research questions. As noted earlier, the instability dimension is currently not well explained by acoustic parameters and it is thus difficult to interpret the reason for the apparent discrepancy. Most likely, there is an interaction between F0 range on the one hand and F0 floor and acoustic duration on the other, as all three parameters show a positive effect on instability (i.e. high F0 floor and high variability with a slow speech rate are seen as instable). However, anger produces faster speech (as also predicted by Scherer <ref type="bibr" target="#b49">[50]</ref>, and empirically confirmed in earlier studies <ref type="bibr" target="#b2">[3]</ref>). It is difficult to interpret these inconsistencies given our current knowledge. Further research is required to disentangle the sources of vocal instability perception and to identify further parameters, including perturbation measures such as jitter, shimmer, or the harmonic-to-noise ratio. The strongly negative semi-direct path EP, bypassing the distal level, from expressed anger to instability, shows that an angry voice is not perceived as instable, which is consistent with the negative effect of instability on anger perception. In any case, these inconsistencies may have reduced the amount of variance explained by the complete indirect mediation paths of type M1+M2+M3, thus accounting for the relatively strong direct path D. On the whole, however, the model is successful for anger and provides strong support for both the feasibility of the modeling of the inference process by the TEEP model and for the earlier predictions based on the component process model of emotion (as described in <ref type="bibr" target="#b49">[50]</ref>).</p><p>The model provides an excellent account for the communication of emotional arousal (as shown in <ref type="bibr">Fig 6)</ref>. Here both the direct (D) and semi-direct paths (EP and AP) are of little import, with over 90% of variance being explained by the indirect mediation paths (M1+M2 +M3), essentially through the loudness and instability percepts and due to the strong effects of expressed arousal on the underlying acoustic parameters F0 and intensity, both with respect to mean and range. This underlines the important role of strong distal connections between different emotions and specific configurations of acoustic parameters. The indirect mediation paths also provide some indication that, in this case, there is a more coherent meaning for the instability dimension, a consistent clustering of F0 floor and range as well as duration (slow speech rate), even though there is no relationship of the latter with expressed arousal.</p><p>These results reflect the strong evidence from past work in this area suggesting that the voice is the privileged modality for the expression and communication of arousal and activation, whereas the face is vastly superior with respect to valence <ref type="bibr" target="#b4">[5]</ref>. It seems plausible that studies involving "extreme" emotional variation (emotion portrayals are often exemplars of very strong and prototypical emotions) will always find emotional arousal to be a "higher order factor" in the voice, given that more emotional arousal is likely to be translated in increased vocal effort and faster speech, which in turn affects many vocal cues. The lack of clearly identified valence cues is also brought out clearly by the current results, the modeling of the happiness family being by far the least satisfactory. This is despite the fact that the accuracy scores for happiness are not much below the average of the other emotions. To the extent that results cannot be explained by guessing strategies given the small number of positive emotions, listeners seem to have been able to correctly infer happiness from voice and speech in our corpora (suggesting that relevant cues were available to them). However, so far we have little evidence concerning the distal and proximal cues that the inference process is based on. One possibility might be the change in lip radiation patterns while speaking with a smile <ref type="bibr" target="#b50">[51]</ref>, or the size-code hypothesis <ref type="bibr" target="#b51">[52]</ref> which implies that higher sociability (and positive emotionality) may be indicated by higher F0 level and larger formant spacing consecutive to shortening of the vocal tract <ref type="bibr" target="#b52">[53]</ref>.</p><p>The results for sadness and fear are somewhat better than for happiness, but in each case, the variance explained by the indirect paths explains less than 50% of the variance in the perceived emotions. The differentiation is provided by acoustic duration/speech rate, decreasing for sadness and increasing for fear (as reported in earlier studies <ref type="bibr" target="#b2">[3]</ref>). In both cases, perceived instability is involved, with an increase making both fear and sadness judgments more likely. This, and the negative relationship for anger, might suggest that perceived instability is seen as an indicator of low power or helplessness. It seems promising to examine this dimension of voice perception more closely.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Limitations</head><p>The expression corpora and corresponding data sets used for the analyses reported here are the first to allow integration of the four sets of measures (expressed emotions, distal cues, proximal cues, and perceived emotions) into single models of vocal emotion communication. However, the nature of the two data sets used here also imposes limitations on our analyses. Sources of limitations, imposed by the established design of emotion recognition studies, include the use of binary variables (present/absent) for the operationalization of expressed emotions, the absence of neutral comparison stimuli (all portrayals represent emotional expressions), and the sample sizes, which are small in relation to the large number of potentially relevant cues.</p><p>The ratings of voice cues and ratings of emotions have been obtained from different raters, using a design that a priori precludes the possibility that the emotional ratings might have conditioned the ratings of voice cues. However, it cannot be excluded that the participants who rated the voice cues were influenced by spontaneously occurring implicit emotion judgments.</p><p>Furthermore the use of independent ratings (for voice cues and perceived emotions) does not allow modeling the perception process with respect to individual listeners (and variance across listeners). In the current models the variance was considered only across emotion portrayals, individual perceptual processes are not addressed.</p><p>The optimal choice of vocal cues (usually acoustic summaries of the speech signal) is a recurrent problem in this research domain. There is an urgent need to develop a principled selection method to allow testing of specific hypotheses. In particular, it seems that different sets of cues might be distinctive for different emotions and that more work is needed to identify the vocal cues that are best suited to describe various emotions in speech. The choice of acoustic measures included in the models described above was limited, lacking, for example, measures relative to formants or voicing stability. Some frequently used spectral measures were included, but there is certainly room for refinement. In general, the field needs a better mapping of the pertinent vocal features involved in the communication of emotion (see Eyben et al. <ref type="bibr" target="#b53">[54]</ref> for a concerted action of the voice research community in this respect).</p><p>Furthermore, the acoustic variables included in our analyses have not been selected (or transformed) for perceptual relevance or saliency. For example, the variations induced by emotion in some of the parameters might fall below discrimination thresholds. Further research should consider adding a psychoacoustic level of representation to the model, situated between the distal and the proximal cues. This could consist of appropriate transformations of the acoustic variables in order to make them more perceptually relevant (e.g. represent F0 in semitones or integrate spectral measure with amplitude measures in order to better approximate perceived loudness).</p><p>Finally, in recent years the use of emotion portrayals has become a common concern. However, the use of enacted material, in an attempt to get as close as possible to natural expressions, is obligatory in this research domain, as it is practically and ethically impossible to obtain a large set of vocalizations for 12 to 18 emotions from the same person in real life. Using a convenience sample of vocal emotion expressions from different individuals recorded on the fly is ruled out by the fact that individual voice qualities are extremely variable, making it impossible to compare acoustic emotion variations across speakers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>The promise of using mediational analysis (in particular path analysis) to understand the processes of expression and impression in social perception-in contrast to designs focusing only on expression or encoding or only on inference and decoding-is increasingly recognized. Most importantly, such research designs focus on the identification of the cues that carry the pertinent information. This approach is of particular importance in the area of vocal markers of identity, personality and emotion given the wealth of information provided by the voice and the complexity of the acoustic markers <ref type="bibr" target="#b5">[6]</ref>. A pertinent example is a recent study showing that women listeners use sexually dimorphic voice cues which correlate with speakers' variation in stature and hormonal status to judge the masculinity of male speakers, highlighting the interdependence of physiological, acoustic and perceptual dimensions <ref type="bibr" target="#b54">[55]</ref>.</p><p>In a similar vein, the present paper shows the utility of applying a modified Brunswikian lens model (TEEP) that separately measures both distal and proximal voice cues to assess the transmission and recoding process, to the vocal communication of emotion. We reported secondary analyses of two data sets that include proximal/perceived voice cues and distal/acoustic measures obtained for two corpora of vocal emotion portrayals. Our main goal was to highlight how the communication process and the contribution of various cues (distal or proximal) can be represented by using LMEs and path analyses.</p><p>The statistical models (LME and path analysis) presented in this paper indicate that this approach to the study of vocal communication of emotion is highly feasible and that the distal and proximal variables used in the models mediate the communication of arousal and negative emotions to a large extent. From the set of proximal percepts, only intonation modulation was indicative for perceived happiness, reflecting the frequently observed absence of specific valence cues in the voice. However, recognition accuracy was comparably large for all emotions, as reflected by strong direct paths from expressed to perceived emotion.</p><p>The expected arousal dominance showed up clearly in our models (in the LME and the path analysis). However, the current results contribute to our understanding of the underlying mechanisms. Emotions characterized by high arousal are characterized by high intensity and high F0 floor and range on the distal side, with corresponding perceptions of loudness and high pitch. This evidence for highly efficient information transmission suggests the existence of inference rules that directly correspond to the empirical associations we found. In contrast, high speech rate, often predicted as a marker of arousal, does not consistently produce the same pattern.</p><p>Importantly, the current work has increased our understanding of the cues that are specific to emotion families, independent of potential arousal differences among the family members (e.g. irritation, cold anger vs. rage, hot anger). We suggest that the cues specific to the anger family as a whole, irrespective of arousal differences, are the following: a comparatively high intensity, a flat spectral slope (perceived as roughness), and a firm, steady voice (the acoustic correlates of which remain unclear for the moment, although perturbation is a likely candidate).</p><p>In sum, we have shown that accounting for the process of vocal emotion communication using the Brunswik-based TEEP model is a promising approach to understanding the processes of emotion production and recognition. It provides insight into which specific cues are used to express an emotion, how the distal indicator variables map to proximal percepts, and which proximal percepts are used to recognize a certain emotion. In particular, our results encourage the search for hitherto unexploited distal voice cues associated with emotions, carrying essential discrimination information for proximal percepts (e.g. instability). A first step in this direction is currently undertaken in the form of the specification of a standardized set of acoustic parameters for research on vocal expression of emotion <ref type="bibr" target="#b53">[54]</ref>. This is particularly pertinent in the case of positive emotions, which have been repeatedly shown to be difficult to characterize by acoustic parameters despite the fact that raters can identify them rather accurately. Distal acoustic cues with a better match to speech production and/or speech perception mechanisms will be needed to improve our models. Correspondingly, further studies of perceived voice cues need to be conducted to be able to include not only a complete set of distal, but also appropriate proximal measures in such models. The TEEP model also suggests new possibilities for research designs using increasingly sophisticated technological tools for voice manipulation through synthesis and morphing, as based on emotion portrayals or realistic speech data. Although this article has focused on vocal communication, this framework can be extended to other areas of interpersonal communication such as the nonverbal communication of emotions in facial or gestural expressions.</p><p>The study of the perception and inference of emotion from nonverbal expressions continues to be highly popular in psychological emotion and social perception research. Unfortunately, much of this research is narrowly confined to the study of recognition accuracy, with little concern for the underlying mechanisms and processes or the nature of the acoustic cues and their perception by listeners. The widening gulf between production studies (which are relatively rare) and recognition studies (which exist in abundance) constitutes a major limitation for progress in this field, particularly with respect to understanding the underlying communication process. Arguably, this research could greatly benefit by a comprehensive, process-oriented approach informed by a theoretical framework and incorporating production and transmission as well as perception and inference. Similarly, the recent surge of activity in the domain of affective computing (involving both engineering and computer sciences), could benefit from the type of modeling described here. This is particularly true for machine learning approaches to automatic emotion detection in the voice and for realistic vocal emotion expression synthesis for avatars in the context of robotics or human-machine-interaction. It is to be hoped that the demonstration of the utility and feasibility of a comprehensive path modeling approach motivates researchers from a wider area in perception and cognition to turn to this important aspect of emotion communication in human social interaction.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig 2</head><label>2</label><figDesc>provides a graphical illustration adapted to the vocal communication of emotion. In a first regression equation, objectively measurable cues are predictors for the distal criterion (expressed emotion in Fig 2). The corresponding multiple correlations (R e ) on the left side of the graph represent the ecological validity (i.e. the extent to which the measured cues account for the variance in the distal criterion). The second regression equation uses the same cues as predictors for the proximal judgments of an individual with regard to the distal criterion. The corresponding multiple correlation (R s ) on the right side of the graph indicate the extent to which the cues in the model can account for the listeners' attributions (cue utilization). The weights of individual cues in the regressions are not part of the LME itself, but are sometimes considered in order to investigate the independent contribution of various cues in the models (both with respect to ecological validity and with respect to cue utilization). A correlation coefficient (between criterion and judgment) is used to represent accuracy (R a in Fig 2). Another</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig 1 .Fig 2 .</head><label>12</label><figDesc>Fig 1. The tripartite emotion expression and perception (TEEP) model (based on Brunswik's lens model). The terms "push" and "pull" refer to the internal and the external determinants of the emotional expression, respectively, distinguished in the lower and upper parts of the figure. D = distal cues; P = percepts. Adapted from p. 120 in Scherer [36]. doi:10.1371/journal.pone.0136675.g001</figDesc><graphic coords="4,200.01,78.01,234.43,164.64" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig 3 .</head><label>3</label><figDesc>Fig 3. Graphic illustration for an extended model (path analysis with separate distal and proximal cues).</figDesc><graphic coords="5,36.00,78.01,418.79,234.94" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Table 1 .</head><label>1</label><figDesc>Eight acoustic parameters selected for the LME analyses.Domain Description LabelFundamental frequency (F0) Minimum or 5 th percentile of the F0 represents the floor/level of the fundamental frequency.F0 floor / F0 5th percentile aRange (difference between minimum and maximum) represents the variability of the fundamental frequency. of the utterance) represents the speech rate (all utterances have the same number of syllables).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Fig 2) computes communication achievement (r a , i.e. the correlation between the expressed and perceived emotion) as the sum of two components: the linear component (i.e. the component of the correlation derived from the linear contributions of the variables entered in the model) and the unmodeled component (which includes systematic and unsystematic variance not accounted for by the linear component). The linear component is a product of speaker consistency (R e , which corresponds to the multiple correlation of enacted emotion on the variables in the model), rater consistency (R s , i.e. the multiple correlation of perceived emotion on the variables in the model), and matching (G, i.e. the correlation between the predicted values of the expressed emotion model and the predicted values of the perceived emotion model).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>the LME. In this model, a value close to 1 for parameter G indicates a good match in terms of the use of vocal features on the two sides of the model. In contrast, a value close to 0 for this parameter indicates that the use of vocal features is different for encoding and decoding. Low values (approaching zero) for the parameters R e and R s may be the consequence of several factors that the model does not allow considering separately: (a) The vocal features in the model are used inconsistently; (b) the vocal features in the model are used in a nonlinear way (i.e. nonlinear functions of these features might allow prediction of the emotion enacted and the emotion perceived); (c) the vocal features important for encoding or decoding are not included in the model; and (d) the measurement errors are large for the variables considered.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>Fig A in S2 File-Data illustrates the case of anger. The standardized beta coefficients for the regressions obtained for these models are reported inTable A of S2 File-Data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig 4 .</head><label>4</label><figDesc>Fig 4. Conceptual representation of the TEEP path model. doi:10.1371/journal.pone.0136675.g004</figDesc><graphic coords="13,36.00,78.01,394.30,147.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig 4 )</head><label>4</label><figDesc>with the proximal percepts as the dependent variable. The path group M2 allows assessment of the contributions of the distal cues with regard to the proximal percepts or, in terms of the TEEP model, the transmission process. For example, intonation is predicted by mean intensity (b = .305), intensity range (b = .127), fundamental frequency (F0 floor 5 th percentile; b = .176), and frequency range (F0 range; b = .288). The path group EP shows the importance of the expressed emotions for predicting a proximal percept in addition to the acoustic measures. Intonation, for example, is predicted by low anger (b = -.180), low sadness (b = -.252), low fear (b = -.187), and high arousal (b = .172). No strict one-to-one relationship between acoustic measures and proximal counterpart is detected except for perceived mean intensity-loudness and duration -speech rate.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Table 4 .</head><label>4</label><figDesc>(Continued) Perceived emotion (DV) Significant predictors (p = &lt; .02) Standardized partial regression coefficient b R2 ΔR2 Expressed arousal .125*** Note: * = p &lt; .02 ** = p &lt; .01 *** = p &lt; .001. Only p-values &lt; .02 are reported. doi:10.1371/journal.pone.0136675.t004</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig 5 .</head><label>5</label><figDesc>Fig 5. Standardized path coefficients of the estimated model for anger (data merged for MUC and GVA). Only significant path coefficients are shown (p &lt; .02). Significant paths with an absolute value &gt; .2 are depicted in black, significant paths with an absolute value &lt; .2 are depicted in black. int. = intensity; r. energy = relative energy. doi:10.1371/journal.pone.0136675.g005</figDesc><graphic coords="19,36.00,78.01,459.55,301.15" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig 6 .</head><label>6</label><figDesc>Fig 6. Standardized path coefficients of the estimated model for arousal (data merged for MUC and GVA). Only significant path coefficients are shown (p &lt; .02). Significant paths with an absolute value &gt; .2 are depicted in black. Significant paths with an absolute value &lt; .2 are depicted in black. int. = intensity; r. energy = relative energy. doi:10.1371/journal.pone.0136675.g006</figDesc><graphic coords="20,36.00,78.01,447.82,299.74" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig 7 .</head><label>7</label><figDesc>Fig 7. Standardized path coefficients of the estimated model for fear (data merged for MUC and GVA). Only significant path coefficients are shown (p &lt; .02). Significant paths with an absolute value &gt; .2 are depicted in black. Significant paths with an absolute value &lt; .2 are depicted in black. doi:10.1371/journal.pone.0136675.g007</figDesc><graphic coords="21,36.00,78.01,464.83,241.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig 8 .</head><label>8</label><figDesc>Fig 8. Standardized path coefficients of the estimated model for sadness (data merged for MUC and GVA). Only significant path coefficients are shown (p &lt; .02). Significant paths with an absolute value &gt; .2 are depicted in black. Significant paths with an absolute value &lt; .2 are depicted in black. doi:10.1371/journal.pone.0136675.g008</figDesc><graphic coords="22,36.00,78.01,438.69,311.98" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2 .</head><label>2</label><figDesc>Scales used for the voice ratings, translations, and terms used in the study with French-speaking raters.</figDesc><table><row><cell>English translation</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 .</head><label>3</label><figDesc>Summary of five LMEs (four emotion families and arousal) for both corpora based on eight acoustic parameters (same parameters used in all models) or eight averaged voice ratings (same voice scales in all models). ×R s ×G/r a</figDesc><table><row><cell>Emotion families and arousal</cell><cell>Corpus</cell><cell cols="4">r a R e Achievement R e R s G Ecological validity Functional validity Matching</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">Models based on eight acoustic parameters (distal cues)</cell><cell></cell><cell></cell><cell></cell></row><row><cell>anger</cell><cell>MUC</cell><cell>.764</cell><cell>.702</cell><cell>.763</cell><cell>.828</cell><cell>0.58</cell></row><row><cell></cell><cell>GVA</cell><cell>.843</cell><cell>.501</cell><cell>.652</cell><cell>.949</cell><cell>0.37</cell></row><row><cell>fear</cell><cell>MUC</cell><cell>.670</cell><cell>.558</cell><cell>.598</cell><cell>.826</cell><cell>0.41</cell></row><row><cell></cell><cell>GVA</cell><cell>.784</cell><cell>.455</cell><cell>.545</cell><cell>.937</cell><cell>0.30</cell></row><row><cell>happiness</cell><cell>MUC</cell><cell>.735</cell><cell>.238</cell><cell>.365</cell><cell>.304</cell><cell>0.04</cell></row><row><cell></cell><cell>GVA</cell><cell>.896</cell><cell>.498</cell><cell>.514</cell><cell>.943</cell><cell>0.27</cell></row><row><cell>sadness</cell><cell>MUC</cell><cell>.774</cell><cell>.549</cell><cell>.670</cell><cell>.896</cell><cell>0.43</cell></row><row><cell></cell><cell>GVA</cell><cell>.786</cell><cell>.380</cell><cell>.388</cell><cell>.926</cell><cell>0.17</cell></row><row><cell>arousal</cell><cell>MUC</cell><cell>.723</cell><cell>.776</cell><cell>.891</cell><cell>.953</cell><cell>0.91</cell></row><row><cell></cell><cell>GVA</cell><cell>.860</cell><cell>.891</cell><cell>.952</cell><cell>.988</cell><cell>0.97</cell></row><row><cell></cell><cell cols="3">Models based on eight perceived voice cues (proximal cues)</cell><cell></cell><cell></cell><cell></cell></row><row><cell>anger</cell><cell>MUC</cell><cell>.764</cell><cell>.756</cell><cell>.858</cell><cell>.841</cell><cell>0.71</cell></row><row><cell></cell><cell>GVA</cell><cell>.843</cell><cell>.799</cell><cell>.870</cell><cell>.948</cell><cell>0.78</cell></row><row><cell>fear</cell><cell>MUC</cell><cell>.670</cell><cell>.582</cell><cell>.773</cell><cell>.788</cell><cell>0.53</cell></row><row><cell></cell><cell>GVA</cell><cell>.784</cell><cell>.617</cell><cell>.751</cell><cell>.942</cell><cell>0.56</cell></row><row><cell>happiness</cell><cell>MUC</cell><cell>.735</cell><cell>.494</cell><cell>.686</cell><cell>.775</cell><cell>0.36</cell></row><row><cell></cell><cell>GVA</cell><cell>.896</cell><cell>.541</cell><cell>.598</cell><cell>.977</cell><cell>0.35</cell></row><row><cell>sadness</cell><cell>MUC</cell><cell>.774</cell><cell>.680</cell><cell>.829</cell><cell>.956</cell><cell>0.70</cell></row><row><cell></cell><cell>GVA</cell><cell>.786</cell><cell>.448</cell><cell>.631</cell><cell>.953</cell><cell>0.34</cell></row><row><cell>arousal</cell><cell>MUC</cell><cell>.723</cell><cell>.812</cell><cell>.961</cell><cell>.927</cell><cell>1.00</cell></row><row><cell></cell><cell>GVA</cell><cell>.860</cell><cell>.897</cell><cell>.965</cell><cell>.971</cell><cell>0.98</cell></row><row><cell>doi:10.1371/journal.pone.0136675.t003</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 5 .</head><label>5</label><figDesc>Prediction of proximal percepts by distal cues and expressed emotions, including standardized regression coefficients, R 2 , and incremental R 2 .</figDesc><table><row><cell>Proximal percept (DV)</cell><cell>Significant predictors (p = &lt; .02)</cell><cell>Standardized partial regression coefficient b</cell><cell>R 2</cell><cell>ΔR 2</cell></row><row><cell>Intonation</cell><cell>Distal cue (M2)</cell><cell></cell><cell>.630***</cell><cell></cell></row><row><cell></cell><cell>Intensity mean</cell><cell>.305***</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Intensity range</cell><cell>.127**</cell><cell></cell><cell></cell></row><row><cell></cell><cell>F0 floor</cell><cell>.176***</cell><cell></cell><cell></cell></row><row><cell></cell><cell>F0 range</cell><cell>.288***</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Expressed emotion (EP)</cell><cell></cell><cell>.684***</cell><cell>.054***</cell></row><row><cell></cell><cell>Anger</cell><cell>-.180***</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Sadness</cell><cell>-.252***</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Fear</cell><cell>-.187***</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Arousal</cell><cell>.172**</cell><cell></cell><cell></cell></row><row><cell>Loudness</cell><cell>Distal cue (M2)</cell><cell></cell><cell>.916***</cell><cell>.</cell></row><row><cell></cell><cell>Intensity mean</cell><cell>.752***</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Intensity range</cell><cell>.095***</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Relative energy</cell><cell>-.082**</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Expressed emotion (EP)</cell><cell></cell><cell>.919***</cell><cell>.003*</cell></row><row><cell></cell><cell>Expressed anger</cell><cell>.077**</cell><cell></cell><cell></cell></row><row><cell>Pitch</cell><cell>Distal cues (M2)</cell><cell></cell><cell>.645***</cell><cell></cell></row><row><cell></cell><cell>Intensity mean</cell><cell>.388***</cell><cell></cell><cell></cell></row><row><cell></cell><cell>F0 floor</cell><cell>.374***</cell><cell></cell><cell></cell></row><row><cell></cell><cell>F0 range</cell><cell>.322***</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Expressed emotion (EP)</cell><cell></cell><cell>.697***</cell><cell>.052***</cell></row><row><cell></cell><cell>Anger</cell><cell>-.325***</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Fear</cell><cell>-.096*</cell><cell></cell><cell></cell></row><row><cell>Roughness</cell><cell>Distal cue (M2)</cell><cell></cell><cell>.078***</cell><cell></cell></row><row><cell></cell><cell>Intensity range</cell><cell>-.167*</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Duration</cell><cell>.167*</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Relative energy</cell><cell>-.243**</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Expressed emotion (EP)</cell><cell></cell><cell>.160***</cell><cell>.082***</cell></row><row><cell></cell><cell>Anger</cell><cell>.346***</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Sadness</cell><cell>.224**</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Fear</cell><cell>.171**</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Arousal</cell><cell>.295**</cell><cell></cell><cell></cell></row><row><cell>Speech rate</cell><cell>Distal cues (M2)</cell><cell></cell><cell>.644***</cell><cell></cell></row><row><cell></cell><cell>F0 range</cell><cell>.104*</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Duration</cell><cell>-.542***</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Expressed emotion (EP)</cell><cell></cell><cell>.711***</cell><cell>.067***</cell></row><row><cell></cell><cell>Fear</cell><cell>.289***</cell><cell></cell><cell></cell></row><row><cell>Instability</cell><cell>Distal cues (M2)</cell><cell></cell><cell>.314***</cell><cell></cell></row><row><cell></cell><cell>F0 floor</cell><cell>.174**</cell><cell></cell><cell></cell></row><row><cell></cell><cell>F0 range</cell><cell>.181***</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Duration</cell><cell>.223***</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Expressed emotion (EP)</cell><cell></cell><cell>.616***</cell><cell>.302***</cell></row><row><cell></cell><cell>Anger</cell><cell>-.421***</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Sadness</cell><cell>.274***</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Fear</cell><cell>.184***</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>(Continued)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 .</head><label>5</label><figDesc>(Continued)    </figDesc><table><row><cell>Proximal percept (DV)</cell><cell>Significant predictors (p = &lt; .02)</cell><cell>Standardized partial regression coefficient b</cell><cell>R 2</cell><cell>ΔR 2</cell></row><row><cell></cell><cell>Arousal</cell><cell>.214**</cell><cell></cell><cell></cell></row><row><cell>Note:</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>* = p &lt; .02</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>** = p &lt; .01</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>*** = p &lt; .001.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Only p-values &lt; .02 are reported.</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">doi:10.1371/journal.pone.0136675.t005</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 6 .</head><label>6</label><figDesc>Prediction of distal cues by expressed emotions including standardized partial regression coefficients and R 2 .</figDesc><table><row><cell>Distal cue (DV)</cell><cell>Significant predictors (p = &lt; .02)</cell><cell>Standardized partial regression coefficient b</cell><cell>R 2</cell></row><row><cell>Intensity mean</cell><cell>Expressed emotion (M1)</cell><cell></cell><cell>.748***</cell></row><row><cell></cell><cell>Anger</cell><cell>.340***</cell><cell></cell></row><row><cell></cell><cell>Sadness</cell><cell>-.135***</cell><cell></cell></row><row><cell></cell><cell>Arousal</cell><cell>.766***</cell><cell></cell></row><row><cell>Intensity range</cell><cell>Expressed emotion (M1)</cell><cell></cell><cell>.321***</cell></row><row><cell></cell><cell>Anger</cell><cell>.160**</cell><cell></cell></row><row><cell></cell><cell>Arousal</cell><cell>.519***</cell><cell></cell></row><row><cell>F0 floor</cell><cell>Expressed emotion (M1)</cell><cell></cell><cell>.508***</cell></row><row><cell></cell><cell>Fear</cell><cell>.195***</cell><cell></cell></row><row><cell></cell><cell>Arousal</cell><cell>.684***</cell><cell></cell></row><row><cell>F0 range</cell><cell>Expressed emotion (M1)</cell><cell></cell><cell>.298***</cell></row><row><cell></cell><cell>Anger</cell><cell>.148**</cell><cell></cell></row><row><cell></cell><cell>Fear</cell><cell>-.148**</cell><cell></cell></row><row><cell></cell><cell>Arousal</cell><cell>.477***</cell><cell></cell></row><row><cell>Duration</cell><cell>Expressed emotion (M1)</cell><cell></cell><cell>.068***</cell></row><row><cell></cell><cell>Anger</cell><cell>-.176**</cell><cell></cell></row><row><cell></cell><cell>Fear</cell><cell>-.283***</cell><cell></cell></row><row><cell>Relative energy</cell><cell>Expressed emotion (M1)</cell><cell></cell><cell>.516***</cell></row><row><cell></cell><cell>Anger</cell><cell>-.317***</cell><cell></cell></row><row><cell></cell><cell>Sadness</cell><cell>.108**</cell><cell></cell></row><row><cell></cell><cell>Arousal</cell><cell>-.619***</cell><cell></cell></row><row><cell>Note:</cell><cell></cell><cell></cell><cell></cell></row><row><cell>* = p &lt; .02</cell><cell></cell><cell></cell><cell></cell></row></table><note><p>** = p &lt; .01 *** = p &lt; .001. Only p-values &lt; .02 are reported. doi:10.1371/journal.pone.0136675.t006</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 7 .</head><label>7</label><figDesc>Standardized direct, total indirect, and total effects from expressed emotion to perceived emotion.</figDesc><table><row><cell>Effects</cell><cell>Anger</cell><cell>Fear</cell><cell>Sad</cell><cell>Arousal</cell></row><row><cell>Total indirect effects</cell><cell>.274*** (0.038)</cell><cell>.161*** (0.030)</cell><cell>.115*** (0.029)</cell><cell>.664*** (0.031)</cell></row><row><cell>Direct effect</cell><cell>.566*** (0.050)</cell><cell>.668*** (0.046)</cell><cell>.630*** (0.049)</cell><cell>.125*** (0.029)</cell></row><row><cell>Total effect</cell><cell>.840*** (0.030)</cell><cell>.829*** (0.034)</cell><cell>.746*** (0.038)</cell><cell>.789*** (0.022)</cell></row><row><cell>Total indirect effects/Total effect</cell><cell>.326</cell><cell>.194</cell><cell>.183</cell><cell>.842</cell></row><row><cell>Note:</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>*** = p &lt; .001.Standard errors in brackets.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>doi:10.1371/journal.pone.0136675.t007</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>PLOS ONE | DOI:10.1371/journal.pone.0136675 September 1, 2015</p></note>
		</body>
		<back>

			<div type="funding">
<div><p>The research program as a whole was supported by funds from <rs type="funder">Swiss National Science Foundation</rs> grant (<rs type="grantNumber">100014-122491</rs>) and <rs type="funder">European Research Council</rs> Advanced grant (no. <rs type="grantNumber">230331</rs>). The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_EKEyR7j">
					<idno type="grant-number">100014-122491</idno>
				</org>
				<org type="funding" xml:id="_wdEUswa">
					<idno type="grant-number">230331</idno>
				</org>
			</listOrg>

			<div type="availability">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>All relevant data are within the paper and its Supporting Information files.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Vocal affect expression: A review and a model for future research</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Scherer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psych. Bull</title>
		<imprint>
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="page" from="143" to="165" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The dynamic architecture of emotion: evidence for the component process model</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Scherer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cogn Emot</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="1307" to="1351" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Communication of emotions in vocal expression and music performance: different channels, same code?</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">N</forename><surname>Juslin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Laukka</surname></persName>
		</author>
		<idno type="PMID">12956543</idno>
	</analytic>
	<monogr>
		<title level="j">Psychol Bull</title>
		<imprint>
			<biblScope unit="volume">129</biblScope>
			<biblScope unit="page" from="770" to="814" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Vocal communication of emotion: a review of research paradigms</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Scherer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Speech Commun</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="227" to="256" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">In the eye of the beholder? Universality and cultural specificity in the expression and perception of emotion</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Scherer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Clark-Polner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mortillaro</surname></persName>
		</author>
		<idno type="DOI">10.1080/00207594.2011.626049</idno>
		<idno type="PMID">22126090</idno>
	</analytic>
	<monogr>
		<title level="j">Int J Psychol</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="401" to="435" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Vocal behaviour</title>
		<author>
			<persName><forename type="first">S</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Scherer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Handbook of nonverbal communication</title>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Hall</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Knapp</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Mouton-DeGruyter</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="167" to="204" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The voice of emotion in Chinese and Italian young adults</title>
		<author>
			<persName><forename type="first">L</forename><surname>Anolli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Mantovani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">De</forename><surname>Toni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Cross Cult Psychol</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="565" to="598" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Vocal expression of emotion: acoustic properties of speech are associated with emotional intensity and context</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Bachorowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Owren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychol Sci</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="219" to="224" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Acoustic profiles in vocal emotion expression</title>
		<author>
			<persName><forename type="first">R</forename><surname>Banse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Scherer</surname></persName>
		</author>
		<idno type="PMID">8851745</idno>
	</analytic>
	<monogr>
		<title level="j">J Pers Soc Psychol</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="614" to="636" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Beyond arousal: valence and potency/control in the vocal expression of emotion</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">B</forename><surname>Goudbeek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Scherer</surname></persName>
		</author>
		<idno type="DOI">10.1121/1.3466853</idno>
		<idno type="PMID">20815467</idno>
	</analytic>
	<monogr>
		<title level="j">J Acoust Soc Am</title>
		<imprint>
			<biblScope unit="volume">128</biblScope>
			<biblScope unit="page" from="1322" to="1336" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Mapping emotions into acoustic space: the role of voice production</title>
		<author>
			<persName><forename type="first">S</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Scherer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Björkner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sundberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biol Psych</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="page" from="93" to="98" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">T</forename><surname>Hawk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Van</forename><surname>Kleef</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H</forename><surname>Van Der Schalk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename></persName>
		</author>
		<idno type="DOI">10.1037/a0015178</idno>
		<idno type="PMID">19485607</idno>
	</analytic>
	<monogr>
		<title level="m">Worth a thousand words&quot;: absolute and relative decoding of nonlinguistic affect vocalizations</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="293" to="305" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Perceiving verbal and vocal emotions in a second language</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">S</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Schirmer</surname></persName>
		</author>
		<idno type="DOI">10.1080/02699931.2010.544865</idno>
		<idno type="PMID">21432625</idno>
	</analytic>
	<monogr>
		<title level="j">Cogn Emot</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1376" to="1392" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Perceptual cues in nonverbal vocal expressions of emotion</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Sauter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Eisner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Calder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Scott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Q J Exp Psychol</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="page" from="2251" to="2272" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Emotion inferences from vocal expression correlate across languages and cultures</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Scherer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Banse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">G</forename><surname>Wallbott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Cross Cult Psychol</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="76" to="92" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Do emotions have distinct vocal profiles? A study of idiographic patterns of expression</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Spackman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">L</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">D</forename><surname>Otto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cogn Emot</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="1565" to="1588" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Levels of valence</title>
		<author>
			<persName><forename type="first">V</forename><surname>Shuman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sander</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Scherer</surname></persName>
		</author>
		<idno type="DOI">10.3389/fpsyg.2013.00261</idno>
		<ptr target="http://dx.doi.org/10.3389/fpsyg.2013.00261" />
	</analytic>
	<monogr>
		<title level="j">Front. Psychol</title>
		<imprint>
			<date type="published" when="2013-05-13">13 May 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Toward a Psychological Theory of Multidimensional Activation (Arousal)</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Thayer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Motiv. Emotion</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1" to="34" />
			<date type="published" when="1978">1978</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Perception and the representative design of psychological experiments</title>
		<author>
			<persName><forename type="first">E</forename><surname>Brunswik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1956">1956</date>
			<publisher>University of California Press</publisher>
			<pubPlace>Berkeley</pubPlace>
		</imprint>
	</monogr>
	<note>nd ed</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A lens-mapping framework for understanding the encoding and decoding of interpersonal dispositions in nonverbal behavior</title>
		<author>
			<persName><forename type="first">R</forename><surname>Gifford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Pers Soc Psychol</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="page" from="398" to="412" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The sounds and the sights of intelligence: a lens model channel analysis</title>
		<author>
			<persName><forename type="first">Daj</forename><surname>Reynolds</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gifford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pers Soc Psychol Bull</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="187" to="200" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Judging rapport: employing Brunswik&apos;s lens model to study interpersonal sensitivity</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">J</forename><surname>Bernieri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Gillis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Interpersonal sensitivity: theory and measurement</title>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Hall</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><forename type="middle">J</forename><surname>Bernieri</surname></persName>
		</editor>
		<meeting><address><addrLine>Mahwah, NJ</addrLine></address></meeting>
		<imprint>
			<publisher>Erlbaum</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="67" to="88" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The dual lens model: a comprehensive framework for understanding self-other agreement of personality judgments at zero acquaintance</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hirschmüller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Egloff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nestler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Back</surname></persName>
		</author>
		<idno type="DOI">10.1037/a0030383</idno>
		<idno type="PMID">23046068</idno>
	</analytic>
	<monogr>
		<title level="j">J Pers Soc Psychol</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="page" from="335" to="353" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">An integrative lens model approach to bias and accuracy in human inferences: hindsight effects and knowledge updating in personality judgments</title>
		<author>
			<persName><forename type="first">S</forename><surname>Nestler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Egloff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Acp</forename><surname>Küfner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Back</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Pers Soc Psychol</title>
		<imprint>
			<biblScope unit="volume">103</biblScope>
			<biblScope unit="page" from="698" to="717" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Accuracy and consensus in judgments of trustworthiness from faces: behavioral and neural correlates</title>
		<author>
			<persName><forename type="first">N</forename><surname>Rule</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krendl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ivcevic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ambady</surname></persName>
		</author>
		<idno type="DOI">10.1037/a0031050</idno>
		<idno type="PMID">23276271</idno>
	</analytic>
	<monogr>
		<title level="j">J Pers Soc Psychol</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="page" from="409" to="426" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A status-enhancement account of overconfidence</title>
		<author>
			<persName><forename type="first">C</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Brion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Kennedy</surname></persName>
		</author>
		<idno type="PMID">22800286</idno>
	</analytic>
	<monogr>
		<title level="j">J Pers Soc Psychol</title>
		<imprint>
			<biblScope unit="volume">103</biblScope>
			<biblScope unit="page" from="718" to="735" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">The sound of power: conveying and detecting hierarchical rank through voice</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Sadler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Galinsky</surname></persName>
		</author>
		<idno type="DOI">10.1177/0956797614553009</idno>
		<idno type="PMID">25413877</idno>
	</analytic>
	<monogr>
		<title level="j">Psychol Sci</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="3" to="14" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Emotional communication in music performance: a functionalist perspective and some data</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">N</forename><surname>Juslin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Music Percept</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="383" to="418" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">A functionalist perspective on emotional communication in music performance</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">N</forename><surname>Juslin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>Acta Universitatis Upsaliensis</publisher>
			<pubPlace>Uppsala, Sweden</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Cue utilization in communication of emotion in music performance: relating performance to perception</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">N</forename><surname>Juslin</surname></persName>
		</author>
		<idno type="PMID">11129375</idno>
	</analytic>
	<monogr>
		<title level="j">J Exp Psychol Hum Percept Perform</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="1797" to="1813" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Play it again with feeling: computer feedback in musical communication of emotions</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">N</forename><surname>Juslin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Karlsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Lindström</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Friberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Schoonderwaldt</surname></persName>
		</author>
		<idno type="PMID">16802890</idno>
	</analytic>
	<monogr>
		<title level="j">J Exp Psychol Appl</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="79" to="95" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Impact of intended emotion intensity on cue utilization and decoding accuracy in vocal expression of emotion</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">N</forename><surname>Juslin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Laukka</surname></persName>
		</author>
		<idno type="PMID">12901399</idno>
	</analytic>
	<monogr>
		<title level="j">Emotion</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="381" to="412" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Personality inference from voice quality: the loud voice of extroversion</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Scherer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Eur J Soc Psychol</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="467" to="487" />
			<date type="published" when="1978">1978</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Voice and emotion</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kappas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Hess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Scherer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fundamentals of nonverbal behavior</title>
		<editor>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Feldman</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Rime</surname></persName>
		</editor>
		<meeting><address><addrLine>Cambridge</addrLine></address></meeting>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="1991">1991</date>
			<biblScope unit="page" from="200" to="238" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Vocal expression of emotion</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Scherer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Johnstone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Klasmeyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Handbook of the affective sciences</title>
		<editor>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Davidson</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Goldsmith</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Scherer</surname></persName>
		</editor>
		<meeting><address><addrLine>Oxford</addrLine></address></meeting>
		<imprint>
			<publisher>Oxford University Press</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="433" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Language, music, and the brain: a mysterious relationship</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Scherer</surname></persName>
		</author>
		<editor>Arbib A</editor>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>MIT Press</publisher>
			<biblScope unit="page" from="107" to="139" />
			<pubPlace>Cambridge (MA</pubPlace>
		</imprint>
	</monogr>
	<note>Emotion in action, interaction, music, and speech</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Determinants of linear judgment: a meta-analysis of lens studies</title>
		<author>
			<persName><forename type="first">N</forename><surname>Karelaia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Hogarth</surname></persName>
		</author>
		<idno type="DOI">10.1037/0033-2909.134.3.404</idno>
		<idno type="PMID">18444703</idno>
	</analytic>
	<monogr>
		<title level="j">Psychol Bull</title>
		<imprint>
			<biblScope unit="volume">134</biblScope>
			<biblScope unit="page" from="404" to="426" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Analyzing the components of clinical inference</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Hammond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Hursch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">J</forename><surname>Todd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychol Rev</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="page" from="238" to="456" />
			<date type="published" when="1964">1964</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A suggested alternative formulation in the developments by</title>
		<author>
			<persName><forename type="first">Lr ;</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName><surname>Hursch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">&amp;</forename><surname>Hammond</surname></persName>
		</author>
		<author>
			<persName><surname>Hursch</surname></persName>
		</author>
		<author>
			<persName><surname>Hammond</surname></persName>
		</author>
		<author>
			<persName><surname>Hursch</surname></persName>
		</author>
		<author>
			<persName><surname>Todd</surname></persName>
		</author>
		<idno type="PMID">14216901</idno>
	</analytic>
	<monogr>
		<title level="j">Psychol Rev</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="page" from="528" to="530" />
			<date type="published" when="1964">1964</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Path analysis: sociological examples</title>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">D</forename><surname>Duncan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Am J Sociol</title>
		<imprint>
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="page" from="1" to="16" />
			<date type="published" when="1966">1966</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">An actor prepares</title>
		<author>
			<persName><forename type="first">K</forename><surname>Stanislavski</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1936">1988. 1936</date>
			<pubPlace>London: Methuen</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Introducing the Geneva multimodal emotion portrayal (GEMEP) corpus</title>
		<author>
			<persName><forename type="first">T</forename><surname>Bänziger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Scherer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Blueprint for affective computing: a source book</title>
		<editor>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Scherer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Bänziger</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><forename type="middle">B</forename><surname>Roesch</surname></persName>
		</editor>
		<meeting><address><addrLine>Oxford</addrLine></address></meeting>
		<imprint>
			<publisher>Oxford University Press</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="271" to="294" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Introducing the Geneva Multimodal Expression corpus for experimental research on emotion perception</title>
		<author>
			<persName><forename type="first">T</forename><surname>Bänziger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mortillaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Scherer</surname></persName>
		</author>
		<idno type="DOI">10.1037/a0025827</idno>
		<idno type="PMID">22081890</idno>
	</analytic>
	<monogr>
		<title level="j">Emotion</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="1161" to="1179" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">The role of perceived voice and speech characteristics in vocal emotion communication</title>
		<author>
			<persName><forename type="first">T</forename><surname>Bänziger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Scherer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Nonverbal Behav</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="31" to="52" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Praat: doing phonetics by computer (Version 5.1.43)</title>
		<author>
			<persName><forename type="first">P</forename><surname>Boersma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Weenink</surname></persName>
		</author>
		<ptr target="http://www.praat.org/" />
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
	<note>Computer program</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Some methodological considerations in multiple-cue probability learning studies</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Hursch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Hammond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Hursch</surname></persName>
		</author>
		<idno type="PMID">14105718</idno>
	</analytic>
	<monogr>
		<title level="j">Psychol Rev</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="page" from="42" to="60" />
			<date type="published" when="1964">1964</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Introduction to structural equation modeling: Issues and practical considerations</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">W</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Educ. Meas</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="33" to="43" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">K</forename><surname>Muthén</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">O</forename><surname>Muthén</surname></persName>
		</author>
		<title level="m">Los Angeles: Muthén &amp; Muthén</title>
		<imprint>
			<date type="published" when="1998">1998-2012</date>
		</imprint>
	</monogr>
	<note>Mplus user&apos;s guide</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Characteristics and recognizability of vocal expressions of emotion. Doctoral dissertation</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Van Bezooijen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1984">1984</date>
			<publisher>Foris Publications</publisher>
			<pubPlace>Dordrecht, The Netherlands</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Vocal affect expression: a review and a model for future research</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Scherer</surname></persName>
		</author>
		<idno type="PMID">3515381</idno>
	</analytic>
	<monogr>
		<title level="j">Psychol Bull</title>
		<imprint>
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="page" from="143" to="165" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Happy talk: perceptual and acoustic effects of smiling on speech</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">C</forename><surname>Tartter</surname></persName>
		</author>
		<idno type="PMID">7367197</idno>
	</analytic>
	<monogr>
		<title level="j">Percept Psychophys</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="24" to="27" />
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">An ethological perspective on common cross-language utilization of F0 of voice</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Ohala</surname></persName>
		</author>
		<idno type="PMID">6204347</idno>
	</analytic>
	<monogr>
		<title level="j">Phonetica</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="1" to="16" />
			<date type="published" when="1984">1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Encoding emotions in speech with the size code. A perceptual investigation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chuenwattanapranithi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Thipakorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Maneewongvatana</surname></persName>
		</author>
		<idno type="DOI">10.1159/000192793</idno>
		<idno type="PMID">19221452</idno>
	</analytic>
	<monogr>
		<title level="j">Phonetica</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="page" from="210" to="230" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">The Geneva Minimalistic Acoustic Parameter Set (GeMAPS) for Voice Research and Affective Computing</title>
		<author>
			<persName><forename type="first">F</forename><surname>Eyben</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Scherer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schuller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sundberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>André</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Busso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Affective Computing</title>
		<imprint/>
	</monogr>
	<note>in press</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">What makes a voice masculine: physiological and acoustical correlates of women&apos;s ratings of men&apos;s vocal masculinity</title>
		<author>
			<persName><forename type="first">V</forename><surname>Cartei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Reby</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.yhbeh.2014.08.006</idno>
		<idno type="PMID">25169905</idno>
	</analytic>
	<monogr>
		<title level="j">Horm. Behav</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="page" from="569" to="576" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
