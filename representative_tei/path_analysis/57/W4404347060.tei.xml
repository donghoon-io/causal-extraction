<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Zero-shot Class Unlearning via Layer-wise Relevance Analysis and Neuronal Path Perturbation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2025-06-20">20 Jun 2025</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Wenhan</forename><surname>Chang</surname></persName>
						</author>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Tianqing</forename><surname>Zhu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ping</forename><surname>Xiong</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yufeng</forename><surname>Wu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Faqian</forename><surname>Guan</surname></persName>
						</author>
						<author>
							<persName><roleName>Life Fellow, IEEE</roleName><forename type="first">Wanlei</forename><surname>Zhou</surname></persName>
						</author>
						<title level="a" type="main">Zero-shot Class Unlearning via Layer-wise Relevance Analysis and Neuronal Path Perturbation</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2025-06-20">20 Jun 2025</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2410.23693v2[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-29T01:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Machine unlearning</term>
					<term>privacy preservation</term>
					<term>neuronal path perturbation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Machine unlearning is a technique that removes specific data influences from trained models without the need for extensive retraining. However, it faces several key challenges, including the lack of explanation, privacy concerns during the unlearning process, and the high demand for time and computational resources. This paper presents a novel unlearning approach to tackle above challenges by employing Layer-wise Relevance Analysis and Neuronal Path Perturbation. Our method balances machine unlearning performance and model utility by identifying and perturbing highly relevant neurons, thus achieving effective unlearning. Using unseen data that has not been presented in the original training set, our method achieves zero-shot unlearning, which allows for the removal of specific class knowledge without accessing the original training data during the unlearning process. This approach ensures robust privacy protection. Experimental results demonstrate that our approach effectively removes targeted data from the target unlearning model while maintaining the model's utility, offering a practical solution for privacy-preserving machine learning.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>I N recent years, with the growing public attention on Artificial Intelligence (AI) models, research has increasingly focused on privacy concerns stemming from user-model interactions. Once a model is trained, users may wish to withdraw their data and remove its influence from the model. Traditionally, this can be achieved by retraining the model, but retraining is time consuming and inefficient for large datasets, such as those used in large language models (LLM) <ref type="bibr" target="#b0">[1]</ref>. The goal of unlearning is to address this issue by selectively removing the model's memory of specific data without retraining the entire model. This can be done through data reorganization or model manipulation, allowing compliance with data deletion requests and safeguarding user privacy <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>.</p><p>Machine unlearning research is primarily divided into class unlearning and sample unlearning <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b3">[4]</ref>. Class unlearning aims to eliminate all knowledge the model has acquired from a specific data class. This type of unlearning is often necessary when data usage policies require the deletion of certain users' data, when the data has lost its relevance, or when it has introduced bias. In contrast, sample unlearning focuses on Wenhan Chang and Ping Xiong are with the School of Information Engineering, Zhongnan University of Economics and Law.</p><p>Yufeng Wu is with the China University of Geosciences, Wuhan, Tianqing Zhu, Faqian Guan and Wanlei Zhou are with the City University of Macau.</p><p>Tianqing Zhu is the corresponding author. E-mail: tqzhu@cityu.edu.mo removing the influence of specific data samples from the model. This approach is commonly applied when users request the deletion of their data or when problematic data samples need to be excluded. Class unlearning methods have been applied to a wide range of AI tasks. For example, Chen et al. <ref type="bibr" target="#b4">[5]</ref> achieved unlearning in image classification models by manipulating the model's decision space. Ma et al. <ref type="bibr" target="#b5">[6]</ref> implemented unlearning using neuron masks derived from gradients. Furthermore, Sun et al. <ref type="bibr" target="#b6">[7]</ref> employed unlearning methods to make StyleGAN forget specific knowledge classes. Additionally, Chen et al. <ref type="bibr" target="#b7">[8]</ref> proposed an unlearning framework for Graph Neural Networks (GNNs). These examples illustrate that class unlearning goes beyond simply removing individual samples-it ensures that entire problematic or sensitive data categories can be eliminated, which is essential for building fair and privacypreserving AI systems.</p><p>As we explore the common issue of class unlearning in neural networks, a fundamental question arises: Can we achieve knowledge unlearning without compromising model utility? Delving into the challenges associated with balancing unlearning and model utility reveals three specific challenges:</p><p>• Lack of explainable machine unlearning methods. A prominent challenge in machine unlearning is the need for more basic explanations regarding the unlearning principle. Even though users submit unlearning requests to model owners, there's still a significant lack of clear explanations on how the unlearning methods influence the models' parameters. • Model owners may need to access users' original data to use unlearning methods. Many existing methods use the users' original data for unlearning, either by finetuning or by directly modifying the parameters. Using original data raises serious privacy concerns because it may expose sensitive information during model updates. • High cost on time and computational resources Many existing unlearning methods necessitate extensive calculations, which can involve retraining or fine-tuning models based on large datasets.</p><p>We address the three challenges by investigating the relevance between the model's neurons and the unlearning information."Relevance" refers to the contribution or importance of each neuron in the neural network to the final prediction. We identify the path of neurons with high relevance, which we define as the classification path. Our goal is to determine the neuron classification path by assessing the relevance of each neuron to the unlearning information. Through layer-wise relevance analysis, we can explain the internal principles of machine unlearning.</p><p>For the second challenge, our solution follows the concept of zero-shot machine unlearning as defined by Chundawat et al. <ref type="bibr" target="#b8">[9]</ref>. In their work, zero-shot unlearning refers to performing the unlearning process without needing access to the original training dataset (including retain data and forget data), rather than not using any data. Based on this definition, our method utilizes "unseen samples", which are data not present in the original training dataset, to perform the unlearning process. These unseen samples can be either directly extracted from the test set or generated by generative models. By relying solely on such data, we effectively adhere to the zero-shot unlearning paradigm and help ensure the privacy of users' original training data during the machine unlearning process.</p><p>In terms of the method's implementation efficiency, the proposed method does not need to modify the overall model parameters, significantly mitigate the time and computational cost. We analyze the data related to the unlearning class information and identify the neurons that show high relevance. Based on this, our method applies Neuronal Path Perturbation by selectively disconnecting these neurons within a single fully connected layer from their connections to preceding and succeeding layers. This action perturbs the model's classification path, causing it to lose the ability to classify the unlearning class data. Due to the inherent redundancy in neural network paths for other classes and our perturbation being limited to a single layer, the model's overall utility is largely preserved.</p><p>In summary, we introduce layer-wise relevance analysis and neuronal path perturbation to fix the privacy and effectiveness challenges. To ensure robust privacy protection, we minimize the exposure of model owners to users' data. Also, we implement unlearning techniques within the neural network architecture to balance unlearning effectiveness and model utility. The key contributions of this paper can be summarized as follows:</p><p>• We propose an explainable zero-shot unlearning method significantly improving unlearning speed and reducing resource consumption. Using layer-wise relevance analysis and neuronal path perturbation, we can adjust neuron weights highly related to unlearning information within the model. Thereby, we can achieve effective unlearning without compromising the model's utility. • Our method can achieve class unlearning without accessing the user's original data. We extract data closely related to the unlearning information from AI-generated contexts or datasets without samples from the training set to identify neurons within the neural network that are relevant to unlearning information without users' data. • We demonstrate that unlearning in neural networks can be approached from the perspective of network inference by conducting experiments. Our method addresses traditional parameter adjustments and delves into the redundant characteristics within neural networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORKS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Machine Unlearning</head><p>As the importance of data privacy protection continues to rise, class unlearning has emerged as a critical technique in machine unlearning, gaining increasing attention from researchers. However, achieving this goal is not straightforward; thoroughly exploring existing unlearning methods can provide valuable insights for our research and enhance our understanding of the challenges.</p><p>First, Bourtoule et al. <ref type="bibr" target="#b9">[10]</ref> introduced the SISA training framework, which accelerates unlearning by limiting the impact of data points during training, making it easier for users to revoke data access and request deletion after sharing. Chen et al. <ref type="bibr" target="#b4">[5]</ref> developed Boundary Unlearning, which removes a class from a DNN by shifting its decision boundary, enabling rapid unlearning. Jia et al. <ref type="bibr" target="#b10">[11]</ref> proposed a model-based approach to unlearning, using weight pruning for model sparsification in a "prune first, then unlearn" method. Researchers can also use unlearning methods to remove model bias. Chen et al. <ref type="bibr" target="#b11">[12]</ref> introduced the FMD framework, which identifies and removes biases from trained models using counterfactual concepts and influence functions. Kurmanji et al. <ref type="bibr" target="#b12">[13]</ref> proposed SCRUB, a versatile unlearning algorithm for removing biases, resolving confusion, and protecting user privacy across various applications. Foster et al. <ref type="bibr" target="#b13">[14]</ref> introduced Selective Synaptic Dampening (SSD), a retrain-free unlearning method that uses the Fisher information matrix to dampen parameters critical to forgotten data. Liu et al. <ref type="bibr" target="#b14">[15]</ref> explored security risks in unlearning, proposing two backdoor attack methods: one exploiting unlearning requests without data poisoning, and another involving poisoned data with a trigger activated by malicious unlearning.</p><p>However, most unlearning methods require the use of original training data, which increases the frequency of privacy data exposure and the risk of leakage. As a result, researchers have begun to focus on unlearning methods that do not rely on the training data. For example, Chundawat et al <ref type="bibr" target="#b8">[9]</ref> introduced zero-shot machine unlearning, tackling unlearning without access to original training data. They propose two methods-error-minimizing noise and gated knowledge transfer-to remove the influence of forgotten data while preserving model performance on retained data. Tarun et al. <ref type="bibr" target="#b15">[16]</ref> presented a fast, scalable unlearning framework using errormaximizing noise and weight manipulation. Their impairrepair approach unlearns specific data classes without the full training dataset, followed by a repair step to recover model performance. Existing research methods have also extended to feature unlearning.</p><p>Beyond the class or sample level, machine unlearning research is expanding. Researchers are now also conducting more thorough investigations at the feature level, seeking more detailed privacy protection. Xu et al. <ref type="bibr" target="#b16">[17]</ref> introduced a "feature unlearning" scheme to address the limitations of existing machine unlearning methods, which typically focus on removing entire instances or classes. Recognizing the need for feature-level unlearning to maintain model utility. They further proposed an approach <ref type="bibr" target="#b17">[18]</ref> constructed an essential graph to describe relationships between critical parameters, identify parameters important to both the forgotten and retained targets and use a pruning-based technique to remove information about the target to be unlearned effectively.</p><p>Our approach differs significantly by operating at the neuron level, focusing on the relevance of individual neurons to the target class to be unlearned. This enables a highly fine-grained and precise unlearning process, thereby enhancing its overall granularity. Furthermore, our method prioritizes user privacy by not requiring access to the original training data, effectively achieving zero-shot unlearning. This distinct approach ensures efficient unlearning while effectively maintaining model utility.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Layer-wise Relevance Propagation</head><p>Layer-wise Relevance Propagation (LRP) aims to provide interpretability for deep learning models by attributing model predictions back to input features. It redistributes the output relevance scores to highlight which features are most influential in driving the model's decisions, enhancing transparency and trust in its behavior.</p><p>Bach et al. <ref type="bibr" target="#b18">[19]</ref> contributed to the development of layer-wise relevance propagation, a methodology that enables pixel-wise decomposition of classification decisions, enhancing the interpretability of automated image classification systems. Binder et al. <ref type="bibr" target="#b19">[20]</ref> proposed an approach to extend layer-wise relevance propagation to neural networks with local renormalization layers, which is a very common product-type non-linearity in convolutional neural networks.</p><p>Today, research on LRP still holds a significant influence. Li et al. <ref type="bibr" target="#b20">[21]</ref> proposed the Weight-dependent Baseline Layer-wise Relevance Propagation algorithm, which improves instancelevel explanations by addressing limitations in existing LRP methods that disregard model weights or sample features. Hatefi et al. <ref type="bibr" target="#b21">[22]</ref> proposed optimizing the hyperparameters of attribution methods from eXplainable AI for network pruning, demonstrating improved model compression rates for both transformer-based and convolutional networks.</p><p>Based on the above work, Layer-wise Relevance Propagation can quantitatively analyze the relevance between neurons in a neural network and specific data categories. This helps us identify which neurons are associated with the categories of unlearning data, allowing for more precise and efficient completion of the unlearning process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. BACKGROUNDS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Machine Unlearning</head><p>Machine unlearning is the process of removing specific data's influence from a trained machine-learning model.</p><p>Consider a machine learning model with a prediction function F , trained on a dataset D via optimizing an accuracy loss function L. The model's parameters after training are represented by θ. When we need to unlearn specific data (x u , y u ), we update the training dataset to D ′ by removing the data that needs to be forgotten as Equation <ref type="formula" target="#formula_0">1</ref>. As shown in Equation <ref type="formula" target="#formula_1">2</ref>, the updated dataset D ′ is then used to retrain the model, resulting in new parameters θ ′ that ideally do not retain any influence from the removed data.</p><formula xml:id="formula_0">D ′ = D \ {(x u , y u )}<label>(1)</label></formula><formula xml:id="formula_1">θ ′ ≈ argmin θ (x,y)∈D ′ L(F (x; θ), y)<label>(2)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Layer-wise Relevance Propagation</head><p>Layer-wise Relevance Propagation is a technique used to interpret the decisions made by deep neural networks. Let F (x) represent the model's prediction function for an input x. Given a neural network, LRP aims to attribute the prediction F (x) to the individual input features by propagating relevance scores through the network layers.</p><p>For a given class C i , the relevance score R o at the output layer l o is initialized as the prediction for that class. If the neural network outputs a probability distribution over classes, the relevance score R o j for each output neuron j corresponding to class C i can be initialized as:</p><formula xml:id="formula_2">R o j = δ ji • F Ci (x), δ ij = 1 if j = i 0 otherwise<label>(3)</label></formula><p>In Equation <ref type="formula" target="#formula_2">3</ref>, δ is the Kronecker delta function, which is 1 if j = i and 0 otherwise, and F Ci (x) is the output of the network for class C i . This initialization means the relevance score is set to the model's output for the target class and zero for other classes.</p><p>The relevance scores are propagated from the output layer back to the input layer using a set of propagation rules. Let R l k be the relevance score of neuron k in layer l, and we denote all the different neurons in layer l as k ′ . The relevance score is propagated from layer l + 1 to the layer l according to the following general rule:</p><formula xml:id="formula_3">R l k = j z kj k ′ z k ′ j R l+1 j , k ∈ l, k ′ ∈ l, j ∈ l + 1<label>(4)</label></formula><p>Unlearning Requests</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Layer-wise Relevance Propagation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Analyse Neurons Occurrence Frequency</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Layer-wise Relevance Analysis</head><p>Confirm the Target Neurons Quantity</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conduct the Perturbation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Neuronal Path Perturbation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Unlearned Model</head><p>Model Owner Prepare Data Fig. <ref type="figure">1</ref>. The overview of the unlearning method. The flowchart outlines the process from data preparation and layer-wise relevance analysis to neuronal path perturbation. Specific steps include preparing data, using layer-wise relevance propagation to analyze neuron occurrence frequency, confirming the target number of neurons, and conducting neuronal path perturbation to create an unlearned model.</p><p>where z kj = a k • w kj represents the contribution of neuron k to j, with a k being the activation of neuron k and w kj being the weight connecting neuron k to j.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. METHODOLOGY A. Threat Model</head><p>Purpose: Our method aims to achieve class unlearning efficiently. We strive to minimize disruption to the model's utility during unlearning. Overall, our method seeks to balance the unlearning results with the utility of the unlearned model.</p><p>Users: Users can know that their data is used to train the model and might be aware of privacy policies and data usage agreements. They can request the unlearning of that specific class and withdraw consent for its use in the model. After requesting class unlearning, users may receive confirmation that the model's knowledge of data from that class has been removed.</p><p>Model owner: Model owners cannot directly access user data and only have insight into the class distribution of the training dataset. They understand the model's performance metrics and how unlearning affects it. They can locate and delete specific user data upon request, retrain, or adjust the model to maintain its performance, and ensure compliance with relevant laws. They may need to provide users with proof of unlearning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Overview of Our Method</head><p>Our method for achieving machine unlearning comprises several steps. As illustrated in Figure <ref type="figure">1</ref>, after receiving unlearning requests from users, the model owner prepares dataset D ′ , extracted from the test set, for subsequent processing. The two primary steps involved are Layer-wise Relevance Analysis and Neuronal Path Perturbation.</p><p>Layer-wise Relevance Analysis. This step obtains and analyzes the relevance of all neurons to the classification task using Layer-wise Relevance Propagation. According to LRP rules r, the contributions of the model's inputs to the predictions are determined through backward propagation through the network layers. By comparing the relevance scores R of neurons in each layer, we can assess their importance to the classification. As depicted in Figure <ref type="figure">1</ref>, analyzing the relevance for each unlearning sample allows us to identify neurons that are frequently highly relevant or critical for the target classification, especially considering the specific characteristics of the samples to be unlearned.</p><p>Neuronal Path Perturbation. This step involves perturbing selected neurons, particularly those identified as relevant or critical, to achieve unlearning. Perturbation can be applied, for example, to neurons in the fully connected layer preceding the output layer. Perturbing only the output layer would be akin to simply suppressing the model's output for a specific class, rather than truly unlearning. As illustrated in Figure <ref type="figure">1</ref>, we first determine the quantity m p of target neurons to be perturbed. We then perturb these neurons to disrupt or sever the model's neuronal pathways responsible for the target classification, thereby achieving machine unlearning.</p><p>Following these two steps, a class-unlearned model is obtained without requiring additional training. The details of our method are described in the following section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Layer-wise Relevance Analysis</head><p>A neural network can be viewed as a complex function that performs a specific task. Network predictions result from internal interactions among its neurons. To achieve unlearning without additional training, we need to understand the internal workings of neural networks. However, obtaining the relevance of neurons to the model's inputs is challenging due to the black-box nature of neural networks.</p><p>To address this challenge, we first apply LRP to obtain an internal relevance score R for each neuron. We utilize dataset D ′ to conduct the relevance analysis. While LRP is typically applied to inputs, here we use D ′ (which contains data related to the unlearning class) to analyze relevance patterns across multiple relevant samples. The relevance R is conserved through the layers during backpropagation, ensuring that the total relevance is preserved from the output layer back to the input layer. This backward propagation process can be described by Equation <ref type="formula" target="#formula_4">5</ref>:</p><formula xml:id="formula_4">R l i = j∈(l+1) R l,l+1 i←j<label>(5)</label></formula><p>Here, based on LRP, R l i is the relevance assigned to neuron i at layer l, and R l,l+1 i←j is the portion of relevance propagated from neuron j at layer l + 1 to neuron i at layer l. For layer l, the total relevance of neuron i (R l i ) is computed by summing all relevance propagated to it from neurons at layer l + 1.</p><p>Using this method, we calculate the relevance R l j for each neuron j in layer l with respect to a given input x. For layer l, assuming there are m neurons with positions p 0 , . . . , p m-1 such that p 0 &lt; p 1 &lt; • • • &lt; p m-1 , we create a list of pairs (R l j , p j ) ordered by neuron position. We represent this list as:</p><formula xml:id="formula_5">List(l, x) = ((R l 0 , p 0 ), . . . , (R l j , p j ), . . . , (R l m-1 , p m-1 )), p 0 &lt; p 1 &lt; . . . &lt; p m-1<label>(6)</label></formula><p>We identify the top k neurons with the highest relevance scores R in List(l, x) as high-relevance neurons for input x. By extracting these top k pairs and sorting them in descending order of R, we obtain List(l, x, k) as defined in Equation <ref type="formula" target="#formula_6">7</ref>. Unlike Equation <ref type="formula" target="#formula_5">6</ref>, the pairs (R, p) in List(l, x, k) are arranged in descending order of R, rather than by neuron position.</p><formula xml:id="formula_6">List(l, x, k) = ((R l 0 , p 0 ), . . . , (R l k-1 , p k-1 )), where R l 0 ≥ R l 1 ≥ . . . ≥ R l k-1 , k ∈ [1, m]<label>(7)</label></formula><p>However, the analysis described above is performed for a single input sample x. To identify a set of neurons consistently relevant across the data associated with the unlearning class, we analyze dataset D ′ . As mentioned, D ′ includes data samples from the unlearning class, either extracted from open-source datasets or generated synthetically to protect user privacy. Let D ′ = {x 1 , . . . , x n } be the set of n data samples in D ′ . After conducting the top-k relevance analysis for layer l on each sample x j ∈ D ′ , we calculate the occurrence frequency of each neuron position to mitigate the influence of samplespecific variations.</p><formula xml:id="formula_7">C(p) = n j=1 I (p ∈ {p | (R, p) ∈ List(l, x j , k)})<label>(8)</label></formula><p>As defined in Equation <ref type="formula" target="#formula_7">8</ref>, C(p) is the occurrence frequency of a specific neuron position p across all n top-k relevance lists List(l, x j , k) for samples x j ∈ D ′ . The indicator function I(•) evaluates to 1 if the condition is true (i.e., if neuron position p is present in the list of top-k neuron positions for sample x j ) and 0 otherwise. For all unique neuron positions that appeared in any of the top-k lists (i.e., positions p with C(p) &gt; 0), we obtain their occurrence frequency C(p) using the method described above. Let P unique = {p | C(p) &gt; 0} be the set of these z = |P unique | unique neuron positions. Next, as represented in Equation <ref type="formula" target="#formula_8">9</ref>, we sort these neuron positions based on their frequencies C(p) in descending order to identify the list of strongly relevant neurons S l for the unlearned class in layer l.</p><p>It is worth mentioning that due to sample-specific variations in relevance, the set of top-k relevant neurons can vary across different samples. If we denote the number of unique neuron positions that appear in at least one of the n top-k lists as z, the value of z will range between k and m. S l = (p 0 , p 1 , . . . , p z-1 ),</p><formula xml:id="formula_8">p ∈ P unique and C(p 0 ) ≥ C(p 1 ) ≥ . . . ≥ C(p z-1 )<label>(9)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Neuronal Path Perturbation</head><p>In neural networks for image classification, the initial layers typically extract low-level features such as edges and textures. Middle layers capture higher-level features like shapes and parts. In contrast, the FC layers transform these features into more abstract representations for classification. Therefore, we perturb the FC layer l to disrupt the neuronal classification path.</p><p>First, we identify the weights connected to the neurons that need to be perturbed. These are the weights connecting the preceding layer l -1 to the selected neurons in layer l. As identified by the relevance analysis in the previous section, the set of strongly relevant neuron positions in layer l for the unlearned class is S l . In Equation <ref type="formula" target="#formula_9">10</ref>, we represent the set containing all incoming weights connected to the neurons identified in S l as W l .</p><formula xml:id="formula_9">W l = {w 0 , w 1 , ..., w z-1 } , z ∈ [k, m]<label>(10)</label></formula><p>To facilitate unlearning of a specific class, we apply targeted dropout by modifying the weights connected to the neurons in layer l responsible for classifying this class. As Equation <ref type="formula" target="#formula_8">9</ref>shows, S l is the list of strongly relevant neuron positions part of the neuronal classification path for the unlearning class in layer l. We implement this by setting the incoming weights to the neurons whose positions are in S l to 0. Specifically, for each neuron position p ∈ S l , all weights connecting from layer l -1 to the neuron at position p in layer l are set to 0. This effectively disrupts the classification path.</p><p>This neuronal path perturbation modifies the forward propagation. We implement this by setting the weights corresponding to neurons in S l to 0. This creates modified network parameters θ ′ , where the weights in layer l connected to neurons in S l are zeroed out. The updated prediction function is then computed using these modified parameters, denoted as</p><formula xml:id="formula_10">F ′ (x) = F (x, θ ′ ).</formula><p>By applying this targeted weight masking (or dropout), the network unlearns the specific class, as the classification path through layer l is effectively severed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Relationship Between Neuron Relevance and Unlearning</head><p>We demonstrate that by disabling key neurons highly relevant to a specific class in a neural network, we can effectively make the network unlearn that class. To identify these neurons, we need relevance scores to analyze the weights of neurons within the neural network. This leads to the following claim:</p><p>Claim 1: Setting the weights w p of neurons p with high relevance scores R p (x u ) for the unlearning class C i to zero effectively achieves unlearning of class C i .</p><p>This operation results in the model's prediction function F (x; θ) losing accuracy for classifying data from class C i , thus realizing effective unlearning. Specifically, for an input sample x u belonging to class C i , the weights are adjusted as follows:</p><formula xml:id="formula_11">w p = 0 if R p (x u ) ≫ R q (x u</formula><p>) for all q / ∈ {p} relevant,u w p otherwise <ref type="bibr" target="#b10">(11)</ref> where {p} relevant,u is the set of neurons with high relevance scores for the input sample x u . This adjustment modifies the model's prediction function as shown in Equation <ref type="formula" target="#formula_12">12</ref>, where f (•) is the activation function applied at the final layer, and a represents the activation values.</p><formula xml:id="formula_12">F (x; θ ′ ) = f     p∈{p}relevant,u Rp(xu)≫Rq(xu) 0 • a p + q / ∈{p}relevant,u w q a q     = f   q / ∈{p}relevant,u w q a q  <label>(12)</label></formula><p>To prove this, consider the effect of changes in the activations of the highly relevant neurons for a given input x, denoted as the set {p} relevant,x , on the model output F (x; θ). The output F (x; θ) can be expressed as a function of the activations of both relevant and irrelevant neurons, as stated in Claim 2.</p><p>Claim 2: The output F (x; θ) is a weighted sum of the activations a p of both relevant and irrelevant neurons.</p><p>The output is then passed through an activation function f (•) to introduce non-linearity. Specifically, the output can be expressed as Equation <ref type="formula">13</ref>, where w p and a p represent the weight and activation associated with neuron p.</p><formula xml:id="formula_13">F (x; θ) = f   p∈{p}relevant,x w p a p + p∈{p}irrelevant,x w p a p   (13)</formula><p>where {p} irrelevant,x represents the set of neurons with low relevance scores for the input x.</p><p>Let's now analyze the effect of perturbing the activations of the relevant neurons for a given input x. We first define the change in the model output as follows:</p><p>Claim 3: The change in the model output ∆F (x; θ) is caused by the change in the activations ∆a p of the neurons, and the magnitude of this change is determined by the sensitivity of the neuron, ∂F (x;θ)  ∂ap . Based on Claim 2, if we introduce a small change ∆a p to each activation a p for p ∈ {p} relevant,x , the resulting change in the output ∆F (x; θ) can be approximated using a first-order Taylor expansion around the initial activations:</p><formula xml:id="formula_14">∆F (x; θ) ≈ p∈{p}relevant, x ∂F (x; θ) ∂a p ∆a p + 1 2 p,q∈{p}relevant, x ∂ 2 F (x; θ) ∂a p ∂a q ∆a p ∆a q + • • •<label>(14)</label></formula><p>For small perturbations ∆a p , higher-order terms can be ignored, leaving the linear approximation:</p><formula xml:id="formula_15">∆F (x; θ) ≈ p∈{p}relevant,x ∂F (x; θ) ∂a p ∆a p (<label>15</label></formula><formula xml:id="formula_16">)</formula><p>where ∂F (x;θ) ∂ap is the sensitivity of the output to the activation a p of neuron p for the input x. Since neurons in {p} relevant,x have high relevance scores R p (x), they typically exhibit large values for ∂F (x;θ)  ∂ap , meaning that even small changes ∆a p can lead to significant changes in F (x; θ).</p><p>Thus, we have:</p><formula xml:id="formula_17">|∆F (x; θ)| = p∈{p}relevant,x ∂F (x; θ) ∂a p ∆a p ≫ 0<label>(16)</label></formula><p>Equation 16 demonstrates that the model's output is highly sensitive to changes in the activations of neurons with high relevance scores R p (x) for the input x. Consequently, these relevant neurons play a crucial role in determining the accuracy of the model's classification performance on the sample x.</p><p>Claim 4: Specific neurons store key information about a class, leading to their high relevance to the data of that class. High-relevance neurons for class C i , denoted as {p} Ci , can be distinguished from low-relevance ones based on their significant relevance scores R p (x), where R p (x) ≫ R q (x) for p ∈ {p} Ci and q / ∈ {p} Ci for data x belonging to class C i . Different neurons exhibit varying sensitivities to features, with some neurons having higher relevance scores R p (x) for key characteristics of specific categories. These neurons, p ∈ {p} Ci , are crucial for classification, and we may observe some overlap between categories:</p><formula xml:id="formula_18">{p} Ci ∩ {p} Cj ̸ = ∅, ∀C i , C j ∈ C, i ̸ = j<label>(17)</label></formula><p>Therefore, identifying neurons highly relevant to the unlearning data x u is essential for machine unlearning. For a sample x u belonging to the unlearning class, let {p} relevant,u represent the subset of neurons with high relevance scores R p (x u ). These neurons are crucial for capturing and representing the key features of x u , as expressed by: R p (x u ) ≫ R q (x u ) for p ∈ {p} relevant,u , q / ∈ {p} relevant,u (18) By identifying the neurons in {p} relevant,u , we locate those primarily responsible for encoding information about the unlearning data.</p><p>Based on Claim 2 to 4, a subset of neurons with high relevance plays a key role in the classification of a specific class. Therefore, to achieve Claim 1, we set the weights w p of these highly relevant neurons for the unlearning sample x u to zero, effectively removing their contributions to the model's output for that sample. Substituting w p = 0 for p ∈ {p} relevant,u in the output function:</p><formula xml:id="formula_19">F (x; θ ′ ) = f   p∈{p}relevant,u 0 • a p + p∈{p}irrelevant,u w p a p   = f   p∈{p}irrelevant,u w p a p   (<label>19</label></formula><formula xml:id="formula_20">)</formula><p>where {p} irrelevant,u represents the set of neurons with low relevance scores for the input sample x u .</p><p>V. EXPERIMENT</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Datasets and Models</head><p>We aaplied MNIST, CIFAR-10, CIFAR-100, and mini-ImageNet datasets for evaluating the performance of our method. Our experiments preprocessed the CIFAR-10, CIFAR-100, and mini-ImageNet datasets to adjust their resolutions, making them more aligned with real-world image scenarios. Table <ref type="table" target="#tab_1">II</ref> provides each dataset's image resolution and the corresponding classification model. Furthermore, to ensure that our method satisfies the definition of "Zero-shot Unlearning," which requires the model owner to unlearn a class without accessing the training data, we extract data from the test set of each dataset as "unseen" samples for layer-wise relevance analysis. Based on this classification result, F r is calculated as the proportion of unlearned data points that this attack fails to identify as members. A higher F r value indicates more successful forgetting. • Gigabyte (G). In this context, Gigabyte is employed to measure the unlearning computational resource consumption of GPU memory. We denote Gigabyte as G. A higher G signifies a higher resource consumption. Specifically, G quantifies the peak GPU memory required by the unlearning algorithm during its execution. • Time. The time spent by the unlearning process is demonstrated as T . A less T indicates greater efficiency. This measures the total wall-clock time required for the unlearning algorithm to complete.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2) Comparing methods list:</head><p>• Retrain from Scratch: Retrains the model from scratch after removing specific data, completely erasing the influence of the removed data. • Random Labels <ref type="bibr" target="#b22">[23]</ref>: Randomizes the target class labels, making it harder for the model to recognize and learn from those data, achieving unlearning. • Amnesiac Unlearning <ref type="bibr" target="#b23">[24]</ref>: Tracks parameter updates during training and selectively undoes updates from batches containing sensitive data to remove its influence. • Boundary Shrink <ref type="bibr" target="#b4">[5]</ref>: Targets the decision boundary by reassigning unlearning samples to incorrect classes, shrinking the decision space of the target class. • Boundary Expanding <ref type="bibr" target="#b4">[5]</ref>: Expands the decision space by introducing a shadow class to disperse activations, then prunes it after finetuning to remove unlearning data's influence.</p><p>• Neuronal Path Noise: Introduces Gaussian (GN) or Laplacian (LN) noise to neurons identified as relevant to the unlearning dataset, disrupting the model's ability to classify those data accurately. When applying Gaussian noise, the parameters were set to a mean of 0.0 and a standard deviation of 1.0; for Laplace noise, the parameters were set to a location of 0.0 and a scale parameter of 1.0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Performance Evaluation</head><p>In image classification tasks, we apply the proposed unlearning method to remove a specific class from the MNIST, CIFAR-10, CIFAR-100, and mini-ImageNet datasets. Figures 2 to 5 show the attribution results of LRP for unlearning and non-unlearning data before and after machine unlearning.</p><p>1) Changes in quantitative metrics: We use retraining as the baseline for unlearning. The retrained model's performance on the target class serves as a reference for evaluating unlearning effectiveness. To assess the method, we apply the ϵ-rule in LRP to analyze neuron relevance. In Table <ref type="table" target="#tab_2">III</ref> to VI, for each column, the bold and underlined values represent the best and second-best results respectively. Our method delivers the most effective and well-balanced results.</p><p>For the MNIST dataset, we perform unlearning on class 1. We extract 36 images belonging to class 1 and have the model classify them while analyzing the relevance patterns among its neurons.</p><p>In Table <ref type="table" target="#tab_2">III</ref>, after applying our unlearning method, the model's accuracy A t on class 1 dropped to 0.00, matching the A t after retraining. This shows the model completely loses the ability to classify data from the unlearning class, cutting off the classification path by disrupting relevant neurons.</p><p>While removing the unlearning class path, the model's ability to classify other classes is preserved. After retraining, the model achieves an overall accuracy A g of 0.97 on the remaining classes. With our unlearning method, A g remains 0.97.</p><p>Our method also performs well in unlearning verification through membership inference. After retraining, the new model achieves a forgetting rate of 1.00 for the unlearning class, indicating full unlearning. Our approach achieves the same forgetting rate, confirming that severing the classification path effectively unlearns the class.</p><p>Moreover, our method is computationally efficient. Retraining requires 71.37 seconds and 0.74 GPU memory, while our unlearning method completes the task in 0.68 seconds with only 0.64 GPU memory. These results show that our method reduces resource consumption without additional training. The results in Table <ref type="table" target="#tab_0">IV</ref> show that Neuronal Path Perturbation is the most effective unlearning method for the CIFAR-10 dataset, achieving near-complete unlearning with A t = 0.03, similar to retraining (A t = 0.00). Its forgetting rate (F r = 0.99) is close to the ideal F r = 1.00, indicating strong unlearning.</p><p>At the same time, it maintains high accuracy on the remaining classes (A g = 0.97), outperforming most methods and matching retraining (A g = 0.96). This demonstrates its ability to preserve model utility while effectively unlearning the target class.</p><p>Additionally, Neuronal Path Perturbation is highly efficient, using only 2.82 GB of GPU memory and taking just 14.44 seconds, compared to retraining's 6.89 GB and 484.17 seconds, making it a practical and efficient choice for unlearning.</p><p>The CIFAR-100 class unlearning results show that Neuronal Path Perturbation strikes the best balance between effective unlearning and model utility. With A t = 0.01 on the target class, it achieves near-complete unlearning, matching retraining and random labels (A t = 0.00). Its forgetting rate (F r = 1.00) fully erases target class information. At the same time, it maintains high accuracy on remaining classes (A g = 0.82), matching the retrained model. This demonstrates its ability to preserve model utility while unlearning the target class.</p><p>In terms of resource efficiency, Neuronal Path Perturbation uses only 2.77 GB of GPU memory, far less than retraining (6.89 GB), and completes the process in 0.53 seconds, compared to retraining's 607.89 seconds. This makes it a highly efficient and practical solution for CIFAR-100 class unlearning. For the VGG-16 model pre-trained on ImageNet, we applied machine unlearning using 33 images from the unlearning class on the mini-ImageNet dataset, which has 1, 000 classes. The goal was to make the model forget class 0.</p><p>As shown in Table <ref type="table" target="#tab_0">VI</ref>, after applying our method, the model's A t dropped to 0.00, while A g remained at 0.83, matching the results of retraining. This confirms that our method effectively unlearned class-specific knowledge.</p><p>Verification with a membership inference attack using SVM also showed a F r of 1.00 after both retraining and our unlearning method, indicating complete unlearning of the target class.</p><p>Retraining with a batch size of 32 used 6.51 GB of GPU memory, while our method only required 0.93 GB. Additionally, our method completed unlearning in 0.68 seconds, compared to 2865.87 seconds for retraining, highlighting its efficiency.</p><p>2) Changes in attribution results: After evaluated our method with various models and datasets, we conducted attribution experiments on both the target model and the unlearned model using different data. The results show that the unlearned  model exhibits more noticeable attribution changes for data in the unlearning class.</p><p>In Figure <ref type="figure" target="#fig_1">2</ref>, the red areas represent parts of the image that positively contribute to the model's classification, while the blue areas indicate negative contributions. Specifically, Figure <ref type="figure" target="#fig_1">2</ref>(a) and Figure <ref type="figure" target="#fig_1">2</ref>(b) show the changes in attribution for the model before and after unlearning the unlearning class data. In Figure <ref type="figure" target="#fig_1">2</ref>(b), the model still extracts the main information from the image. However, there are noticeably more pixels with negative contributions (blue pixels) compared to Figure <ref type="figure" target="#fig_1">2(a)</ref>. This shows that the key information used for classification does not follow the original classification path in the unlearned model. Therefore, the model has successfully achieved unlearning.</p><p>In Figure <ref type="figure" target="#fig_1">2</ref>(c) and Figure <ref type="figure" target="#fig_1">2</ref>(d), we can see that after unlearning, the model's attribution results for the same nonunlearning class data show almost no changes. Additionally, the model's classification results for the non-unlearning class remain unchanged. This indicates that our method successfully performs unlearning while preserving the model's utility on non-unlearning class data.</p><p>In Figures <ref type="figure" target="#fig_4">3(a</ref> Meanwhile, the unlearned model's classification on nonunlearning class data remains unaffected. In Figures <ref type="figure" target="#fig_4">3(c</ref>) and 3(d), we observe that the attributions for non-unlearning data show no significant differences between the original model and the unlearned model. This is because the classification paths for non-unlearning class data remain undisturbed, allowing normal forward and backward propagation. These results demonstrate that the unlearned model preserves its utility on non-unlearning data.</p><p>When performing class unlearning on data from CIFAR-100, the changes in attribution maps are relatively small, but the specified categories can still be effectively forgotten. In Figures <ref type="figure" target="#fig_5">4(a</ref>) and 4(b), we can see that the model extracts the main objects in the images both before and after unlearning, and the attribution results under the same rules have also changed. We believe that the minimal changes in attribution results are because, after training on a dataset with many classes, the model has lower parameter redundancy and only needs to perturb a few neurons to achieve unlearning. When a large number of neurons remain, the model can more comprehensively analyze the contribution of more pixels to image classification.</p><p>Similarly to the previous results, our unlearned model shows nearly no difference in attribution results for non-unlearning class data in CIFAR-10 compared to before unlearning. Our method effectively preserves the utility of the unlearned model.</p><p>In Figure <ref type="figure" target="#fig_7">5</ref>, we can see the attribution results of our method validated on the mini-ImageNet dataset. Figure <ref type="figure" target="#fig_7">5(a)</ref> shows the attribution of data from the unlearning class before unlearning. After unlearning, in Figure <ref type="figure" target="#fig_7">5</ref>(b), we can observe significant     changes in pixel details. This is because neurons strongly related to the unlearning class data have been perturbed, causing the model to fail to classify normally.</p><p>Figure <ref type="figure" target="#fig_7">5</ref>(c) and Figure <ref type="figure" target="#fig_7">5</ref>(d) didn't show any difference that can be directly observed. This is because the classification paths related to non-unlearning class data didn't get perturbed during machine unlearning, indicating that our unlearning method is effective and robust.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Parameters Evaluation</head><p>After validating our method, we attempted to use different parameters to verify the effectiveness of our approach. We will adjust the algorithm parameters from four perspectives to validate the effectiveness and universality of our unlearning algorithm. These perspectives are: different propagation rules, different amount of perturbed neurons, different amount of analyzed neurons.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Different Propagation Rules:</head><p>We use different rules to analyze and perturb different numbers of neurons to reveal the effectiveness and robustness of our approach. The relevance propagation rules that we chose are as follows:</p><p>1) Epsilon <ref type="bibr" target="#b18">[19]</ref> rule is one of the basic LRP rules. It helps to prevent numerical instability by ensuring that small denominators do not result in large relevance scores. 2) Gamma+epsilon <ref type="bibr" target="#b18">[19]</ref> rule is an extension of the epsilon rule, where a positive bias γ is added to enhance the relevance of positive contributions.</p><p>3) Alpha1beta0 <ref type="bibr" target="#b18">[19]</ref> rule explicitly separates the positive and negative contributions, using only the positive contributions for relevance propagation. 4) Alpha2beta1 <ref type="bibr" target="#b18">[19]</ref> rule is a generalized form of the alpha1beta0 rule, where both positive and negative contributions are considered but with different weights. 5) PatternNet S(x) <ref type="bibr" target="#b24">[25]</ref> is designed to provide a more interpretable gradient by isolating the patterns in the input data the neural network has learned to respond to. 6) PatternNet S(x) +- <ref type="bibr" target="#b24">[25]</ref> is a variant of PatternNet that distinguishes between positive and negative patterns, allowing for a more detailed analysis. 7) PatternAttribution S(x) <ref type="bibr" target="#b24">[25]</ref> extends PatternNet by attributing the network's output prediction to the input features, similar to LRP but focusing on learned patterns. 8) PatternAttribution S(x) +- <ref type="bibr" target="#b24">[25]</ref> rule is similar to Pat-ternAttribution but allows for the separation of positive and negative contributions, similar to the S(x) +-rule in PatternNet.</p><p>Table VII presents the unlearning performance of our method on the MNIST dataset under various relevance propagation rules, evaluating the accuracy on the remaining data (A g ) and the unlearning data (A t ) with increasing proportions of perturbed neurons (M p ). Across a range of propagation rules, including epsilon, gamma+epsilon, alpha1beta0, and alpha2beta1, we consistently observe a significant reduction in A t as M p increases to 20%, typically dropping from an initial 0.99 to near 0.001. Simultaneously, these rules maintain a high A g around 0.97, indicating a robust unlearning process that preserves performance on the unperturbed data. While PatternNet S(x) and PatternNet S(x) +-show a more gradual decrease in A t (to 0.19 and 0.28 respectively at M p = 20%), they still achieve substantial unlearning, albeit with a slight impact on A g (dropping to 0.95 and 0.96). Interestingly, PatternAttribution S(x) exhibits a less pronounced unlearning effect (A t reduces to 0.83 at M p = 20%) while maintaining high A g (0.97), suggesting a trade-off between unlearning efficacy and retention. However, the modified Pat-ternAttribution S(x) +-demonstrates a sharp decline in A t to 0.00, aligning with the performance of the epsilon-based rules.</p><p>The collective results from Table VII underscore the versatility of our unlearning approach. Despite the diverse mechanisms of the tested relevance propagation rules, our method consistently demonstrates the ability to effectively unlearn targeted class information. The varying rates and degrees of unlearning achieved across different rules highlight the adaptability of our framework, suggesting its potential applicability across different scenarios and model architectures where different unlearning characteristics might be desired.</p><p>Our method's efficacy extends to more complex datasets, as demonstrated by the results on CIFAR-10 (Table <ref type="table" target="#tab_7">VIII</ref>). Across various layer-wise relevance propagation rules, we observe a consistent pattern of effective unlearning. Specifically, as the proportion of perturbed neurons (M p ) reaches 25% and 30%, the A g remains high at approximately 0.97, while the A t significantly drops to the range of 0.03-0.00 for the epsilon, gamma+epsilon, and alpha-series rules. This indicates that our method can successfully unlearn specific information from more intricate image datasets while preserving the model's overall utility.</p><p>The robustness of our approach is further validated by experiments on the CIFAR-100 dataset (Table <ref type="table" target="#tab_8">IX</ref>). Across all tested propagation rules, at M p = 10% and 12.5%, the A g is consistently maintained at 0.81, signifying stable performance on the remaining classes. Concurrently, the A t decreases substantially to 0.01 and 0.00, confirming the successful unlearning of the designated target class within a more finegrained classification task.</p><p>Finally, evaluations on the challenging mini-ImageNet dataset (Table <ref type="table" target="#tab_8">X</ref>) reinforce the broad applicability of our method. With M p at 10% and 12%, all tested relevance propagation rules demonstrate effective unlearning by reducing the A t to 0.00, while maintaining a high A g = 0.83 on the non-target classes. These results across CIFAR-10, CIFAR-100, and mini-ImageNet highlight the consistent ability of our method, regardless of the specific propagation rule employed, to achieve significant unlearning of target information without compromising the model's performance on other data, underscoring its versatility and potential for widespread application.</p><p>2) Different Amount of Perturbed Neurons: Our experiments, employing the epsilon rule within the layer-wise relevance propagation framework, investigated the effect of varying the proportion of perturbed neurons (M p ) on unlearning performance across different datasets.</p><p>Consistent across MNIST (Table <ref type="table" target="#tab_6">VII</ref>), CIFAR-10 (Table VIII), CIFAR-100 (Table <ref type="table" target="#tab_8">IX</ref>), and mini-ImageNet (Table X), we observed a general trend: as M p increases, the target accuracy (A t ) steadily declines. Notably, A t eventually reaches a stable low point, beyond which further perturbation yields minimal additional unlearning. For instance, on MNIST, A t stabilizes around 0.005 at M p = 16%, with only marginal reduction at 20%. Similar stabilization trends are evident on CIFAR-10 (around M p = 25%-30%), CIFAR-100 (M p ≥ 10%), and mini-ImageNet (M p ≥ 10%). Throughout This behavior suggests that perturbing a small fraction of neurons does not drastically impair the model's overall memory capabilities. We attribute this resilience to the inherent redundancy in neural network representations, where information is distributed across numerous neurons and layers. Consequently, perturbing a limited number of neurons is often insufficient to overcome this redundancy, as the unperturbed neurons can compensate for the induced disruptions. Our findings support this hypothesis, as significant memory disruption, indicated by substantial and stable drops in A t , typically occurs only when M p reaches a certain threshold (e.g., ≥ 16% for MNIST, ≥ 10% for mini-ImageNet). This demonstrates that a critical mass of perturbed neurons is required to effectively break through the distributed nature of learned information and achieve stable unlearning.</p><p>3) Different Amount of Analyzed Neurons: The analysis of different quantities of neurons identified as relevant to the unlearning target significantly impacts the effectiveness of the unlearning process, as demonstrated by the trends in A g and A t across varying M p and N .</p><p>Examining the results on MNIST (Table <ref type="table" target="#tab_6">VII</ref>), we observe a clear influence of the number of analyzed neurons, top N . For the first four rules (epsilon, gamma+epsilon, alpha1beta0, alpha2beta1), when analyzing the top 50 neurons, a substantial drop in A t occurs rapidly as M p increases beyond 8%, reach-ing near zero values by M p = 16%. However, when analyzing the top 100 neurons, a comparable level of A t reduction is only achieved at higher perturbation levels, typically at M p = 20%. This suggests that for MNIST, increasing the number of analyzed neurons beyond a certain point (N = 50 in this case) might include less relevant neurons. As a result, a higher M p is needed across the entire larger group of neurons to achieve the desired unlearning effect, compared to focusing on a smaller, more critical subset of neurons.</p><p>The relationship between N and unlearning effectiveness appears dataset-dependent. On CIFAR-10 (Table <ref type="table" target="#tab_7">VIII</ref>) and mini-ImageNet (Table <ref type="table" target="#tab_8">X</ref>), increasing N seems beneficial up to a certain point. For CIFAR-10, comparing N = 150 and N = 200 under the four rules, N = 200 leads to a more complete reduction in A t at the highest M p values (e.g., A t reaches 0.00 at M p = 30% for N = 200, whereas it's around 0.06 for N = 150). Similarly, on mini-ImageNet, increasing N from 100 to 150 leads to a faster and more complete A t drop, reaching 0.00 by M p = 10%, compared to 0.08 for N = 100. However, further increasing N from 150 to 200 on mini-ImageNet does not yield significant additional improvement in A t reduction at high M p . These findings suggest there may be an optimal number of neurons to analyze for unlearning, where increasing N initially helps capture more relevant information, but exceeding this optimum introduces noise or targets neurons less crucial for the unlearning task.</p><p>On CIFAR-100 (Table <ref type="table" target="#tab_8">IX</ref>), within the tested range of N When selecting the number of neurons to analyze, there exists a critical balance point. Analyzing too few neurons may fail to capture the truly strongly relevant neurons, resulting in incomplete unlearning. Conversely, analyzing too many neurons can introduce many irrelevant or weakly related neurons, creating noise and reducing the efficiency of the unlearning process. Therefore, finding an appropriate value of N -balancing the capture of key neurons and the avoidance of noise-is essential for achieving effective and efficient unlearning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Experiment Analysis</head><p>1) Impact of Neural Network Redundancy on the Number of Perturbed Neurons: Deep learning models, particularly over-parameterized ones, often exhibit significant parameter redundancy. This means the model contains more parameters than strictly necessary to achieve a given level of performance on the training data, leading to information being distributed across many neurons and pathways. This inherent redundancy can influence how effectively specific information can be unlearned by perturbing a subset of neurons.</p><p>Our experimental results suggest that the degree of unlearning required, measured by M p , is related to the apparent redundancy in the model's representation of the target class, which may vary with dataset complexity. For datasets with fewer classes, such as MNIST and CIFAR-10, where models might develop more redundant representations for each class, achieving effective unlearning typically required perturbing a higher proportion of neurons, with A t dropping significantly only at M p values ranging from 16% to 20% (for MNIST) or higher (for CIFAR-10).</p><p>This necessity for higher M p in seemingly more redundant representations can be attributed to the distributed nature of the learned information. When knowledge about a target class is spread across a larger number of parameters or neurons, perturbing only a small fraction of them has a limited impact on the overall representation of that class. Therefore, a larger proportion of neurons must be perturbed to effectively disrupt the redundant encoding and significantly reduce the model's ability to recognize the target class.</p><p>In contrast, for datasets with a larger number of classes, such as CIFAR-100 and mini-ImageNet, where the model might develop less redundant or more specialized representations due to the complexity of the task, effective unlearning was achieved at lower M p values. In these cases, perturbing around 10% of neurons was often sufficient to cause a drastic drop in A t . This observation supports the idea that in less redundant representations, the information might be more concentrated, making the model more sensitive to perturbations of fewer, more critical neurons.</p><p>2) Impact of Neural Network Redundancy on the Model Utility: A key finding from our experiments is that the neuron perturbation unlearning methods generally have minimal impact on the model's overall utility, as measured by A g . This robustness of A g during targeted unlearning is largely a consequence of the inherent redundancy present in deep neural networks. In these over-parameterized models, data inference and feature extraction are distributed across numerous layers, neurons, and intricate pathways <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref>.</p><p>We conducted experiments on the layers of different neural networks trained on four datasets to analyze the distribution of class-related information. We selected the top ten classes from four datasets and conducted experimental analysis under the condition of minimizing the parameters required for unlearning. We defined the following metrics to analyze the information overlap between neurons:</p><p>• X most @2 represents the maximum number of shared neurons among strongly relevant neurons in the perturbed FC layer between any two classes. The more shared relevant neurons required to store information for both classes, the more likely unlearning one class will affect the model's classification utility for the other class. • X @10 represents the number of shared neurons in the FC layer perturbed by our method that exhibit strong relevance with the classification of ten classes. A higher value indicates that path perturbation on one class is more likely to adversely affect the model's classification utility for other classes. • Share@most indicates the maximum number of classes whose classifications are correlated with a single neuron among the strongly correlated neurons we analyzed. In Table <ref type="table" target="#tab_9">XI</ref>, we observe that the overlap of neurons strongly relevant to two different classes in a single FC layer of neural networks trained on four datasets, X most @2, is less than 0.5. This phenomenon indicates that, although neurons in a single layer do share information for certain classes, each class has its own strongly correlated neurons, allowing perturbation of one class's classification path without affecting the model's utility for other classes.</p><p>Furthermore, we aimed to verify whether any neurons are strongly relevant to all ten classes. As shown in Table XI, the experimental results indicate that X @10 is 0 for all models upon completing unlearning, meaning no single neuron simultaneously serves the classification of these ten classes. This further confirms the specific relevance of neurons with different class data. Does a single fully connected layer (FC layer) in a neural network contain neurons strongly relevant to multiple classes? We further analyzed this by calculating the Share@most for each model. We observed that, when analyzing the strongly relevant neurons for the ten classes in the MNIST dataset, only two neurons (No. 135 and No. 345) were shared by a maximum of six classes. In two ResNet-50 models trained on CIFAR-10 and CIFAR-100, only one neuron each (No. 1447 and No. 1064) was shared by a maximum of seven and five classes, respectively. This result suggests that the knowledge learned for each class is represented by specific strongly relevant neurons forming distinct classification paths. For VGG-16 trained on the mini-ImageNet dataset, which has a single layer with 4096 neurons, we calculated the top 400 strongly relevant neurons for each of the ten classes, but found only one neuron (No. 3452) serving a maximum of nine classes' classification tasks.</p><p>The low neuron overlap, absence of universally critical neurons demonstrate that neural networks have sufficient redundancy to maintain distinct classification paths for each class. Due to this redundancy, no single neuron or a small set of neurons is solely responsible for the model's entire performance or the inference for general classes. Instead, the model's ability to generalize relies on the collective computation and interaction of a vast number of parameters. Consequently, perturbing or modifying a limited subset of neurons specifically identified for unlearning the target class does not cause a catastrophic breakdown of the network's fundamental functionality or its ability to correctly classify examples from other classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION</head><p>In this paper, we present a novel approach to machine unlearning via Layer-wise Relevance Analysis and Neuronal Path Perturbation. Layer-wise Relevance Analysis provides a detailed understanding of the internal workings of the neural network during the unlearning process. Assessing the relevance of each neuron to the unlearning information allows for more precise identification of the neurons that need to be perturbed. Neuronal Path Perturbation further enhances the unlearning process. By perturbing only the highly relevant neurons, it effectively disrupts the model's memory of the specific data without overwriting or unnecessarily modifying a large portion of the network. Compared with traditional methods, our method addresses critical challenges of machine unlearning methods, such as lack of explanation, privacy guarantees, and the balance between unlearning effectiveness and model utility.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>(a) The attribution of data from the unlearning class before unlearning, using various propagation rules. (b) The attribution of data from the unlearning class after unlearning, using various propagation rules. (c) The attribution of data from the non-unlearning class before unlearning, using various propagation rules. (d) The attribution of data from the non-unlearning class after unlearning, using various propagation rules.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. The attribution of the model's classification on unlearning and non-unlearning class data from MNIST, before and after unlearning, using different propagation rules.</figDesc><graphic coords="9,69.44,199.36,231.31,115.65" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>) and 3(b), we compare the attributions of the ResNet50 model for unlearning class data in the CIFAR-10 dataset. Figure3(a)shows the model's attributions for unlearning class data before unlearning. After unlearning, Figure3(b) displays changes in the attribution distribution. In Figure3(b), when using attribution rules such as epsilon, gamma + epsilon, and alpha1beta0, we observe that more pixels with altered attributions in the unlearning data influence the classification of the unlearned model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>(a) The attribution of data from the unlearning class before unlearning, using various propagation rules. (b) The attribution of data from the unlearning class after unlearning, using various propagation rules. (c) The attribution of data from the non-unlearning class after unlearning, using various propagation rules. (d) The attribution of data from the non-unlearning class after unlearning, using various propagation rules.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. The attribution of the model's classification on unlearning and non-unlearning class data from CIFAR-10, before and after unlearning, using different propagation rules.</figDesc><graphic coords="10,104.91,407.29,195.85,130.57" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. The attribution of the model's classification on unlearning and non-unlearning class data from CIFAR-100, before and after unlearning, using different propagation rules.</figDesc><graphic coords="10,104.91,565.48,195.85,130.57" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>(a) The attribution of data from the unlearning class before unlearning, using various propagation rules. (b) The attribution of data from the unlearning class after unlearning, using various propagation rules. (c) The attribution of data from the non-unlearning class after unlearning, using various propagation rules. (d) The attribution of data from the non-unlearning class after unlearning, using various propagation rules.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. The attribution of the model's classification on unlearning and non-unlearning class data from mini-ImageNet, before and after unlearning, using different propagation rules.</figDesc><graphic coords="11,104.91,214.26,195.85,130.57" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I ABBREVIATIONS</head><label>I</label><figDesc>AND ACRONYMS. THIS TABLE PROVIDES EXPLANATIONS FOR THE ABBREVIATIONS AND ACRONYMS USED IN THE ARTICLE.</figDesc><table><row><cell cols="2">Symbol Description</cell></row><row><cell>F</cell><cell>Model's prediction function</cell></row><row><cell>L</cell><cell>Accuracy loss function</cell></row><row><cell>θ</cell><cell>Model's parameters</cell></row><row><cell>θ ′</cell><cell>Approximate parameters</cell></row><row><cell>D/D ′</cell><cell>Training/Updated dataset</cell></row><row><cell>Nc</cell><cell>Number of classes</cell></row><row><cell>C i</cell><cell>Class i</cell></row><row><cell>(x, y)</cell><cell>Input-output pairs</cell></row><row><cell>η</cell><cell>Learning rate</cell></row><row><cell>∇</cell><cell>Gradient</cell></row><row><cell>r</cell><cell>Propagation rules</cell></row><row><cell>w</cell><cell>Weight</cell></row><row><cell>p</cell><cell>Neuron position</cell></row><row><cell>R</cell><cell>Relevance score</cell></row><row><cell>l</cell><cell>Layer l</cell></row><row><cell>m i</cell><cell>Amount of neurons in Layer i</cell></row><row><cell>mp</cell><cell>Amount of perturbed neurons in Layer i</cell></row><row><cell>G</cell><cell>Gigabyte</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II DATASETS</head><label>II</label><figDesc>WITH IMAGE RESOLUTION AND CLASSIFICATION MODEL The model's average accuracy on the remaining classes is denoted as A g . A high A g reflects the improved utility of the unlearned model for the remaining tasks. Maintaining a high A g ensures that the unlearning process does not significantly degrade the model's performance.• Forgetting rate (F r). The forgetting rate (F r) of the unlearned model is a metric derived from a membership inference attack (MIA). This attack involves training an SVM attack model using the target model's posterior probabilities from its training data to learn how to identify unlearned class members. Subsequently, this trained SVM is used to classify the posterior probabilities produced by the unlearned model when applied to the unlearned class data.</figDesc><table><row><cell>Dataset</cell><cell cols="2">Image Resolution Classification Model</cell></row><row><cell>MNIST</cell><cell>28x28</cell><cell>ALLCNN</cell></row><row><cell>CIFAR-10</cell><cell>224x224</cell><cell>ResNet-50</cell></row><row><cell>CIFAR-100</cell><cell>224x224</cell><cell>ResNet-50</cell></row><row><cell>mini-ImageNet</cell><cell>256x256</cell><cell>VGG-16</cell></row><row><cell cols="2">B. Evaluation Metrics</cell><cell></cell></row><row><cell cols="3">1) Evaluation Metrics for Images unlearning:</cell></row></table><note><p>• Unlearning class accuracy (A t ). The accuracy of the model on the unlearning class. A low A t indicates successful unlearning to the target class. This metric directly quantifies the model's reduced ability to correctly classify the data points intended to be forgotten. • Global accuracy (A g ).</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE III CLASS</head><label>III</label><figDesc>UNLEARNING PERFORMANCE ON THE MNIST DATASET.</figDesc><table><row><cell>Method</cell><cell>At</cell><cell>Ag</cell><cell>F r</cell><cell>G</cell><cell>T ime</cell></row><row><cell>Retrain from scratch</cell><cell>0.00</cell><cell>0.97</cell><cell>1.00</cell><cell cols="2">0.74 71.37s</cell></row><row><cell>Random labels</cell><cell>0.00</cell><cell>0.97</cell><cell>1.00</cell><cell cols="2">0.70 15.64s</cell></row><row><cell>Amnesiac</cell><cell>0.38</cell><cell>0.98</cell><cell>0.78</cell><cell>0.44</cell><cell>38.72s</cell></row><row><cell>Boundary expanding</cell><cell cols="5">0.13 0.95 0.91 0.65 69.21s</cell></row><row><cell>Boundary shrink</cell><cell cols="2">0.05 0.58</cell><cell>1.00</cell><cell cols="2">0.71 18.55s</cell></row><row><cell>Neuronal path noise (GN)</cell><cell cols="4">0.35 0.68 0.61 0.64</cell><cell>0.77s</cell></row><row><cell>Neuronal path noise (LN)</cell><cell cols="4">0.21 0.53 0.78 0.64</cell><cell>0.83s</cell></row><row><cell>Neuronal path perturbation</cell><cell>0.00</cell><cell>0.97</cell><cell>1.00</cell><cell>0.64</cell><cell>0.68s</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE VII PERFORMANCE</head><label>VII</label><figDesc>OF UNLEARNING ON MNIST WITH VARYING PROPAGATION RULES r, ANALYZED NEURON COUNT N , AND PERTURBED NEURON PROPORTION Mp</figDesc><table><row><cell>r</cell><cell cols="2">Top N</cell><cell></cell><cell></cell><cell cols="3">Ag / At at Different Mp (≈) Levels</cell></row><row><cell></cell><cell></cell><cell></cell><cell>0%</cell><cell>4%</cell><cell>8%</cell><cell>12%</cell><cell>16%</cell><cell>20%</cell></row><row><cell>epsilon</cell><cell></cell><cell>50 100</cell><cell>0.98 / 0.99 0.98 / 0.99</cell><cell>0.98 / 0.99 0.98 / 0.99</cell><cell>0.97 / 0.64 0.98 / 0.99</cell><cell>0.96 / 0.13 0.97 / 0.89</cell><cell>0.96 / 0.005 0.97 / 0.34</cell><cell>0.97 / 0.001 0.96 / 0.09</cell></row><row><cell>gamma+epsilon</cell><cell></cell><cell>50 100</cell><cell>0.98 / 0.99 0.98 / 0.99</cell><cell>0.98 / 0.99 0.98 / 0.99</cell><cell>0.98 / 0.59 0.98 / 0.99</cell><cell>0.96 / 0.13 0.97 / 0.89</cell><cell>0.96 / 0.001 0.96 / 0.38</cell><cell>0.97 / 0.001 0.96 / 0.09</cell></row><row><cell>alpha1beta0</cell><cell></cell><cell>50 100</cell><cell>0.98 / 0.99 0.98 / 0.99</cell><cell>0.98 / 0.99 0.98 / 0.99</cell><cell>0.97 / 0.56 0.98 / 0.99</cell><cell>0.96 / 0.13 0.97 / 0.89</cell><cell>0.96 / 0.003 0.97 / 0.36</cell><cell>0.97 / 0.001 0.96 /</cell></row><row><cell>alpha2beta1</cell><cell></cell><cell>50 100</cell><cell>0.98 / 0.99 0.98 / 0.99</cell><cell>0.98 / 0.99 0.98 / 0.99</cell><cell>0.97 / 0.61 0.98 / 0.99</cell><cell>0.96 / 0.13 0.97 / 0.89</cell><cell>0.96 / 0.007 0.96 / 0.40</cell><cell>0.97 / 0.001 0.96 / 0.09</cell></row><row><cell>PatternNet S(x)</cell><cell></cell><cell>50 100</cell><cell>0.98 / 0.99 0.98 / 0.99</cell><cell>0.98 / 0.99 0.98 / 0.99</cell><cell>0.98 / 0.95 0.98 / 0.95</cell><cell>0.97 / 0.59 0.96 / 0.59</cell><cell>0.97 / 0.38 0.96 / 0.38</cell><cell>0.95 / 0.19 0.95 / 0.19</cell></row><row><cell>PatternNet S(x) +-</cell><cell></cell><cell>50 100</cell><cell>0.98 / 0.99 0.98 / 0.99</cell><cell>0.98 / 0.99 0.98 / 0.99</cell><cell>0.98 / 0.93 0.98 / 0.93</cell><cell>0.97 / 0.62 0.97 / 0.62</cell><cell>0.96 / 0.52 0.96 / 0.52</cell><cell>0.96 / 0.28 0.96 / 0.28</cell></row><row><cell>PatternAttribution S(x)</cell><cell></cell><cell>50 100</cell><cell>0.98 / 0.99 0.98 / 0.99</cell><cell>0.98 / 0.99 0.98 / 0.99</cell><cell>0.97 / 0.99 0.98 / 0.99</cell><cell>0.97 / 0.97 0.97 / 0.97</cell><cell>0.97 / 0.93 0.97 / 0.93</cell><cell>0.97 / 0.83 0.97 / 0.83</cell></row><row><cell cols="2">PatternAttribution S(x) +-</cell><cell>50 100</cell><cell>0.98 / 0.99 0.98 / 0.99</cell><cell>0.98 / 0.99 0.98 / 0.99</cell><cell>0.97 / 0.55 0.97 / 0.55</cell><cell>0.96 / 0.21 0.96 / 0.21</cell><cell>0.96 / 0.02 0.96 / 0.02</cell><cell>0.95 / 0.00 0.95 / 0.00</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE VIII PERFORMANCE</head><label>VIII</label><figDesc>OF UNLEARNING ON CIFAR-10 WITH VARYING PROPAGATION RULES r, ANALYZED NEURON COUNT N , AND PERTURBED NEURON PROPORTION Mp</figDesc><table><row><cell>r</cell><cell>Top N</cell><cell></cell><cell></cell><cell cols="3">Ag / At at Different Mp (≈) Levels</cell><cell></cell></row><row><cell></cell><cell></cell><cell>0%</cell><cell></cell><cell>10%</cell><cell>15%</cell><cell>20%</cell><cell>25%</cell><cell>30%</cell></row><row><cell>epsilon</cell><cell>150 200</cell><cell>0.98 / 0.99 0.98 / 0.99</cell><cell cols="2">0.98 / 0.67 0.98 / 0.67</cell><cell>0.98 / 0.34 0.98 / 0.33</cell><cell>0.98 / 0.11 0.98 / 0.11</cell><cell>0.97 / 0.03 0.97 / 0.01</cell><cell>0.97 / 0.06 0.95 / 0.00</cell></row><row><cell>gamma+epsilon</cell><cell>150 200</cell><cell>0.98 / 0.99 0.98 / 0.99</cell><cell cols="2">0.98 / 0.67 0.98 / 0.67</cell><cell>0.98 / 0.34 0.98 / 0.33</cell><cell>0.98 / 0.11 0.97 / 0.11</cell><cell>0.97 / 0.01 0.97 / 0.01</cell><cell>0.93 / 0.00 0.95 / 0.00</cell></row><row><cell>alpha1beta0</cell><cell>150 200</cell><cell>0.98 / 0.99 0.98 / 0.99</cell><cell cols="2">0.98 / 0.67 0.98 / 0.67</cell><cell>0.98 / 0.34 0.98 / 0.33</cell><cell>0.98 / 0.11 0.98 / 0.11</cell><cell>0.97 / 0.02 0.97 / 0.02</cell><cell>0.93 / 0.01 0.96 / 0.00</cell></row><row><cell>alpha2beta1</cell><cell>150 200</cell><cell>0.98 / 0.99 0.98 / 0.99</cell><cell cols="2">0.98 / 0.67 0.98 / 0.67</cell><cell>0.98 / 0.33 0.98 / 0.33</cell><cell>0.98 / 0.11 0.98 / 0.11</cell><cell>0.97 / 0.02 0.97 / 0.02</cell><cell>0.95 / 0.01 0.95 / 0.00</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">TABLE IX</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="5">PERFORMANCE OF UNLEARNING ON CIFAR-100 WITH</cell><cell></cell></row><row><cell cols="9">VARYING PROPAGATION RULES r, ANALYZED NEURON COUNT N , AND PERTURBED NEURON PROPORTION Mp</cell></row><row><cell></cell><cell>r</cell><cell cols="2">Top N</cell><cell cols="4">Ag / At at Different Mp (≈) Levels</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>0%</cell><cell>7.5%</cell><cell>10%</cell><cell>12.5%</cell></row><row><cell></cell><cell cols="2">epsilon</cell><cell>150 200</cell><cell cols="4">0.82 / 0.96 0.81 / 0.07 0.81 / 0.01 0.81 / 0.00 0.82 / 0.96 0.81 / 0.08 0.81 / 0.01 0.81 / 0.00</cell></row><row><cell></cell><cell cols="2">gamma+epsilon</cell><cell>150 200</cell><cell cols="4">0.82 / 0.96 0.81 / 0.07 0.81 / 0.01 0.81 / 0.00 0.82 / 0.96 0.81 / 0.10 0.81 / 0.01 0.81 / 0.00</cell></row><row><cell></cell><cell cols="2">alpha1beta0</cell><cell>150 200</cell><cell cols="4">0.82 / 0.96 0.81 / 0.05 0.81 / 0.01 0.81 / 0.00 0.82 / 0.96 0.81 / 0.09 0.81 / 0.01 0.81 / 0.00</cell></row><row><cell></cell><cell cols="2">alpha2beta1</cell><cell>150 200</cell><cell cols="4">0.82 / 0.96 0.81 / 0.07 0.81 / 0.01 0.81 / 0.00 0.82 / 0.96 0.81 / 0.09 0.81 / 0.01 0.81 / 0.00</cell></row><row><cell cols="5">this process, the non-target accuracy (A g ) remains relatively</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">high, indicating preserved model utility.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE X PERFORMANCE</head><label>X</label><figDesc>OF UNLEARNING ON MINI-IMAGENET WITH VARYING PROPAGATION RULES r, ANALYZED NEURON COUNT N , AND PERTURBED NEURON PROPORTION Mp The A t values drop sharply to nearly zero by M p = 10% for both N = 150 and N = 200, with A g remaining stable. For CIFAR-100, within this range, the number of analyzed neurons has less impact on the unlearning performance compared to the level of M p .</figDesc><table><row><cell>r</cell><cell>Top N</cell><cell></cell><cell></cell><cell cols="2">Ag / At at Different Mp (≈) Levels</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>0%</cell><cell>2%</cell><cell>4%</cell><cell>8%</cell><cell>10%</cell><cell>12%</cell></row><row><cell></cell><cell>100</cell><cell>0.83 / 0.88</cell><cell>0.83 / 0.88</cell><cell>0.83 / 0.61</cell><cell>0.83 / 0.32</cell><cell>0.83 / 0.08</cell><cell>0.83 / 0.08</cell></row><row><cell>epsilon</cell><cell>150</cell><cell>0.83 / 0.88</cell><cell>0.83 / 0.85</cell><cell>0.83 / 0.56</cell><cell>0.83 / 0.18</cell><cell>0.83 / 0.00</cell><cell>0.83 / 0.00</cell></row><row><cell></cell><cell>200</cell><cell>0.83 / 0.88</cell><cell>0.83 / 0.88</cell><cell>0.83 / 0.56</cell><cell>0.83 / 0.18</cell><cell>0.83 / 0.00</cell><cell>0.83 / 0.00</cell></row><row><cell></cell><cell>100</cell><cell>0.83 / 0.88</cell><cell>0.83 / 0.88</cell><cell>0.83 / 0.62</cell><cell>0.83 / 0.32</cell><cell>0.83 / 0.08</cell><cell>0.83 / 0.08</cell></row><row><cell>gamma+epsilon</cell><cell>150</cell><cell>0.83 / 0.88</cell><cell>0.83 / 0.85</cell><cell>0.83 / 0.56</cell><cell>0.83 / 0.18</cell><cell>0.83 / 0.00</cell><cell>0.83 / 0.00</cell></row><row><cell></cell><cell>200</cell><cell>0.83 / 0.88</cell><cell>0.83 / 0.88</cell><cell>0.83 / 0.56</cell><cell>0.83 / 0.18</cell><cell>0.83 / 0.00</cell><cell>0.83 / 0.00</cell></row><row><cell></cell><cell>100</cell><cell>0.83 / 0.88</cell><cell>0.83 / 0.88</cell><cell>0.83 / 0.62</cell><cell>0.83 / 0.32</cell><cell>0.83 / 0.08</cell><cell>0.83 / 0.08</cell></row><row><cell>alpha1beta0</cell><cell>150</cell><cell>0.83 / 0.88</cell><cell>0.83 / 0.85</cell><cell>0.83 / 0.56</cell><cell>0.83 / 0.18</cell><cell>0.83 / 0.00</cell><cell>0.83 / 0.00</cell></row><row><cell></cell><cell>200</cell><cell>0.83 / 0.88</cell><cell>0.83 / 0.88</cell><cell>0.83 / 0.56</cell><cell>0.83 / 0.18</cell><cell>0.83 / 0.00</cell><cell>0.83 / 0.00</cell></row><row><cell></cell><cell>100</cell><cell>0.83 / 0.88</cell><cell>0.83 / 0.88</cell><cell>0.83 / 0.62</cell><cell>0.83 / 0.32</cell><cell>0.83 / 0.08</cell><cell>0.83 / 0.08</cell></row><row><cell>alpha2beta1</cell><cell>150</cell><cell>0.83 / 0.88</cell><cell>0.83 / 0.85</cell><cell>0.83 / 0.56</cell><cell>0.83 / 0.18</cell><cell>0.83 / 0.00</cell><cell>0.83 / 0.00</cell></row><row><cell></cell><cell>200</cell><cell>0.83 / 0.88</cell><cell>0.83 / 0.88</cell><cell>0.83 / 0.56</cell><cell>0.83 / 0.18</cell><cell>0.83 / 0.00</cell><cell>0.83 / 0.00</cell></row><row><cell cols="4">(150 and 200), both values exhibit very similar unlearning</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">characteristics under all rules.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE XI ANALYSIS</head><label>XI</label><figDesc>OF THE OVERLAP OF RELEVANT NEURONS IN THE PERTURBED LAYER</figDesc><table><row><cell>D</cell><cell>Top N</cell><cell cols="2">Mp (≈)</cell><cell cols="2">Xmost@2</cell><cell>X @10</cell><cell></cell><cell cols="2">Share@most</cell></row><row><cell></cell><cell></cell><cell cols="8">Proportion Quantity Proportion Quantity Proportion Quantity Quantity Position</cell></row><row><cell>MNIST</cell><cell>50</cell><cell>0.16</cell><cell>80</cell><cell>0.43</cell><cell>34</cell><cell>0</cell><cell>0</cell><cell>6</cell><cell>[155, 345]</cell></row><row><cell>CIFAR-10</cell><cell>200</cell><cell>0.20</cell><cell>400</cell><cell>0.45</cell><cell>178</cell><cell>0</cell><cell>0</cell><cell>7</cell><cell>[1447]</cell></row><row><cell>CIFAR-100</cell><cell>150</cell><cell>0.10</cell><cell>200</cell><cell>0.30</cell><cell>60</cell><cell>0</cell><cell>0</cell><cell>5</cell><cell>[1064]</cell></row><row><cell cols="2">mini-ImageNet 150</cell><cell>0.10</cell><cell>400</cell><cell>0.47</cell><cell>189</cell><cell>0</cell><cell>0</cell><cell>9</cell><cell>[3452]</cell></row></table></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Wanlei Zhou received the B.Eng and M.Eng degrees from Harbin Institute of Technology, Harbin, China in 1982 and 1984, respectively, and the PhD degree from The Australian National University, Canberra, Australia, in 1991, all in Computer Science and Engineering. He also received a DSc degree from Deakin University in 2002. He is currently the Vice Rector and Dean of Institute of Data Science, City University of Macau, Macao SAR, China. He has authored or coauthored more than 400 papers in refereed international journals and refereed international conferences proceedings, including many articles in IEEE transactions and journals. His research interests include security and privacy, parallel and distributed systems, and e-learning.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Class machine unlearning for complex data via concepts inference and data poisoning</title>
		<author>
			<persName><forename type="first">W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2405.15662" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Machine unlearning: A survey</title>
		<author>
			<persName><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
		<idno type="DOI">10.1145/3603620</idno>
		<ptr target="https://doi.org/10.1145/3603620" />
	</analytic>
	<monogr>
		<title level="j">ACM Comput. Surv</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2023-08">aug 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Zero-shot class unlearning via layer-wise relevance analysis and neuronal path perturbation</title>
		<author>
			<persName><forename type="first">W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2410.23693" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">A survey of machine unlearning</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">T</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">T</forename><surname>Huynh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">L</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W</forename></persName>
		</author>
		<author>
			<persName><forename type="first">-C</forename><surname>Liew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V H</forename><surname>Nguyen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2209.02299</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Boundary unlearning: Rapid forgetting of deep networks via shifting the decision boundary</title>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="7766" to="7775" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learn to forget: Machine unlearning via neuron masking</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Dependable and Secure Computing</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Generative adversarial networks unlearning</title>
		<author>
			<persName><forename type="first">H</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2308.09881" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Graph unlearning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Backes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Humbert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2022 ACM SIGSAC conference on computer and communications security</title>
		<meeting>the 2022 ACM SIGSAC conference on computer and communications security</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="499" to="513" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Zeroshot machine unlearning</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">S</forename><surname>Chundawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Tarun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mandal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kankanhalli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Forensics and Security</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="2345" to="2354" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Machine unlearning</title>
		<author>
			<persName><forename type="first">L</forename><surname>Bourtoule</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Chandrasekaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Choquette-Choo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Travers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Papernot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE Symposium on Security and Privacy (SP)</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="141" to="159" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Model sparsity can simplify machine unlearning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Neural Information Processing Systems, ser. NIPS &apos;23</title>
		<meeting>the 37th International Conference on Neural Information Processing Systems, ser. NIPS &apos;23<address><addrLine>Red Hook, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates Inc</publisher>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Fast model debias with machine unlearning</title>
		<author>
			<persName><forename type="first">R</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Neural Information Processing Systems, ser. NIPS &apos;23</title>
		<meeting>the 37th International Conference on Neural Information Processing Systems, ser. NIPS &apos;23<address><addrLine>Red Hook, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates Inc</publisher>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Towards unbounded machine unlearning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kurmanji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Triantafillou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hayes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Triantafillou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Neural Information Processing Systems, ser. NIPS &apos;23</title>
		<meeting>the 37th International Conference on Neural Information Processing Systems, ser. NIPS &apos;23<address><addrLine>Red Hook, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates Inc</publisher>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Fast machine unlearning without retraining through selective synaptic dampening</title>
		<author>
			<persName><forename type="first">J</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Schoepf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Brintrup</surname></persName>
		</author>
		<ptr target="https://ojs.aaai.org/index.php/AAAI/article/view/29092" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2024-03">Mar. 2024</date>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page">51</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Backdoor attacks via machine unlearning</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Huai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Miao</surname></persName>
		</author>
		<ptr target="https://ojs.aaai.org/index.php/AAAI/article/view/29321" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2024-03">Mar. 2024</date>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="14" to="115" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Fast yet effective machine unlearning</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Tarun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">S</forename><surname>Chundawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mandal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kankanhalli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page">55</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Don&apos;t forget too much: Towards machine unlearning on feature level</title>
		<author>
			<persName><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Dependable and Secure Computing</title>
		<imprint>
			<biblScope unit="page" from="1" to="16" />
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Towards efficient target-level machine unlearning based on essential graph</title>
		<author>
			<persName><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhao</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2406.10954" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Binder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Montavon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Klauschen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-R</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Samek</surname></persName>
		</author>
		<idno type="DOI">10.1371/journal.pone.0130140</idno>
		<ptr target="https://doi.org/10.1371/journal.pone.0130140" />
	</analytic>
	<monogr>
		<title level="j">PLOS ONE</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1" to="46" />
			<date type="published" when="2015-07">07 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Layer-wise relevance propagation for neural networks with local renormalization layers</title>
		<author>
			<persName><forename type="first">A</forename><surname>Binder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Montavon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lapuschkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-R</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Samek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial Neural Networks and Machine Learning -ICANN 2016</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Villa</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Masulli</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Pons Rivero</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="63" to="71" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Wb-lrp: Layer-wise relevance propagation with weight-dependent baseline</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<ptr target="https://www.sciencedirect.com/science/article/pii/S0031320324007076" />
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">158</biblScope>
			<biblScope unit="page">110956</biblScope>
			<date type="published" when="2025">2025</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Pruning by explaining revisited: Optimizing attribution methods to prune cnns and transformers</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M V</forename><surname>Hatefi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dreyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Achtibat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wiegand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Samek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lapuschkin</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2408.12568" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Random relabeling for efficient machine unlearning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ghosh</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2305.12320" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Amnesiac machine learning</title>
		<author>
			<persName><forename type="first">L</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Nagisetty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ganesh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="11" to="516" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Learning how to explain neural networks: Patternnet and patternattribution</title>
		<author>
			<persName><forename type="first">P.-J</forename><surname>Kindermans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">T</forename><surname>Schütt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Alber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-R</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dähne</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1705.05598" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Network dissection: Quantifying interpretability of deep visual representations</title>
		<author>
			<persName><forename type="first">D</forename><surname>Bau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2017.354</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2017.354" />
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Honolulu, HI, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2017-07-21">2017. July 21-26, 2017. 2017</date>
			<biblScope unit="page" from="3319" to="3327" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">His research interests are information security, machine learning and privacy preservation. Yufeng Wu received his B.Eng degree in Management from Wuhan University of Science and Technology in 2023. Currently, he is pursuing a Master&apos;s degree at China University of Geosciences</title>
		<author>
			<persName><forename type="first">S</forename><surname>Girish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Maiya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<ptr target="https://openaccess.thecvf.com/content/CVPR2021/html/Girish\The\" />
	</analytic>
	<monogr>
		<title level="m">His research interests include security and privacy in machine learning, and graph neural network</title>
		<meeting><address><addrLine>Wuhan, China; China; China; Australia; Macao SAR, China; China; China; Wuhan, China; China; China; China; Macao, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000">June 19-25, 2021. 2021. 2000 and 2004. 2014. 2002 and 2005. 2017. 2020</date>
			<biblScope unit="page" from="762" to="771" />
		</imprint>
		<respStmt>
			<orgName>Zhongnan University of Economics and Law ; PhD in computer science from Deakin University ; City University of Macau ; Minnan Normal University ; Fuzhou University ; Computer Science from China University of Geosciences</orgName>
		</respStmt>
	</monogr>
	<note>Tianqing Zhu received her B.Eng. degree and her M.Eng. degree from Wuhan University</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
