<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Policy Improvement Methods: Between Black-Box Optimization and Episodic Reinforcement Learning</title>
				<funder ref="#_KRsUa3T">
					<orgName type="full">French ANR</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Freek</forename><surname>Stulp</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Robotics and Computer Vision</orgName>
								<orgName type="institution">ENSTA-ParisTech</orgName>
								<address>
									<country>Paris</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">FLOWERS Research Team</orgName>
								<orgName type="institution">INRIA Bordeaux Sud-Ouest</orgName>
								<address>
									<settlement>Talence</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Olivier</forename><surname>Sigaud</surname></persName>
							<affiliation key="aff2">
								<orgName type="laboratory" key="lab1">Institut des Systèmes Intelligents et de Robotique</orgName>
								<orgName type="laboratory" key="lab2">UMR 7222</orgName>
								<orgName type="institution">Université Pierre Marie Curie CNRS</orgName>
								<address>
									<settlement>Paris</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Policy Improvement Methods: Between Black-Box Optimization and Episodic Reinforcement Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-29T01:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Policy improvement methods seek to optimize the parameters of a policy with respect to a utility function. There are two main approaches to performing this optimization: reinforcement learning (RL) and black-box optimization (BBO). Whereas BBO algorithms are generic optimization methods that, due to there generality, may also be applied to optimizing policy parameters, RL algorithms are specifically tailored to leveraging the structure of policy improvement problems. In recent years, benchmark comparisons between RL and BBO have been made, and there has been several attempts to specify which approach works best for which types of problem classes.</p><p>In this article, we make several contributions to this line of research: 1) We define four algorithmic properties that further clarify the relationship between RL and BBO: action-perturbation vs. parameter-perturbation, gradient estimation vs. rewardweighted averaging, use of only rewards vs. use of rewards and state information, actor-critic vs. direct policy search. 2) We show how the chronology of the derivation of ever more powerful algorithms displays a trend towards algorithms based on parameter-perturbation and reward-weighted averaging. A striking feature of this trend is that it has moved RL methods closer and closer to BBO. 3) We continue this trend by applying two modifications to the state-of-the-art "Policy Improvement with Path Integrals" (PI 2 ), which yields an algorithm we denote PI BB . We show that PI BB is a BBO algorithm, and, more specifically, that it is a special case of the "Covariance Matrix Adaptation -Evolutionary Strategy" algorithm. Our empirical evaluation demonstrates that the simpler PI BB outperforms PI 2 on simple evaluation tasks in terms of convergence speed and final cost. 4) Although our evaluation implies that, for these five tasks, BBO outperforms RL, we do not hold this to be a general statement, and provide an analysis of why these tasks are particularly well-suited for BBO. Thus, rather than making the case for BBO or RL, one of the main contributions of this article is rather to provide an algorithmic framework in which such cases may be made, as PI BB and PI 2 use identical perturbation and parameter update methods, and differ only in being BBO and RL approaches respectively.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Over the last two decades, the convergence speed and robustness of policy improvement methods has increased dramatically, such that they are now able to learn a variety of challenging robotic tasks <ref type="bibr" target="#b30">(Theodorou et al., 2010;</ref><ref type="bibr">Rückstiess et al., 2010b;</ref><ref type="bibr" target="#b29">Tamosiumaite et al., 2011;</ref><ref type="bibr" target="#b11">Kober and Peters, 2011;</ref><ref type="bibr" target="#b1">Buchli et al., 2011;</ref><ref type="bibr">Stulp et al., 2012)</ref>. Several underlying trends have accompanied this performance increase. The first is related to exploration, where there has been a transition from action perturbing methods, which perturb the output of the policy at each time step, to parameter perturbing methods, which perturb the parameters of the policy itself <ref type="bibr">(Rückstiess et al., 2010b)</ref>. The second trend pertains to the parameter update, which has moved from gradient-based methods towards updates based on reward-weighted averaging <ref type="bibr" target="#b26">(Stulp and Sigaud, 2012)</ref>.</p><p>A striking feature of these trends, visualized in Figure <ref type="figure" target="#fig_0">1</ref>, and described in detail in Section 2, is that they have moved reinforcement learning (RL) approaches to policy improvement closer and closer to black-box optimization (BBO). This class of algorithms is depicted in the right-most column of Figure <ref type="figure" target="#fig_0">1</ref>. In fact, two state-of-the-art algorithms that have been applied to policy improvement -PI 2 <ref type="bibr" target="#b30">(Theodorou et al., 2010)</ref> and CMA-ES <ref type="bibr" target="#b5">(Hansen and Ostermeier, 2001)</ref> -are so similar that a line-by-line comparison of the algorithms is feasible <ref type="bibr" target="#b26">(Stulp and Sigaud, 2012)</ref>. The main difference is that whereas PI 2 is an RL algorithm -it uses information about rewards received at each time step during exploratory policy executions -whereas CMA-ES is a BBO algorithm -it uses only the total reward received during execution, which enables it to treat the utility function as a black box that returns one scalar value. In this article, we make the relation between RL and BBO even more explicit by taking these trends one (ultimate) step further. We do so by introducing PI BB (in Section 3), which simplifies the exploration and parameter update methods of PI 2 . These modifications are consistent with PI 2 's derivation from stochastic optimal control. An important insight is that PI BB is actually a BBO algorithm, as discussed in Section 4.1. More specifically, we show that PI BB is a special case of CMA-ES. One of the main contributions of this article is thus to draw an explicit bridge from RL to BBO approaches to policy improvement, as visualized by the dark line from PI 2 to PI BB in Figure <ref type="figure" target="#fig_0">1</ref>.</p><p>We thus have a pair of algorithms -PI 2 and PI BB -that use the same method for exploration (parameter perturbation) and parameter updating (reward-weighted averaging), and differ only in being RL (PI 2 ) or BBO (PI BB ) approaches to policy improvement. This allows for a more objective comparison of RL/BBO algorithms than, for instance, comparing eNAC and CMA-ES <ref type="bibr">(Heidrich-Meisner and Igel, 2008a;</ref><ref type="bibr">Rückstiess et al., 2010b)</ref>, as eNAC is an action-perturbing, gradient-based RL algorithm, and CMA-ES is a parameterperturbing, reward-weighted averaging BBO algorithm. If one is found to outperform the other on a particular task, does it do so due to the different parameter update methods? Or is it due to the difference in the policy perturbation? Or because one is an RL method and the other BBO? Using the PI 2 /PI BB pair allows us to specifically investigate the latter question, whilst keeping the other algorithmic features the same.</p><p>The PI 2 /PI BB pair may thus be the key to providing "[s]trong empirical evidence for the power of evolutionary RL and convincing arguments why certain evolutionary algorithms are particularly well suited for certain RL problem classes" <ref type="bibr">(Heidrich-Meisner and Igel, 2008a)</ref>, and could help verify or falsify the five conjectures proposed by <ref type="bibr">Togelius et al. (2009, Section 4.1)</ref>, about which types of problems are particularly suited for RL and BBO approaches to policy improvement. As a first step in this direction, we compare the performance of PI 2 and PI BB in terms of convergence speed and final cost on the evaluation tasks from <ref type="bibr" target="#b30">(Theodorou et al., 2010)</ref> in Section 3. Although PI BB has equal or better performance than PI 2 on these tasks, our aim in this article is not to make a case for either RL or BBO approaches to policy improvement -in general we expect the most appropriate method to vary from task to task, as discussed in Section 5 -but rather to provide a pair of algorithms that allow such targeted comparisons in the first place.</p><p>In summary, the main contributions of this article are:</p><p>• Providing an overview and classification of policy improvement algorithms.</p><p>• Deriving PI BB by simplifying the perturbation and update methods of PI 2 .</p><p>• Empirically comparing PI 2 and PI BB on the five tasks proposed by <ref type="bibr" target="#b30">Theodorou et al. (2010)</ref>, and showing that PI BB has equal or superior performance.</p><p>• Demonstrating that PI BB is a BBO algorithm. In particular, it is a special case of CMA-ES.</p><p>• Providing an algorithmic pair (PI 2 and PI BB ) with which it is easier to verify the conjectures proposed by <ref type="bibr" target="#b31">Togelius et al. (2009)</ref>.</p><p>The rest of this article is structured as follows. In the next section, we describe the policy improvement algorithms depicted in Figure <ref type="figure" target="#fig_0">1</ref>, explain their key differences, and classify them according to these differences. In Section 3, we show how PI BB is derived by applying two simplifications to PI 2 , and we compare the algorithms empirically on five tasks. The PI BB algorithm is analyzed more closely in Section 4; in particular, we show that PI BB is a BBO algorithm, and discuss several reasons why it outperforms PI 2 on the tasks used. In Section 6 we summarize the main contributions of the article, and present future research opportunities instigated by this article.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>In RL, the policy π maps states to actions. The optimal policy π * chooses the action that optimizes the cumulative discounted reward over time. When the state and actions sets of the system are discrete, finding the optimal policy π * can be cast in the framework of discrete Markov Decision Processes (MDPs) and solved with Dynamic Programming (DP) or RL methods <ref type="bibr" target="#b28">(Sutton and Barto, 1998)</ref>. For problems where the state is continuous, many state approximation techniques exist in the field of Approximate Dynamic Programming (ADP) methods <ref type="bibr" target="#b19">(Powell, 2007)</ref>. But when the action space also becomes continuous, the extension of DP or ADP methods results in optimization problems that have proven hard to solve in practice <ref type="bibr" target="#b24">(Santamaría et al., 1997)</ref>.</p><p>In such contexts, a policy cannot be represented by enumerating all actions, so parametric policy representations π θ are required, where θ is a vector of parameters. Thus, finding the optimal policy π * corresponds to finding the optimal policy parameters θ * , i.e. those that maximize cumulative discounted reward. As finding the θ corresponding to the global optimum is generally too expensive, policy improvement methods are local methods that rather search for a local optimum of the expected reward.</p><p>In episodic RL, on which this article focusses, the learner executes a task until a terminal state is reached. Executing a policy from an initial state until the terminal state, called a "roll-out", leads to a trajectory τ , which contains information about the states visited, actions executed, and rewards received. Many policy improvements use an iterative process of exploration, where the policy is executed K times leading to trajectories τ k=1...K , and parameter updating, where the policy parameters θ are updated based on this batch of trajectories. This process is explained in more detail in the generic policy improvement loop in Figure <ref type="figure" target="#fig_1">2</ref>. In each iteration, the policy is executed K times. One execution of a policy is called a 'Monte Carlo roll-out', or simply 'roll-out'. Because the policy is perturbed (different perturbation methods are described in Section 2.1.3), each execution leads to slightly different trajectories in state/action space, and potentially different rewards. The exploration phase thus leads to a set of different trajectories τ k=1...K . Based on these trajectories, policy improvement methods then update the parameter vector θ → θ new such that the policy is expected to incur lower costs/higher rewards. The process then continues with the new θ new as the basis for exploration.</p><p>In this section, we give an overview of algorithms that implement this loop. We distinguish between three main classes of algorithms, based on whether their derivation is based mainly -they are not mutually exclusive -on principles based on lower bounds on the expected return (Section 2.1), path integral stochastic optimal control (Section 2.2) or BBO (Section 2.3).</p><p>For each algorithm, we make a 'fact sheet', in which the following questions are answered:</p><p>Fact sheet (to be filled in by all algorithms) Perturbation: How is the policy perturbed? Trajectory: What information must be stored in the trajectory resulting from the execution of the policy? Actor-Critic: Is the method an actor-critic method, or a direct policy search method? Update: How are the parameters updated?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Policy Improvement through Lower Bounds on the Expected Return</head><p>We now briefly describe three algorithms that build on one another to achieve ever more powerful policy improvement methods, being REINFORCE <ref type="bibr" target="#b34">(Williams, 1992)</ref>, eNAC <ref type="bibr">(Peters and Schaal, 2008b)</ref>, and POWER <ref type="bibr" target="#b11">(Kober and Peters, 2011)</ref>. All these algorithms may be derived from a common framework based on the lower bound on the expected return, as demonstrated by <ref type="bibr" target="#b11">Kober and Peters (2011)</ref>. In this section, we focus on properties of the resulting algorithms, rather than on their derivations.</p><p>Our main aim here is to use a set of known algorithms to answer the questions in the fact sheet, thus providing a basis for considering the questions in perhaps unfamiliar and more intricate contexts in Section 2.2 and 3. Since these algorithms have already been covered in extensive surveys <ref type="bibr">(Peters and</ref><ref type="bibr">Schaal, 2008b, 2007;</ref><ref type="bibr" target="#b11">Kober and Peters, 2011)</ref>, we do not present them in full detail here, as this does not serve the particular aim of this section.</p><p>An underlying assumption of the algorithms presented in Section 2.1 and 2.2 is that the policies are represented as u t = g(x, t) ⊺ θ; g is a set of basis functions, for instance Gaussian kernels, θ are the policy parameters, x is the state, and t is time since the roll-out started.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1">REINFORCE</head><p>The REINFORCE algorithm <ref type="bibr" target="#b34">(Williams, 1992)</ref> ("reward increment = nonnegative factor × offset reinforcement × characteristic eligibility") uses a stochastic policy to foster exploration (1), where π θ (x) returns the nominal motor command<ref type="foot" target="#foot_0">foot_0</ref> , and ǫ t is a perturbation of this command at time t. In REINFORCE, this policy is executed K times with the same θ, and the states/actions/rewards that result from a roll-out are stored in a trajectory.</p><p>Given K such trajectories, the parameters θ are then updated by first estimating the gradient ∇θ J(θ) (2) of the expected return J(θ) = E N i=1 r t i |π θ . Here, the trajectories are assumed to be of equal length, i.e. having N discrete time steps t i=1...N . The notation in (2) estimates the gradient ∇θ d J(θ) for each parameter entry d in the vector θ separately. <ref type="bibr" target="#b20">Riedmiller et al. (2007)</ref> provides a concise yet clear explanation how to derive (2). The baseline ( <ref type="formula">3</ref>) is chosen so that it minimizes the variation in the gradient estimate <ref type="bibr">(Peters and Schaal, 2008a)</ref>. Finally, the parameters are updated through steepest gradient ascent (4), where the open parameter α is a learning rate.</p><p>Policy perturbation during a roll-out</p><formula xml:id="formula_0">u t = π θ (x) + ǫ t (1) Parameter update, given K roll-outs ∇θ d J(θ) = 1 K K k=1 N i=1 i j=1 ∇ θ d log π(u t j ,k |x t j ,k )(r t i ,k -b d t i ) (2) b d t i = K k=1 i j=1 ∇ θ d log π(u t j ,k |x t j ,k ) 2 r t i ,k K k=1 i j=1 ∇ θ d log π(u t j ,k |x t j ,k ) 2 (3) θ new = θ + α ∇θ J(θ)<label>(4)</label></formula><p>Fact sheet for REINFORCE Perturbation: A stochastic policy is used, i.e. the output of the nominal policy is perturbed, cf. (1).</p><p>Trajectory: To compute (2) and (3), the trajectory needs to contain, for each time step i, the reward r ti , as well as the state x ti and action u ti , as they are required to compute ∇ θ logπ θ (ut|xt).</p><p>Actor-Critic: The value function is not approximated, so it is a direct policy search method.</p><p>Update: The update is based on the gradient ∇ θ J(θ), cf. (4). Note that in order to compute this gradient, the policy must be differentiable w.r.t. its parameters: ∇ θ logπ θ (ut|xt).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2">eNAC</head><p>One issue with REINFORCE is that it requires many roll-outs for one parameter update, and the resulting trajectories cannot be reused for later updates. This is because we need to perform a roll-out each time we want to compute N i=1 [. . . ]r t i in (2). Such methods are known as 'direct policy search' methods. Actor-critic methods, such as "Episodic Natural Actor Critic" (eNAC), address this issues by using a value function V π θ as a more compact representation of long-term reward than sample episode R(τ ), allowing them to make more efficient use of samples.</p><p>In continuous state-action spaces, V π θ cannot be represented exactly, but must be estimated from data. Actor-critic methods therefore update the parameters in two steps: 1) approximate the value function from the point-wise estimates of the cumulative rewards observed in the trajectories acquired from roll-outs of the policy; 2) update the parameters using the value function. In contrast, direct policy search updates the parameters directly using point-wise estimates<ref type="foot" target="#foot_2">foot_2</ref> , as visualized in Figure <ref type="figure" target="#fig_2">3</ref>. The main advantage of having a value function is that it generalizes; whereas K roll-outs provide only K point-wise estimates of the cumulative reward, a value function approximated from these K point-wise estimates is also able to provide estimates not observed in the roll-outs. Another issue is that in REINFORCE the 'naive', or 'vanilla'<ref type="foot" target="#foot_3">foot_3</ref> , gradient ∇ θ J(θ) is sensitive to different scales in parameters. To find the true direction of steepest descent towards the optimum, independent of the parameter scaling, eNAC uses the Fischer information matrix F to determine the 'natural gradient':</p><formula xml:id="formula_1">θ new = θ + αF -1 (θ)∇ θ J(θ).</formula><p>In practice, the Fischer information matrix need not be computed explicitly <ref type="bibr">(Peters and Schaal, 2008b)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fact sheet for eNAC</head><p>Perturbation and Trajectory: Same as for REINFORCE. Actor-critic: As the name implies, eNAC is an actor-critic approach, because it first approximates the value function with LSTD(1) (critic), and then uses the value function to update the policy parameters (actor). Update: eNAC uses the natural gradient to update the policy parameters.</p><p>Thus, going from REINFORCE to eNAC represents a transition from direct policy search to actor-critic, and from vanilla to natural gradients.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.3">POWER</head><p>REINFORCE and eNAC are both 'action perturbing' methods which perturb the nominal command at each time step u t = u nominal t + ǫ t , cf. (1). Action-perturbing algorithms have several disadvantages: 1) Samples are drawn independently from one another at each time step, which leads to a very noisy trajectory in action space <ref type="bibr">(Rückstiess et al., 2010b)</ref>. 2) Consecutive perturbations may cancel each other and are thus washed out <ref type="bibr" target="#b11">(Kober and Peters, 2011)</ref>. The system also often acts as a low-pass filter, which further reduces the effects of perturbations that change with a high frequency. 3) On robots, high-frequency changes in actions, for instance when actions represent motor torques, may lead to dangerous behavior, or damage to the robot <ref type="bibr">(Rückstiess et al., 2010b)</ref>. 4) It causes a large variance in parameter updates, an effect which grows with the number of time steps <ref type="bibr" target="#b11">(Kober and Peters, 2011)</ref>.</p><p>The "Policy Learning by Weighting Exploration with the Returns" (POWER) algorithm therefore implements a different policy perturbation scheme first proposed by <ref type="bibr">Rückstiess et al. (2010a)</ref>, where the parameters θ of the policy, rather than its output, are perturbed, i.e. π θ + ǫt (x) rather than π θ (x) + ǫ t . This distinction has been illustrated in Figure <ref type="figure">4</ref>.</p><formula xml:id="formula_2">policy execution loop π + parameter perturbation θ ǫ t θ + ǫ t + action perturbation u t ǫ t E u t + ǫ t x t</formula><p>Figure <ref type="figure">4</ref>: Illustration of action and policy parameter perturbation. Action perturbation is applied to the output ut of the policy, whereas policy parameter perturbation is applied to the parameters θ of the policy. The perturbations are sampled at each time step, inside the loop in which the policy is executed.</p><p>REINFORCE and eNAC estimate gradients, which is not robust when noisy, discontinuous utility functions are involved. Furthermore, they require the manual tuning of the learning rate α, which is not straight-forward, but critical to the performance of the algorithms <ref type="bibr" target="#b30">(Theodorou et al., 2010;</ref><ref type="bibr" target="#b11">Kober and Peters, 2011)</ref>. The POWER algorithm proposed by <ref type="bibr" target="#b11">Kober and Peters (2011)</ref> addresses these issues by using reward-weighted averaging, which rather takes a weighted average of a set of K exploration vectors ǫ k=1...K as follows:</p><p>Policy perturbation during a roll-out</p><formula xml:id="formula_3">u t = π θ + ǫt (x), with ǫ t ∼ N (0, Σ)<label>(5)</label></formula><p>Parameter update, given K roll-outs</p><formula xml:id="formula_4">S k t i = N j=i r k j<label>(6)</label></formula><formula xml:id="formula_5">θ new = θ + E N i=1 WS t i -1 E N i=1 Wǫ t i S t i (7) ≈ θ + K k=1 N i=1 W t i S k t i -1 K k=1 N i=1 W t i ǫ k t i S k t i (8) with W t i = g t i g ⊺ t i (g ⊺ t i Σg t i ) -1</formula><p>where K refers to the number of roll-outs, and g t i is a vector of the basis function activations at time t i . The update (8) may be interpreted as taking the average of the perturbation vectors ǫ k , but weighting them with S k / K l=1 S l , which is a normalized version of the reward-to-go S k . Hence the name reward-weighted averaging. An important property of reward-weighted averaging is that it follows the natural gradient <ref type="bibr" target="#b0">(Arnold et al., 2011)</ref>, without having to actually compute the gradient or the Fischer information matrix. This leads to more robust updates.</p><p>The final main difference between REINFORCE/eNAC and POWER is that the former require roll-out trajectories to contain information about the state and actions, as they must compute ∇ θ logπ θ (ut|xt) to perform an update. In contrast, POWER only uses information about the rewards r t from the trajectory, as these are necessary to compute the expected return (6). The basis function g are parameters of the algorithm, and must not be stored in the trajectory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fact sheet for POWER</head><p>Perturbation: The parameters of the policy are perturbed (θ + ǫ t ), rather than the output of the policy, cf. (5). Trajectory: To determine the expected return (6), the trajectory must contain the reward received at each time step r t i=1...N . The state and actions are not needed to perform an update. Actor-Critic: The value function is not approximated, so it is a direct policy search method. Update: The update is based on reward-weighted averaging (8), rather than estimating a gradient.</p><p>In summary, going from eNAC to POWER represents a transition from action perturbation to policy parameter perturbation, from estimating the gradient to reward-weighted averaging, and from actor-critic back to direct policy search (as in REINFORCE).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Policy Improvement with Path Integrals</head><p>In this section, we describe the second stream (SOC→GPIC→PI 2 ) in Figure <ref type="figure" target="#fig_0">1</ref>. The aim of this section is to lay the foundation for Section 3, in which we analyze how the transition from action perturbation to policy parameter perturbation is made within the PI 2 derivation. We only explain those parts of the derivation that serve this aim; for the complete PI 2 derivation on which this section is based, we refer to <ref type="bibr" target="#b30">Theodorou et al. (2010)</ref>. Note that in the previous section, algorithms aimed at maximizing rewards. In optimal control, the convention is rather to define costs, which should be minimized.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Source: Stochastic Optimal Control</head><p>The PI 2 derivation is based on stochastic optimal control (SOC). Note that SOC in itself does not involve parameterized policies, exploration or learning, and is not an algorithm. It is rather the definition of a domain in which, given a model of the control system and a cost function, a set of equations (Hamilton Jacobi Bellman) is derived which must be solved in order to compute the optimal controls. In Section 2.2.2, we present a path-integralbased solution to this problem formulation, and in Section 2.2.3 we apply this solution to parameterized policies, which yields the PI 2 algorithm.</p><p>In SOC, the dynamics of the control system is assumed to take the following form:</p><formula xml:id="formula_6">ẋt = f (x t ) + G(x t ) (u t + ǫ t ) = f t + G t (u t + ǫ t )<label>(9)</label></formula><p>where x t denotes the state of the system, G t = G(x t ) the control matrix, f t = f (x t ) the passive dynamics, u t the control vector and ǫ t zero-mean Gaussian noise with covariance Σ.</p><p>In the system model of SOC, we see that actions are perturbed, because of u t + ǫ t (9). However, the nature of this perturbation is quite different to those in the algorithms in Section 2. In SOC, these perturbations represent additive motor stochasticity that arises when applying the command u t to the system, for instance due to imperfectly calibrated motors or wear-and-tear of the system. This is quite distinct from the interpretation in for instance REINFORCE or eNAC, where the policy is stochastic because the algorithm adds perturbations itself to foster exploration. The aim in SOC is to find the commands that minimize cost despite motor stochasticity that arises in the system, whereas in RL methods like RE-INFORCE and eNAC learn these commands because of stochasticity that these algorithms introduce themselves.</p><p>For the finite horizon problem, the goal is to determine the control inputs u t i :t N which minimize the value function</p><formula xml:id="formula_7">V (x t i ) = V t i = min ut i :t N E τ i [R(τ i )] (10) R(τ i ) = φ t N + t N t i r t dt (11) r t = r(x t , u t , t) = q t + 1 2 u ⊺ t Ru t dt (<label>12</label></formula><formula xml:id="formula_8">)</formula><p>where R is the finite horizon cost over a trajectory starting at time t i in state x t i and ending at time t N and where φ t N = φ(x t N ) is a terminal reward at time t N , r t = r(x t , t) is an arbitrary state-dependent immediate reward function, and R is the control cost matrix. τ i are trajectory pieces starting at x t i and ending at time t N .</p><p>From SOC <ref type="bibr" target="#b25">(Stengel, 1994)</ref>, it is known that the associated Hamilton Jacobi Bellman (HJB) equation is</p><formula xml:id="formula_9">∂ t V t = q t + (∇ x V t ) ⊺ f t - 1 2 (∇ x V t ) ⊺ G t R -1 G ⊺ t (∇ x V t ) + 1 2 trace ((∇ xx V t )G t ΣG ⊺ t ) (13) u(x t i ) = u t i = -R -1 G ⊺ t i (∇ xt i V t i )<label>(14)</label></formula><p>where u(x t i ) is the corresponding optimal control. The rest of the PI 2 derivation is dedicated to finding a solution to this non-linear, 2 nd order partial differential equation (Section 2.2.2), and applying this solution to parameterized policies (Section 2.2.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">From Stochastic Optimal Control to Generalized Path Integral Control</head><p>Let us now briefly sketch the three main steps in the derivation of Generalized Path Integral Control (GPIC) <ref type="bibr" target="#b30">(Theodorou et al., 2010)</ref> from the HJB equations:</p><p>1. Linearize the HJB into a Chapman Kolmogorov partial differential equation (PDE) by substituting V t = -λ log Ψ t (15) <ref type="bibr" target="#b10">(Kappen, 2005)</ref> and introducing a simplification λR -1 = Σ <ref type="bibr" target="#b30">(Theodorou et al., 2010)</ref>.</p><p>2. Transform the Chapman Kolmogorov PDE into a path integral <ref type="bibr" target="#b10">(Kappen, 2005)</ref>, by using the Feynman-Kac theorem.</p><p>3. Generalize the path integral by partitioning the control transition matrix</p><formula xml:id="formula_10">G t into directly G (c)</formula><p>t and indirectly actuated parts <ref type="bibr" target="#b30">(Theodorou et al., 2010)</ref>.</p><p>This leads to the following path integral formulation of the value function:</p><formula xml:id="formula_11">V t = -λ log Ψ t (<label>15</label></formula><formula xml:id="formula_12">)</formula><formula xml:id="formula_13">Ψ t i = lim dt→0 exp - 1 λ S(τ i ) dτ (c) i (16) with S(τ i ) = φ t N + N -1 j=i q t j dt + C t i (17)</formula><p>where the integral is over paths, i.e. dτ</p><formula xml:id="formula_14">(c) i = (dx t i , . . . , dx t N ).</formula><p>The accumulated cost S(τ i ) may be interpreted as the cost-to-go from time step i, i.e. the cost accumulated during the rest of the trajectory starting at t i . Thus at the end when i = N , S(τ i ) only consists of the terminal cost S(τ N ) = φ t N . At the beginning i = 1, and S(τ 1 ) corresponds to the sum of the costs over the entire trajectory 4 . The path integral over trajectories dτ i in ( <ref type="formula">16</ref>) may thus be interpreted as "the value at time step i is (the logarithm of) the exponentiation of the cost-to-go at time step i over all possible trajectories." Generating all possible trajectories on a robot to exactly compute V t would be a timeconsuming enterprise indeed. Instead, Eq. ( <ref type="formula">16</ref>) may be approximated by sampling trajectories. However, in practical applications with high-dimensional state spaces, this sampling would generate primarily trajectories of high cost, and finding low cost trajectories would 4 For completeness, the term <ref type="bibr" target="#b30">(Theodorou et al., 2010)</ref>. be a question of luck, rather than wisdom. Also, the dynamics of the system may bias the trajectories to be sampled in only a small part of the state space, which may not necessarily be the part where low-cost trajectories are to be found. But having a path integral that can, in principle, be estimated by performing Monte-Carlo roll-outs of the system is a big step forward from having the value function V t represented as HJB: a non-linear, second order partial differential equation which does not have a general solution (13).</p><formula xml:id="formula_15">Ct i is 1 2 N -1 j=i x (c) t j+1 -x (c) t j dt -f (c) t j 2 G (c) t j R -1 G (c)⊺ t j + λ 2 N -1 j=i log G (c) t j G (c)⊺ t j , cf.</formula><p>Given the value function in (15), we are able to compute the optimal command. We do so by inserting the value function in (15) into the optimal command equation ( <ref type="formula" target="#formula_9">14</ref>) and acquire (18). Replacing Ψ t i in ( <ref type="formula">18</ref>) with ( <ref type="formula">16</ref>) and simplifying leads to (19).</p><formula xml:id="formula_16">u t i = λR -1 G t i ∇ xt i Ψ t i Ψ t i (18) = P (τ i )D(τ i )ǫ(τ i )dτ (c) i (19) with P (τ i ) = e -1 λ S(τ i ) e -1 λ S(τ i ) dτ i<label>(20)</label></formula><p>and</p><formula xml:id="formula_17">D(τ i ) = R -1 G (c)⊺ t i (G (c) t i R -1 G (c)⊺ t i )G (c) t i<label>(21)</label></formula><p>Here, P (τ i ) is the probability of trajectory τ at time step i, and is inversely proportional to the cost-to-go. Lower costs thus lead to higher probabilities. The optimal command at time step i is then computed as weighted average of the observed perturbation ǫ(τ i ) of the trajectory, weighted by the probability of the trajectory P (τ i ). Thus, we have inverse-cost weighted averaging, which is analogous to reward-weighted averaging as done in POWER ( <ref type="formula">8</ref>), as the inverse of a cost may be considered a reward.</p><p>As (21) makes clear, GPIC is model-based, and assumes knowledge of the system model, i.e. control matrix G(x t ) and passive dynamics f (x t ). In Section 2.2.3, we see that applying GPIC to parameterized policies and making the update rule iterative leads to a very powerful, model-free policy improvement algorithm.</p><p>Although GPIC does not involve a parameterized policy, we can still very loosely apply the 'fact sheet' questions from Section 2.</p><p>Fact sheet for GPIC Perturbation: Action perturbation, but determined by the system. Follows directly from SOC formulation in (9). It is not to be confused with the perturbations that REINFORCE and eNAC generate themselves to foster exploration. Trajectory: To perform an update, the cost and state at each time must be known. A substantial, infeasible amount of roll-outs may be required to achieve a good approximation of ( <ref type="formula">16</ref>); using all possible paths leads to the exact solution.</p><p>Actor-critic: Although the value function is at the heart of GPIC's derivation, it is no longer explicitly represented, or approximated by a function approximator, in ( <ref type="formula">19</ref>)-( <ref type="formula" target="#formula_17">21</ref>). Therefore, it cannot be an actor-critic. Update: The concept of reward-weighted averaging is already apparent in GPIC ( <ref type="formula">19</ref>), but it is not yet applied to policy parameters (GPIC does not use parameterized policies), as for instance in POWER. To perform the update, the system model must be known.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.3">From Generalized Path Integral Control to PI 2</head><p>The PI 2 algorithm is a special case of the GPIC optimal control solution in ( <ref type="formula">19</ref>), applied to control systems with parameterized control policy <ref type="bibr" target="#b30">(Theodorou et al., 2010)</ref> as in ( <ref type="formula">22</ref>).</p><p>That is, the control command is generated from the inner product of a perturbed parameter vector θ + ǫ t with a vector of basis functions g t : g ⊺ t (θ + ǫ t ). The noise ǫ t is interpreted as exploration noise sampled from a normal distribution N (0, Σ), where Σ is a user controlled parameter. Since parameters rather than actions are perturbed, PI 2 is a parameter perturbing approach.</p><p>The path integral formulation in ( <ref type="formula">19</ref>) applied to parameterized policies provides us with the following parameter update rule for PI 2 :</p><p>Policy perturbation during a roll-out</p><formula xml:id="formula_18">u t = g ⊺ t (θ + ǫ t ) (22)</formula><p>Parameter update for each time step through reward-weighted averaging</p><formula xml:id="formula_19">δθ t i = M t i ,k K k=1 [P (τ i,k ) ǫ t i ,k ] (23) with P (τ i,k ) = e -1 λ S(τ i,k ) K k=1 [e -1 λ S(τ i,k ) ]<label>(24)</label></formula><p>and</p><formula xml:id="formula_20">S(τ i,k ) = φ t N ,k + N -1 j=i q t j ,k + 1 2 N -1 j=i+1 (θ + M t j ,k ǫ t j ,k ) ⊺ R(θ + M t j ,k ǫ t j ,k )<label>(25)</label></formula><p>and</p><formula xml:id="formula_21">M t j ,k = R -1 g t j ,k g T t j ,k g T t j ,k R -1 g t j ,k<label>(26)</label></formula><p>Weighted average over time steps</p><formula xml:id="formula_22">[δθ] j = N -1 i=0 (N -i) w j,t i [δθ t i ] j N -1 i=0 w j,t i (N -i)<label>(27)</label></formula><p>Actual parameter update</p><formula xml:id="formula_23">θ new = θ + δθ<label>(28)</label></formula><p>The cost-to-go S(τ i,k ) is computed for each of the K roll-outs and each time step i = 1 . . . N . The terminal cost φ t N , immediate costs q t i and command cost matrix R are task-dependent and provided by the user. M t j ,k is a projection matrix onto the range space of g t j under the metric R -1 , cf. <ref type="bibr" target="#b30">(Theodorou et al., 2010)</ref>. The probability of a roll-out P (τ i,k ) is computed as the normalized exponentiation of the cost-to-go. This assigns high probabilities to low-cost roll-outs and vice versa. The intuition behind this step is that trajectories of lower cost should have higher probabilities. The interpretation of P k as a probability follows from applying the Feynman-Kac theorem to the SOC problem, cf. <ref type="bibr" target="#b30">(Theodorou et al., 2010)</ref>.</p><p>The key algorithmic step is in ( <ref type="formula">23</ref>), where the parameter update δθ is computed for each time step i through probability weighted averaging over the exploration ǫ of all K trials. Trajectories with higher probability, and thus lower cost, therefore contribute more to the parameter update.</p><p>A different parameter update δθ t i is computed for each time step. To acquire one parameter vector θ, the time-dependent updates must be averaged over time, one might simply use the mean parameter vector over all time steps: δθ = 1 N N i=1 δθ t i . Although temporal averaging is necessary, the particular weighting scheme used in temporal averaging does not follow from the derivation. Rather than a simple mean, <ref type="bibr" target="#b30">Theodorou et al. (2010)</ref> suggest the weighting scheme in Eq. ( <ref type="formula" target="#formula_22">27</ref>). It emphasizes updates earlier in the trajectory, and also makes use of the activation of the j th basis function at time step i, i.e. w j,t i (32).</p><p>Apart from being applied to parameterized policies rather than determining optimal controls, the key differences between GPIC and PI 2 are: PI 2 is model-free. In SOC, the passive dynamics f (x t ) and control matrix G(x t ) represent models of the system. In the parameterized policies on which PI 2 acts, these are replaced with the linear spring-damper system f t and the basis functions g t . These are not models of the control system, i.e. the gravity vector or the inertia matrix, but rather functions that can be parameterized freely by the user.</p><p>Perturbations are generated by PI 2 , not the system. Whereas in GPIC perturbations arise from the stochasticity in the system, PI 2 rather samples perturbations itself to actively foster exploration. These perturbation are sampled from a Gaussian ǫ t ∼ N (0, Σ), where Σ is an open parameter set by the user. Because PI 2 perturbs the policy parameters (θ + ǫ t ), PI 2 is a parameter perturbing method.</p><p>PI 2 is iterative. The path integral ( <ref type="formula">19</ref>) is not iterative, and computes the optimal controls from a large batch of trajectories in one go. In high-dimensional systems, many of these trajectories will be 'useless' trajectories with high cost, which do not contribute because they are assigned a low probability. To have a sufficient amount of useful trajectories, an often infeasible amount of trajectories must be sampled. PI 2 addresses this by applying an iterative strategy, which starts with an initial estimate of the optimal parameters θ init . In robotics, this estimate is typically acquired through supervised imitation learning. PI 2 then samples K perturbations locally around θ init . Because θ init is assumed to already be a relatively good parameterization, local samples around θ init will in general also not be too bad. The update is therefore based on the variance within a set of trajectories that are all quite useful, rather than containing many useless trajectories. This local strategy is also applied to all subsequent parameters arising from updates. The 'single shot' global sampling from ( <ref type="formula">19</ref>) has thus been replaced by a local iterative approach that searches incrementally amongst mainly good roll-outs.</p><p>As demonstrated in <ref type="bibr" target="#b30">(Theodorou et al., 2010)</ref>, PI 2 is able to outperform the previous RL algorithms for parameterized policy learning described in Section 2.1 by at least one order of magnitude in learning speed (number of roll-outs to converge) and also lower final cost performance. As an additional benefit, PI 2 has no open algorithmic parameters, except for the magnitude of the exploration noise Σ, and the number of trials per update K.</p><p>Although applying our four 'fact sheet' questions to GPIC was quite forced -it does not use a parameterized policy -we may readily construct one for PI 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fact sheet for PI 2</head><p>Perturbation: The parameters of the policy are perturbed: θ +ǫ t . Trajectory: To compute the cost-to-go (25), the trajectory must contain the reward received at each time step r t i=1...N . The states and actions are not needed to perform an update. Actor-critic: No value function is approximated, so PI 2 is a direct policy search method. Update: The update is based on reward-weighted averaging.</p><p>Application to Dynamic Movement Primitives So far, the most impressive results of PI 2 have been demonstrated when using Dynamic Movement Primitives (DMPs) <ref type="bibr" target="#b8">(Ijspeert et al., 2002)</ref> as the underlying parameterized control policy. DMPs consist of a set of dynamical system equations:</p><formula xml:id="formula_24">1 τ ẍt = f t + g ⊺ t θ</formula><p>Transform. system (29)</p><formula xml:id="formula_25">f t = α(β(g -x t ) -ẋt ) Spring-damper (30) [g t ] j = w j (s t ) • s t p k=1 w k (s t ) (g -x 0 )</formula><p>Basis functions (31)</p><formula xml:id="formula_26">w j = exp -0.5h j (s t -c j ) 2 Gaussian kernel (32) 1 τ ṡt = -αs t Canonical. system (33)</formula><p>The core idea behind DMPs is to perturb a simple linear spring-damper system f t with a non-linear component g ⊺ t θ to acquire smooth movements of arbitrary shape. In the context of PI 2 , the commands u t = g ⊺ t (θ + ǫ t ) are thus taken to be the output of the non-linear system in (29).</p><p>The intuition of this approach is to create desired trajectories x d,t , ẋd,t , ẍd,t for a motor task out of the time evolution of a nonlinear attractor system, where the goal g is a point attractor and x 0 the start state. The (policy) parameters θ determine the shape of the attractor landscape within a nonlinear function approximator, which allows to represent almost arbitrary smooth trajectories, e.g., a tennis swing, a reaching movement, or a complex dance movement. The canonical system s t is the phase of the movement, which is 1 at the beginning, and decays to 0 over time. The multiplication of g t with s t in (31) ensures that the effect of g ⊺ t θ disappears at the end of the movement when s = 0. The entire system thus converges to the goal g.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Policy Improvement through Black-box Optimization</head><p>Policy improvement may also be achieved with BBO. In general, the aim of BBO is to find the solution x * ∈ X that optimizes the objective function J : X → R <ref type="bibr" target="#b0">(Arnold et al., 2011)</ref>. As in Stochastic Optimal Control and PI 2 , J is usually chosen to be a cost function, such that optimization corresponds to minimization. Let us highlight three aspects that define BBO: 1) Input: no assumptions are made about the search space X; 2) Objective function: the function J is treated as a 'black box', i.e. no assumptions are made about, for instance, its differentiability or continuity; 3) Output: the objective function returns only one scalar value. A desirable property of BBO algorithms that are applicable to problems with these conditions is that they are able to find x * using as few samples from the objective function J(x) as possible. Many BBO algorithms, such as CEM <ref type="bibr" target="#b21">(Rubinstein and Kroese, 2004)</ref>, CMA-ES <ref type="bibr" target="#b5">(Hansen and Ostermeier, 2001)</ref> and NES <ref type="bibr" target="#b33">(Wierstra et al., 2008)</ref>, use an iterative strategy, where the current x is perturbed x+ǫ k=1...K , and a new solution x new is computed given the evaluations J k=1...K = J(x + ǫ k=1...K ).</p><p>BBO is applicable to policy improvement <ref type="bibr">(Rückstiess et al., 2010b)</ref> as follows: 1) Input: the input x is interpreted as being the policy parameter vector θ. Whereas RL algorithms are tailored to leveraging the problem structure to update the policy parameters θ, BBO algorithms used for policy improvement are completely agnostic about what the parameters θ represent; θ might represent the policy parameters for a motion primitive to grasp an object, or simply the 2-D search space to find the minimum of a quadratic function.</p><p>2) Objective function: the function f executes the policy π with parameters θ, and records the rewards r t i=1:N . 3) Output: J must sum over these rewards after a policy execution: R = N i=1 r t i to achieve an output of only one scalar value. Examples of applying BBO to policy improvement include <ref type="bibr" target="#b15">(Ng and Jordan, 2000;</ref><ref type="bibr" target="#b2">Busoniu et al., 2011;</ref><ref type="bibr">Heidrich-Meisner and Igel, 2008a;</ref><ref type="bibr">Rückstiess et al., 2010b;</ref><ref type="bibr" target="#b12">Marin and Sigaud, 2012;</ref><ref type="bibr" target="#b3">Fix and Geist, 2012)</ref>.</p><p>Given the definition of BBO, we see that applying it to policy improvement already allows us categorize it in terms of the four questions in the fact sheet, without considering specific algorithms.</p><p>Perturbation: Since J takes the policy parameters as an argument, the perturbation must also take place in this space. Therefore, all BBO approaches to policy improvement must be policy parameter perturbing methods. Furthermore, since the policy is executed within the function J, the parameter perturbation θ + ǫ can be passed only once as an argument to J before the policy is executed. The perturbations ǫ therefore cannot vary over time, as is the case in REINFORCE/eNAC/POWER; this difference is apparent when comparing the left and right illustrations in Figure <ref type="figure" target="#fig_3">5</ref>. As Heidrich-Meisner and Igel (2008a) note: "in evolutionary strategies <ref type="bibr">[BBO]</ref> there is only one initial stochastic variation per episode, while the stochastic policy introduces perturbations in every step of the episode.". Trajectory: Since the cost function returns only the total cost R = N i=1 r t i , it is a rather minimalist degenerate trajectory, representing the cost-to-go for t i=1 only. Because states and actions are not stored in this 'trajectory', BBO by definition cannot make use of state/action information, as visualized in Figure <ref type="figure" target="#fig_4">6</ref>. This is a further reason why BBO must be parameter perturbing; using a stochastic policy inside the cost function J would lead to J returning different values for the same θ. If no information about the states visited/actions performed is available, no algorithm is able to map these differences in cost to differences in policy parameters. A defining feature of RL approaches is that they leverage information about the states that were visited, and about which states yielded which rewards <ref type="bibr" target="#b31">(Togelius et al., 2009)</ref>.  Algorithms that use only the aggregate scalar reward are considered to be BBO approaches to policy improvement.</p><formula xml:id="formula_27">policy execution loop π + parameter perturbation θ ǫ t θ + ǫ t E u t x t policy execution loop π + parameter perturbation θ ǫ θ + ǫ E u t x t</formula><formula xml:id="formula_28">r t1 r t2 . . . r t N x t1 x t2 . . . x t N u t1 u t2 . . . u t N r t1 r t2 . . .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Generic fact sheet for BBO</head><p>Perturbation: The parameters of the policy are perturbed (θ +ǫ), and must be constant during the execution of the policy. Trajectory: A degenerate trajectory consisting of only one scalar value, representing the total cost of the trajectory: N i=1 r ti . Actor-Critic: Since no information about states is available in the trajectories, and value functions are defined over states, BBO methods cannot approximate a value function. Therefore, they cannot be actor-critic. Update: The update method depends on the algorithm. Vanilla gradients (gradient descent), natural gradients (NES) and reward-weighted averaging (CMA-ES,CEM) are all used.</p><p>In fact, if a policy improvement algorithm 1) uses policy perturbation, where the perturbation is constant during policy execution, and 2) stores only the scalar total cost of a roll-out in the trajectory, then it is by definition a BBO approach to policy improvement, because the algorithm that works under these conditions is by definition a BBO algorithm. As is pointed out by <ref type="bibr">Rückstiess et al. (2010b)</ref>: "the return of a whole RL episode can be interpreted as a single fitness evaluation. In this case, parameter-based exploration in RL is equivalent to black-box optimization.". However, this distinction is not always made so clearly in the literature, where algorithms are referred to as RL or BBO based rather on their derivation and community that is being addressed. What also makes this distinction less clear is that RL is sometimes also considered to be a problem definition, rather than a solution or algorithm in itself; some of these solutions are considered to be 'typical' RL solutions to RL problems, whereas others are considered to be BBO solutions to RL problems. Where the line is drawn is a topic of vigorous debate.</p><p>We use the term 'policy improvement' for the general problem of optimizing policy parameters with respect to a utility function. BBO approaches to policy improvement are defined by the two properties above. All other approaches are RL. If a particular algorithm is preferably considered to be a RL approach, we are agnostic about this. But if the two conditions above (one scalar return, constant parameter perturbation) hold, it must at least be acknowledged that the algorithm may also be interpreted as being a BBO algorithm. Within the scope of this article, we define BBO and RL algorithms to be mutually exclusive; we do so for clarity only, and not to take sides in the debate.</p><p>The relative advantages and disadvantages of using the states and rewards encountered during a roll-out rather than treating policy improvement as BBO depend on the domain and problem structure, and are not yet well understood <ref type="bibr">(Heidrich-Meisner and Igel, 2008a)</ref>. <ref type="bibr">Togelius et al. (2009, Section 4</ref>.1) list five conjectures about which approach outperforms the other for which types of RL problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.1">Finite-difference methods</head><p>We now briefly present finite-difference (FD) methods, as they are one of the oldest and perhaps simplest policy gradient approaches, and may be interpreted as performing BBO on policy parameters. Here, policy parameters θ are varied K times with perturbations ǫ k , and a regression of ǫ k on the resulting performance differences δJ k is performed:</p><formula xml:id="formula_29">J ref = J(θ) Reference (34) J k = J(θ + ǫ k ) with k = 1 . . . K Perturb (35) δJ k=1:K = J k=1:K -J ref Difference (36) ∇ θ J(θ) = (∆Θ ⊺ ∆Θ) -1 ∆Θ ⊺ ∆J Gradient (37) with ∆Θ = [ǫ 1 , . . . , ǫ K ] ⊺ and ∆J = [δJ 1 , . . . , δJ K ] ⊺ θ new = θ + α∇ θ J(θ) Update (<label>38</label></formula><formula xml:id="formula_30">)</formula><p>This algorithm performs BBO, because θ is passed to J, and J is a black-box objective function that returns a scalar value. In FD, this value is a reward, rather than a cost. Note that in none of the equations above we see a policy, states, or actions; all this information is dealt with within the objective function J. The equations above may in principle also be used to find the minimum of a quadratic function. For FD, we see that K + 1 evaluations of the black-box objective function J, corresponding to K + 1 executions of the policy, are required to perform an update of θ. The PEGASUS algorithm <ref type="bibr" target="#b15">(Ng and Jordan, 2000)</ref> is an example of applying the concept of finite-differencing to policy improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.2">Covariance Matrix Adaptation -Evolutionary Strategy (CMA-ES)</head><p>CMA-ES <ref type="bibr" target="#b5">(Hansen and Ostermeier, 2001)</ref> is an example of an existing BBO method that was applied only much later to the specific domain of policy improvement <ref type="bibr">(Heidrich-Meisner and Igel, 2008a)</ref>. In BBO, CMA-ES is considered to be a de facto standard <ref type="bibr">(Rückstiess et al., 2010b)</ref>. We describe it a bit more extensively here, because we use CMA-ES for a comparison in Section 4.2.</p><p>CMA-ES searches for the global minimum as listed in Algorithm 1. First, CMA-ES samples K exploration vectors ǫ k=1...K from a Gaussian distribution N (0, σ 2 Σ) (line 5), where σ is the 'step-size', and Σ is the covariance matrix. The cost J k of each of these samples is then computed with the black-box cost function J (line 7). The exploration vectors are then sorted with respect to their cost, and only the best K e samples are kept (line 9). Each of these K e 'elite' samples is assigned a weight. The function that maps the cost J k to the weight P k are chosen by the user, and must satisfy the constraints Ke k=1 P k = 1 and P 1 ≥ • • • ≥ P Ke . Thus, samples with lower cost are assigned larger weights than those with higher cost. The default suggested by <ref type="bibr" target="#b5">Hansen and Ostermeier (2001)</ref> is listed in line 11, i.e. P k = ln (max (K/2, K e ) + 0.5))ln(k). The parameter update is then computed with </p><formula xml:id="formula_31">pσ ← (1 -cσ) pσ + cσ(2 -cσ)µ P Σ -1 (δθ/σ) 19 σnew = σ × exp cσ dσ pσ E N (0,I) -1 20 p Σ ← (1 -c Σ ) p Σ + hσ c Σ (2 -c Σ )µ P (δθ/σ) 21 Σ new = (1 -c 1 -cµ) Σ + c 1 (p Σ p T Σ + δ(hσ)Σ) + cµΣ tmp 22</formula><p>Algorithm 1: The CMA-ES algorithm. reward-weighted averaging using the weights P k (line 13), i.e. δθ = K k=1 [P k ǫ k ]. The covariance matrix is also updated using reward-weighted averaging (line 15).</p><p>The last part (lines 19-22) further adapts the step-size σ and covariance matrix Σ, using the so-called 'evolution paths' p σ and p Σ , which store information about previous parameter updates. Although these last lines lead to more robust convergence in practice, we do not elaborate on them here, as they do not involve the 'core' of the algorithm, and are not relevant for our purposes. For a full explanation of the algorithm we refer to <ref type="bibr" target="#b5">Hansen and Ostermeier (2001)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.3">Other BBO Algorithms Applied to Policy Improvement</head><p>Further BBO algorithms that have been applied to policy improvement include the Cross-Entropy Method (CEM) <ref type="bibr" target="#b21">(Rubinstein and Kroese, 2004;</ref><ref type="bibr" target="#b2">Busoniu et al., 2011;</ref><ref type="bibr" target="#b7">Heidrich-Meisner and Igel, 2008b)</ref>, which is very similar to CMA-ES, but has simpler methods for determining the weights P k and performing the covariance matrix update. For a comparison of further BBO algorithms such as PGPE <ref type="bibr">(Rückstiess et al., 2010b)</ref> and Natural Evolution Strategies (NES) <ref type="bibr" target="#b33">(Wierstra et al., 2008)</ref> with CMA-ES and eNAC, please see the overview article by <ref type="bibr">Rückstiess et al. (2010b)</ref>.</p><p>NEAT+Q is a hybrid algorithm that actually combines RL and BBO in a very original way <ref type="bibr" target="#b32">(Whiteson and Stone, 2006)</ref>. Within our classification, NEAT+Q is first and foremost an actor-critic approach, as function approximation is used to explicitly represent the value function. What sets NEAT+Q apart is that the representation of the value function is evolved through BBO, with the "Neuro Evolution of Augmenting Topologies" (NEAT). This alleviates the user from having to design this representation; unsuitable representations may keep RL algorithms from being able to learn the problem, even for algorithms with proven convergence properties <ref type="bibr" target="#b32">(Whiteson and Stone, 2006)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Classification of Policy Improvement Algorithms</head><p>Figure <ref type="figure" target="#fig_6">7</ref> is an extended version of Figure <ref type="figure" target="#fig_0">1</ref>, which now includes all the questions of our fact sheet. This table will be especially useful for our discussion in Section 5 about which classes of algorithms are appropriate for which classes of problems. The figure also reminds us of the main aim of this article: continuing the SOC →GPIC →PI 2 stream to acquire the PI BB algorithm. 3 From PI 2 to PI BB Figure 8 repeats the 'stream' from SOC to GPIC to PI 2 . In terms of exploration and parameter updates, it becomes apparent that PI 2 is similar to the BBO algorithm CMA-ES. In fact, they are so similar that a line-by-line comparison of the algorithms is feasible, as is done by <ref type="bibr" target="#b26">Stulp and Sigaud (2012)</ref>. Figure <ref type="figure" target="#fig_7">8</ref> suggests that two modifications are required to convert PI 2 into a BBO algorithm. First of all, the policy perturbation method must be adapted. PI 2 and BBO both use policy parameter perturbation, but in PI 2 it varies over time (θ + ǫ t ), whereas it must remain constant over time in BBO (θ + ǫ). In Section 3.1, we therefore simplify the perturbation method in PI 2 to be constant over time, which yields the algorithm variation PI 2 .</p><p>Second, we must adapt PI 2 such that it is able to update the parameters based only on the scalar aggregated cost J = N i=1 r t i , rather than having access to the reward r t at each time step t. This is done in Section 3.2, and yields the PI BB algorithm. An important aspect of these two simplifications for deriving PI BB from PI 2 is that they do not violate any of the assumptions required for the derivation of PI 2 from SOC, which we motivate in more detail throughout this section.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BBO</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Simplifying the Exploration Method</head><p>This section is concerned with analyzing exploration in PI 2 . In particular we: 1) Argue on theoretical grounds why exploration vectors ǫ must not be sampled anew at each time step during a roll-out, and present two previously proposed alternative exploration methods <ref type="bibr" target="#b30">(Theodorou et al., 2010;</ref><ref type="bibr" target="#b29">Tamosiumaite et al., 2011;</ref><ref type="bibr" target="#b26">Stulp and Sigaud, 2012)</ref>. These exploration variants of PI 2 are denoted PI 2 /PI 2 /PI 2 . 2) Show that these three methods lead to different levels of exploration in policy space, and that compensating for this effect leads the methods to perform essentially identical.</p><p>Especially the last point provides a deeper insight into the underlying cause for the performance differences that have been observed for the three exploration methods, and is one of the contributions of this article. Furthermore, demonstrating that constant exploration (PI 2 ) is not outperformed by the other two exploration methods (PI 2 /PI 2 ) paves the way for our comparison between episodic RL and BBO in Section 4.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Parameter Perturbation in the Context of the PI 2 Derivation</head><p>As discussed in Section 2.2.3, applying GPIC to parameterized policies, which yields PI 2 , has several consequences:</p><p>• In GPIC, actions are perturbed, whereas PI 2 is a parameter perturbing method. This represents the left-to-right stream in Figure <ref type="figure" target="#fig_7">8</ref>.</p><p>• Perturbations are no longer caused by the system, but rather generated by the PI 2 . Therefore, Σ is an open parameter, rather than determined by the system. In PI 2 , Σ = λR -1 , where the λ is the parameter that controls the magnitude of exploration, and R is the command cost matrix. Thus, the scalar λ determines the magnitude of the exploration.</p><p>• In SOC, it is assumed that the stochasticity is independent of time, and ǫ t i is therefore different for each time step t i . If ǫ t i would be constant over time, it would be a bias (e.g. a 1 degree offset due to a calibration error in a robot joint) rather than noise (e.g. stochasticity arising from noisy encoders). The PI 2 algorithm inherits this property through its derivation from SOC, and thus also has time-varying perturbations ǫ t i . Note a subtle difference between the two: in SOC a time-varying perturbation ǫ t is added to a time-varying command u t , whereas in PI 2 a time-varying perturbation ǫ t is added to a constant parameter vector θ.</p><p>An important result of this last point is that the stochasticity must, in principle, no longer be time-independent, as is the case when applying it to motor commands. In practice, time-varying exploration has several disadvantages, which have been pointed out in Section 2.1.3. In practical applications of PI 2 , the noise is therefore not varied at every time step <ref type="bibr" target="#b30">(Theodorou et al., 2010;</ref><ref type="bibr" target="#b29">Tamosiumaite et al., 2011;</ref><ref type="bibr">Stulp et al., 2012)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Three Proposed Methods for Parameter Perturbation</head><p>We refer to the 'canonical' version of PI 2 , which samples different exploration vectors ǫ t for each time step, as PI 2 . The small blue symbol serves as a mnemonic to indicate that exploration varies at a high frequency, as seen in the upper left graph of Figure <ref type="figure" target="#fig_8">9</ref>, which plot ǫ t against time t.</p><p>As an alternative to time-varying exploration, <ref type="bibr" target="#b30">Theodorou et al. (2010)</ref> propose to generate exploration noise only for the basis function with the highest activation. We refer to this second method as PI 2 with exploration per basis function, or PI 2 , where the green graphs serves as a mnemonic of the shape of the exploration for one basis function. The difference between PI 2 and PI 2 is visualized in the top row of Figure <ref type="figure" target="#fig_8">9</ref>, which depicts ǫ t over time for an exploration magnitude<ref type="foot" target="#foot_4">foot_4</ref> of λ = 0.05.</p><p>Alternatively, ǫ t i ,k can be set to have a constant value during a roll-out. Thus, for each of the K roll-outs, we generate ǫ k exploration vectors before executing the policy, and keep it constant during the execution, i.e. ǫ t i ,k = ǫ k . We call this 'PI 2 with constant exploration', and denote it as PI 2 , where the horizontal line indicates a constant value over time. Note that ǫ k will still have a temporally extended effect, because it is multiplied with a basis function that is active throughout an extended part of the movement, as depicted in the right graph, second row in Figure <ref type="figure" target="#fig_8">9</ref>.</p><p>Exploration in Parameter Space vs. Exploration in Policy Output Space The third row of Figure <ref type="figure" target="#fig_8">9</ref> depicts the cumulative activation of the (third) perturbed basis function: t s=0 g t ǫ t . For PI 2 (left), we see that, since consecutive positive/negative perturbations cancel each other out, the cumulative activation is quite low. For PI 2 (center), where there is no canceling out, the cumulative activation is much higher. For PI 2 (right) it is the highest, because the exploration vector ǫ t is never 0, and thus has the largest effect when multiplied with g t .</p><p>Since g t ǫ t directly determines the acceleration of the DMP output (29), higher cumulative activations lead to higher accelerations. This becomes apparent in the fourth row of Figure <ref type="figure" target="#fig_8">9</ref>, which depicts 50 roll-outs of the DMP, all sampled with an exploration magnitude of λ = 0.05. Upon visual inspection of the trajectories in the fourth row of Figure <ref type="figure" target="#fig_8">9</ref>, the variance for PI 2 is higher than for the other two. To quantify this effect, we perform K = 1000 roll-outs, and determine the standard deviation of the DMP output at time t as:</p><formula xml:id="formula_32">σ x t = 1 K-1 K k=1 x k t -xt 2 .</formula><p>The solid dark graphs in the final row in Figure <ref type="figure" target="#fig_8">9</ref> depict this standard deviation for the 1000 roll-outs for the three exploration methods.</p><p>If we set the exploration magnitude for PI 2 to a higher value of λ = 0.170, we see that the standard deviation (dashed blue line, lower left graph) in policy output space (σ develops the same as PI 2 for λ = 0.05. We use PI 2 with λ = 0.05 as a reference, because this is the exploration method and exploration magnitude used by <ref type="bibr" target="#b30">Theodorou et al. (2010)</ref>. For PI 2 , the story is the converse, and a lower exploration magnitude of λ = 0.025 is required to achieve the same exploration magnitude in policy output space, cf. the red dashed graph.</p><formula xml:id="formula_33">x t ) PI 2 PI 2 PI 2</formula><p>Summary: By setting the policy parameter exploration magnitude λ appropriately, we achieve essentially the same exploration in the policy output for PI 2 , PI 2 and PI 2 . The practical implications and relation to previous work <ref type="bibr" target="#b30">(Theodorou et al., 2010;</ref><ref type="bibr" target="#b29">Tamosiumaite et al., 2011;</ref><ref type="bibr" target="#b26">Stulp and Sigaud, 2012)</ref> of this is made clear in the empirical comparison that follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">Empirical Comparison</head><p>The experiments in this article are based on the same tasks as presented by <ref type="bibr" target="#b30">Theodorou et al. (2010)</ref>. The main advantage of using these tasks is that it allows for a direct comparison with the results reported by <ref type="bibr" target="#b30">Theodorou et al. (2010)</ref>. The tasks are described in Appendix A.</p><p>For each learning session, we are interested in comparing the convergence speed and final cost, i.e. the value to which the learning curve converges. Convergence speed is measured as the parameter update after which the cost drops below 5% of the initial cost before learning. The final cost is the mean cost over the last 100 updates. For all tasks and algorithm settings, we execute 10 learning sessions (which together we call an 'experiment'), and report the µ ± σ over these 10 learning sessions. For all experiments, the DMP and PI 2 parameters are the same as in <ref type="bibr" target="#b30">(Theodorou et al., 2010)</ref>, and listed in Appendix A.</p><p>Figure <ref type="figure" target="#fig_9">10</ref> summarizes the results of comparing the different exploration methods on the example Task 2; Figure <ref type="figure" target="#fig_10">11</ref> presents the results for the other tasks. The top graph represents the learning curves (µ ± σ) over 10 learning sessions, with exploration magnitude λ = 0.05 for all exploration methods.</p><p>The left graphs enables the comparison of convergence speed. To evaluate the convergence speed, we determine when each of the learning curves drops below 5% of the cost before learning. The means of these values for the three exploration methods are visualized as vertical lines in the left graph of Figure <ref type="figure" target="#fig_9">10</ref>. At the top of these lines, a horizontal bar represents the standard deviation. For convergence we see that PI 2 &lt;PI 2 &lt;PI 2 (p-value &lt; 0.001), i.e. constant exploration converges quickest.</p><p>The right graphs compare the final cost of each method, but depicts the average learning curve during the last 100 updates, after which all learning curves have converged. The vertical lines and horizontal bars in the right graphs visualize the µ ± σ of the final cost over the 10 learning sessions, where the final cost is defined as the mean over a learning curve during the last 100 updates. For the value and variance in the final cost, we see that PI 2 &lt;PI 2 &lt;PI 2 (p-value &lt; 0.001), i.e. this time PI 2 performs significantly better than the other two methods.</p><p>Generating exploration only for the basis function with the highest approximation (PI 2 ) thus provides a good trade-off between achieving fast convergence and a low final cost, which is why it has been independently recommended in different applications of PI 2 <ref type="bibr" target="#b30">(Theodorou et al., 2010;</ref><ref type="bibr" target="#b29">Tamosiumaite et al., 2011)</ref>. If convergence speed is the most important feature, we have argued that constant exploration is best <ref type="bibr" target="#b26">(Stulp and Sigaud, 2012)</ref>.</p><p>However, the second row of graphs shows that, when we normalize for the variance in the policy output by setting λ = 0.170/0.050/0.025 for PI 2 /PI 2 /PI 2 respectively, as discussed in Section 3.1.2, that the difference between the PI 2 variations do not differ signicantly (p-value &gt; 0.07 for all pairwise comparisons) and the mean and variance in the final cost is almost identical (p-value &gt; 0.67).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Decaying Exploration as Learning Progresses</head><p>The value and variance in final cost is still quite high when considering the top two rows in Figure <ref type="figure" target="#fig_9">10</ref>. A typical reason for high variance in the final cost is that the high exploration that was suitable at the beginning of learning prevents the algorithm from converging to the lowest possible cost at the end of learning. For this reason, exploration is often decayed exponentially as learning progresses <ref type="bibr" target="#b26">(Stulp and Sigaud, 2012)</ref>. The bottom graphs of Figure <ref type="figure" target="#fig_9">10</ref> depicts the results for the three exploration methods, with a decay factor of γ = 0.98, i.e. the exploration at update u is determined by Σ u = γ u λI. Here λ = {0.05, 0.170, 0.025} as above, for normalized exploration in policy output space. Again, differences in convergence speed are not significant (p-value &gt; 0.05), and final cost the final cost is 0 for all exploration methods. This is the minimal possible cost, corresponding to passing through the via-point perfectly.</p><p>Results on All Tasks Figure <ref type="figure" target="#fig_10">11</ref> summarizes the convergence speed and final cost for all five tasks described in the Appendix A, where the λ has been normalized to achieve the same variance in policy output, and with decaying exploration. For each task, all values have been normalized w.r.t. the value for PI 2 . For instance, for Task 2, the convergence below 5% of the initial cost in the bottom graph of Figure <ref type="figure" target="#fig_9">10</ref> was on average at updates 14.7, 13.7, and 13.0 for PI 2 ,PI 2 , and PI 2 . Normalized for PI 2 , this becomes 1.07, 1.00 and 0.95, as highlighted in Figure <ref type="figure" target="#fig_10">11</ref>. From this bar plot, we derive the following conclusions:</p><p>• The final costs do not differ much between the exploration methods. On average it is 2.2% higher than for PI 2 , with a maximum of 6.6% for Task 5. The differences are only significant for Task 1 &amp; 5 (p-value &lt; 0.05).</p><p>Same variance in parameter space PI 2 /PI 2 /PI 2 : λ = 0.05</p><p>Significantly different convergence speed and final cost between all methods (p-value &lt; 0.001 for all pair-wise comparisons) • The convergence speed varies by 10% between exploration methods, except for Task 1, where they differ by almost 20%. The convergence speed differs significantly between methods only for Task 1 (p-value &lt; 0.05). • Convergence speed is inversely related to final cost. That is, if a method has a faster convergence than the baseline, it will have a higher final cost. This represents the general trade-off between convergence speed and final cost. Note that this does not hold for Task 2 and 3, because the final cost goes to 0 for all methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.4">Conclusion for Exploration Methods</head><p>In conclusion, the faster convergence as observed with per-basis <ref type="bibr" target="#b30">(Theodorou et al., 2010;</ref><ref type="bibr" target="#b29">Tamosiumaite et al., 2011)</ref> or constant <ref type="bibr" target="#b26">(Stulp and Sigaud, 2012)</ref> exploration noise does not seem to be caused by intrinsic properties of the exploration method, but rather by the higher level of exploration they lead to in the output of the policy. When choosing the parameter exploration magnitude such that it leads to the same amount of exploration in task space, the exploration methods have much more similar convergence speed and final cost, and which is faster or slower depends on the task. For all methods, the advantages of high initial exploration for fast convergence and exploitation of the learned policy after learning may be achieved by decaying exploration over time.</p><p>PI 2 /PI 2 /PI 2 : λ = 0.170/0.050/0.025 λu = γ u λ, with γ = 0.98  </p><formula xml:id="formula_34">1</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Simplifying the Parameter Update</head><p>In this section, we simplify the parameter update rule of PI 2 which yields the simpler PI BB algorithm. We motivate why this simplication is valid within the PI 2 derivation, and empirically compare PI 2 and PI BB .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Temporal Averaging in the Context of the PI 2 Derivation</head><p>In PI 2 , a different parameter update δθ t i is computed for each time step i. This is caused by its derivation from GPIC, where motor commands u t are different at each time step; it is difficult to imagine a task that requires a constant motor command during a roll-out. But since the policy parameters θ are constant during a roll-out, there is a need to condense the N parameters updates δθ t i =0:N into one update δθ. This step is called temporal averaging, and was proposed by <ref type="bibr" target="#b30">Theodorou et al. (2010)</ref> as:</p><formula xml:id="formula_35">[δθ] d = N i=1 (N -i + 1) w d,t i [δθ t i ] d N i=1 w d,t i (N -i + 1) . (<label>39</label></formula><formula xml:id="formula_36">)</formula><p>temporal averaging scheme emphasizes updates earlier in the trajectory, and also makes use of the basis function weights w d,t i . However, since this does not directly follow from the derivation "[u]sers may develop other weighting schemes as more suitable to their needs." <ref type="bibr" target="#b30">(Theodorou et al., 2010)</ref>. As an alternative, we now choose a weight of 1 at the first time step, and 0 for all others. This means that all updates δθ t i are ignored, except the first one δθ t 1 , which is based on the cost-to-go at the first time step S(τ 1,k ). By definition, the cost-to-go at t 1 represents the cost of the entire trajectory. This implies that we must only compute the cost-to-go S(τ i,k ) and probability P (τ i,k ) for i = 1. This simplified PI 2 variant, which does not use temporal averaging, and which we denote 'PI BB ' is presented in more detail in Section 4.</p><p>Note that this simplification depends strongly on using constant exploration noise during a roll-out. If the noise varies at each time step or per basis function, the variation at the first time step ǫ t 1 ,k is not at all representative for the variations throughout the rest of the trajectory. It is therefore more accurate to consider PI BB as a variant of PI 2 , rather than of PI 2 ≡PI 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Empirical Comparison</head><p>We evaluate the effect of temporal averaging by comparing PI 2 (with constant exploration) and PI BB (which does not use temporal averaging and has constant exploration by default), which is also executed in 10 learning sessions which 1000 updates each. These learning curves (µ±σ) for both non-decaying (γ = 1.0) and decaying (γ = 0.98) exploration are depicted in Figure <ref type="figure" target="#fig_11">12</ref>. We see that PI BB converges almost twice as fast as PI 2 , and that both converge to a final cost of 0. Figure <ref type="figure" target="#fig_12">13</ref> repeats the convergence speed and final cost from Figure <ref type="figure" target="#fig_10">11</ref> for all tasks, but adds the values for PI BB for comparison. This bar plot reveals the following:</p><p>• PI BB achieves a substantially faster convergence speed than the other PI 2 variants methods. On average it is 53% of the convergence speed for PI 2 . This is significant for all pair-wise comparisons (p &lt; 0.01), except for the difference between PI 2 and PI BB for Task 1 (p &gt; 0.894). • PI BB achieves equivalent or better final costs than PI 2 . On average, the final cost is 6% lower than for the reference PI 2 . This is significant for all pair-wise comparisons (p &lt; 0.002), except for the difference between PI 2 and PI BB for Task 5 (p &gt; 0.150) and Task 2 and 3, where all methods converge to a final cost of 0.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Conclusion for Parameter Update</head><p>For the five tasks considered in this article (and thus those in <ref type="bibr" target="#b30">(Theodorou et al., 2010)</ref>, we see that PI BB achieves equal or better performance in terms of convergence speed and final cost than the other PI 2 variants, if we decay exploration over time.</p><p>4 The PI BB Algorithm</p><p>In Section 3, we have defined a variant of PI 2 called PI BB , in which: 1) Exploration noise is constant over time, i.e. as in PI 2 . 2) Temporal averaging uses only the first update, i.e.</p><p>δθ new = δθ new t 1 . We also refer to this simply as 'no temporal averaging'. As previously discussed in Section 3, these simplifications do not violate any of the assumptions made when deriving PI 2 from SOC. In this section, we perform a closer analysis of PI BB . Figure <ref type="figure" target="#fig_13">14</ref> lists both the PI 2 (left) and PI BB (center) algorithms. Since PI BB is a simplified version of PI 2 , we have visualized the simplifications as dark red areas, that indicate that these lines are dropped from the PI 2 algorithm. In Figure <ref type="figure" target="#fig_13">14</ref>, simplifications have been labeled: C1 -keep exploration constant; C2 -do not use temporal averaging; M -drop the projection matrix M from the parameter update. We now demonstrate that the PI BB algorithm is equivalent to applying a BBO algorithm to the policy parameters.</p><p>The effect of using constant exploration is that PI BB has only one remaining loop over time (to execute the policy), and that temporal averaging (the penultimate line of PI 2 ) disappears, cf. Figure <ref type="figure" target="#fig_13">14</ref>. As a consequence, determining the cost of a vector θ + ǫ k may now be interpreted as a black-box cost function, i.e. S k = J(θ + ǫ k ) with the perturbed policy parameters (which do not vary over time due to C1 ) as input, and the scalar trajectory cost S k as output (only the entire trajectory cost is needed due to C2 ). In PI BB , the cost function S k = J(θ + ǫ k ) thus does the following: 1) integrate and execute the policy with constant parameters (θ k + ǫ k ); 2) record the costs at each time step during the execution; 3) when the roll-out is done, sum over the costs, and return them as S k .</p><p>4.2 PI BB is a special case of CMA-ES Now, we show more specifically that PI BB is a special case of the CMA-ES algorithm.</p><p>We now describe several simplifications/specializations of the CMA-ES algorithm, listed in Section 2.3.2. Our intention is not to create a more efficient algorithm -we will remove some core functionality of CMA-ES in the process -but rather to highlight the relationship between PI BB and CMA-ES.</p><p>In CMA-ES, the weighting function may be chosen freely, as long as the conditions Ke k=1 P k = 1 and P 1 ≥ • • • ≥ P Ke hold. Since the exponentiation of the cost in PI 2 meets these conditions, we set K e = K, and the function that maps S k to P k to that of PI 2 . This step is labeled S1 in Figure <ref type="figure" target="#fig_13">14</ref>.</p><p>The next step is to disable the covariance matrix adaptation in CMA-ES, which is done as follows: S2 Set the initial step-size σ = 1 S3 Disable step-size updating, by setting the time horizon c σ = 0. This makes (20) collapse to σ new = σ × exp(0), which means the step-size stays equal over time. Since the initial step-size σ = 1, σ simply drops from all equations. S4 Disable covariance matrix updating, by setting c 1 = 0 and c µ = 0. The second and third terms of ( <ref type="formula">22</ref>) then drop, and what remains is (1 -0 -0)Σ. Therefore, the covariance matrix is not adapted, and remains constant during learning. Setting the parameters as listed above thus makes the entire covariance matrix adaptation drop<ref type="foot" target="#foot_5">foot_5</ref> .</p><p>Finally, CMA-ES can readily be applied to policy improvement, as is done in <ref type="bibr">(Heidrich-Meisner and Igel, 2008a)</ref>, by considering the cost S k to be the cost of the trajectory that arises when executing the policy, labeled S5 in Figure <ref type="figure" target="#fig_13">14</ref>.</p><p>Simplifications S1 -S5 are again visualized as red areas in Figure <ref type="figure" target="#fig_13">14</ref>. Interestingly, the algorithm that arises from applying S1 -S5 to CMA-ES is equivalent to PI BB . Note that CMA-ES was not modified in any way, we simply set certain open parameters of CMA-ES to specific values.</p><p>Summary: PI BB , which is a variant of PI 2 with constant exploration and without temporal averaging, is a BBO algorithm and in particular a special case of CMA-ES without covariance matrix updating.</p><p>It is important to recognize that PI BB is, compared to other state-of-the-art BBO algorithms, quite simplistic. Our reasons for introducing PI BB are: 1) To be able to compare two algorithms (PI 2 /PI BB ) that differ only in being RL or BBO methods, but are identical otherwise. 2) To demonstrate that a degenerate version of PI 2 (with constant exploration and without temporal averaging) is equivalent to a degenerate version of CMA-ES (without covariance matrix adaptation), and that PI 2 and CMA-ES thus share a common core. An obvious extension of our current work is to include other BBO algorithms in the empirical comparison. For instance, we would certainly expect CMA-ES to outperform its degenerate sibling PI BB on these tasks, and thus, by extension, also PI 2 . In fact, it is likely that a large amount of BBO algorithms are able to outperform PI BB on the tasks considered. In this article, we have refrained from including these BBO methods in our comparison, to be able to specifically focus on the difference that arises when keeping all algorithmic features the same, except being RL or BBO methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion</head><p>So why, on these five tasks, is PI 2 outperformed by the much simpler BBO algorithm PI BB ? It is rather counter-intuitive that an algorithm that uses less information is able to converge as fast or quicker than an algorithm that uses more information. This intuition is captured well in the following quote from <ref type="bibr" target="#b14">Moriarty et al. (1999)</ref> "In this sense, EA <ref type="bibr">[BBO]</ref> methods pay less attention to individual decisions than TD [RL] methods do. While at first glance, this approach appears to make less efficient use of information, it may in fact provide a robust path toward learning good policies."</p><p>In this discussion section, we first describe previous comparisons of RL and BBO algorithms, and then explain how our results extend the knowledge obtained in this previous work. In particular, we re-consider the trends in Figure <ref type="figure" target="#fig_0">1</ref> and Figure <ref type="figure" target="#fig_6">7</ref>. We also discuss how the results we have obtained are influenced by the choice of policy representation and tasks used by <ref type="bibr" target="#b30">Theodorou et al. (2010)</ref> and ourselves, which are tailored to the domain of learning skills on physical robots.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Previous Work on Empirically Comparing RL and BBO</head><p>The earliest empirical comparison of RL and BBO that we are aware of is the work of <ref type="bibr" target="#b14">Moriarty et al. (1999)</ref>. They compare "Evolutionary Algorithms for Reinforcement Learning" (EARL) with Q-learning on a simple MDP grid world, and conclude that these two methods "while complementary approaches, are by no means mutually exclusive." and that BBO approaches are advantageous "in situations where the sensors are inadequate to observe the true state of the world." <ref type="bibr" target="#b14">(Moriarty et al., 1999)</ref>.</p><p>Heidrich-Meisner and Igel (2008b) compare the performance of CMA-ES and NAC on a single pole balancing task. They conclude that "Our preliminary comparisons indicate that the CMA-ES is more robust w.r.t. to the choice of hyperparameters and initial policies. In terms of learning speed, the natural policy gradient ascent performs on par for fine-tuning and may be preferable in this scenario." This work was later extended to a double pole balancing task <ref type="bibr">(Heidrich-Meisner and Igel, 2008a)</ref>, where similar conclusions were drawn. A more extensive evaluation on single-and double pole balancing tasks is performed by <ref type="bibr" target="#b4">(Gomez et al., 2008)</ref>. They also conclude that "in real world control problems, neuroevolution [. . . ] can solve these problems much more reliably and efficiently than non-evolutionary reinforcement learning approaches".</p><p>In an extensive comparison, <ref type="bibr">Rückstiess et al. (2010b)</ref> compare PGPE, REINFORCE, NES, eNAC, NES and CMA-ES one pole-balancing, biped standing, object grasping, and ball catching. Their focus is particularly on comparing action-perturbing and parameterperturbing algorithms. Their main conclusion is that parameter-perturbation outperforms action-perturbation: "We believe that parameter-based exploration should play a more important role not only for PG methods but for continuous RL in general, and continuous value-based RL in particular" <ref type="bibr">(Rückstiess et al., 2010b</ref>).</p><p>An issue with such comparisons is that "each of these efforts typically only compares a few algorithms on a single problem, leading to contradictory results regarding the merits of different RL methods." <ref type="bibr" target="#b31">(Togelius et al., 2009)</ref>. It is this issue that we referred to in the introduction: if CMA-ES outperforms eNAC on a particular task, is it because of their different perturbation methods, their different parameter update methods, or because one is BBO and the other is RL? One of the main goals of this article is to provide an algorithmic framework that allows us to specifically investigate the latter question, whilst keeping the other algorithmic features the same. <ref type="bibr" target="#b9">Kalyanakrishnan and Stone (2011)</ref> aim at "characterizing reinforcement learning methods through parameterized learning problems". They compare Sarsa, ExpSarsa, Q-learning, CEM and CMA-ES on problems consisting of simple square grids with a finite number of states. One interesting conclusion is that they are able to partially corroborate several of the conjectures by <ref type="bibr" target="#b31">Togelius et al. (2009)</ref>. The main difference to previous work is that "our parameterized learning problem enables us to evaluate the effects of individual parameters while keeping others fixed." <ref type="bibr" target="#b9">(Kalyanakrishnan and Stone, 2011)</ref>. Our work is orthogonal to this, in that it provides a pair of algorithms in which experimenters may switch between BBO and RL, whilst keeping other algorithmic features fixed. We thus focus on 'parameterizable algorithms', rather than parameterizable learning problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Reconsidering the Observed Trends</head><p>We now reconsider and evaluate the trends in Figure <ref type="figure" target="#fig_6">7</ref>, given the empirical results presented in this article.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">From Gradient-based Methods to Reward-weighted averaging</head><p>We believe the trend from gradient-based methods to reward-weighted averaging to have been an important step in enabling policy improvement methods to become robust towards noisy, discontinuous cost functions. From a theoretical perspective, we find it striking that reward-weighted averaging may be derived from fundamental principles in a wide variety of domains: reinforcement learning <ref type="bibr" target="#b11">(Kober and Peters, 2011)</ref>, stochastic optimal con-trol <ref type="bibr" target="#b30">(Theodorou et al., 2010)</ref>, rare-event theory <ref type="bibr" target="#b21">(Rubinstein and Kroese, 2004)</ref>, and a basic set of optimality principles in BBO <ref type="bibr" target="#b0">(Arnold et al., 2011)</ref>. In practice, two algorithms that use this principle in BBO (CMA-ES) and RL (PI 2 ) turn out to be state-of-the-art in terms of empirical performance.</p><p>Previous work has focussed on comparing BBO methods that use reward-weighted averaging with gradient-based RL methods <ref type="bibr">(Heidrich-Meisner and Igel, 2008a,b;</ref><ref type="bibr">Rückstiess et al., 2010b)</ref>. This, however, is a rather unfair comparison, as RL methods based on rewardweighted averaging -such as POWER and PI 2 -have been shown to substantially outperform gradient-based RL methods <ref type="bibr" target="#b11">(Kober and Peters, 2011;</ref><ref type="bibr" target="#b30">Theodorou et al., 2010)</ref>. The reason such comparisons have not yet been made is that POWER and PI 2 have only been introduced recently. To the best of our knowledge, this work is the first in comparing RL and BBO algorithms that are both based on reward-weighted averaging. Our conclusion is that BBO (PI BB ) is still able to outperform reward-weighted averaging RL (PI 2 ) on the tasks considered, but the margin is much smaller than when comparing BBO with gradient-based RL (e.g. eNAC). We expect this margin to increase again when using more sophisticated BBO algorithms, such as CMA-ES, than PI BB . This is part of our current work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">From Action Perturbation to Parameter Perturbation</head><p>Going from action perturbation to parameter perturbation seems to have been a fruitful trend, as confirmed by <ref type="bibr">Rückstiess et al. (2010b)</ref>. Mapping action perturbations to parameter updates requires a mapping from action space to parameter space, and requires knowledge of the derivative of the policy. In contrast, parameter perturbing methods perform exploration in the same space as in which the parameter update takes place. Thus, the controlled variable that leads to variations in the cost is also directly the variable that will be updated. Empirically, algorithms based on parameter perturbation substantially outperform those based on action perturbation <ref type="bibr">(Rückstiess et al., 2010b;</ref><ref type="bibr">Heidrich-Meisner and Igel, 2008a;</ref><ref type="bibr" target="#b30">Theodorou et al., 2010)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.3">From Rewards at Each Time Step to Aggregrated Costs</head><p>Although PI BB , which uses only a scalar aggregrated cost, outperforms PI 2 , which uses the costs at each time step, on the tasks presented by <ref type="bibr" target="#b30">Theodorou et al. (2010)</ref> and used in this article, we do not hold this to be a general result. We believe the cause must lie in the chosen tasks themselves, or the particular policy representation we have chosen. These tasks and representations in their turn are biased by our particular interest in applying policy improvement to acquire robotic skills. In fact, we have (informally) compared PI BB and PI 2 on several other robotic tasks not reported here, and have not found one instance where PI 2 outperforms PI BB . Thus, understanding why BBO outperforms RL on these types of tasks may be related to understanding if and how the properties of typical tasks and policy representations used in robotic skill learning make them particularly amenable to BBO. <ref type="bibr" target="#b9">Kalyanakrishnan and Stone (2011)</ref>: "[T]he relationships between problem instances and the performance properties of algorithms are unclear, it becomes a worthwhile pursuit to uncover them". The results presented in this article are a first step in the pursuit to uncover the relationship between typical robotic tasks and the performance properties of BBO/RL. This topic, of particular interest to roboticists, is at the center of our current investigations. <ref type="bibr" target="#b31">Togelius et al. (2009)</ref> Our experiments, as do those of <ref type="bibr" target="#b9">(Kalyanakrishnan and Stone, 2011)</ref>, corroborate several of the conjectures by <ref type="bibr" target="#b31">Togelius et al. (2009)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Relation to the Conjectures by</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Continuous state and actions "[BBO] method [. . . ] generally outperform [RL] methods</head><p>on problems with continuous state spaces". We have studied only continuous state and action spaces, and BBO has equal or better performance than RL, which is a corroboration of this conjecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Intermediate rewards Since [RL] methods, unlike [BBO] method</head><p>, can use all experiential information obtained during interaction with the environment, [RL] methods outperform <ref type="bibr">[BBO]</ref> algorithms in applications where it is helpful to exploit intermediate rewards . . . [quote continued below]" <ref type="bibr" target="#b31">(Togelius et al., 2009)</ref>. For the tasks in <ref type="bibr" target="#b30">(Theodorou et al., 2010)</ref>, exploiting intermediate rewards hardly plays a role. Therefore, we have added an extra task in which three via-points must be passed through. However, for this task, BBO (PI BB ) also converges much faster than RL (PI 2 ), and to the same final cost (cf. Figure <ref type="figure" target="#fig_12">13</ref>). We cannot corroborate this conjecture.</p><p>Short, episodic tasks [Quote continued from above] . . . especially if episodes are long." Our research interest is learning skills for robots, with a particular focus on manipulation <ref type="bibr">(Stulp et al., 2012;</ref><ref type="bibr" target="#b13">Marin et al., 2011)</ref>. Typical robotic skills for manipulation -reaching for an object, transporting it to another location -typically do not take longer than about a second. This also holds for the simulated tasks described in Appendix A. For such short tasks, it is less likely that intermediate rewards play an important role.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>Based on four algorithmic properties, we have provided a classification of policy improvement algorithms. By defining these properties concisely and clearly, one of the results of this article is to distinguish between RL and BBO based on these properties alone, rather than specific algorithms. For instance, we argued that action perturbing methods cannot use a BBO update rule. Furthermore, algorithms that require information about states/actions in the trajectories arising from policy roll-outs, such as actor-critic methods, cannot be BBO algorithms. Also, although finite-differencing methods (FD) are often to be considered an RL approach to policy improvement, it must be acknowledged that the algorithm may also be interpreted as being a BBO algorithm, given the definition of a BBO problem.</p><p>A second result is that, within this classification, we observe three trends in the chronology and derivation paths of algorithms: from gradient-based methods to reward-weighted averaging, from action to parameter perturbation, and towards algorithms that use only reward information from policy roll-outs.</p><p>We have continued this trend by applying two simplifications to the PI 2 algorithm: 1) keep exploratory parameter perturbations constant during a roll-out; 2) eliminate temporal averaging by considering only the entire cost of the trajectory, rather than the cost-to-go at each time step. This leads to a novel, much simpler algorithm, called PI BB . We show that PI BB is a BBO algorithm, and a specific degenerate case of CMA-ES.</p><p>In previous work, it was shown that PI 2 is able to outperform PEGASUS, REIN-FORCE, and eNAC and POWER <ref type="bibr" target="#b30">(Theodorou et al., 2010)</ref>. Using exactly the same tasks, we observe rather surprisingly that the much simpler BBO algorithm PI BB has equal or better performance than PI 2 still. Previous work on comparing RL and BBO shows that BBO often wins by a wide margin; the caveat being that, in those experiments, the BBO methods use reward-weighted averaging whereas the RL methods use gradient estimation. An important conclusion of our results is that the margin, though still existent, is much smaller when both RL and BBO are based on the powerful concept of reward-weighted averaging.</p><p>Although BBO thus trumps RL on several tasks, we do not believe this to be a general result, and further investigations are needed, especially into the bias that typical tasks and policy representations used in robotics -the types used in this article -introduce into RL problems.</p><p>Rather than making the case for BBO or RL, one of the main contributions of this article is to provide an algorithmic framework in which such cases may be made. Because PI BB and PI 2 use identical perturbation and parameter update methods, and differ only in being BBO and RL approaches respectively, this allows for a more objective comparison of BBO and RL than for instance comparing algorithms that differ in many respects. Therefore, we believe this algorithmic pair is an excellent basis for comparing BBO and RL approaches to policy improvement, and further investigating the five conjectures in <ref type="bibr" target="#b31">(Togelius et al., 2009)</ref>.</p><p>A.4 Task 4 &amp; 5 <ref type="bibr" target="#b30">Theodorou et al. (2010)</ref> used this task to evaluate the scalability of PI 2 to high-dimensional action spaces and learning problems with high redundancy. Here, an 'arm' with D rotational joints and D links of length 1 D is kinematically simulated in 2D Cartesian space. Figure <ref type="figure" target="#fig_6">17</ref> visualizes the movement by showing the configuration of the arm at each time step. The goal is again to pass through a viapoint (0.5,0.5) , this time in end-effector space, whilst minimizing accelerations. The D joint trajectories are initialized with a minimum-jerk trajectory, and then optimized with respect to the following cost function:</p><formula xml:id="formula_37">r t = D i=1 (D + 1 -i)(0.1f 2 i,t + 0.5θ ⊺ i θ i ) D j=1 (D + 1 -j)<label>(42)</label></formula><p>r 300ms = 10 8 ((0.5x t 300ms ) 2 + (0.5y t 300ms ) 2 ) (43)</p><formula xml:id="formula_38">φ t N = 0<label>(44)</label></formula><p>The weighting term (D + 1i) places more weight on proximal joints than distal ones, which is motivated by the fact that proximal joints have lower mass and therefore less inertia, and are therefore more efficient to move <ref type="bibr" target="#b30">(Theodorou et al., 2010)</ref>. Figure <ref type="figure" target="#fig_6">17</ref> depicts the movements before and after learning for arms with D = 2 and D = 10 links respectively.</p><p>Figure <ref type="figure" target="#fig_6">17</ref>: Task 4 (left, 2-DOF) and 5 (right, 10-DOF): minimizing the distance to a viapoint in end-effector space whilst minimizing joint accelerations.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure1: Classification of policy improvement algorithms. The vertical dimension categorizes the update method used, and the horizontal dimension the method used to perturb the policy. The two streams represent both the derivation history of policy improvement algorithms. The algorithms are discussed in Section 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure2: Generic policy improvement loop. In each iteration, the policy is executed K times. One execution of a policy is called a 'Monte Carlo roll-out', or simply 'roll-out'. Because the policy is perturbed (different perturbation methods are described in Section 2.1.3), each execution leads to slightly different trajectories in state/action space, and potentially different rewards. The exploration phase thus leads to a set of different trajectories τ k=1...K . Based on these trajectories, policy improvement methods then update the parameter vector θ → θ new such that the policy is expected to incur lower costs/higher rewards. The process then continues with the new θ new as the basis for exploration.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Actor-critic (above) and direct policy search (below).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Figure5: Left: Parameter perturbation at each time step, inside the policy execution loop (repeated from Figure4). Right: Since policy parameters are perturbed outside the policy execution loop in BBO, they cannot vary over time.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Figure6: Illustration of the different types of information that may be stored in the trajectories that arise from policy roll-outs. Algorithms that use only the aggregate scalar reward are considered to be BBO approaches to policy improvement.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>..K ← sort ǫ k=1...K w.r.t J k=1...K 9 foreach k in K do 10 P k = ln (max (K/2, Ke) + 0.5))ln(k) if k ≤ Ke 0 if k &gt; Ke 11Update mean: Reward-weighted averaging over K trials12 δθ = K k=1 [P k ǫ k ] 13Update covariance matrix: Reward-weighted averaging 14 Σ tmp = Ke k=1 P k ǫ k ǫ ⊺</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Classification of policy improvement algorithms, given their fact sheets. The two arrows represent chronology and order of derivation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Simplifications to PI 2 to derive PI BB , a BBO algorithm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Visualization of the three different forms of exploration, using one roll-out of a 1-D DMP of duration 0.5s. The DMP has 10 basis functions, of which the third is highlighted. Top row: exploration vector ǫt over time. Second row: The 10 basis functions activations gt (light grey and highlighted dashed), and the exploration vector multiplied with the basis function activation g ⊺ t ǫt (solid). Third row: Cumulative activation of the third (highlighted) basis functions. Fourth row: Output of the DMP xt for 50 roll-outs. Final row: Standard deviation in the policy output during the movement, averaged over 1000 roll-outs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 10 :</head><label>10</label><figDesc>Figure10: Learning curves for the three different methods of generating exploration (PI 2 , PI 2 and PI 2 ) for Task 2. The three rows represent different parameter settings of the learning algorithm. The left graphs, which highlight differences in convergence speed, shows the learning curves (µ ± σ over 10 separate learning sessions) during the first 32 updates, which corresponds to 480 trials. The right graphs highlight the cost after convergence, and depicts the learning curves (only µ for clarity) between updates 900 and 1000. The y-axis is zoomed ×2000 in comparison to the left graph. Annotations are described in the text.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: Summary of the results on all five experiments with normalized and decaying exploration.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 12 :</head><label>12</label><figDesc>Figure 12: As Figure 10, but for PI 2 (repeated in red) and PI BB (cyan) only.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 13 :</head><label>13</label><figDesc>Figure 13: Summary of the results on all five experiments, including PI BB .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 14 :</head><label>14</label><figDesc>Figure 14: Comparison of PI 2 (left), PI BB (center) and CMA-ES (right)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>r t N</figDesc><table><row><cell>states/actions/rewards</cell><cell>only rewards</cell></row></table><note><p><p><p>N</p>J</p>aggregrate scalar reward</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>With this notation, the policy π θ (x) is actually deterministic. A truly stochastic policy is denoted as ut ∼ π θ (u|x) = µ(x)+ǫt(Riedmiller et al.,  </p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1"><p>2007), where µ(x) is a deterministic policy that returns the nominal command. We use our notation for consistency with parameter perturbation, introduced in Section 2.1.3. For now, it is best to consider the sum π θ (x) + ǫt to be the stochastic policy, rather than just π θ (x).</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_2"><p>The point-wise estimates are sometimes considered to be a special type of critic; in this article we use the term 'critic' only when it is a function approximator.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_3"><p>'Vanilla' refers to the canonical version of an entity. The origin of this expression lies in ice cream flavors; i.e. 'plain vanilla' vs. 'non-plain' flavors, such as strawberry, chocolate, etc.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4"><p>In this section, we choose R = IB, where B is the number of basis functions, such that Σ = λI -1 = λI. This is convenient, because the magnitude of exploration in parameter space is determined solely by the scalar λ. Higher λ thus means more exploration in parameter space, and therefore more variance in the output trajectories of the DMP.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5"><p>In this article, we show that removing covariance matrix adaptation from CMA-ES reduces it to PI BB : "PI BB = CMA-ES minus CMA". In an orthogonal line of research<ref type="bibr" target="#b26">(Stulp and Sigaud, 2012)</ref>, we demonstrated the advantages of adding CMA-ES-style covariance matrix updating to PI 2 , which yields the PI 2 -CMAES algorithm: "PI 2 -CMAES = PI 2 plus CMA". For a discussion of the advantages of adding covariance matrix updating in RL and BBO, we refer to<ref type="bibr" target="#b26">(Stulp and Sigaud, 2012)</ref>.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>We would like to thank <rs type="person">Mrinal Kalakrishnan</rs>, <rs type="person">Jonas Buchli</rs>, <rs type="person">Nikolaus Hansen</rs> and <rs type="person">Balázs Kégl</rs> for fruitful discussions and suggestions for improvement. A special thanks to <rs type="person">Matthieu Geist</rs> for proofreading an earlier version of the article. We thank <rs type="person">Stefan Schaal</rs> for providing the source code for running the experiments and tasks in <ref type="bibr" target="#b30">(Theodorou et al., 2010)</ref>. This work is supported by the <rs type="funder">French ANR</rs> program <rs type="projectName">MACSi</rs> (<rs type="grantNumber">ANR 2010 BLAN 0216 01</rs>), <ref type="url" target="http://macsi.isir.upmc.fr">http://macsi.isir.upmc.fr</ref> </p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_KRsUa3T">
					<idno type="grant-number">ANR 2010 BLAN 0216 01</idno>
					<orgName type="project" subtype="full">MACSi</orgName>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Evaluation Tasks</head><p>In this section, we describe the tasks used for the empirical evaluations in Section 3.1.3 and 3.2.2. These tasks are taken from the article by <ref type="bibr" target="#b30">Theodorou et al. (2010)</ref>. The implementations are based on the same source code as in <ref type="bibr" target="#b30">Theodorou et al. (2010)</ref>, and all tasks and algorithms parameters are the same unless stated otherwise. This allows for a direct comparison of the results in this article and those acquired by <ref type="bibr" target="#b30">Theodorou et al. (2010)</ref>. Due to the similarity, this appendix is very similar to Section 5 of <ref type="bibr" target="#b30">(Theodorou et al., 2010)</ref>, and added for completeness only.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 DMP and PI 2 Parameterization</head><p>In all the tasks below, the DMPs have 10 basis functions per dimension, and a duration of 0.5s. During learning, K = 15 roll-outs are performed for one update. Although 10 roll-outs has usually proven to be sufficient, <ref type="bibr" target="#b30">Theodorou et al. (2010)</ref> choose 15 roll-outs to allow comparison with eNAC, which requires at least 1 roll-out more than the number of basis functions to perform its matrix inversion without numerical instabilities. The initial exploration magnitude is λ = 0.05 for all tasks except Task 1, where it is λ = 0.01. The exploration decay, which was tuned separately for each task, is 0.98, 0.98, 0.99, 0.99, 0.999 for Task 1. . . 5 respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Task 1</head><p>This task considers a 1-dimensional DMP of duration 0.5s, which starts at x 0 = 0 and ends at the goal g = 1. In this task as in all others, the initial movement is acquired by training the DMP with a minimum-jerk movement. The aim of Task 1 is to reach the goal g with high accuracy, whilst minimizing acceleration, which is expressed with the following immediate (r t ) and terminal (φ t N ) costs:</p><p>where f t refers to the linear spring-damper system in the DMP, cf. (29). Figure <ref type="figure">15</ref> visualizes the movement before and after learning.</p><p>Figure <ref type="figure">15</ref>: Task 1: Reaching the goal accurately whilst minimizing accelerations before (light green) and after (black) learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Task 2 &amp; 3</head><p>In Task 2, the aim is for the output of the 1-dimensional DMP (same parameters as in Task 1) to pass through the viapoint 0.25 at time t = 300ms. Which is expressed with the costs:</p><p>The costs are thus 0 at each time step except at t 300ms . This cost function was chosen by <ref type="bibr" target="#b30">Theodorou et al. (2010)</ref> to allow for the design of a compatible function for POWER.</p><p>Task 3 is equivalent except that it uses 3 viapoints [0.5 -0.5 1.0] at times [100ms 200ms 300ms] respectively. Figure <ref type="figure">16</ref> visualizes the movement before and after learning for Task 2 and Task 3. Note that Task 3 was not evaluated by <ref type="bibr" target="#b30">Theodorou et al. (2010)</ref>. We have included it as we expected that it is a task where it may be "helpful to exploit intermediate rewards" <ref type="bibr" target="#b31">(Togelius et al., 2009)</ref>, and where RL approaches are conjectured to outperform BBO <ref type="bibr" target="#b31">(Togelius et al., 2009)</ref>. As Figure <ref type="figure">13</ref> reveals, this is not the case for this particular task, and PI BB also outperforms PI 2 for this task.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Information-geometric optimization algorithms: A unifying picture via invariance principles</title>
		<author>
			<persName><forename type="first">L</forename><surname>Arnold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Auger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ollivier</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
		<respStmt>
			<orgName>INRIA Saclay</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning variable impedance control</title>
		<author>
			<persName><forename type="first">J</forename><surname>Buchli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Stulp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Theodorou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Schaal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Robotics Research</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="820" to="833" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Cross-entropy optimization of control policies with adaptive basis functions</title>
		<author>
			<persName><forename type="first">L</forename><surname>Busoniu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ernst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">D</forename><surname>Schutter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Babuska</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Systems, Man, and Cybernetics-Part B: Cybernetics</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="196" to="209" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Monte-carlo swarm policy search</title>
		<author>
			<persName><forename type="first">J</forename><surname>Fix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Geist</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Symposium on Swarm Intelligence and Differential Evolution, Lecture Notes in Artificial Intelligence (LNAI), page 9 pages</title>
		<meeting><address><addrLine>Zakopane</addrLine></address></meeting>
		<imprint>
			<publisher>Springer Verlag -Heidelberg Berlin</publisher>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Accelerated neural evolution through cooperatively coevolved synapses</title>
		<author>
			<persName><forename type="first">F</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Miikkulainen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="937" to="965" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Completely derandomized self-adaptation in evolution strategies</title>
		<author>
			<persName><forename type="first">N</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ostermeier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Evolutionary Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="159" to="195" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Evolution strategies for direct policy search</title>
		<author>
			<persName><forename type="first">V</forename><surname>Heidrich-Meisner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Igel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th international conference on Parallel Problem Solving from Nature: PPSN X</title>
		<meeting>the 10th international conference on Parallel Problem Solving from Nature: PPSN X<address><addrLine>Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="428" to="437" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Similarities and differences between policy gradient methods and evolution strategies</title>
		<author>
			<persName><forename type="first">V</forename><surname>Heidrich-Meisner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Igel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ESANN 2008, 16th European Symposium on Artificial Neural Networks</title>
		<meeting><address><addrLine>Bruges, Belgium</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">April 23-25, 2008. 2008b.</date>
			<biblScope unit="page" from="149" to="154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Movement imitation with nonlinear dynamical systems in humanoid robots</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Ijspeert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Nakanishi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Schaal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Robotics and Automation (ICRA)</title>
		<meeting>the IEEE International Conference on Robotics and Automation (ICRA)</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Characterizing reinforcement learning methods through parameterized learning problems</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kalyanakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Stone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">84</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="205" to="247" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Path integrals and symmetry breaking for optimal control theory</title>
		<author>
			<persName><forename type="first">H</forename><surname>Kappen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">P11011</title>
		<imprint>
			<date type="published" when="2005">2005. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Policy search for motor primitives in robotics</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kober</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Peters</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">84</biblScope>
			<biblScope unit="page" from="171" to="203" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Towards fast and adaptive optimal control policies for robots: A direct policy search approach</title>
		<author>
			<persName><forename type="first">D</forename><surname>Marin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Sigaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings Robotica</title>
		<meeting>Robotica<address><addrLine>Guimaraes, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="21" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning cost-efficient control policies with XCSF: Generalization capabilities and further improvement</title>
		<author>
			<persName><forename type="first">D</forename><surname>Marin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Decock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Rigoux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Sigaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th annual conference on Genetic and evolutionary computation (GECCO&apos;11)</title>
		<meeting>the 13th annual conference on Genetic and evolutionary computation (GECCO&apos;11)</meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1235" to="1242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Evolutionary algorithms for reinforcement learning</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Moriarty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Schultz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Grefenstette</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence (JAIR)</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="241" to="276" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Pegasus: A policy search method for large mdps and pomdps</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th Conference on Uncertainty in Artificial Intelligence</title>
		<meeting>the 16th Conference on Uncertainty in Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="406" to="415" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Applying the episodic natural actor-critic architecture to motor primitive learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Schaal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th European Symposium on Artificial Neural Networks (ESANN 2007)</title>
		<meeting>the 15th European Symposium on Artificial Neural Networks (ESANN 2007)</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Reinforcement learning of motor skills with policy gradients</title>
		<author>
			<persName><forename type="first">J</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Schaal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural networks : the official journal of the International Neural Network Society</title>
		<idno type="ISSN">0893-6080</idno>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="682" to="697" />
			<date type="published" when="2008-05">May 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Natural actor-critic</title>
		<author>
			<persName><forename type="first">J</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Schaal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="issue">7-9</biblScope>
			<biblScope unit="page" from="1180" to="1190" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Approximate Dynamic Programming: Solving the curses of dimensionality</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">B</forename><surname>Powell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
			<publisher>Wiley-Blackwell</publisher>
			<biblScope unit="volume">703</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Evaluation of Policy Gradient Methods and Variants on the Cart-Pole Benchmark</title>
		<author>
			<persName><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Schaal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2007 IEEE International Symposium on Approximate Dynamic Programming and Reinforcement Learning</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007-04">Apr. 2007</date>
			<biblScope unit="page" from="254" to="261" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">The Cross-Entropy Method: A Unified Approach to Combinatorial Optimization, Monte-Carlo Simulation, and Machine Learning</title>
		<author>
			<persName><forename type="first">R</forename><surname>Rubinstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kroese</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
			<publisher>Springer-Verlag</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">State-dependent exploration for policy gradient methods</title>
		<author>
			<persName><forename type="first">T</forename><surname>Rückstiess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Felder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">19th European Conference on Machine Learning (ECML)</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
	<note>a</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Exploring parameter space in reinforcement learning</title>
		<author>
			<persName><forename type="first">T</forename><surname>Rückstiess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Sehnke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Paladyn. Journal of Behavioral Robotics</title>
		<idno type="ISSN">2080-9778</idno>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="14" to="24" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Experiments with reinforcement learning in problems with continuous state and action spaces</title>
		<author>
			<persName><forename type="first">J</forename><surname>Santamaría</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ram</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adaptive behavior</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="163" to="217" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Optimal Control and Estimation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Stengel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994">1994</date>
			<publisher>Dover Publications</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Path integral policy improvement with covariance matrix adaptation</title>
		<author>
			<persName><forename type="first">F</forename><surname>Stulp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Sigaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th International Conference on Machine Learning (ICML)</title>
		<meeting>the 29th International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Reinforcement learning with sequences of motion primitives for robust manipulation</title>
		<author>
			<persName><forename type="first">F</forename><surname>Stulp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Theodorou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Schaal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Robotics</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note>Accepted for publication</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Reinforcement Learning: an Introduction</title>
		<author>
			<persName><forename type="first">R</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Barto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning to pour with a robot arm combining goal and shape learning for dynamic movement primitives</title>
		<author>
			<persName><forename type="first">M</forename><surname>Tamosiumaite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Nemec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ude</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wörgötter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Robots and Autonomous Systems</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="910" to="922" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A generalized path integral control approach to reinforcement learning</title>
		<author>
			<persName><forename type="first">E</forename><surname>Theodorou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Buchli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Schaal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="3137" to="3181" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Ontogenetic and phylogenetic reinforcement learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Togelius</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Igel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Zeitschrift Künstliche Intelligenz -Special Issue on Reinforcement Learning</title>
		<imprint>
			<biblScope unit="page" from="30" to="33" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Evolutionary function approximation for reinforcement learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Whiteson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Stone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="877" to="917" />
			<date type="published" when="2006-05">May 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Natural evolution strategies</title>
		<author>
			<persName><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Congress on Evolutionary Computation (CEC)</title>
		<meeting>IEEE Congress on Evolutionary Computation (CEC)</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Simple statistical gradient-following algorithms for connectionist reinforcement learning</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="229" to="256" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
