<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">InkSight: Leveraging Sketch Interaction for Documenting Chart Findings in Computational Notebooks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2023-07-16">16 Jul 2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yanna</forename><surname>Lin</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Science and Technology</orgName>
								<orgName type="institution" key="instit2">Harvard University</orgName>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Science and Technology</orgName>
								<orgName type="institution" key="instit2">Harvard University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Haotian</forename><surname>Li</surname></persName>
							<email>haotian.li@connect.ust.hk</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Science and Technology</orgName>
								<orgName type="institution" key="instit2">Harvard University</orgName>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Science and Technology</orgName>
								<orgName type="institution" key="instit2">Harvard University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Leni</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Science and Technology</orgName>
								<orgName type="institution" key="instit2">Harvard University</orgName>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Science and Technology</orgName>
								<orgName type="institution" key="instit2">Harvard University</orgName>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Science and Technology</orgName>
								<orgName type="institution" key="instit2">Harvard University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Aoyu</forename><surname>Wu</surname></persName>
							<email>aoyuwu@seas.harvard.edu</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Science and Technology</orgName>
								<orgName type="institution" key="instit2">Harvard University</orgName>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Science and Technology</orgName>
								<orgName type="institution" key="instit2">Harvard University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Huamin</forename><surname>Qu</surname></persName>
							<email>huamin@cse.ust.hk</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Science and Technology</orgName>
								<orgName type="institution" key="instit2">Harvard University</orgName>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Science and Technology</orgName>
								<orgName type="institution" key="instit2">Harvard University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hong</forename><surname>Kong</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Science and Technology</orgName>
								<orgName type="institution" key="instit2">Harvard University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">InkSight: Leveraging Sketch Interaction for Documenting Chart Findings in Computational Notebooks</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2023-07-16">16 Jul 2023</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2307.07922v1[cs.HC]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-29T00:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Computational Notebook</term>
					<term>Sketch-based Interaction</term>
					<term>Documentation</term>
					<term>Visualization</term>
					<term>Exploratory Data Analysis</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Fig. 1: This figure illustrates the process of using InkSight to document findings in a computational notebook. (A) displays the chart created by the user for data analysis. (B) demonstrates that when the user sketches atop the chart to identify areas of interest, InkSight automatically generates corresponding documentation on the right. (C) reveals how users can add more sketches and refine the documentations by performing interactions such as deleting, ordering, grouping, and editing.</p><p>Abstract-Computational notebooks have become increasingly popular for exploratory data analysis due to their ability to support data exploration and explanation within a single document. Effective documentation for explaining chart findings during the exploration process is essential as it helps recall and share data analysis. However, documenting chart findings remains a challenge due to its time-consuming and tedious nature. While existing automatic methods alleviate some of the burden on users, they often fail to cater to users' specific interests. In response to these limitations, we present InkSight, a mixed-initiative computational notebook plugin that generates finding documentation based on the user's intent. InkSight allows users to express their intent in specific data subsets through sketching atop visualizations intuitively. To facilitate this, we designed two types of sketches, i.e., open-path and closed-path sketch. Upon receiving a user's sketch, InkSight identifies the sketch type and corresponding selected data items. Subsequently, it filters data fact types based on the sketch and selected data items before employing existing automatic data fact recommendation algorithms to infer data facts. Using large language models (GPT-3.5), InkSight converts data facts into effective natural language documentation. Users can conveniently fine-tune the generated documentation within InkSight. A user study with 12 participants demonstrated the usability and effectiveness of InkSight in expressing user intent and facilitating chart finding documentation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>tedious process, causing some data scientists to disregard it for fear of interrupting their analysis flow <ref type="bibr" target="#b35">[36]</ref>. Previous research found that data analysts strongly desire assistance in documenting analysis results <ref type="bibr" target="#b26">[28]</ref>.</p><p>Recently, documenting findings in computational notebooks has drawn the attention of researchers. Wang et al. <ref type="bibr" target="#b45">[46]</ref> conducted interview studies and found that automatic methods are needed to reduce the burden of documentation. They further developed Themisto, which facilitates documentation in a mix-initiative way. Though it suggests code documentation automatically, it only provides a start of a sentence as a prompt-based approach to encourage users to document findings, leaving users to complete it on their own. To alleviate users' burden, Notable <ref type="bibr" target="#b26">[28]</ref> generates documentation of chart findings automatically by adopting a data fact recommendation algorithm. However, it fails to allow users to specify interest in specific data subsets, forcing users to manually document their findings from scratch when the automated documentation deviates from their focus. To fill this gap, our work aims to provide a tool that allows users to specify their intent for automatically documenting chart analysis results in computational notebooks.</p><p>The documentation process should have little workload and should be well integrated into the exploration process. With this goal in mind, we apply a sketch-based interaction for users to indicate data items of their interest. We decided upon this design mainly for two reasons:</p><p>(1) It has been found that data analysts prefer sketching over keyboard input to maintain their mental flow while externalizing their thoughts during data analysis <ref type="bibr" target="#b19">[21]</ref>. <ref type="bibr" target="#b0">(2)</ref> Sketching allows for a more accurate and flexible representation of users' intentions; for instance, it allows analysts to draw any shape to indicate their areas of interest within a chart and captures the nuances of users' drawings.</p><p>We present InkSight, a mixed-initiative and on-the-fly computational notebook plugin. The tool supports users to indicate data items of their interests through easy sketching on charts for document findings. As shown in Figure <ref type="figure">1</ref>, when users create a chart for data analysis (Figure <ref type="figure">1 (A)</ref>), they can sketch their area of interest directly atop the chart, prompting InkSight to automatically generate the documentation for the related data facts (Figure <ref type="figure">1 (B)</ref>). The tool allows users to add more intent and refine the generated documentation through editing, deleting, grouping, and ordering (Figure <ref type="figure">1</ref> (C)). To facilitate the intuitive expression of user intent, we designed two types of sketches: open-path and closed-path sketches. Upon receiving a user's sketch, InkSight aims to infer the user's intent and generate corresponding documentation. Specifically, InkSight first identifies the sketch type and corresponding selected data items. Next, InkSight filters data fact types based on data types, the number of selected data items, and the sketch type. InkSight then employs existing automatic data fact recommendation algorithms <ref type="bibr" target="#b14">[16,</ref><ref type="bibr" target="#b48">49]</ref> to infer data facts. Finally, InkSight leverages the natural language generation models <ref type="bibr">[1,</ref><ref type="bibr" target="#b38">39]</ref> to convert data facts into natural language, making the documentation more accessible and effective for users. To demonstrate the usefulness of InkSight, we conducted a user study with 12 participants. The results show that sketching is efficient and effective in catching the users' intent, and InkSight can help document findings. Finally, we further summarized the lessons we learned and discussed potential future directions. In summary, our main contributions are as follows:</p><p>• We provide a convenient and efficient approach for users to express intent by sketching atop the visualizations directly;</p><p>• We present InkSight, a computational notebook plugin, enabling users to document findings easily;</p><p>• We conduct a user study to demonstrate the usefulness and effectiveness of InkSight.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Our research is related to prior studies on visual data analysis for notebooks, documentation, visualization summarization, and intent expression.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Visual Data Analysis in Computational Notebook</head><p>With the popularity of computational notebooks for exploratory data analysis, facilitating visual data analysis in computational notebook environments has become a trending research topic in the visualization and HCI community. Their focuses range from creating data visualizations to presenting the analysis results. For example, researchers have developed Jupyter Notebook extensions that generate visualizations from dataframe transformation code <ref type="bibr" target="#b52">[53]</ref>, printed dataframes <ref type="bibr" target="#b24">[26]</ref>, and data queries <ref type="bibr" target="#b9">[11,</ref><ref type="bibr" target="#b24">26]</ref>. Overall, much effort has been put into releasing users from laborious code writing for data wrangling and visualization during the data exploration process. Besides exploration, the role of computational notebooks for documenting and sharing analysis results raises another critical issue. That is, how to facilitate results documentation which could interrupt the analysis flow and be discarded by users as being too demanding <ref type="bibr" target="#b7">[9,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b45">46]</ref>. Themisto <ref type="bibr" target="#b45">[46]</ref> applied deep-learning algorithms to generate text for explaining codes in computational notebooks. However, for documenting analysis results, it only generates a few beginning words to prompt users to start writing. More work introduced Human-AI collaboration tools for generating presentation slides from computational notebooks. For example, NB2Slides <ref type="bibr" target="#b57">[58]</ref> and Slide4N <ref type="bibr" target="#b46">[47]</ref> automatically distill key messages from cells in computational notebooks and organize them into slides. Nonetheless, users still need to manually write their findings from analyzing charts. Notable <ref type="bibr" target="#b26">[28]</ref> takes a step further by extracting data facts from the data in user-specified charts. Yet, some participants in the user study suggested dissatisfaction with the divergence between automatically generated results and their intentions. This observation necessitates intuitive and efficient interaction to convey user intent as input for automatic algorithms.</p><p>We present InkSight, a tool that provides a sketch-based interaction to allow users to suggest data items of their interests through easy sketching atop data visualizations. Compared with previous work, InkSight captures user intent at a more fine-grained level without a significant increase in user interaction and workload. Instead of generating a whole slide presentation, we focused on integrating the documented findings in the same notebook for sharing and future recall.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Natural Language and Visualization</head><p>Visualizations are frequently coupled with text to convey information effectively with data and tell compelling stories <ref type="bibr" target="#b53">[54]</ref>. Some research efforts have been invested in linking text and visual elements within visualization to eliminate the need for shifting users' attention and enhancing comprehension <ref type="bibr" target="#b20">[22,</ref><ref type="bibr" target="#b21">23]</ref>. Diverging from linking text with visualizations, another line of research aims at generating one form of communication from the other. Much of this research centers on generating visualizations given NL utterances. For example, systems such as Text-to-Viz <ref type="bibr" target="#b13">[15]</ref>, FlowSense <ref type="bibr" target="#b54">[55]</ref>, GVQA <ref type="bibr" target="#b40">[41]</ref> and NL4DV <ref type="bibr" target="#b31">[32]</ref> apply a semantic parser to convert NL queries into visualization queries. More recent research like <ref type="bibr" target="#b47">[48]</ref> improves the performance by training deep learning models.</p><p>Relatively less research has studied the reverse problem, that is, to generate NL descriptions given a visualization. Researchers have proposed datasets for chart-to-text generation <ref type="bibr" target="#b18">[20]</ref>. However, their benchmark results show that state-of-the-art methods still yield unsatisfactory results and often suffer factual errors. Therefore, many systems first generate factually correct data findings and subsequently generate NL descriptions. For example, Voder <ref type="bibr" target="#b41">[42]</ref> implements heuristics to extract data insights from tables and generate corresponding NL descriptions and visualizations using templates. AutoCaption <ref type="bibr" target="#b28">[30]</ref> extends this workflow to recognize significant features of the chart images and subsequently generate NL descriptions. Those techniques are deployed for an array of applications such as iterative data analysis <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b56">57]</ref>, providing text summaries <ref type="bibr" target="#b28">[30]</ref>, and making charts accessible to individuals with visual impairments <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b32">33]</ref>. Although those methods capture interesting data findings, they fall short of capturing user intent, which is critical for effective human-computer collaboration. To address this problem, Choi and Jo <ref type="bibr" target="#b11">[13]</ref> introduced Intentable, a mixed-initiative system that allows users to specify their intent by selecting pre-defined intent types and clicking the data items of interest in the chart. However, its interaction is limited to clicking, which is inefficient when dealing with a large number of data items of interest. For instance, selecting numerous data items in scatterplots, as shown in Figure <ref type="figure" target="#fig_7">6</ref>, becomes impractical and time-consuming. Moreover, it primarily focuses on developing a grammar for intent specification and training a neural network to generate caption sentences. In contrast, our approach contributes a new interaction design, leveraging sketching as a more natural and expressive method for specifying user intent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Sketching Interaction in Visual Analysis</head><p>Introduced by Sutherland in the 1960s with the Sketchpad concept <ref type="bibr" target="#b42">[43]</ref>, sketching has been widely studied due to its central role in the design process and visual thinking. In visualizations, sketch-based interaction was first employed for data queries <ref type="bibr" target="#b17">[19,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b49">50]</ref>. For instance, ShapeSearch <ref type="bibr" target="#b39">[40]</ref> and QuerySketch <ref type="bibr" target="#b49">[50]</ref> allowed users to draw freeform sketches to query time-series data with matching patterns. Recognizing the benefits of promoting thinking, insight, and inspiration <ref type="bibr" target="#b43">[44]</ref>, sketch-based interactions have been further employed for data exploration. Examples include NapkinVis <ref type="bibr" target="#b6">[8]</ref>, which enables users to sketch predefined gestures for rapid and effortless visualization creation, and SketchVis <ref type="bibr" target="#b5">[7]</ref>, which allows hand-drawn sketch inputs to explore data through simple charts. Some research has extended these interactions to pen and touch for creating and manipulating data visualizations <ref type="bibr" target="#b22">[24,</ref><ref type="bibr" target="#b44">45]</ref>.</p><p>Beyond data exploration, sketching has been identified as a flexible and lightweight way to convey users' high-level intents <ref type="bibr" target="#b8">[10,</ref><ref type="bibr" target="#b12">14,</ref><ref type="bibr" target="#b15">17]</ref>. For example, TaleBrush <ref type="bibr" target="#b12">[14]</ref> allows users to generate stories by sketching the protagonist's fortune changes throughout the narrative. Forte <ref type="bibr" target="#b8">[10]</ref> enables users to generate structures for creating 3D fabrication-ready models by sketching the draft plan and loads. Kim et al. <ref type="bibr" target="#b19">[21]</ref> have observed that users prefer to use digital pens to express their intentions for identifying trends, anomalies, and more. Given the advantages of sketch-based interactions, such as flexibility, expressiveness, and the ability to convey high-level intentions, InkSight chooses to utilize sketching as a means for users to express their intent. Our work aims at a new application scenario where sketch interaction will be integrated into the visual data analysis process with computational notebooks of which most interactions are keyboard based. We believe our study results can complement previous work by providing insights into sketch interactions for visual data analysis in a scenario.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">DESIGN GOAL</head><p>Our tool aims at reducing both users' mental and labor effort of documenting chart findings in computational notebooks. We set ourselves the following design goals.</p><p>G1: Offer automatically generated documentation of chart findings. Manually documenting findings can be both time-consuming and tedious, thus some data scientists disregard it <ref type="bibr" target="#b35">[36]</ref>. Data analysts have expressed a preference for automated approaches that assist in documentation <ref type="bibr" target="#b45">[46]</ref>. The tool should automatically generate chart findings, reducing the burden of starting the documentation from scratch.</p><p>G2: Support flexible and effortless user specification of intent. Prior research indicates the importance of considering user intent in subsets of data and data fact types to optimize automatic algorithms for creating data visualizations and generating chart findings <ref type="bibr" target="#b24">[26,</ref><ref type="bibr" target="#b26">28,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b45">46]</ref>. Critically, the forms of specification affect the efficiency of expressing user intent. The tool should offer a flexible and effortless way for users to specify their intent.</p><p>G3: Reduce context switch between exploration and explanation. Context switching between exploration and explanation can be costly <ref type="bibr" target="#b10">[12,</ref><ref type="bibr" target="#b26">28]</ref>. The tool should provide a smooth and natural interaction that minimizes disruption to users' mental flow to reduce users' mental effort required to transition between exploration and explanation tasks.</p><p>G4: Facilitate iterative refinement of the documentation. Providing users with some control over the generated results can serve as a means of fine-tuning and refining the outcomes to better meet individual user requirements and compensate for the limitations of the automated methods. This can improve user satisfaction and increase user engagement with the system <ref type="bibr" target="#b45">[46]</ref>. Thus, the tool should facilitate iterative refinement of the generated results conveniently with a suite of interactions.</p><p>G5: Integrating with existing platform. The tool should be developed as a plugin for computational notebooks (i.e., Jupyter Notebook in our case), such that users do not need to be familiar with a new interface. The chart findings documentation is directly inserted into the notebook as a whole for recalling and sharing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">INKSIGHT</head><p>In this section, we first present an overview of InkSight (Section 4.1). Then we introduce the interactive modules and computational modules of InkSight (Section 4.2 and Section 4.3, respectively).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">System Overview</head><p>In accordance with the design goals, we developed InkSight, a mixedinitiative Jupyter Notebook plugin (G5) designed to offer a seamless experience for documenting chart findings during data exploration (G3). In this section, we present the overview of InkSight and elaborate on the details in subsequent sections. The tool consists of interactive and computational modules. Interactive modules define the interface and interaction designs. Computational modules support the functionalities of interactive modules.</p><p>As shown in Figure <ref type="figure">1</ref> (C), our tool has two interactive modules: the sketch panel and the documentation panel. For every code cell that creates a data visualization, the two panels are inserted below it. In the left sketch panel, users can sketch their intent in subsets of data atop the chart directly (G2). We pre-defined sketch types tailored to different types of data facts. For example, users can circle out an outlier point in a scatter plot or draw a line following the trend of a line chart, and the algorithms will generate outlier or trend data facts respectively. When users explore the chart and identify interesting patterns, they can immediately sketch related areas in the chart to smoothly transition to the task of documenting chart findings (G3).</p><p>The right documentation panel displays the generated data facts corresponding to each sketch (G1). Its design centers around the goal of facilitating the iterative refinement of the documentation (G4). Users then can refine the documentation by revising, reorganizing, deleting, and grouping multiple data facts. Furthermore, the sketch panel and documentation panel are linked visually and interactively, inspired by the use of embellishments in Voder to enhance users' understanding <ref type="bibr" target="#b41">[42]</ref>. For example, InkSight assigned a color mark to the generated findings with the same color as the corresponding sketch. It also enables users to hover over documentation to highlight the corresponding sketch and to click on a sketch to automatically scroll its associated documentation into view. The final documentation of chart findings is presented in a hierarchical list structure.</p><p>To support the interactive modules in the front-end user interface, we developed three computational modules (i.e., sketch identification, intent inference, and finding documentation generation) for automatic findings documentation generation (G1). Once a user completes a sketch, the sketch identification module interprets it to determine the sketch type and associated data items. The intent inference module then infers potential data facts related to these data items and sketch types. The finding documentation generation module accepts these data facts and employs a template-based approach to generate descriptive text narratives for each. It utilizes the advanced language model, GPT-3.5 [1], to organize and refine all text narratives. The generated narrative for each sketch is displayed in the documentation panel within the interface.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Interactive Modules</head><p>This section presents two interactive modules in the front-end user interface: the sketch panel and the documentation panel. The sketch panel enables users to express their intents through sketches atop the charts, while the documentation panel provides a suite of interactions to support the convenient refinement of the generated documentation.</p><p>Sketch Panel. As shown in Figure <ref type="figure">1</ref> (C), the sketch panel displays the chart (in SVG format) from which users have findings to be documented. Our tool supports basic statistical charts, including bar charts, line charts, pie charts, grouped bar charts, multiple line charts, and scatter plots. In the sketch panel, users can sketch directly on top of the visualizations. The sketching feature enables users to express their intents in data subsets of interest. Inspired by <ref type="bibr" target="#b3">[5]</ref>, we support two sketch types for expressing intent: closed path and open path.  data items of interest by ensuring they lie within or intersect the sketch area as shown in A1 and A2. However, grouped bar charts and multiple line charts involve several data groups each of which could be hard to single out by a close path. Thus, we enhance users to select a group of interested data items through legends (Figure <ref type="figure" target="#fig_0">2</ref>  Regarding the open path, users can draw a line to follow each data item of interest in the charts in a loose way. To be more specific, B1 showcases how users can identify a data range of interest using an open path for simple charts. Similar to the closed path, the tool offers a convenient way to select a group of data from the same category in multiple line charts and grouped bar charts. To address this, we enable users to draw a sketch resembling the path of a specific group of data items they find interesting (Figure <ref type="figure" target="#fig_0">2</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(B2)).</head><p>During drawing, to assist users in checking whether the sketch captures their data subsets of interest accurately, the sketch panel highlights the selected data items in red (Figure <ref type="figure" target="#fig_0">2</ref>  Once the documentation for each sketch is generated, it is automatically inserted as the first card in the documentation panel, as shown in Figure <ref type="figure" target="#fig_2">3 (B)</ref>. Each card is accompanied by two buttons on its right side. At the top is a deletion button for deleting the documentation card and the corresponding sketch (Figure <ref type="figure" target="#fig_2">3 (A2)</ref>). Alternatively, users can delete a documentation card by clicking on the corresponding sketch and clicking the deletion button that pops up subsequently (Figure <ref type="figure" target="#fig_2">3</ref> (A1)). At the bottom is a checkbox button for users to select multiple documentation cards and group them together to create a simple hierarchical structure by clicking the grouping button (Figure <ref type="figure" target="#fig_2">3 (C)</ref>). The grouping button is the first button at the top toolbar of the panel Figure <ref type="figure" target="#fig_2">3</ref> (C3)). To further organize the documentation cards, users can utilize the drag-and-drop operations to reorder the cards within, outside, and between the merged groups and documentation cards (Figure <ref type="figure" target="#fig_2">3</ref> (D)). At the top toolbar, three buttons from left to right are for users to group selected cards, group all cards, or delete all cards, respectively. Last but not least, users can edit the content of documentation cards by simply clicking on them (Figure <ref type="figure" target="#fig_2">3 (E)</ref>).</p><p>To enhance the linkage between the documentations and the sketches, our tools provide the following designs. First, the border of each card is assigned the same color as the corresponding sketch. Moreover, when users click a sketch on the chart, the corresponding documentation card will scroll into view, thereby facilitating their navigation. Inspired by a previous study <ref type="bibr" target="#b55">[56]</ref>, InkSight highlights key messages in data facts to enhance readability and improve efficiency. To highlight the key messages, we first identify all key messages in data facts, including the related data variables' names and values, as well as data patterns revealed by the data facts like increasing or decreasing trends. Then, utilizing an exact match method, these key messages are located within the sentences and subsequently highlighted for emphasis. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Computation Modules</head><p>This section describes the three computation modules that support the front-end user interface, as depicted in Figure <ref type="figure" target="#fig_5">4</ref>. First, the sketch identification module takes the Vega-Lite chart specification and the sketch atop the chart as input to identify the sketch types and infer the data items of interest. Subsequently, the intent inference module infers a list of potential data facts related to these items. Finally, the finding documentation generation module automatically generates and organizes text descriptions for the data facts, resulting in the final documentation. Sketch Identification. The sketch identification module aims to determine the type of sketch and extract the data that binds to the sketch. Once users complete sketching, the sketch identification module first detects the path type (i.e., close or open) and then calculates the data items that bind to the sketch.</p><p>For a closed path, the module identifies data items of interest by examining the visual graphical elements included within or intersecting the sketch. Specifically, if these graphical elements are visual representations of data items, then these data items are considered to be of interest. Alternatively, if the graphical elements are data labels For open path, the module identifies data items of interest differently for simple charts (i.e., line chart, bar chart, scatterplot, pie chart) and complex charts (i.e., multiple line chart and group bar chart), which have several data groups from distinct categories. For simple charts, the module recognizes the selected data range and filters data items within the specified range. However, in the case of complex charts such as multiple line charts and grouped bar charts, the selected data range may encompass several data groups from distinct categories, necessitating the determination of which data group is of primary interest to the user. To address this, the module infers that the data group with the highest spatial similarity to the user's sketch is the target group. The similarity between each data group and the sketch is evaluated by calculating the average of the shortest distance between each point in the data group and the sketch. We use the average distance instead of the sum of distances because we observe that different groups may have varying numbers of points due to missing values, within the selected range. Consequently, the average distance is employed to mitigate the impact of missing values. In summary, with the sketch and chart specification as input, the sketch identification module calculates the sketch type and the data items of interest.</p><p>Intent Inference. Given the chart specification and the output of the sketch identification module (including sketch types and data items of interest) as input, the intent inference module generates relevant data facts. InkSight supports eight data fact types defined by Calliope <ref type="bibr" target="#b38">[39]</ref>, as shown in Table <ref type="table" target="#tab_0">1</ref>. InkSight adopts the existing data fact generation algorithm <ref type="bibr" target="#b14">[16,</ref><ref type="bibr" target="#b48">49]</ref> to generate data facts. For example, for the data fact outlier, we utilize the Interquartile Range (IQR) method <ref type="bibr" target="#b0">[2]</ref> to detect outliers. Outliers are identified as data items with values that are either smaller than Q1 -1.5IQR or larger than Q3 + 1.5IQR. The detailed methods are included in the supplementary material.</p><p>The module first eliminates meaningless data facts based on data types, the size of the data subset, and sketch types. For data types, some data fact types can only be from certain types of data (Table <ref type="table" target="#tab_0">1</ref>: column "Breakdown" ). For example, a trend must be from a chart that encodes temporal data and numerical data. Similarly, some data facts have constraints on the minimal size of the data subset (Table <ref type="table" target="#tab_0">1</ref>: column "#Focus"). For example, difference compares at least two data items. Regarding sketch types, we assume that users utilize open-path sketches to express specific interests in trends or associations related to shapes rather than points. For closed-path sketches, we apply almost all fact types. However, if there are multiple selected data items, it is impractical to report all the ranks for each data item in the subset and compare each pair of data items. In this case, we only consider the top one and the last one for the rank fact and their difference for the difference fact. This distinction between sketch types helps ensure that the inferred data facts are relevant and meaningful.</p><p>Next, the module defines the following special cases. First, when a single data item is selected in complex charts comprising multiple groups of data from different categories, inferring users' intent can be challenging. It is hard to identify whether the users aim to compare the particular data item with others in the same group, with those that have the same independent variable, or with all other data items. To resolve this, we present all three possibilities to the user, enabling them to select the most appropriate option by deleting the unwanted ones. Second, when users draw a closed-path sketch involving multiple items, we assume they are only interested in data items inside the selected data subset without comparing them with data items outside. In other words, the intent inference module treats the selected data items as a completely separate dataset to calculate the data facts.</p><p>Finding Documentation Generation. Given the generated data facts from the intent inference module, the finding documentation generation module organizes them into the final generated documentation. The module initially employs template-based methods, commonly used for natural language generation, to generate text descriptions for each data fact. Table <ref type="table" target="#tab_0">1</ref> displays the templates employed for each data fact. Moreover, we add a description of the selected range of data when users select multiple items, such as "among the selected x items", where x represents the number of selected items. Subsequently, to improve the coherence and naturalness of the generated results, we leverage an advanced language model, GPT-3.5 [1], to help organize and refine the data facts. When there is only one data fact, we ask GPT-3.5 to polish it to be more accurate, concise, and human-like. In the case of multiple data facts, we have additional requirements. We instruct GPT-3.5 to synthesize the facts by merging those with similar information and comparing them as instructed in the prompt. An example instruction is "merge those with similar information". Further details about the prompt can be found in the supplementary material. The generated documentation can also be edited by the users in the documentation panel. In this way, users can revise or remove the data facts that are not aligned with their intent.</p><p>When designing the function of grouping user-selected cards in the documentation panel (Section 4.2), we also considered merging selected cards into a single sentence. However, most documentation results already comprise several data facts. Merging them will generate an excessively long and complex sentence that would be difficult for users to read and comprehend. Furthermore, the merged documentation can introduce ambiguity or confusion, as users may struggle to identify the relation between the original documentation and the merged result. In light of these concerns, we ultimately opted to group multiple documentation cards into a simple hierarchical structure without merging the content. This approach preserves clarity and maintains the original information, ensuring that each data fact can be easily understood.</p><p>In this section, we present a scenario to illustrate how InkSight assists users in documenting findings. Suppose that Alice is a data analyst at an investment company. She uses Jupyter Notebook to explore and analyze the movie market. Figure <ref type="figure" target="#fig_6">5</ref> shows the finding documentation process of Alice, with further details provided below. Alice is particularly interested in two movie genres: Action and Drama. To visualize the number of movies released in these genres between 2006 and 2010, Alice employs a grouped bar chart (Figure <ref type="figure" target="#fig_6">5 (A)</ref>).</p><p>Upon examining the chart, Alice is first drawn to the decreasing trend of drama movies. She sketches a line following the trend of drama movies (Figure <ref type="figure" target="#fig_6">5</ref> (1)). InkSight generates a documentation card with the description of this decreasing trend (Figure <ref type="figure" target="#fig_6">5 (B)</ref>). Compared to drama movies, the number of released action movies appears more steady and wavering. Alice draws another line that resembles the trend (Figure <ref type="figure" target="#fig_6">5</ref> (2)). The tool generates the second documentation card inserted at the top of the documentation panel (Figure <ref type="figure" target="#fig_6">5</ref> (C)). It not only outlines the overall wavering and decreasing pattern but also specifies each time period an increasing or decreasing trend appears (Figure <ref type="figure" target="#fig_6">5</ref> (C)). Although accurate, Alice considers this level of detail excessive. She clicks the documentation card to edit it and removes details about every small time interval (Figure <ref type="figure" target="#fig_6">5</ref> (3)). Satisfied with the content, Alice thinks both the two documentation cards (Figure <ref type="figure" target="#fig_6">5 (A)-(B)</ref>) are about trends. Thus, she clicks the Group All button at the top of the panel to group them (Figure <ref type="figure" target="#fig_6">5 (4)</ref>).</p><p>Wanting to emphasize the changes in the differences between these two genres at the beginning (2006) and the end (2010) of the data range, Alice encircles the two bars in 2006 and the two bars in 2010 (Figure <ref type="figure" target="#fig_6">5</ref> (5)-( <ref type="formula">6</ref>)). She subsequently obtains two documentation cards about the comparison of the two genres. They together reveal a contrast that the count of drama movies in 2006 was four times greater than that of action movies (Figure <ref type="figure" target="#fig_6">5</ref> (D)), while in 2010, the count of drama movies was one less than that of action movies (Figure <ref type="figure" target="#fig_6">5 (E)</ref>). Alice groups these two documentation cards about the comparisons together by selecting the corresponding cards and clicking the Group button at the top of the panel (Figure 5 <ref type="bibr" target="#b5">(7)</ref>).</p><p>Furthermore, Alice highlights the number of action movies in 2007 by drawing a circle atop the corresponding blue bar 2007 (Figure <ref type="figure" target="#fig_6">5</ref>  <ref type="bibr" target="#b6">(8)</ref>). In response, InkSight offers three comparison options: comparing the selected bar to all other bars, to those representing action movies, and to those also from the year 2007. It generates three documentation cards grouped together to form a larger card (Figure <ref type="figure" target="#fig_6">5</ref> (F)). Alice finds that the second one highlighting that in 2007 the number of action movies is largest compared to other years aligns with her intent. She thus decides to delete the other two (Figure <ref type="figure" target="#fig_6">5 (9)</ref>).</p><p>Finally, Alice feels satisfied with the content of these documentation cards in a hierarchical structure. However, she finds that the cards could be reordered to form a better narrative flow. She drags and drops the documentation cards to order them (Figure <ref type="figure" target="#fig_6">5</ref>  <ref type="bibr" target="#b8">(10)</ref>), beginning with the general trends of the two genres, followed by comparing the genres at specific time points, and ending with highlighting the highest number of action movies in 2007. This way, Alice effectively documents her findings for this chart with the help of InkSight (Figure <ref type="figure" target="#fig_6">5 (G)</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">USER STUDY</head><p>To evaluate the usability and effectiveness of InkSight, we conducted a within-group user study with 12 participants. They were asked to use InkSight and a baseline method with two datasets and give both qualitative and quantitative feedback. We introduce the study setting in Section 6.1 and then analyze the results obtained from the user study in Section 6.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Study Setup</head><p>Participants. Participants (P1-P12, 7 males and 5 females, aged 22 to 30, at an average age 25.5) were recruited through online advertisements on social networks and word of mouth. They were welleducated Ph.D. students of diverse backgrounds, including visualization, human-computer interaction, database, robotics, and software engineering. When asked about their prior experience with Jupyter Notebook, Python, and data analysis, their self-rated average scores were 5.6, 5.9, and 5.3, respectively (1 = "no experience", 7 = "highly experienced"). None of them had color blindness. Each participant received compensation of US $9/hour for completing the user study.</p><p>Baseline Method. InkSight enables users to express their intents by sketching. The baseline approach required users to write code to express their intent in JSON format. This code-based method, having been employed in prior research to specify user intent, such as Lux <ref type="bibr" target="#b24">[26]</ref> and Voyager2 <ref type="bibr" target="#b50">[51]</ref>, offers comparable expressiveness to sketch. Despite the difference in how users express their intent in InkSight and the baseline, other aspects remain consistent; that is, selecting the same data of interest via either method will yield identical findings in the generated documentation. Specifically, in InkSight, users sketch their intent atop the visualization directly. In the baseline approach, users create a new code cell to express their intent in JSON format, which is then executed to generate the corresponding documentation of findings alongside the visualization. Figure <ref type="figure" target="#fig_7">6</ref> showcases three examples of how users can write codes to specify their intent in baseline, which was also used as tutorial material in the user study.</p><p>Task.</p><p>We designed an open-ended data analysis task that involved both exploring the data and documenting findings from data charts created by themselves during the exploration. We chose this design instead of directly giving participants charts for them to document for two reasons: (1) we attempted to verify whether our tool can be naturally and smoothly integrated into an exploration flow; (2) it was found that individuals would analyze the data charts with more intentions when the charts are created by themselves compared to when the charts are made by others <ref type="bibr" target="#b19">[21]</ref>. Participants were asked to explore a given dataset with charts and utilize the provided tools to document their findings, which they would later share with others. Participants would finish the tasks with two tools in two different datasets. The order of the tools and datasets was counterbalanced and randomly assigned to participants to reduce the learning effect. We required that the participants document their findings using at least 4 charts or within 30 minutes. During our own exploration and a pilot study with two other researchers out of this project, we found this number of charts was more likely to allow the tool to be fully explored without tiring people.</p><p>Data. We selected the movies and cars datasets from the Vega dataset<ref type="foot" target="#foot_0">foot_0</ref> and house price prediction dataset from the Kaggle dataset<ref type="foot" target="#foot_1">foot_1</ref> . These datasets have been widely used to evaluate the effectiveness and usability of visualization tools <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b51">52]</ref>. The cars dataset was used only in the tutorial to familiarize participants with the tools. To make the task manageable in the user study, we reduced the dataset size to 10 attributes and approximately 500 rows. All datasets contain the three common data types, i.e., temporal, categorical, and numerical.</p><p>Procedure. The user study lasted about 1.5 hours and all of them were conducted through one-to-one offline meetings. We started by introducing the procedure and asked for the participant's consent for recording the entire session. Participants then completed a brief tutorial session to become familiar with InkSight or the baseline. They first watched a short video about the components and functionalities and then tried them on a sample notebook with the cars dataset. This tutorial session took around 10 minutes until participants felt familiar enough to use the tool. Participants were then given a dataset with attribute descriptions and asked to explore data and document chart findings for approximately 30 minutes. After that, they were asked to briefly share their recorded findings. Then, participants were asked to complete a 7-point Likert questionnaire in a think-aloud protocol, where users were encouraged to justify their ratings. Specifically, a rating of 1 means "strongly disagree", while a rating of 7 means "strongly agree". As shown in Figure <ref type="figure" target="#fig_8">7</ref>, the questionnaires concerned three aspects, i.e., the usability and effectiveness of the tools (Q1-Q5 and Q6-Q7, respectively), the quality of the generated documentation (Q8-Q9), and the usability and effectiveness of the way to express  with "*" for p &lt; 0.05, and "**" for p &lt; 0.01. As shown in the results, InkSight received overall higher ratings than the baseline in all perspectives.</p><p>For the usability and effectiveness of the tools (Q1-Q7), InkSight outperformed the baseline statistically significantly in terms of usability (Q1-Q5), while being slightly better in effectiveness (Q6-Q7), as depicted in Figure <ref type="figure" target="#fig_8">7</ref>. Both InkSight and the baseline were recognized as being helpful and efficient for documenting findings, with all average ratings larger than 5.3 (Q6-Q7). As a result, all participants expressed their willingness to use the tools in their work, with average ratings larger than 5.4 (Q4). However, the difference in the ways of expressing intent led to InkSight outperforming the baseline statistically significantly in terms of its seamless integration into the data analysis workflow, ease of use and learning, and recommendation to others (with all p &lt; 0.05) (Q1-Q3 and Q5).</p><p>For the quality of generated documentation (Q8-Q9), participants overall recognized the generated documentation. InkSight statistically significantly outperformed the baseline in capturing the users' intents (p &lt; 0.01). However, no statistically significant differences were observed in the overall satisfaction with the generated documentation. Since InkSight and the baseline shared the same computation modules, the difference in performance was likely due to the difference in the interaction method used to express intents.</p><p>For the way to express intent (Q10-Q15), the user study results revealed that the sketching interaction of InkSight statistically significantly outperformed the baseline interaction. Specifically, sketching was more helpful (Q10), intuitive (Q12), natural (Q13), and easier to use (Q14) in expressing users' intent, with all of these aspects showing statistically significant differences (p &lt; 0.05). As for efficiency (Q11) and ease to learn (Q15), there was no statistically significant difference between the two interactions.</p><p>In conclusion, the results suggest that InkSight consistently outperformed the baseline across various aspects related to the overall tool, the quality of documentation, and interactions for expressing intent. Although some aspects did not show statistically significant differences, InkSight consistently achieved higher average ratings in the user study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.2">Qualitative results</head><p>This section presents participants' qualitative feedback from their justifications of questionnaire ratings and post-interview responses.</p><p>InkSight streamlines the documentation process and provides additional benefits, including enhancing data analysis and fostering effective collaboration. All participants in the study appreciated InkSight for its ability to help them document findings. They recognized the tool's well-designed interactions and provided documentation effectively reducing their efforts in documenting findings. They specifically appreciated the seamless integration of InkSight into their analysis workflow, as it did not disrupt their data analysis process (G3 and G5). For instance, P11 noted that "InkSight with sketch aligns well with my analysis habits. When I record findings, I observe the points and trends and take notes on the charts directly." Similarly, P3 mentioned that "The code-free sketch makes InkSight seamlessly integrate into my data analysis workflow without any additional workload...I even did not realize when I was switching between exploring the data and recording findings." Furthermore, participants recognized how the documentation of findings by InkSight can benefit collaboration and self-recall. P1 mentioned that "Placing the documentation next to the visualization instead of below it, which distinguished it from the analysis code, helps my future self and my cooperators to better locate the documentation." P6 noted that "Sharing my findings with my cooperators through InkSight allowed them to better understand what I have found, and they can easily add their own findings using the same method."</p><p>In addition to its benefits in documenting findings, participants highlighted other advantages brought by InkSight, such as facilitating data analysis and fostering a sense of partnership. InkSight sometimes serves as a tool for users to obtain and inspect related data findings regarding a data subset of interest, which in turn helps some participants analyze data. P5 mentioned that "InkSight helped me test my hypotheses on interesting data subsets quickly. I even did not need to write the code to analyze it". Interestingly, InkSight can also give a feeling of being accompanied by serving as an AI partner, transforming the typically mundane documentation process into a more engaging experience. P12 said, "Every time I draw a sketch, the tool responds to my input. It feels like I'm having a conversation with a partner. I documenting". The sketch interaction in InkSight is considered an easy and flexible way to express intent by the majority of the participants. 10 out of 12 participants preferred the sketching method in InkSight, with the remaining two participants (P2 and P12) expressing their equal preferences for both methods. Participants found the code-free sketching to be more natural, intuitive, and flexible, allowing them to work directly on the chart with little workload (G2). In particular, the sketch-based interaction enabled users to express their intent by drawing any shapes to highlight what they see in the chart, while the coding method demands additional mental efforts to identify attribute names and values carefully. For example, P8 mentioned that "When sketching, I just highlight the area that my eye catches. While for coding, I need to further process what I see into codes." P5 noted that "When coding my intent, I just feel like I work as a compiler to translate my intent to a grammar, which is really unnatural and unfriendly." Additionally, when talking about the baseline method, participants said that they often made mistakes when typing out their intent in a rigorous programming format. Sometimes the tedious process could cause participants to give up using the tool, as P1 pointed out "I would prefer to spend my time documenting findings directly rather than writing extra codes first for expressing intent."</p><p>Despite the advantages of sketch interaction, we find a personal preference for the baseline method. P2 and P12, who rated their Python programming experience highly (7 out of 7), stated that they enjoyed programming and felt comfortable and more familiar with using programming to accomplish tasks. However, P12 added that "The convenience of the sketch is more beneficial to open exploration since I can draw any shapes to see the results with a low burden. While coding is more suitable for focused analysis since I need to think about it carefully."</p><p>The quality of documentation generated by InkSight is thought highly by the participants. In the user study, participants expressed overall satisfaction with the generated documentation, finding it helpful to reduce their efforts in documenting findings (G1). They appreciated the natural language used in the documentation, the efficiency gained from using it, and the fact that they could easily edit and customize it (G4). P5 commented that "The generated ones is what I needed, and most of the time, I can share to others directly." Some participants acknowledged that the data facts covered many common and useful aspects, providing a comprehensive understanding of the data. P6 remarked "The tool provides almost all the basic data facts that I need for data analysis."</p><p>However, the generated documentation did not always fully capture participants' intent. For example, P10 would like to highlight two peak points in a line by drawing an open-path sketch following the whole line. Unexpectedly, InkSight only described the general trend without explicitly mentioning the two peaks. Nonetheless, unexpected data facts could sometimes have positive effects. Three participants (P4, P8, P12) realized that these facts beyond their expectations could provide them with a new perspective of data analysis and inspire their next-step exploration. For instance, P4 noted that "Unexpected information can help me avoid overlooking important details."</p><p>Overall, while the generated documentation was well-rated, there is still room for improvement in terms of comprehensiveness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">DISCUSSION</head><p>In this section, we discuss the lessons learned, the limitation of this study, and future research directions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Design Lessons</head><p>Human-AI collaboration in InkSight. InkSight applies a Human-AI collaboration workflow for documenting chart findings. When using InkSight, humans express their intent through sketching first. AI then generates initial drafts of documentation, and humans finally refine the documentation. All participants appreciate the assistance of AI in reducing manual work in this tedious task. Previous human-AI collaboration tools in computational notebooks <ref type="bibr" target="#b26">[28,</ref><ref type="bibr" target="#b45">46]</ref> mainly focus on defining the roles of humans and AI for more efficient and effective task assignments. Unlike them, we target interaction designs for facilitating the communication between humans and AI, which is an important challenge in facilitating human-AI collaboration <ref type="bibr" target="#b4">[6,</ref><ref type="bibr" target="#b25">27]</ref>. The user study results suggest that our tool design can help AI capture participants' intent with little input from them. Participants further provide valuable insights into future improvements in human-AI communication. Some participants desired a way to further specify their needs on the basis of the original input of intent when AI generates documentation that has redundant or irrelevant information. In other words, they would like AI to revise the documentation based on updated intent instead of editing the documentation themselves. Furthermore, several participants appreciate personalized AI assistance, such as generating documentation aligned with their individual habits and styles. We will keep working on facilitating AI's understanding of human intent in a more iterative and personalized manner.</p><p>Enabling user to express intents through multi-modal interaction methods. All participants in the user study appreciated the sketch interaction for expressing their intent, praising its intuitive and engaging nature. The sketching method also offered a higher degree of freedom, enabling users to draw any shape of interest without the constraints imposed by coding. However, users may accidentally select the data items incorrectly when selecting points distant or surrounded by undesired ones. While the code-based approach was found to be more accurate, it process of reading the chart in detail to obtain attribute names and values and writing lengthy codes for filtering. Moreover, the code-based approach is more restrictive and demanding in terms of chart interpretation. Each interaction method has its own advantages and disadvantages. As such, one potential direction for future work is to support multi-modal interactions (e.g., <ref type="bibr" target="#b44">[45]</ref>) simultaneously, maximizing the benefits of each method while minimizing their respective drawbacks. This would enable users to choose the most suitable interaction method based on their personal preferences, skills, and the specific task at hand. Documenting findings in computational notebooks. In our research, we have some observations that may help deepen the understanding of documenting findings in computational notebooks. First, our study participants mentioned that documenting findings is essential, as they usually had some initial ideas when examining a chart that either inspired their next-step exploration or those they wanted to share with others. Due to the tedious process of documentation, they often prioritized data exploration over spending time on explanation. As a result, they sometimes forgot the rationale behind their analysis or the findings they wanted to share, leading to additional time spent revisiting charts to jog their memory. Thus, documenting findings promptly during data exploration can help record their findings when the memory is fresh and save time for future reference. Second, participants appreciated the placement of the documentation panel to the right of the visualization. This layout offers several benefits. It enables users to locate corresponding documentation swiftly. Additionally, the horizontal placement of the documentation separates it from the vertically arranged code analysis, effectively distinguishing between data analysis and data exploration, facilitating users' focus on each aspect of their work. Third, it is noteworthy that participants tended not to edit the documentation during the data analysis process. Instead, they briefly scanned the generated findings and simply deleted unwanted ones, leaving detailed documentation revision after completing data exploration. This observation indicates the importance of striking a balance between documentation and data analysis, ensuring that the documentation process does not impede the exploration flow.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Limitation and Future Work</head><p>Enhancing the quality of documentation created by InkSight. Firstly, it will be helpful to support documenting findings based on multiple code cells and related charts. Currently, InkSight focuses on applying sketch interaction to allow users to document findings based on one code cell and the associated chart. However, participants (P4, P8, P10, P12) in our user study expressed interest in enhancing documentation that spans multiple code cells and corresponding charts, i.e., integrating previous codes or prior finding documentation as context. Such an enhancement would align with the interconnected nature of findings often observed during exploratory data analysis, where analysts frequently base subsequent explorations on previous findings <ref type="bibr" target="#b23">[25]</ref>.</p><p>In the future, it will be interesting to enhance InkSight by integrating techniques like understanding code histories <ref type="bibr" target="#b16">[18]</ref> and chart relationships <ref type="bibr" target="#b27">[29]</ref> for more contextualized documentation based on the entire exploration workflow. Secondly, there is a need for InkSight to support more types of charts and data facts. Currently, InkSight is limited to six chart types and eight types of data facts (see Section 4). However, participants expressed the need for more chart types (P4, P8, P10, P12), like heatmaps, and more diverse types of data facts (P2, P5, P9, P10), such as local minima and maxima. Future work should focus on extending the coverage of supported chart types and data facts to enhance InkSight's utility. Lastly, the documentation generated by InkSight can be more natural. Currently, though we employ GPT-3.5 to organize and polish the description of data facts, some generated documentation can look template-based and not intuitive, such as the expression "Genre=Action" in Figure <ref type="figure" target="#fig_6">5</ref> (C) and (E). To improve the documentation quality, it is possible to explore the usage of more advanced natural language processing models such as GPT-4 <ref type="bibr" target="#b33">[34]</ref> or fine-tune the prompt in the future.</p><p>Improving the interaction design of InkSight. Firstly, enhancing the interaction design of InkSight to minimize user effort in documenting findings can be an improvement avenue. Presently, InkSight generated all data facts related to the data subset identified by users, allowing users to edit out unwanted ones. While this strategy could save user effort in expressing intent, it might result in information overload when there are too many findings for users to consume and select from. Another approach is to ask users to put more effort into specifying their intents. For example, Intentable <ref type="bibr" target="#b11">[13]</ref> asks users to click intent types (e.g., "overview", "trend") and data items of interest to generate more focused results. Therefore, it remains an open question of how to strike a balance between users' efforts in specifying their intent and their efforts in editing the automatically generated results. Additional user studies could help understand user preferences and behaviours regarding these two different intent inference strategies for generating documentation, ultimately informing iterative interface improvements. Secondly, enhancing the interaction with sketch in InkSight offers a promising direction for improvement. Currently, to refine a sketch, users have to delete it, which limits users' ability to refine their intent. Future improvement of InkSight could consider integrating additional interactive features, such as dragging, resizing, and erasing sketches <ref type="bibr" target="#b5">[7,</ref><ref type="bibr" target="#b12">14]</ref>, thereby providing users with greater flexibility in adjusting their intent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">CONCLUSION</head><p>Computational notebooks provide data analysts with a convenient approach to exploratory data analysis by combining code cells and outputs within a single document. However, effectively documenting chart findings remains a challenge due to its time-consuming and tedious nature. To address the challenge, this study explores a natural and convenient way to reduce the workload of document chart findings. Specifically, we develop InkSight, a computational notebook plugin, to document chart findings according to users' intent leveraging sketch interaction. It allows users to sketch their interested data items and creates corresponding documentation with language generation models. InkSight was considered effective by the participants in our user study. They agreed that InkSight offered an effortless experience of finding documentation through natural and engaging sketch interaction. Furthermore, they pointed out future improvements of InkSight, such as capturing intent more accurately and generating documentation that considers context and semantics. In the future, we hope to continue the research on facilitating documenting chart findings. Potential directions enhancing the documentation generation and refining the interface and interaction design of InkSight. Additionally, we hope to conduct long-term and real-world user studies with more diverse participants. Through these studies, we can deepen our understanding of users' workflow and experience of documenting and communicating data analysis results with InkSight and other tools (e.g., Notable <ref type="bibr" target="#b26">[28]</ref>) in computational notebooks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 :</head><label>2</label><figDesc>Fig. 2: This figure showcases how users can employ both closed-path sketches (A1-A3) and open-path sketches (B1-B2) to express their intent by selecting data subsets of interest. Specifically, the selected data items are highlighted in red, while the selected range features a red background.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2</head><label>2</label><figDesc>showcases six examples that illustrate how users can express their intent using open-path sketches (Figure 2 (A1)-(A3)) and closedpath sketches (Figure 2 (B1)-(B2)). For the closed path, users encircle</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>Fig. 3: A suite of interactions is provided for refining documentation, illustrating how users can interact with the interface and obtain corresponding results. Specifically, (A1) or (A2) are used to delete the sketch and the corresponding documentation card, (B) denotes drawing a sketch to insert a documentation card, (C) refers to selecting multiple documentation cards for grouping, (D) represents ordering the documentation cards by dragging and dropping, and (E) signifies clicking to edit.</figDesc><graphic coords="4,45.00,49.50,513.01,152.96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>(A3)) to improve usability. Circling out a label in the legend represents the selection of all data items that belong to the category.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>(A1)-(A3) and (B1)-(B2)) or the selected range with a red background (Figure 2 (B1)-(B2)). To avoid visual clutter, this highlighting effect lasts for one second before disappearing. Documentation Panel. The documentation panel on the right is for display and refinement of the generated documentation (Figure 1 (C)). The documentation panel presents findings in a hierarchical structure consisting of different cards. In particular, a card at the lowest level contains findings generated from one sketch. The card-based visual design is to resemble the code cell-based design in Jupyter Notebook, to ensure a consistent style throughout the interface.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 4 :</head><label>4</label><figDesc>Fig. 4: This figure shows the workflow of InkSight. With the user's sketch as input, InkSight contains three computation modules to generate the corresponding documentation, i.e., sketch identification, intent inference, and finding documentation generation.</figDesc><graphic coords="4,497.63,392.96,57.92,76.83" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 5 :</head><label>5</label><figDesc>Fig. 5: This figure illustrates a usage scenario in which Alice, an analyst, utilizes InkSight to document her findings during analyzing the movie market. (1-10) represent Alice's interactions with the chart, such as sketching, editing, and ordering. (A) corresponds to the chart that Alice created for her analysis, while (B-G) refer to the documentation results after the interaction.</figDesc><graphic coords="7,54.00,49.50,513.01,220.85" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 6 :</head><label>6</label><figDesc>Fig. 6: This figure shows the baseline condition in user study where users write code</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 7 :</head><label>7</label><figDesc>Fig. 7: This figure displays the user ratings for two tools: InkSight and the baseline, with * indicating p &lt; 0.05 and ** indicating p &lt; 0.01. In general, InkSight outperforms the baseline across all aspects, with some aspects showing statistically significant differences.</figDesc><graphic coords="8,45.00,49.50,512.99,118.16" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="1,77.76,131.53,465.48,157.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>The table displays eight data fact types and their corresponding attributes, including the data types involved, the number of data points related, and the text description template. C, T, N indicate the categorical, temporal, and numerical data types, respectively. Breakdown represents the independent variables of a chart, measure refers to the dependent variables, focus denotes the data points related to the data fact, and others such as rank and corr are parameters which describe the data fact.</figDesc><table><row><cell cols="5">Data Fact Type Breakdown Measure #Focus Template</cell></row><row><cell>Value</cell><cell>C/T/N</cell><cell>N</cell><cell>1</cell><cell>{focus}.</cell></row><row><cell>Proportion</cell><cell>C/T</cell><cell>N</cell><cell>1</cell><cell>The {focus} accounts for {proportion} of the {measure}.</cell></row><row><cell>Outlier</cell><cell>C/T</cell><cell>N</cell><cell>1</cell><cell>The {focus} is an outlier in {measure}.</cell></row><row><cell>Outlier_scatter</cell><cell>N</cell><cell>N</cell><cell>1</cell><cell>The {focus} is an outlier.</cell></row><row><cell>Rank</cell><cell>C/T</cell><cell>N</cell><cell>1</cell><cell>The {focus} ranks {rank} in {measure}.</cell></row><row><cell>Difference</cell><cell>C/T</cell><cell>N</cell><cell>2</cell><cell>The {measure} of {focus1} is {ratio times/difference} higher than that of {focus2}.</cell></row><row><cell>Trend</cell><cell>T</cell><cell>N</cell><cell>&gt;=3</cell><cell>During {range}, the {measure} (of the category of {focus}) has an {increas-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>ing/decreasing/flat/wavering} trend over the {breakdown} with a slope of {slope}.</cell></row><row><cell>Association</cell><cell>N</cell><cell>N</cell><cell>&gt;=3</cell><cell>{measure1} has a {strong/moderate/weak} {positive/negative} influence to {mea-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>sure2}, with a Pearson correlation as {corr}.</cell></row><row><cell cols="5">in the legend, the data items belonging to the selected categories are</cell></row><row><cell>considered of interest.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https://vega.github.io/vega-datasets/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>https://www.kaggle.com/competitions/ house-prices-advanced-regression-techniques/data</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<idno>2023-07-01. 5</idno>
		<ptr target="https://online.stat.psu.edu/stat200/lesson/3/3.2" />
		<title level="m">Interquartile Range (IQR) method</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName><surname>Posit</surname></persName>
		</author>
		<idno>2023-07-01. 1</idno>
		<ptr target="https://posit.co/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Project</forename><surname>Jupyter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Home</forename></persName>
		</author>
		<idno>2023-07-01. 1</idno>
		<ptr target="https://jupyter.org/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">CINCH: A cooperatively designed marking interface for 3d pathway selection</title>
		<author>
			<persName><forename type="first">D</forename><surname>Akers</surname></persName>
		</author>
		<idno type="DOI">10.1145/1166253.1166260</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2006 Annual ACM Symposium on User interface Software and Technology</title>
		<meeting>the 2006 Annual ACM Symposium on User interface Software and Technology</meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="33" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Guidelines for Human-AI interaction</title>
		<author>
			<persName><forename type="first">S</forename><surname>Amershi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Vorvoreanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fourney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Nushi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Collisson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Suh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">T</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">N</forename><surname>Bennett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Inkpen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Teevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kikin-Gil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Horvitz</surname></persName>
		</author>
		<idno type="DOI">10.1145/3290605.3300233</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems</title>
		<meeting>the 2019 CHI Conference on Human Factors in Computing Systems<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Data analysis on interactive whiteboards through sketch-based interaction</title>
		<author>
			<persName><forename type="first">J</forename><surname>Browne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Carpendale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Riche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sherwood</surname></persName>
		</author>
		<idno type="DOI">10.1145/2076354.2076383</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 ACM International Conference on Interactive Tabletops and Surfaces</title>
		<meeting>the 2011 ACM International Conference on Interactive Tabletops and Surfaces<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
	<note>Association for Computing Machinery</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">NapkinVis: Rapid pen-centric authoring of improvisational visualizations</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">O</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Munzner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Panne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 2010 IEEE Infovis Conference -Poster</title>
		<meeting>2010 IEEE Infovis Conference -Poster</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">What&apos;s wrong with computational notebooks? Pain points, needs, and design opportunities</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chattopadhyay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Prasad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Z</forename><surname>Henley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sarma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Barik</surname></persName>
		</author>
		<idno type="DOI">10.1145/3313831.3376729</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems</title>
		<meeting>the 2020 CHI Conference on Human Factors in Computing Systems<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Forte: User-driven generative design</title>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">A</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Grossman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Coros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Hudson</surname></persName>
		</author>
		<idno type="DOI">10.1145/3173574.3174070</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems</title>
		<meeting>the 2018 CHI Conference on Human Factors in Computing Systems<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">PI2: End-to-end interactive visualization interface generation from queries</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Wu</surname></persName>
		</author>
		<idno type="DOI">10.1145/3514221.3526166</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2022 International Conference on Management of Data</title>
		<meeting>the 2022 International Conference on Management of Data<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">CrossData: Leveraging text-data connections for authoring data documents</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xia</surname></persName>
		</author>
		<idno type="DOI">10.1145/3491102.3517485</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems</title>
		<meeting>the 2022 CHI Conference on Human Factors in Computing Systems<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Intentable: A mixed-initiative system for intent-based chart captioning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jo</surname></persName>
		</author>
		<idno type="DOI">10.1109/VIS54862.2022.00017</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 2022 IEEE Visualization and Visual Analytics Conference</title>
		<meeting>2022 IEEE Visualization and Visual Analytics Conference</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Tale-Brush: sketching stories with generative pretrained language models</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J Y</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Adar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chang</surname></persName>
		</author>
		<idno type="DOI">10.1145/3491102.3501819</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems</title>
		<meeting>the 2022 CHI Conference on Human Factors in Computing Systems<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Text-to-Viz: Automatic generation of infographics from proportion-related natural language statements</title>
		<author>
			<persName><forename type="first">W</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-G</forename><surname>Lou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2019.2934785</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="906" to="916" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">QuickInsights: Quick and automatic discovery of insights from multi-dimensional data</title>
		<author>
			<persName><forename type="first">R</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1145/3299869.3314037</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 International Conference on Management of Data</title>
		<meeting>the 2019 International Conference on Management of Data<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">How do humans sketch objects?</title>
		<author>
			<persName><forename type="first">M</forename><surname>Eitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Alexa</surname></persName>
		</author>
		<idno type="DOI">10.1145/2185520.2185540</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">2012</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Leveraging analysis history for improved in-situ visualization recommendation</title>
		<author>
			<persName><forename type="first">W</forename><surname>Epperson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>.-L. Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Parameswaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Moritz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Perer</surname></persName>
		</author>
		<idno type="DOI">10.1111/cgf.14529</idno>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Relaxed selection techniques for querying timeseries graphs</title>
		<author>
			<persName><forename type="first">C</forename><surname>Holz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Feiner</surname></persName>
		</author>
		<idno type="DOI">10.1145/1622176.1622217</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd Annual ACM Symposium on User Interface Software and Technology</title>
		<meeting>the 22nd Annual ACM Symposium on User Interface Software and Technology<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="213" to="222" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Chart-to-Text: A large-scale benchmark for chart summarization</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kantharaj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">T</forename><surname>Leong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Masry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Thakkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hoque</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Joty</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.acl-long.277</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2022 Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 2022 Annual Meeting of the Association for Computational Linguistics<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Inking your insights: Investigating digital externalization behaviors during data analysis</title>
		<author>
			<persName><forename type="first">Y.-S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Henry Riche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Brehmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pahud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hinckley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hullman</surname></persName>
		</author>
		<idno type="DOI">10.1145/3343055.3359714</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 ACM International Conference on Interactive Surfaces and Spaces</title>
		<meeting>the 2019 ACM International Conference on Interactive Surfaces and Spaces<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Extracting references between text and charts via crowdsourcing</title>
		<author>
			<persName><forename type="first">N</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Hearst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Agrawala</surname></persName>
		</author>
		<idno type="DOI">10.1145/2556288.2557241</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 CHI Conference on Human Factors in Computing Systems</title>
		<meeting>the 2014 CHI Conference on Human Factors in Computing Systems<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="31" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Interactive synthesis of text and charts in data documents</title>
		<author>
			<persName><forename type="first">S</forename><surname>Latif</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Beck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><surname>Kori</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2021.3114802</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="184" to="194" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">SketchStory: Telling more engaging stories with data through freeform sketching</title>
		<author>
			<persName><forename type="first">B</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">H</forename><surname>Kazi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Smith</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2013.191</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2416" to="2425" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Avoiding drill-down fallacies with VisPilot: Assisted exploration of data subsets</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename></persName>
		</author>
		<author>
			<persName><forename type="first">.-L</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Dev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Elmeleegy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Parameswaran</surname></persName>
		</author>
		<idno type="DOI">10.1145/3301275.3302307</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Conference on Intelligent User Interfaces</title>
		<meeting>the 24th International Conference on Intelligent User Interfaces<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="186" to="196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Lux: Always-on visualization recommendations for exploratory dataframe workflows</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename></persName>
		</author>
		<author>
			<persName><forename type="first">.-L</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Boonmark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Mukhopadhyay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Hearst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Parameswaran</surname></persName>
		</author>
		<idno type="DOI">10.14778/3494124.3494151</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the VLDB Endowment</title>
		<meeting>the VLDB Endowment</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Why is AI not a panacea for data workers?</title>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Qu</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2304.08366</idno>
		<idno type="arXiv">arXiv:2304.08366</idno>
	</analytic>
	<monogr>
		<title level="m">An interview study on Human-AI collaboration in data storytelling</title>
		<imprint>
			<biblScope unit="page">2023</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Notable: On-the-fly assistant for data storytelling in computational notebooks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1145/3544548.3580965</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems</title>
		<meeting>the 2023 CHI Conference on Human Factors in Computing Systems<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<biblScope unit="page">2023</biblScope>
		</imprint>
	</monogr>
	<note>1, 2, 3, 8, 9</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Dashboard design mining and recommendation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Qu</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2023.3251344</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">AutoCaption: An approach to generate natural language description from visualization automatically</title>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 2020 IEEE Pacific Visualization Symposium</title>
		<meeting>2020 IEEE Pacific Visualization Symposium</meeting>
		<imprint>
			<biblScope unit="page" from="191" to="195" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title/>
		<author>
			<persName><surname>Ieee</surname></persName>
		</author>
		<idno type="DOI">10.1109/PacificVis48177.2020.1043</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">ChartVi: Charts summarizer for visually impaired</title>
		<author>
			<persName><forename type="first">P</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Chaube</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Shrawankar</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.cola.2022.101107</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Computer Languages</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">NL4DV: A toolkit for generating analytic specifications for data visualization from natural language queries</title>
		<author>
			<persName><forename type="first">A</forename><surname>Narechania</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Stasko</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2020.3030378</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="369" to="379" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Chart-to-Text: Generating natural language descriptions for charts by adapting the transformer model</title>
		<author>
			<persName><forename type="first">J</forename><surname>Obeid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hoque</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 International Conference on Natural Language Generation</title>
		<meeting>the 2020 International Conference on Natural Language Generation<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="138" to="147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">OpenAI. Gpt-4 technical report</title>
		<imprint>
			<biblScope unit="issue">9</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">MEDLEY: Intent-based recommendations to support dashboard composition</title>
		<author>
			<persName><forename type="first">A</forename><surname>Pandey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Setlur</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2022.3209421</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1135" to="1145" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Exploration and explanation in computational notebooks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Rule</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tabard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Hollan</surname></persName>
		</author>
		<idno type="DOI">10.1145/3173574.3173606</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems</title>
		<meeting>the 2018 CHI Conference on Human Factors in Computing Systems<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Com-puting Machinery</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">QueryLines: Approximate query for visual browsing</title>
		<author>
			<persName><forename type="first">K</forename><surname>Ryall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Lesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lanning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Leigh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Miyashita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Makino</surname></persName>
		</author>
		<idno type="DOI">10.1145/1056808.1057017</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2005 CHI Conference on Human Factors in Computing Systems -Extended Abstracts</title>
		<meeting>the 2005 CHI Conference on Human Factors in Computing Systems -Extended Abstracts<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="1765" to="1768" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">TaskVis: Task-oriented visualization recommendation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.2312/evs.20211061</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st Eurographics Conference on Visualization -Short Papers. The Eurographics Association</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Agus</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Garth</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Kerren</surname></persName>
		</editor>
		<meeting>the 21st Eurographics Conference on Visualization -Short Papers. The Eurographics Association</meeting>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Calliope: Automatic visual data story generation from a spreadsheet</title>
		<author>
			<persName><forename type="first">D</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Cao</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2020.3030403</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Shape-Search: A flexible and efficient system for shape-based exploration of trendlines</title>
		<author>
			<persName><forename type="first">T</forename><surname>Siddiqui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Luh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Karahalios</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Parameswaran</surname></persName>
		</author>
		<idno type="DOI">10.1145/3318464.3389722</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 SIGMOD International Conference on Management of Data</title>
		<meeting>the 2020 SIGMOD International Conference on Management of Data<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="51" to="65" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">GVQA: Learning to answer questions about graphs with visualizations via knowledge base</title>
		<author>
			<persName><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems</title>
		<meeting>the 2023 CHI Conference on Human Factors in Computing Systems<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Augmenting visualizations with interactive data facts to facilitate interpretation and communication</title>
		<author>
			<persName><forename type="first">A</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Drucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Endert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Stasko</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2018.2865145</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Sketchpad: A man-machine graphical communication system</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">E</forename><surname>Sutherland</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1963">1963</date>
		</imprint>
		<respStmt>
			<orgName>Massachusetts Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD Thesis</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Getting the design right and the right design</title>
		<author>
			<persName><forename type="first">M</forename><surname>Tohidi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Buxton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Baecker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sellen</surname></persName>
		</author>
		<idno type="DOI">10.1145/1124772.1124960</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2006 CHI Conference on Human Factors in Computing Systems</title>
		<meeting>the 2006 CHI Conference on Human Factors in Computing Systems<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="1243" to="1252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Understanding pen and touch interaction for data exploration on interactive whiteboards</title>
		<author>
			<persName><forename type="first">J</forename><surname>Walny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Johns</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">H</forename><surname>Riche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Carpendale</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2012.275</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Documentation Matters: Human-centered AI system to assist data science code documentation in computational notebooks</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Drozdal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Weisz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Dugan</surname></persName>
		</author>
		<idno type="DOI">10.1145/3489465</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Computer-Human Interaction</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="33" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note>2, 3, 6, 8</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Slide4N: Creating presentation slides from computational notebooks with Human-AI collaboration</title>
		<author>
			<persName><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Neshati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="DOI">10.1145/3544548.3580753</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems</title>
		<meeting>the 2023 CHI Conference on Human Factors in Computing Systems<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Towards natural language-based visualization authoring</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2022.3209357</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1222" to="1232" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Datashot: Automatic generation of fact sheets from tabular data</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2019.2934398</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Sketching a graph to query a time-series database</title>
		<author>
			<persName><forename type="first">M</forename><surname>Wattenberg</surname></persName>
		</author>
		<idno type="DOI">10.1145/634067.634292</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2001 CHI Conference on Human Factors in Computing Systems -Extended Abstracts</title>
		<meeting>the 2001 CHI Conference on Human Factors in Computing Systems -Extended Abstracts<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="381" to="382" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Voyager 2: Augmenting visual analysis with partial view specifications</title>
		<author>
			<persName><forename type="first">K</forename><surname>Wongsuphasawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Moritz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Ouk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Anand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mackinlay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Howe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Heer</surname></persName>
		</author>
		<idno type="DOI">10.1145/3025453.3025768</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems</title>
		<meeting>the 2017 CHI Conference on Human Factors in Computing Systems<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2648" to="2659" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">MultiVision: Designing analytical dashboards with deep learning based recommendation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2021.3114826</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="162" to="172" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">B2: Bridging code and interactive visualization in computational notebooks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Hellerstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Satyanarayan</surname></persName>
		</author>
		<idno type="DOI">10.1145/3379337.3415851</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology</title>
		<meeting>the 33rd Annual ACM Symposium on User Interface Software and Technology<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="152" to="165" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">A design space for applying the freytag&apos;s pyramid structure to data stories</title>
		<author>
			<persName><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="922" to="932" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">FlowSense: A natural language interface for visual data exploration within a dataflow system</title>
		<author>
			<persName><forename type="first">B</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">T</forename><surname>Silva</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2019.2934668</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Readability enhancement for PDF documents</title>
		<author>
			<persName><forename type="first">C.-H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Shelton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Abou Nassif Mourad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Oulal</surname></persName>
		</author>
		<idno type="DOI">10.3389/fcomp.2021.628832</idno>
	</analytic>
	<monogr>
		<title level="j">Frontiers in Computer Science</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">ChartStory: Automated partitioning, layout, and captioning of charts into comic-style narratives</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chandrasegaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bryan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-L</forename><surname>Ma</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2021.3114211</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1384" to="1399" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Telling stories from computational notebooks: Ai-assisted presentation slides creation for presenting data science work</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<idno type="DOI">10.1145/3491102.3517615</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems</title>
		<meeting>the 2022 CHI Conference on Human Factors in Computing Systems<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
