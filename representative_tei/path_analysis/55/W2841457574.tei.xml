<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Beyond Pixels: Image Provenance Analysis Leveraging Metadata</title>
				<funder ref="#_WFCZGx8">
					<orgName type="full">CNPq</orgName>
				</funder>
				<funder>
					<orgName type="full">DARPA</orgName>
				</funder>
				<funder ref="#_kYQJugd">
					<orgName type="full">FAPESP</orgName>
				</funder>
				<funder ref="#_KG5CeP4">
					<orgName type="full">Air Force Research Laboratory</orgName>
					<orgName type="abbreviated">AFRL</orgName>
				</funder>
				<funder ref="#_Vd5Cmtg">
					<orgName type="full">CAPES</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2019-03-06">6 Mar 2019</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Aparna</forename><surname>Bharati</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Notre Dame</orgName>
								<address>
									<region>IN</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Daniel</forename><surname>Moreira</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Notre Dame</orgName>
								<address>
									<region>IN</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Joel</forename><surname>Brogan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Notre Dame</orgName>
								<address>
									<region>IN</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Patricia</forename><surname>Hale</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Notre Dame</orgName>
								<address>
									<region>IN</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kevin</forename><forename type="middle">W</forename><surname>Bowyer</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Notre Dame</orgName>
								<address>
									<region>IN</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Patrick</forename><forename type="middle">J</forename><surname>Flynn</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Notre Dame</orgName>
								<address>
									<region>IN</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Anderson</forename><surname>Rocha</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Campinas</orgName>
								<address>
									<region>SP</region>
									<country key="BR">Brazil</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Walter</forename><forename type="middle">J</forename><surname>Scheirer</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Notre Dame</orgName>
								<address>
									<region>IN</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Beyond Pixels: Image Provenance Analysis Leveraging Metadata</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2019-03-06">6 Mar 2019</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:1807.03376v3[cs.CV]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-29T00:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Creative works, whether paintings or memes, follow unique journeys that result in their final form. Understanding these journeys, a process known as "provenance analysis," provides rich insights into the use, motivation, and authenticity underlying any given work. The application of this type of study to the expanse of unregulated content on the Internet is what we consider in this paper. Provenance analysis provides a snapshot of the chronology and validity of content as it is uploaded, re-uploaded, and modified over time. Although still in its infancy, automated provenance analysis for online multimedia is already being applied to different types of content. Most current works seek to build provenance graphs based on the shared content between images or videos. This can be a computationally expensive task, especially when considering the vast influx of content that the Internet sees every day. Utilizing non-contentbased information, such as timestamps, geotags, and camera IDs can help provide important insights into the path a particular image or video has traveled during its time on the Internet without large computational overhead. This paper 1 tests the scope and applicability of metadata-based inferences for provenance graph construction in two different scenarios: digital image forensics and cultural analytics.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Understanding the story behind a visual object is an activity of broad interest. Whether it is determining the palette used to make a painting, the style of a sculptor, or the authenticity of an artwork, deriving the origin and composition of the object at hand has been a difficult but important task for many examiners. Subtle clues derived from the nature of works of art have long been used to provide answers to provenance related questions <ref type="bibr" target="#b7">[8]</ref>. Off-white colors The examples in this case are meme-style images similar to the ones from the photoshopbattles community on the Reddit social media site <ref type="bibr" target="#b58">[60]</ref>. The transformations can be as simple as increasing the brightness or as complex as multi-composition. In this paper, we consider the incorporation of meta-data to improve the construction of such graphs.</p><p>found in the painting Darby and Joan by Laurence Stephen Lowry brought into question its authenticity <ref type="bibr" target="#b9">[10]</ref>. Lead content in the paint of Danseuse Bleue et Contrebasses and careful scrutiny of the painter's signature allowed experts to rightly restore the validity of Edgar Degas's most famous work <ref type="bibr" target="#b8">[9]</ref>. Provenance analysis of this sort has helped historians, cultural analysts and art enthusiasts to analyze the origin, content and growth of works such as these. Although the techniques used to perform provenance analysis have evolved over time <ref type="bibr" target="#b24">[25]</ref>, it is, in general, still an unsolved problem <ref type="bibr" target="#b61">[63]</ref>. In the domain of art history, it is one of the most active and important areas of research <ref type="bibr" target="#b62">[64]</ref> as there are still complicated cases where provenance has yet to be established (e.g., the painting Bords de la Seine à Argenteuil <ref type="bibr" target="#b50">[51]</ref>) and new avenues for the interpretation of relationships between artworks. The above case studies might lead one to believe that provenance analysis is a tool to decipher events far in the past. On the contrary, with the growth in popularity of online digital media, the need for provenance analysis has never been more timely. Current social sentiment can often only be fully understood within the context of online memes and other viral movements <ref type="bibr" target="#b57">[59]</ref>. Further, as the lines between real and fake images blur, the extent to which these types of online phenomena can be deployed towards the deception of the public has become deeply concerning <ref type="bibr" target="#b31">[32]</ref>. With high quality cameras and image editing software at anybody's disposal, photographs have become easier to forge than paintings or sculptures. We have reached a point where digital forgeries can be produced with finegrained detail, down to photographic style and sensor noise <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b43">44]</ref>. These advancements in anti-forensics undermine the content's credibility, ownership, and authenticity. The current scale at which images and videos are shared requires an automated way of answering such questions.</p><p>Image processing and computer vision techniques can be employed to detect correspondences between images or other digital art forms <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b66">68]</ref>. This kind of correspondence can range from object matching in images <ref type="bibr" target="#b45">[46]</ref> to comparing the style <ref type="bibr" target="#b28">[29]</ref> and semantics <ref type="bibr" target="#b55">[57]</ref> of the two. Provenance analysis can be thought of as ordering pair similarities between multiple image pair sets, and is therefore a natural extension to pairwise image comparison. These subsequent ordered parings can be modeled as a graph, where each edge denotes a correspondence between a pair, and the end vertices of the edge signify the two respective images. An example of such a graph can be seen in Figure <ref type="figure" target="#fig_0">1</ref>. This example shows that a provenance analysis algorithm could be analyzing multiple very close-looking realistic versions of the same visual object. Complex scenarios like this can make content-based similarity metrics unreliable.</p><p>Due to the vast range of possible versions of a single original image, the metrics for quantifying the similarity between pairs of images can be noisy. Relying solely upon visual cues to order the different versions into a graph can result in poor provenance reconstructions <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b51">52]</ref>. Therefore, it becomes pertinent to utilize other sources of data to determine connections. For example, it is difficult to point out a semantic difference between the two images in Figure <ref type="figure" target="#fig_1">2</ref>, but the images can be differentiated by inspecting the metadata of the image files. Such a pair of images can be termed semantically similar, as they are related to each other in a semantic way but do not originate from the same source <ref type="bibr" target="#b54">[56,</ref><ref type="bibr" target="#b10">11]</ref>. Matching difficulty can also arise within sets of near-duplicate images, which are generated from a single origin having undergone a series of transformations (e.g., crop→saturate→desaturate). The pixel-level data within these image sets can exhibit ambiguous provenance directionality. Information beyond pixel-level data may be required to detect differences between such images.</p><p>To handle scenarios where image content fails to explain  Right: Photo of the replica of the monument in Las Vegas taken at night. Note that both photos depict the same visual object -only the image file metadata in this case can help us understand that they are completely different scenes. Photos and their metadata were obtained from Flickr <ref type="bibr" target="#b12">[13]</ref> and Wikimedia Commons <ref type="bibr" target="#b64">[66]</ref>.</p><p>image evolution, file metadata can be used to help fill in the gaps. In this work, we explore the use of commonly present file metadata tags to improve image provenance analysis. We compare these results against image contentbased methods and highlight the advantages and disadvantages of both.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Provenance analysis is a widely known and studied phenomenon in various data-based domains such as the semantic web and data warehousing <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b60">62]</ref>. However, provenance analysis for online multimedia has not been as extensively studied in the existing literature. The types of work most relevant and related to the problem of image provenance analysis come from three established concepts in the digital forensics literature: near-duplicate detection <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b40">41]</ref>, image splicing detection <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b14">15]</ref> and image phylogeny <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b21">22]</ref>. Most of the proposed methods work towards classifying whether an image is a near-duplicate of the query image in a retrieval context and do not determine the original image among the set of the near-duplicates. However, that particular problem has been studied by the image phylogeny community.</p><p>Image phylogeny solutions aim at finding kinship relations between different versions of an image <ref type="bibr" target="#b23">[24]</ref>. Similar to provenance analysis, image phylogeny limits its representation to a single-root tree with the original image as the root, even though there can be multiple original images contributing towards the creation of an image. The algorithm receives a query image and outputs the Image Phylogeny Tree (IPT). That method has also been extended to handle multiple (two) roots by taking spliced images into consideration <ref type="bibr" target="#b54">[56]</ref>. An example of this multiple parent scenario can be observed in Figure <ref type="figure" target="#fig_0">1</ref> where four images (donors) contribute to the content of the central composite image. A constraint of these image phylogeny approaches that solve very specific cases of image provenance analysis is that they have dealt with constrained datasets using a limited set of transformations and image formats <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b21">22]</ref>. In addition to that, most of them only consider two images to form a composite, thereby limiting the solutions for large-scale general applicability. Thus new image provenance algorithms must generalize and be evaluated across different forgery datasets, image transformations, file formats and image resolutions to be applicable in real-world situations.</p><p>As a step towards a more general framework for image provenance analysis inspired by image phylogeny works, recent work on undirected provenance graph construction <ref type="bibr" target="#b10">[11]</ref> adopted a more general taxonomy and dataset proposed by the American National Institute of Standards and Technology (NIST) <ref type="bibr" target="#b53">[55]</ref>. It offered the U-phylogeny pipeline as a preliminary approach towards solving provenance analysis, which is not restricted to either a closed set of image transformations, or the number of donor images to form multi-parent composites. Results are presented for scenarios with and without the presence of distractors (images that are not related to the provenance history of the query image) showing the approach to be tolerant to irrelevant images. A limitation of the U-phylogeny approach is that it does not provide a directed provenance graph, which is required to understand the evolution of the media object.</p><p>In order to overcome the direction limitation and propose a scalable approach, a more complete end-to-end pipeline for image provenance analysis was described in <ref type="bibr" target="#b51">[52]</ref>. That method for graph construction first builds dissimilarity matrices based on local image features, and then employs hierarchical clustering to group nodes and draw edges within the final provenance graph. As stated in Section 1, relying solely on image content can lead to noisy edge inference. This is especially true for directed edges, which have been shown to be more difficult to derive than undirected edges <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b51">52]</ref>. An option for addressing this is the use of metadata related to the images. File metadata has been predominantly used for data and software provenance analysis <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b29">30]</ref>, as such information reveals important clues about a file that cannot be directly derived from the data. Secondly, metadata related to online posts which include images can also be utilized for this purpose.</p><p>In the image domain, metadata often stores information regarding the device used to capture the image and the software used to process the image. Information provided by these types of tags has been utilized to improve the effec-tiveness of tasks such as image grouping <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b44">45]</ref>, contentbased image retrieval <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b65">67]</ref>, photo classification <ref type="bibr" target="#b13">[14]</ref>, image annotation <ref type="bibr" target="#b39">[40]</ref> and copyright protection <ref type="bibr" target="#b32">[33]</ref>. Among these, algorithms establishing semantic correspondences between images, such as automatic grouping or classification, may utilize tags such as date, location, content originator, camera type and scene type <ref type="bibr" target="#b34">[35]</ref> whereas those that detect tampering may rely on detecting inconsistencies within the values of these and other tags containing source and copyright information <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b32">33]</ref>. While metadata has been successfully used for forensics tasks in the past <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b47">48]</ref>, it has not been used for provenance analysis before.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Approach</head><p>Image provenance analysis algorithms aim at constructing a provenance graph with related images, given a query image. The provenance graph <ref type="bibr" target="#b51">[52]</ref> is a Directed Acyclic Graph (DAG) where each node corresponds to an image in the set of related images and the edges stand for the relationship of sharing duplicate content. The direction of an edge denotes the direction of the flow of content between each pair of images and the overall provenance case. In this section, we explain in detail each of the three stages (as seen in Figure . 3) of image provenance analysis used in the proposed approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Filtering</head><p>The first step required to perform provenance analysis for a given query image involves collecting the set of top-k relevant images. In this work, we follow the solution proposed in <ref type="bibr" target="#b51">[52]</ref>, which selects a subset of a large source of images (such as millions of images from the Internet), whose elements include samples sharing full content with the query with slight modifications (i.e., nearduplicate images), samples sharing partial content with the query in any form (single or multiple foreground objects, or background), and samples transitively related to the query (e.g., near duplicates of the images sharing content with the query). In summary, this solution utilizes Optimized Product Quantization (OPQ) to store local Speeded-Up Robust Features (SURF) <ref type="bibr" target="#b6">[7]</ref> in an Inverted File index (IVF), with a large number (e.g., ∼400k) of representative centroids. Each image is described through at most 5k SURF features, which are fed to constitute the IVF index. To search the index, multi-stage query expansion is utilized. The first stage mainly retrieves the hosts, while the second stage retrieves donors; further stages retrieve the images transitively related to the query, by replacing the original query with samples retrieved in the previous stages. The 'k' most relevant images are selected for pairwise image comparison. This step is not present in an oracle scenario where we assume to have been provided with the perfectly correct set of 'k' related images. The images are compared in terms of visual content and metadata, yielding two types of adjacency matrices. The obtained matrices are then combined in the graph construction step to form an IPG.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Adjacency Matrix Computation</head><p>Upon receiving the set of top-k related images, denoted by R, to the query image Q, we build N × N (here, N = |R| + 1) adjacency matrices D, in which each indexed value D[i, j] is the similarity (or dissimilarity) quotient between images i and j. The full matrices are obtained by comparing (n 2 -n)/2 pairs. Different from previous work, though, besides using a matrix that relies solely on visual content, we propose the employment of an additional metadata-based asymmetric adjacency matrix that is used to determine the orientation of the pairwise image relations. To the best of our knowledge, this is the first work proposing a way to leverage metadata to complement visual information for the problem of provenance analysis.</p><p>For visual comparison, the images can be described using interest point detectors and descriptors (such as SURF <ref type="bibr" target="#b6">[7]</ref>) or learned from data using a Convolutional Neural Network. Image description for provenance analysis typically avoids using computationally expensive methods such as deep learning because of scalability concerns <ref type="bibr" target="#b66">[68]</ref>. An empirical evaluation we conducted comparing SURF <ref type="bibr" target="#b6">[7]</ref> and ShuffleNet <ref type="bibr" target="#b67">[69]</ref>, one of the most efficient deep learning frameworks, highlights this. Ignoring training time, Shuf-fleNet took 3.5 minutes to describe 10k images using two Nvidia Quadro GPUs, while SURF (a C++ implementation) took 39 seconds for the same images using one GPU. This motivates the usage of SURF-based detection and description of keypoints for the visual comparison between images. Once the images are described, for each image pair, the p most relevant interest points of each image are matched using brute-force pairwise comparison based on the L2 distance between the descriptors. The best matched correspondences are filtered to retain the geometrically consistent ones, as described in <ref type="bibr" target="#b10">[11]</ref>. As a consequence, a symmetric adjacency matrix is obtained with the quantity of matched interest points between each pair of images.</p><p>Commonly, the value of mutual information is used as the degree of pairwise association between images, or as asymmetric weights of the edges in a complete graph among the N images, with no self loops <ref type="bibr" target="#b51">[52]</ref>. In this work, in order to incorporate metadata information at this stage, we introduce a heuristic-based normalized voting to attribute weights to each pairwise image relationship. The voting method is chosen as a complement to the similarity comparison in the visual domain. The heuristics used to obtain the scores for each pair are straightforward metadata-related assumptions in the context of image provenance and rely upon the content of the tags. They include:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Date.</head><p>To check for the temporal order of content creation, we individually compare the date-related tags -DateTimeOriginal, ModifyDate and CreateDate. Considering two images i and j, for each one of the three dates, whenever available, the provenance relation (i, j) gets one vote if the date of image i is earlier or equal than the respective date of image j. The relationship in the opposite direction (j, i) is also analogously evaluated.</p><p>Location. Near-duplicates of an image (e.g., cropped versions) should have the same geographic location as the original one. Hence, we cast one vote for the pairwise image relationship (i, j), and one vote for the relationship (j, i), if image i shares with image j exactly the same non-null values for the four location-related tags -GPSLatitude, GPSLatitudeRef, GPSLongitude, GPSLongitudeRef. Although this does not help to define the direction of the provenance between images i and j, since both (i, j) and (j, i) relationships get one vote, it does help to give them more weight than the other image pairs that do not share location content. In addition, in very complex image compositions where there is not a clear presence of a foreground donor, the location-related metadata tags might be null or missing, contrary to the donors of the composition. Thus, we alternatively cast one vote to the relationship (i, j), if image i has non-null location information and image j is missing it.</p><p>Camera. We propose to use camera-based metadata information in a way that is analogous to the location case. If image i and image j share the same non-null content for the camera's Make, Model and Software tags simultaneously, we cast one vote for both the (i, j) and (j, i) relationships, suggesting near-duplication that maintained image metadata. Similarly, we cast one vote to (i, j) if image i has camera information and image j does not.</p><p>Editing. We use the editing-related metadata tags to figure out if either image i or image j were ever manipulated. Given that the provenance direction might occur from a non-manipulated to a manipulated image, we give one vote to the relationship (i, j) if image j has information for any of the ProcessingSoftware, Artist, HostComputer, ImageResources. The relationship in the opposite direction (j, i) is also evaluated in the same manner.</p><p>Thumbnail. We extract the respective thumbnails of images i and j. If the thumbnails are exactly the same, both relationships (i, j) and (j, i) get one vote, since it means one image might be generated from the other. Alternatively, if image i has a thumbnail and image j does not have one, then the relationship (i, j) gets one vote, indicating that image i is probably the original one.</p><p>These heuristics are used to generate a metadata-based image pairwise adjacency matrix M . For instance, taking images i and j and the possible provenance relationship from i to j, whenever a heuristic is satisfied, the respective value M [i, j] is increased in one unity, meaning the cast of one vote to the (i, j) relationship.</p><p>Aiming to keep the solution as widely applicable as possible, the tags are selected based on their availability and relevance to the provenance problem. An example of such relevance has been shown in Figure <ref type="figure">4</ref>. It depicts an image pair example that is directionally ambiguous. After performing interest-point-based pairwise analysis between the two images in Figure <ref type="figure">4</ref>(a), a valid argument for either a splicing (left-to-right edge) or removal (right-to-left edge) operation between the two could be made. Utilizing the "DateTimeOriginal" tag from both images disambiguates Figure <ref type="figure">4</ref>. Usage of metadata information for determining direction in image pairwise provenance relationships. In (a), the output of interest-point-based analysis between two images is shown. The operation can be either a splicing or removal of the male lion. In (b), according to the date-based metadata, the operation is revealed to be a splice, since the image on the left is older.</p><p>the relationship, revealing that the lion was indeed spliced into the image at a later time. While a large array of metadata tags are often present in many images, only a small subset of these tags provide pertinent information useful for discerning inter-image relationships. Furthermore, using tags provided by only specific camera firmwares or only applicable for certain formats (e.g., JPEG) reduces the generalizability of the proposed approach. The tags mentioned here are EXIF tags (details provided in supplemental material) but the information provided by their values is what holds relevance to provenance. In case EXIF metadata is missing or tampered, information provided by online image posts, such as date of submission, can also be utilized in a similar way.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Graph Construction</head><p>Based on the values of the adjacency matrix, the final graph construction step chooses the most feasible set of directed edges (i.e., the set of edges that best represents the sequence of image operations). Each chosen directed edge denotes a parent-child relationship in the graph.</p><p>Once the vision-based and metadata-based adjacency matrices are available, one can either individually use them to directly generate a provenance graph, through, for example, the application of Kruskal's Maximum Spanning Tree (MST) algorithm <ref type="bibr" target="#b42">[43]</ref>, or, as we are proposing, use a specialized algorithm for constructing a directed provenance graph, such as clustered provenance graph expansion, proposed in <ref type="bibr" target="#b51">[52]</ref>. In the latter case, we suggest using the metadata-based asymmetric adjacency matrix to determine the directions of the edges. In the experiments herein re-ported, we investigate both strategies. In the end, the output graph can be represented as a binary adjacency matrix (BAM). BAM[i, j] is set to 1 whenever there is an edge between images i and j, indicating i → j flow of content.</p><p>Understandably, none of the proposed rules guarantee correct inference as metadata can be manipulated, wrong or missing. Using multiple tags reduces the impact of an incorrect inference and makes the process more robust. To mitigate circumstances where file metadata is unavailable, we demonstrate provenance in an online setting in our experiments using an alternative approach that can harvest metadata from website users' comments as opposed to the file itself. In both scenarios, the proposed approach is designed to tolerate events such as data tampering. As the metadatacompliance score is a cumulative score metric, each rule and the corresponding tags contribute to the value used to make the edge decision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Setup</head><p>Here we detail the two evaluation scenarios and describe the characteristics of the corresponding datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Provenance Analysis for Digital Forensics</head><p>NIST has recently released a dataset curated for the tasks of provenance image filtering and graph construction in a forensics context, which is devoid of most of the limitations of the existing datasets. Similar to the experimental setup described in <ref type="bibr" target="#b51">[52]</ref>, we rely on the development partition of this dataset since it provides a full set of groundtruth graphs. Named NC2017-Dev1-Beta4, the dataset contains 65 queries, and the ground-truth is released in the form of journals depicting provenance graphs. The provenance graph journals were created manually with the help of a proprietary image-editing journaling tool. The graphs include links corresponding to simple image transformations such as cropping, scaling, sharpening, blurring, and rotation, to complex ones such as splicing from multiple sources and object removal. The total number of related images per case ranges from <ref type="bibr" target="#b1">[2,</ref><ref type="bibr">81]</ref>. In addition to the images relevant to the provenance of each of the query images, the dataset also contains distractors (i.e., images not related to any query).</p><p>Following the protocol proposed by NIST [54], we perform both end-to-end and oracle-filter provenance analysis over this dataset. End-to-end analysis requires performing provenance filtering prior to graph construction <ref type="bibr" target="#b56">[58]</ref>. In this case, for each query image, graphs are built upon a list of ranked images that might include distractors and miss genuinely related images due to imperfect image filtering. To obtain these filtered image rank lists, we employ the best solution proposed in <ref type="bibr" target="#b51">[52]</ref> and retrieve the top-100 ranked images to the query, which may contain unrelated distractors. Conversely, the oracle analysis does not require a filtering step, but instead starts with perfect ranks, i.e., ranks containing all the relevant images and no distractors.</p><p>Orthogonal to the end-to-end versus oracle comparison, we also compare results for both metadata only and visual + metadata solutions. When using only metadata, we compute the vote-based metadata adjacency matrix, as explained in Section 3.2. We use ExifTool <ref type="bibr" target="#b30">[31]</ref> to perform file metadata extraction. A table listing the tags used and their details has been provided in the supplemental material. Once the adjacency matrix is computed, we apply Kruskal's maximum spanning tree algorithm <ref type="bibr" target="#b42">[43]</ref> to obtain the final provenance graph.</p><p>For fused metadata and visual solutions, we start with visual content-based adjacency matrices, which are generated according to the method explained in Section 3. We perform two different computations, one based on SURF <ref type="bibr" target="#b6">[7]</ref> and the other based on Maximally Stable Extremal Regions (MSER) <ref type="bibr" target="#b49">[50]</ref>. Both solutions were proposed and evaluated in <ref type="bibr" target="#b51">[52]</ref>, hence we follow their pipeline: (1) extraction of at most 5k interest points (either with SURF or MSER), (2) computation of adjacency matrices based on the number of geometrically consistent interest-point matches, (3) computation of adjacency matrices based on mutual information, and (4) application of the cluster-based method for generating provenance graphs. For combining visual content and metadata, we proceed as suggested in Section 3: within the cluster-based algorithm, in the step of establishing the directions of edges, instead of using the mutual-informationbased adjacency matrix <ref type="bibr" target="#b51">[52]</ref>, we consider the metadatabased one and keep the directions with more votes.</p><p>The provenance graphs generated using the proposed approach for both oracle and end-to-end scenarios are evaluated using the metrics proposed by NIST for the provenance task <ref type="bibr" target="#b53">[55]</ref>. The metrics focus on comparing the nodes and edges from both ground-truth and candidate graphs. The corresponding measures of Vertex Overlap (VO) and Edge Overlap (EO) are the harmonic mean of precision and recall (F1 score) for the nodes and edges retrieved by our method. In addition to these, a unified metric representing one score for the graph overlap namely the Vertex Edge Overlap (VEO) is also reported. The VEO is the combined F1 score for nodes and edges. All the metrics are computed through the NIST MediScore tool <ref type="bibr" target="#b52">[53]</ref>. The values of these metrics lie in the range [0, 1] where higher values are better.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Provenance Analysis for Cultural Analytics</head><p>To include experiments with more realistic examples, we also evaluate the approaches from Section 3 on the Reddit dataset introduced in <ref type="bibr" target="#b51">[52</ref>] and maintained at <ref type="bibr">[20]</ref>. This dataset contains provenance cases created from images extracted from the photoshopbattles community on the Reddit website <ref type="bibr" target="#b58">[60]</ref>. This community provides a platform for users to experiment with image manipulation in a friendly context. Each thread begins with a single image submitted by one user, which serves as the base image for the manipu-Table <ref type="table" target="#tab_3">1</ref>. Results of provenance graph construction over the NIST NC2017-Dev1-Beta4 dataset. We report the mean and the standard deviation for the metrics on the provided 65 queries. Visual results are from Moreira et al. <ref type="bibr" target="#b51">[52]</ref>. Best results are in bold. lations of others, whose contributions appear as comments on the original post. For the purpose of provenance, Moreira et al. <ref type="bibr" target="#b51">[52]</ref> utilize this comment structure to obtain 184 provenance graphs with an average graph order of 56. For the sake of fair comparison, we evaluate the variants of the proposed approach on the exact same set. The full set of images from Reddit do not contain distractors. This restricts our experiments for provenance analysis in this setting to oracle-filter analysis only, in contrast to the NC2017-Dev1-Beta4 dataset. Since the images in the Reddit dataset are collected from the web, the availability of metadata is restricted by the policies of the Reddit website and image hosting services, such as imgur.com <ref type="bibr" target="#b35">[36]</ref>. For that reason, the metadata extraction through ExifTool <ref type="bibr" target="#b30">[31]</ref> does not deliver useful tags for provenance analysis. As an alternative, we use the Reddit users' comments and posts to estimate the date and time of image uploads, thus treating them as DateTimeOriginal values, making it possible to invoke the date-based heuristics.</p><p>Here, one important comment can be made about the restricted availability of metadata and the apparent limited possibility of application of the present solution. Although metadata might not be available to the general public, image hosting websites might still be storing them, hence being able to apply the method in their headquarters or under legal demand. Other image hosting websites such as Flickr and Picasa can be used as image sources that preserve metadata tags, but they do not provide structured information for provenance ground-truth extraction. This promotes Reddit as a choice for obtaining graphs and evaluating provenance in a cultural setting. To evaluate our experimental results on the Reddit dataset, we employ the same metrics and scorer used in the case of the NC2017-Dev1-Beta4 dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experimental Results</head><p>The experiments performed on both datasets show that utilizing knowledge from metadata helps in the process of edge inference for provenance. As it can be observed from the values reported in Table <ref type="table" target="#tab_3">1</ref>, the proposed method significantly improves total edge overlap, and thereby total graph overlap, since it uses image-content-based information to initially establish connections between images, then relies on metadata to refine edge direction. The tags and checks used in this work yield an edge overlap of 44.5% and graph overlap (VEO) of ∼70% for provenance in the oracle scenario, improving notably over current state-of-the-art <ref type="bibr" target="#b51">[52]</ref> by ∼15 percentage points (pp). More notably, metadata fusion provides a ∼30pp increase in EO in the oracle cases, when compared to <ref type="bibr" target="#b51">[52]</ref>.</p><p>In the end-to-end scenario, metadata usage also shows improvements in edge overlap by ∼3pp, aiding the overall graph overlap to reach &gt;60%. Provenance analysis solutions thus far have struggled at obtaining good edge reconstruction, as can be seen from the disparity between the vertex and edge overlap. Furthermore, the addition of distractors reduces performance by ∼5pp, implying that semantically similar images within the distractor sets can lead to high inter-image similarity between pairs that should not be related. This can negatively impact greedy graph construction approaches. Some success and failure provenance cases are presented in the supplemental material, including the graph visualizations.</p><p>To understand the contribution of each type of metadata information, we conduct an ablation study on the oracle and end-to-end scenarios using the Visual+Metadata, Cluster-SURF method from Section 4.1. We perform the experiment seven times, for each scenario, using only a subset of heuristics for each run. Results are presented in Table <ref type="table" target="#tab_2">2</ref>. In the oracle scenario, while all five tags individually benefit graph EO, the date-based one performs best, followed by thumbnail usage. For that reason, we also present, in the last two rows of Table <ref type="table" target="#tab_2">2</ref>, the results of having all heuristics combined except for date (to assess the impact of avoiding the best one), as well as combination of date and thumbnail (the two best ones). Indeed, the date-based heuristic alone slightly surpasses the combination of heuristics, in this particular dataset and scenario. In the end-to-end scenario, in turn, observations are somewhat different. Metadata tags alone do not improve the results of the visual solution, except for date and the date-thumbnail fusion, with the latter showing the best results. Again, this might be particular to the dataset, where the added distractors probably present more unreliable metadata (due to tampering or removal). That reveals the importance of combining tags, since it leads to a more robust solution to metadata tampering. Provenance analysis becomes significantly more difficult when dealing with real-world scenarios, such as those presented in the Reddit dataset. Although metadata doubles the number of correctly retrieved edges, as seen in Table <ref type="table" target="#tab_4">3</ref>, the edge overlap is still much lower than for the NC2017-Dev1-Beta4 dataset. In the Reddit cases, images can be connected by visual puns, inside jokes, and purely associative content without any direct visual correspondence between them. This is very common in meme-style imagery. Understanding the quirks and sentiments of human language can further help provenance analysis in these contexts, but it has not yet been explored. To perform complex relationship retrieval using image provenance analysis, input from other modalities, such as text comments, may be required.</p><p>Since all experiments calculate initial correspondences using only visual image content, the purely visual method and visual + metadata based method perform identically with respect to VO. This metric is generally high with a low standard deviation whereas the EO has very high standard deviation. Due to the vast range of possible transformations, the provenance analysis approaches are not able to detect and map certain image relationships as well as others. The results of the experiments for both scenarios show that SURF detections for image matching are better than MSER detections, which is consistent with the results in <ref type="bibr" target="#b51">[52]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Discussion</head><p>Image metadata is a valuable asset for improving results for vision-based problems such as image retrieval <ref type="bibr" target="#b59">[61]</ref>, semantic segmentation <ref type="bibr" target="#b4">[5]</ref>, and manipulation detection <ref type="bibr" target="#b33">[34]</ref>. Our work demonstrates that the task of image provenance analysis also benefits from metadata. External context can corroborate evidence from purely visual techniques, creating an overall better solution to provenance graph reconstruction.</p><p>In addition to utilizing information that cannot be derived from the images themselves, metadata-based approaches are computationally very cheap. Furthermore, unlike complex, data-driven, vision-based techniques that require large amounts of training resources, methods like ours require no training at all. Such methods can be deployed easily on a large scale, incurring very little performance overhead. Approaches that require large amounts of training data can suffer due to the relatively small sizes of currently available provenance datasets. And most datasets published in this field so far are indeed small.</p><p>Even though external information can improve imagebased approaches, provenance analysis is still far from being solved. This work only presents a preliminary exploration of utilizing metadata in provenance analysis. While our results show improvement, metadata-based approaches have higher chances of being rendered unreliable due to their absence or manipulation. Further advancements in solving the problem must focus on the examination of content-derived metadata as well. Future work could include estimating missing metadata information from the content and available tags <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b63">65]</ref>. For now, our findings suggest that image-content-based methods should be the fallback option, as metadata alone is more useful for determining edge directions instead of edge selection. We surmise that going forward, the best provenance approaches should rely primarily on image content, but utilize metadata analysis as a secondary refinement system in scenarios where it is present and provides ample evidence.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Example of an Image Provenance Graph (IPG) showing some common operations performed on images and how they are manifested in the case of provenance. The examples in this case are meme-style images similar to the ones from the photoshopbattles community on the Reddit social media site<ref type="bibr" target="#b58">[60]</ref>. The transformations can be as simple as increasing the brightness or as complex as multi-composition. In this paper, we consider the incorporation of meta-data to improve the construction of such graphs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Left: Photo of the Eiffel Tower taken at night in Paris.Right: Photo of the replica of the monument in Las Vegas taken at night. Note that both photos depict the same visual object -only the image file metadata in this case can help us understand that they are completely different scenes. Photos and their metadata were obtained from Flickr<ref type="bibr" target="#b12">[13]</ref> and Wikimedia Commons<ref type="bibr" target="#b64">[66]</ref>.</figDesc><graphic coords="2,417.48,221.32,93.29,56.79" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Stages of image provenance analysis. The proposed method starts with filtering images related to the provided query image Q.The 'k' most relevant images are selected for pairwise image comparison. This step is not present in an oracle scenario where we assume to have been provided with the perfectly correct set of 'k' related images. The images are compared in terms of visual content and metadata, yielding two types of adjacency matrices. The obtained matrices are then combined in the graph construction step to form an IPG.</figDesc><graphic coords="4,50.11,72.00,495.00,175.59" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Ablation results for oracle and end-to-end provenance. We repeat the experiments seven times for the best solution presented in</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 (</head><label>1</label><figDesc>Visual + metadata, Cluster-SURF) in both scenarios, keeping only a subset of heuristics activated at a time. Best results in bold.</figDesc><table><row><cell>Heuristic</cell><cell></cell><cell>Oracle Filtering</cell><cell></cell><cell></cell><cell>End-to-End Analysis</cell><cell></cell></row><row><cell></cell><cell>VO</cell><cell>EO</cell><cell>VEO</cell><cell>VO</cell><cell>EO</cell><cell>VEO</cell></row><row><cell>Date only</cell><cell>0.931±0.075</cell><cell>0.446±0.265</cell><cell>0.700±0.147</cell><cell>0.853±0.157</cell><cell>0.389±0.244</cell><cell>0.630±0.169</cell></row><row><cell>Location only</cell><cell>0.931±0.075</cell><cell>0.394±0.282</cell><cell>0.674±0.154</cell><cell>0.853±0.157</cell><cell>0.348±0.241</cell><cell>0.611±0.164</cell></row><row><cell>Camera only</cell><cell>0.931±0.075</cell><cell>0.388±0.269</cell><cell>0.672±0.147</cell><cell>0.853±0.157</cell><cell>0.350±0.234</cell><cell>0.612±0.164</cell></row><row><cell>Editing only</cell><cell>0.931±0.075</cell><cell>0.396±0.281</cell><cell>0.675±0.153</cell><cell>0.853±0.157</cell><cell>0.353±0.237</cell><cell>0.613±0.163</cell></row><row><cell>Thumbnail only</cell><cell>0.931±0.075</cell><cell>0.411±0.285</cell><cell>0.683±0.155</cell><cell>0.853±0.157</cell><cell>0.363±0.238</cell><cell>0.618±0.167</cell></row><row><cell>All but Date</cell><cell>0.931±0.075</cell><cell>0.394±0.280</cell><cell>0.675±0.152</cell><cell>0.853±0.157</cell><cell>0.345±0.247</cell><cell>0.610±0.168</cell></row><row><cell>Date + Thumbnail</cell><cell>0.931±0.075</cell><cell>0.444±0.268</cell><cell>0.699±0.148</cell><cell>0.853±0.157</cell><cell>0.391±0.245</cell><cell>0.632±0.169</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .</head><label>3</label><figDesc>Results of provenance graph construction over the Reddit dataset. We report the average values of the metrics over the 184 cases, as well as the standard deviations. This dataset only allows us to report oracle-filtering results. Visual results are from Moreira et al.<ref type="bibr" target="#b51">[52]</ref>. Best results are in bold.</figDesc><table><row><cell>Solution</cell><cell>VO</cell><cell>EO</cell><cell>VEO</cell></row><row><cell>Visual [52]:</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Cluster-SURF</cell><cell>0.757±0.341</cell><cell>0.037±0.034</cell><cell>0.401±0.181</cell></row><row><cell>Cluster-MSER</cell><cell>0.509±0.388</cell><cell>0.027±0.034</cell><cell>0.271±0.207</cell></row><row><cell>Metadata:</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Kruskal</cell><cell>0.969±0.073</cell><cell>0.034±0.086</cell><cell>0.506±0.056</cell></row><row><cell>Visual + Metadata:</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Cluster-SURF</cell><cell>0.757±0.341</cell><cell>0.085±0.065</cell><cell>0.424±0.193</cell></row><row><cell>Cluster-MSER</cell><cell>0.509±0.388</cell><cell>0.061±0.063</cell><cell>0.288±0.220</cell></row></table></figure>
		</body>
		<back>

			<div type="funding">
<div> <ref type="bibr" target="#b0">1</ref> <p>This material is based on research sponsored by <rs type="funder">DARPA</rs> and <rs type="funder">Air Force Research Laboratory (AFRL)</rs> under agreement number <rs type="grantNumber">FA8750-16-2-0173</rs>. Hardware support was generously provided by the <rs type="institution">NVIDIA Corporation</rs>. We also thank the financial support of <rs type="funder">FAPESP</rs> (Grant <rs type="grantNumber">2017/12646-3</rs>, <rs type="projectName">DéjàVu</rs> Project), <rs type="funder">CAPES</rs> (<rs type="grantName">DeepEyes Grant</rs>) and <rs type="funder">CNPq</rs> (Grant <rs type="grantNumber">304472/2015-8</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_KG5CeP4">
					<idno type="grant-number">FA8750-16-2-0173</idno>
				</org>
				<org type="funded-project" xml:id="_kYQJugd">
					<idno type="grant-number">2017/12646-3</idno>
					<orgName type="project" subtype="full">DéjàVu</orgName>
				</org>
				<org type="funding" xml:id="_Vd5Cmtg">
					<orgName type="grant-name">DeepEyes Grant</orgName>
				</org>
				<org type="funding" xml:id="_WFCZGx8">
					<idno type="grant-number">304472/2015-8</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A graph model of data and workflow provenance</title>
		<author>
			<persName><forename type="first">U</forename><surname>Acar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Buneman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cheney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Van Den Bussche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kwasnikowska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vansummeren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">USENIX Workshop on the Theory and Practice of Provenance</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Content-based image retrieval in radiology: current status and future directions</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">B</forename><surname>Akgül</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Rubin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Napel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">F</forename><surname>Beaulieu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Greenspan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Acar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Springer Journal of Digital Imaging</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="208" to="222" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Using extended file information (exif) file headers in digital evidence analysis</title>
		<author>
			<persName><forename type="first">P</forename><surname>Alvarez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Digital Evidence</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Provenance browser: Displaying and querying scientific workflow provenance graphs</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Anand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bowers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ludäscher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Data Engineering</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1201" to="1204" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Geosemantic segmentation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ardeshir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Malcolm Collins-Sibley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2792" to="2799" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Blurred image splicing localization by exposing blur type inconsistency</title>
		<author>
			<persName><forename type="first">K</forename><surname>Bahrami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Kot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Forensics and Security</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="999" to="1009" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Speeded-up robust features (SURF)</title>
		<author>
			<persName><forename type="first">H</forename><surname>Bay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Image Understanding</title>
		<imprint>
			<date type="published" when="2008-06">June 2008</date>
			<biblScope unit="volume">110</biblScope>
			<biblScope unit="page" from="346" to="359" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title/>
		<author>
			<persName><surname>Bbc One</surname></persName>
		</author>
		<ptr target="https://www.bbc.co.uk/programmes/b01mxxz6" />
		<imprint>
			<biblScope unit="page" from="6" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><surname>Bbc One</surname></persName>
		</author>
		<ptr target="https://www.bbc.co.uk/programmes/p00xym5k" />
		<title level="m">Fake or Fortune? Degas and the Little Dancer</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><surname>Bbc One</surname></persName>
		</author>
		<ptr target="https://www.bbc.co.uk/programmes/p02whms0" />
		<title level="m">Fake or Fortune? Lowry</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">U-phylogeny: Undirected provenance graph construction in the wild</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bharati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Moreira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pinto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Brogan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Bowyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Scheirer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rocha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Image Processing</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Digital image forgery detection using passive techniques: A survey</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">K</forename><surname>Birajdar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">H</forename><surname>Mankar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>Elsevier Digital Investigation</publisher>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="226" to="245" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Bon</surname></persName>
		</author>
		<idno>06-25-2018</idno>
		<ptr target="https://www.flickr.com/photos/girolame/3220601379/" />
		<title level="m">Paris Eiffel Tower</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Photo classification by integrating image content and camera metadata</title>
		<author>
			<persName><forename type="first">M</forename><surname>Boutell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="901" to="904" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Spotting the difference: Context retrieval and analysis for improved forgery detection and localization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Brogan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bestagini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bharati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pinto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Moreira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Bowyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rocha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Scheirer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Image Processing</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4078" to="4082" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Why and where: A characterization of data provenance</title>
		<author>
			<persName><forename type="first">P</forename><surname>Buneman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Khanna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wang-Chiew</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Springer International conference on database theory</title>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="316" to="330" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Image splicing detection via camera response function analysis</title>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mccloskey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5087" to="5096" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Estimation of color modification in digital images by cfa pattern change</title>
		<author>
			<persName><forename type="first">C.-H</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-K</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>Elsevier Forensic Science International</publisher>
			<biblScope unit="volume">226</biblScope>
			<biblScope unit="page" from="94" to="105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Near duplicate image detection: min-hash and tf-idf weighting</title>
		<author>
			<persName><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="volume">810</biblScope>
			<biblScope unit="page" from="812" to="815" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title/>
		<author>
			<persName><surname>Provenance_Datasets</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="7" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Splicebuster: A new blind image splicing detector</title>
		<author>
			<persName><forename type="first">D</forename><surname>Cozzolino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Poggi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Verdoliva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Workshop on Information Forensics and Security</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Multiple parenting phylogeny relationships in digital images</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>De Oliveira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ferrara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">De</forename><surname>Rosa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Piva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Barni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Goldenstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Dias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rocha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Forensics and Security</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="328" to="343" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Toward image phylogeny forests: Automatically recovering semantically similar image relationships</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Dias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Goldenstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rocha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Elsevier Forensic science international</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">231</biblScope>
			<biblScope unit="page" from="178" to="189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Image phylogeny by minimal spanning trees</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Dias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rocha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Goldenstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Forensics and Security</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="774" to="788" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Origins: evolving ideas about the principle of provenance. Currents of Archival Thinking</title>
		<author>
			<persName><forename type="first">J</forename><surname>Douglas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">24</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Exiv2</title>
		<ptr target="http://www.exiv2.org/metadata.html" />
	</analytic>
	<monogr>
		<title level="m">Metadata reference tables</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="6" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Estimating exif parameters based on noise features for image manipulation detection</title>
		<author>
			<persName><forename type="first">J</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Kot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Forensics and Security</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="608" to="618" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Image forgery detection</title>
		<author>
			<persName><forename type="first">H</forename><surname>Farid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="16" to="25" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Gatys</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Ecker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bethge</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.06576</idno>
		<title level="m">A neural algorithm of artistic style</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Annotation and provenance tracking in semantic web photo libraries</title>
		<author>
			<persName><forename type="first">C</forename><surname>Halaschek-Wiener</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Golbeck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Schain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Grove</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Parsia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hendler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Springer International Provenance and Annotation Workshop</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="82" to="89" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">P</forename><surname>Harvey</surname></persName>
		</author>
		<author>
			<persName><surname>Exiftool</surname></persName>
		</author>
		<ptr target="https://www.sno.phy.queensu.ca/˜phil/exiftool/" />
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="7" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Holan</surname></persName>
		</author>
		<ptr target="http://www.politifact.com/truth-o-meter/article/2016/dec/13/2016-lie-year-fake-news/" />
		<title level="m">lie of the year: Fake news</title>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
			<biblScope unit="page" from="6" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Metadata-based image watermarking for copyright protection</title>
		<author>
			<persName><forename type="first">H.-C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-C</forename><surname>Fang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Elsevier Simulation Modelling Practice and Theory</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="436" to="445" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Fighting fake news: Image splice detection via learned self-consistency</title>
		<author>
			<persName><forename type="first">M</forename><surname>Huh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Owens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.04096</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">The mir flickr retrieval evaluation</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Huiskes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Lew</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Multimedia Information Retrieval</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="39" to="43" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<ptr target="https://help.imgur.com/hc/en-us/articles/201746817-Post-privacy" />
		<title level="m">Imgur Privacy Standards</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="6" to="25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Applying perceptual grouping to content-based image retrieval: Building images</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Aggarwall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="42" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Image splicing detection based on general perspective constraints</title>
		<author>
			<persName><forename type="first">M</forename><surname>Iuliani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Fabbri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Piva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Workshop on Information Forensics and Security (WIFS)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Hamming embedding and weak geometric consistency for large scale image search</title>
		<author>
			<persName><forename type="first">H</forename><surname>Jegou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Springer European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="304" to="317" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Love thy neighbors: Image annotation by exploiting image metadata</title>
		<author>
			<persName><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ballan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="4624" to="4632" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Efficient near-duplicate detection and sub-image retrieval</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Huston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Multimedia</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="5" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Digital image authentication from jpeg headers</title>
		<author>
			<persName><forename type="first">E</forename><surname>Kee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Farid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Forensics and Security</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="1066" to="1075" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">On the shortest spanning subtree of a graph and the traveling salesman problem</title>
		<author>
			<persName><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the American Mathematical Society</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="48" to="50" />
			<date type="published" when="1956">1956</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Anti-forensics of camera identification and the triangle test by improved fingerprint-copy attack</title>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.07795</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Automatic digital image grouping using criteria based on image metadata and spatial information</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">K</forename><surname>Logan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Szeliski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Uyttendaele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">US Patent</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">952</biblScope>
			<date type="published" when="2009-08">August 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Object recognition from local scale-invariant features</title>
		<author>
			<persName><forename type="first">D</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1150" to="1157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Distinctive image features from scale-invariant keypoints</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">A bibliography on blind methods for identifying image forgery</title>
		<author>
			<persName><forename type="first">B</forename><surname>Mahdian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Saic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Elsevier Signal Processing: Image Communication</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="389" to="399" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Attacking the triangle test in sensor-based camera identification</title>
		<author>
			<persName><forename type="first">F</forename><surname>Marra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Roli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cozzolino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sansone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Verdoliva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Image Processing</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="5307" to="5311" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Robust Wide Baseline Stereo from Maximally Stable Extremal Re-gions</title>
		<author>
			<persName><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Urban</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pajdla</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
			<publisher>Elsevier Image and Vision Computing</publisher>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="761" to="767" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Is this Monet real or fake-and who gets to decide?</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mohdin</surname></persName>
		</author>
		<ptr target="https://qz.com/588932/is-this-monet-real-or-fake-and-who-gets-to-decide/" />
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="6" to="25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Image provenance analysis at scale</title>
		<author>
			<persName><forename type="first">D</forename><surname>Moreira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bharati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Brogan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pinto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Parowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">W</forename><surname>Bowyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rocha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Scheirer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.06510</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<ptr target="https://github.com/usnistgov/MediScore" />
		<title level="m">MediScore:Scoring tools for Media Forensics Evaluations</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="7" to="8" />
		</imprint>
		<respStmt>
			<orgName>National Institute of Standards and Technology</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<ptr target="https://www.nist.gov/sites/default/files/documents/2017/09/07/\nc2017evaluationplan_20170804.pdf" />
		<title level="m">Nimble Challenge 2017 Evaluation Plan</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="6" to="18" />
		</imprint>
		<respStmt>
			<orgName>National Institute of Standards and Technology</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Multiple parenting identification in image phylogeny</title>
		<author>
			<persName><forename type="first">A</forename><surname>Oliveira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ferrara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">De</forename><surname>Rosa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Piva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Barni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Goldenstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Dias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rocha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Image Processing</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="5347" to="5351" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Automatic image captioning</title>
		<author>
			<persName><forename type="first">J.-Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Duygulu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Faloutsos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Multimedia and Expo</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1987" to="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Provenance filtering for multimedia phylogeny</title>
		<author>
			<persName><forename type="first">A</forename><surname>Pinto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Moreira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bharati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Brogan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Bowyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Scheirer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rocha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Image Processing</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1502" to="1506" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Pskowski</surname></persName>
		</author>
		<ptr target="https://www.theverge.com/2018/6/27/17503444/mexico-election-fake-news-facebook-twitter-whatsapp" />
		<title level="m">Mexico struggles to weed out fake news ahead of its biggest election ever</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="6" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title/>
		<author>
			<persName><forename type="middle">Com</forename><surname>Reddit</surname></persName>
		</author>
		<author>
			<persName><surname>Photoshopbattles</surname></persName>
		</author>
		<ptr target="https://www.reddit.com/r/photoshopbattles/" />
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="8" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Efficient content based image retrieval system with metadata processing</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sasikala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Gandhi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Innovative Research in Science and Technology</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="72" to="77" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">A survey of data provenance techniques</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">L</forename><surname>Simmhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Plale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gannon</surname></persName>
		</author>
		<ptr target="ftp://ftp.extreme.indiana.edu/pub/techreports/TR618.pdf" />
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="6" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Provenance: Important, Yes, But Often Incomplete and Often Enough</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Spencer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">D</forename><surname>Sesser</surname></persName>
		</author>
		<ptr target="https://news.artnet.com/market/the-importance-of-provenance-in-determining-authenticity-29953" />
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="6" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">The Metropolitan Museum of Art</title>
		<ptr target="https://www.metmuseum.org/about-the-met/policies-and-documents/provenance-research-project" />
	</analytic>
	<monogr>
		<title level="m">The Met&apos;s Provenance Research Project</title>
		<imprint>
			<date>200</date>
			<biblScope unit="page" from="6" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Extent: Inferring image metadata from context and content</title>
		<author>
			<persName><forename type="first">C.-M</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Qamra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">Y</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-F</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Multimedia and Expo</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="1270" to="1273" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">)_at_ night.jpg</title>
		<author>
			<persName><forename type="first">Wikimedia</forename><surname>Commons</surname></persName>
		</author>
		<ptr target="https://commons.wikimedia.org/wiki/File" />
	</analytic>
	<monogr>
		<title level="m">Las Vegas&apos; Eiffel Tower</title>
		<meeting><address><addrLine>Paris</addrLine></address></meeting>
		<imprint>
			<publisher>Eiffel_Tower_in_Las_Vegas</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="6" to="25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Faceted metadata for image search and browsing</title>
		<author>
			<persName><forename type="first">K.-P</forename><surname>Yee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Swearingen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hearst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGCHI Conference on Human Factors in Computing Systems</title>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="401" to="408" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Learning to compare image patches via convolutional neural networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="4353" to="4361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Shufflenet: An extremely efficient convolutional neural network for mobile devices</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
