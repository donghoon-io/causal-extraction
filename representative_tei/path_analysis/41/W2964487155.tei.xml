<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multi-Target Embodied Question Answering</title>
				<funder>
					<orgName type="full">Facebook</orgName>
				</funder>
				<funder ref="#_k4p29CA #_ef8rcW8 #_WKEQrm6">
					<orgName type="full">National Science Foundation</orgName>
					<orgName type="abbreviated">NSF</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2019-04-09">9 Apr 2019</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Licheng</forename><surname>Yu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of North Carolina at Chapel Hill</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Facebook AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Facebook AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of North Carolina at Chapel Hill</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tamara</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of North Carolina at Chapel Hill</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Facebook AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
							<affiliation key="aff1">
								<address>
									<country>Georgia Tech</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Facebook AI</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Multi-Target Embodied Question Answering</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2019-04-09">9 Apr 2019</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:1904.04686v1[cs.CV]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-29T00:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Embodied Question Answering (EQA) is a relatively new task where an agent is asked to answer questions about its environment from egocentric perception. EQA as introduced in [8] makes the fundamental assumption that every question, e.g. "what color is the car?", has exactly one target ("car") being inquired about. This assumption puts a direct limitation on the abilities of the agent.</p><p>We present a generalization of EQA -Multi-Target EQA (MT-EQA). Specifically, we study questions that have multiple targets in them, such as "Is the dresser in the bedroom bigger than the oven in the kitchen?", where the agent has to navigate to multiple locations ("dresser in bedroom", "oven in kitchen") and perform comparative reasoning ("dresser" bigger than "oven") before it can answer a question. Such questions require the development of entirely new modules or components in the agent. To address this, we propose a modular architecture composed of a program generator, a controller, a navigator, and a VQA module. The program generator converts the given question into sequential executable sub-programs; the navigator guides the agent to multiple locations pertinent to the navigation-related sub-programs; and the controller learns to select relevant observations along its path. These observations are then fed to the VQA module to predict the answer. We perform detailed analysis for each of the model components and show that our joint model can outperform previous methods and strong baselines by a significant margin. Project page: <ref type="url" target="https://embodiedqa.org">https://embodiedqa.org</ref>.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>One of the grand challenges of AI is to build intelligent agents that visually perceive their surroundings, communicate with humans via natural language, and act in their environments to accomplish tasks. In the vision, language, and AI communities, we are witnessing a shift in focus from internet vision to embodied AI -with the creation of new tasks and benchmarks <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b34">35]</ref>, instantiated on new simulation platforms <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b5">6]</ref>.</p><p>The focus of this paper is one such embodied AI task, Figure <ref type="figure">1</ref>: Difference between EQA-v1 and MT-EQA. While EQA-v1's question asks about a single target "car", MT-EQA's question involves multiple targets (e.g., bedroom, dressing table, bathroom, sink) to be navigated, and attribute comparison between multiple targets (e.g., dressing table and sink).</p><p>Embodied Question Answering (EQA) <ref type="bibr" target="#b7">[8]</ref>, which tests an agent's overall ability to jointly perceive its surrounding, communicate with humans, and act in a physical environment. Specifically, in EQA, an agent is spawned in a random location within an environment and is asked a question about something in that environment, for example "What color is the lamp?". In order to answer the question correctly, the agent needs to parse and understand the question, navigate to a good location (looking at the "lamp") based on its first-person perception of the environment and predict the right answer (e.g. "blue"). However, there is still much left to be done in EQA. In its original version, the EQA-v1 dataset only consists of single-target question-answer pairs, such as "What color is the car?". The agent just needs to find the car then check its color based on its last observed frames. However, the single target constraint places a direct limitation on the possible set of tasks that the AI agent can tackle. For example, consider the question "Is the kitchen larger than the bedroom?" in EQA-v1; the agent would not be able to answer this question because it involves navigating to multiple targets -"kitchen" and "bedroom" -and the answer requires comparative reasoning between the two rooms, where all of these skills are not part of the original EQA task.</p><p>In this work, we present a generalization of EQA -  multi-target EQA (MT-EQA). Specifically, we study questions that have multiple implicit targets in them, such as "Is the dresser in the bedroom bigger than the oven in the kitchen?". At a high-level, our work is inspired by the visual reasoning work of Neural Modular Networks <ref type="bibr" target="#b3">[4]</ref> and CLEVR <ref type="bibr" target="#b17">[18]</ref>. These works study compositional and modular reasoning in a fully-observable environment (an image).</p><p>Our work may be viewed as embodied visual reasoning, where an agent is asked a question involving multiple modules and needs to gather information before it can execute them. In MT-EQA, we propose 6 types of compositional questions which compare attribute properties (color, size, distance) between multiple targets (objects/rooms). Fig. <ref type="figure">1</ref> shows an example from the MT-EQA dataset and contrasts it to the original EQA-v1 dataset.</p><p>The assumption in EQA-v1 of decoupling navigation from question-answering not only makes the task simpler but is also reflected in the model used -the EQA-v1 model simply consists of an LSTM navigator which after stopping, hands over frames to a VQA module. In contrast, MT-EQA introduces new modeling challenges that we address in this work. Consider the MT-EQA question in Fig. <ref type="figure">1</ref> -"Does the table in the bedroom have same color as the sink in the bathroom?". From this example, it is clear that not only is it necessary to have a tighter integration between navigator and VQA, but we also need to develop fundamentally new modules. An EQA-v1 <ref type="bibr" target="#b7">[8]</ref> agent would navigate to the final target location and run the VQA module based on its last sequence of frames along the path. In this case, only the "sink" would be observed from the final frames but dressing table would be lost. Instead, we propose a new model that consists of 4 components: (a) a program generator, (b) a navigator, (c) a controller and (d) a VQA module. The program generator converts the given question into sequential executable sub-programs, as shown in Fig. <ref type="figure" target="#fig_1">2</ref>. The controller executes these sub-programs sequentially and gives control to the navigator when the navigation sub-programs are invoked (e.g. nav room(bedroom)). During navigation, the controller processes the first-person views observed by the agent and predicts whether the target of the sub-program (e.g. bedroom) has been reached. In addition, the controller extracts cues pertinent to the questioned property of the sub-target, e.g. query(color). Finally, these cues are fed into the VQA module which deals with the comparison of different attributes, e.g. executing equal color() by comparing the color of dressing table and sink (Fig. <ref type="figure">1</ref> ).</p><p>Empirically, we show results for our joint model and analyze the performance of each of our components. Our full model outperforms the baselines under almost every navigation and QA metric by a large margin. We also report performance for the navigator, the controller, and the VQA module, when executed separately in an effort to isolate and better understand the effectiveness of these components. Our ablation studies show that our full model is better at all sub-tasks, including room navigation, object navigation and final EQA accuracy. Additionally, we find quantitative evidence that MT-EQA questions on closer targets are relatively easier to solve as they require shorter navigation, while questions for farther targets are harder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Our work relates to research in embodied perception and modular predictive models for program execution. Embodied Perception. Visual recognition from images has witnessed tremendous success in recent years with the advent of deep convolutional neural networks (CNNs) <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b14">15]</ref> and large-scale datasets, such as ImageNet <ref type="bibr" target="#b25">[26]</ref> and COCO <ref type="bibr" target="#b23">[24]</ref>. More recently, we are beginning to witness a resurgence of active vision. For example, end-to-end learning methods successfully predict robotic actions from raw pixel data <ref type="bibr" target="#b22">[23]</ref>. Gupta et al. <ref type="bibr" target="#b13">[14]</ref> learn to navigate via mapping and planning. Sadeghi &amp; Levine <ref type="bibr" target="#b26">[27]</ref> teach an agent to fly in simulation and show its performance in the real world. Gandhi et al. <ref type="bibr" target="#b10">[11]</ref> train self-supervised agents to fly from examples of drones crashing.</p><p>At the intersection of active perception and language understanding, several tasks have been proposed, including instruction-based navigation <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b1">2]</ref>, target-driven navigation <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b13">14]</ref>, embodied question answering <ref type="bibr" target="#b7">[8]</ref>, interactive question answering <ref type="bibr" target="#b12">[13]</ref>, and task planning <ref type="bibr" target="#b34">[35]</ref>. While these tasks are driven by different goals, they all require training agents that can perceive their surroundings, understand the goal -either presented visually or in language instructions -and act in a virtual environment. Furthermore, the agents need to show strong generalization ability when deployed in novel unseen environments <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b31">32]</ref>. Environments. There is an overbearing cost to developing real-world interactive benchmarks. Undoubtedly, this cost has hindered progress in studying embodied tasks. On the contrary, virtual environments that offer rich, efficient simulations of real-world dynamics, have emerged as promising alternatives to potentially overcome many of the challenges faced in real-world settings.</p><p>Recently there has been an explosion of simulated 3D environments in the AI community, all tailored towards different skill sets. Examples include ViZDoom <ref type="bibr" target="#b19">[20]</ref>, TorchCraft <ref type="bibr" target="#b29">[30]</ref> and DeepMind Lab <ref type="bibr" target="#b4">[5]</ref>. Just in the last year, simulated environments of semantically complex, realistic 3D scenes have been introduced, such as HoME <ref type="bibr" target="#b5">[6]</ref>, House3D <ref type="bibr" target="#b31">[32]</ref>, MINOS <ref type="bibr" target="#b27">[28]</ref>, Gibson <ref type="bibr" target="#b32">[33]</ref> and AI2THOR <ref type="bibr" target="#b20">[21]</ref>. In this work, we use House3D, following the original EQA task <ref type="bibr" target="#b7">[8]</ref>. House3D is a rich, interactive 3D environment based on human-designed indoor scenes sourced from SUNCG <ref type="bibr" target="#b28">[29]</ref>.</p><p>Modular Models. Neural module networks were originally introduced for visual question answering <ref type="bibr" target="#b3">[4]</ref>. These networks decompose a question into several components and dynamically assemble a network to compute the answer, dealing with variable compositional linguistic structures. Since their introduction, modular networks have been applied to several other tasks: visual reasoning <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b18">19]</ref>, relationship modeling <ref type="bibr" target="#b16">[17]</ref>, embodied question answering <ref type="bibr" target="#b8">[9]</ref>, multitask reinforcement learning <ref type="bibr" target="#b2">[3]</ref>, language grounding on images <ref type="bibr" target="#b33">[34]</ref> and video understanding <ref type="bibr" target="#b11">[12]</ref>. Inspired by <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b18">19]</ref>, we cast EQA as a partially observable version of CLEVR and extend the modular idea to this task, which we believe requires an increasingly modular model design to address visual reasoning within a 3D environment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Multi-Target EQA Dataset</head><p>We now describe our proposed Multi-Target Embodied Question Answering (MT-EQA) task and associated dataset, contrasting it against EQA-v1. In v1 <ref type="bibr" target="#b7">[8]</ref>, the authors select 750 (out of about 45,000) environments for the EQA task. Four types of questions are proposed, each questioning a property (color, location, preposition) of a single target (room, object), as shown at the top of Table . 1. Our proposed MT-EQA task generalizes EQA-v1 and involves comparisons of various attributes (color, size, distance) between multiple targets, shown at the bottom of Table . 1. Next, we describe in detail the generation process, as well as useful statistics of MT-EQA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Multi-Target EQA Generation</head><p>We generate question-answer pairs using the annotations available on SUNCG. We use the same number of rooms and objects as EQA-v1 (see Figure <ref type="figure" target="#fig_1">2</ref> in <ref type="bibr" target="#b7">[8]</ref>). Each question in MT-EQA is represented as a series of functional programs, which can be executed on the environment to yield a ground-truth answer. The functional programs consist of some elementary operations, e.g., select(), unique(), object color pair(), query(), etc., that operate on the room and object annotations.</p><p>Each question type is associated with a question template and a sequence of operations. For example, consider the question type in MT-EQA object color compare, whose template is "Does &lt;OBJ1&gt; share same color as &lt;OBJ2&gt; in &lt;ROOM&gt;?". Its sequence of elementary operations is:</p><formula xml:id="formula_0">select(rooms) → unique(rooms) → select(objects) → unique(objects) → pair(objects) → query(color compare).</formula><p>The first function, select(rooms), returns all rooms in the environment. The second function, unique(rooms), selects   <ref type="table" target="#tab_2">2</ref>.</p><p>In some cases, a question instantiation returned from the corresponding program, as shown above, might not be executable, as rooms might be disconnected or not reachable. To check if a question is feasible, we execute the corresponding nav room() and nav object() programs and compute shortest paths connecting the targets in the question. If there is no path<ref type="foot" target="#foot_0">foot_0</ref> , it means the agent would not be able to look at all targets starting from its given spawn location. We filter out such impossible questions.</p><p>For computing the shortest path connecting the targets, we need to find the position (x, y, z, yaw) that best views each target. In order to do so, we first sample 100 positions near the target. For each position, we pick the yaw angle that looks at the target with the highest Intersection-Over-Union (IOU), computed using the target's mask<ref type="foot" target="#foot_1">foot_1</ref> and a centered rectangular mask. Fig. <ref type="figure" target="#fig_3">3</ref> shows 4 IOU scores of coffee machine and refrigerator from different positions. We sort the 100 positions and pick the one with highest IOU as the best-view position of the target, which is used to connect the shortest-path. For each object, its highest IOU value IOU best is recorded for evaluation purposes (as a reference of the target's best-view).</p><p>To minimize the bias in MT-EQA, we perform entropyfiltering, similar to <ref type="bibr" target="#b7">[8]</ref>. Specifically for each unique question, we compute its answer distribution across the whole dataset. We exclude questions whose normalized answer distribution entropy is below 0.9 <ref type="foot" target="#foot_2">3</ref> . This prevents the agent from memorizing easy question-answer pairs without looking at the environment. For example, the answer to "is the    bed in the living room bigger than the cup in the kitchen?" is always Yes. Such questions are excluded from our dataset.</p><p>After the two filtering stages, the MT-EQA questions are both balanced and feasible.</p><p>In addition, we check if MT-EQA is easily addressed by question-only or prior-only baselines. For this, we evaluate four question-based models: (a) an LSTM-based questionto-answer model, (b) a nearest neighbor (NN) baseline that finds the NN question from the training set and uses its most frequent answer as the prediction, (c) a bag-of-words (BoW) model that encodes a question followed by a learned linear classifier to predict the answer and (d) a naive "no" only answer model, since "no" is the most frequent answer by a slight margin. Table . 3 shows the results. There exists very little bias on the "yes/no" distribution (53.28%), and all question-based models make close to random predictions. In comparison, and as we empirically show in Sec. 5, our results are far better than these baselines, indicating the necessity to explore the environment in order to answer the question. Besides, the results also address the concern in <ref type="bibr" target="#b0">[1]</ref> where language-only models (BoW and NN) already form competitive baselines for EQA-v1. In MT-EQA, these baselines perform close to chance as a result of the balanced binary question-answer pairs in MT-EQA.</p><p>Overall, our MT-EQA dataset consists of 19,287 questions across 588 environments <ref type="foot" target="#foot_3">4</ref> , referring to a total of 61 unique object types in 8 unique room types. Fig. <ref type="figure" target="#fig_4">4</ref> shows the question type distribution. Approximately 32 questions are asked for each house on average, 209 at most and 1 at fewest. There are relatively fewer object size compare and room size compare questions as many frequently occurring comparisons are too easy to guess without exploring the environment and thus fail the entropy filtering. We will release the MT-EQA dataset and the generation pipeline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Model</head><p>Our model is composed of 4 modules: the question-toprogram generator, the navigator, the controller, and the VQA module. We describe these modules in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Program Generator</head><p>The program generator takes the question as input and generates sequential programs for execution. We define   7 types of executable programs for the MT-EQA task in Table. <ref type="bibr" target="#b3">4</ref>. For example, "Is the bathtub the same color as the sink in the bathroom?" is decomposed into a series of sequential sub-programs: nav room(bathroom) → nav object(bathtub) → query color() → nav object(sink) → query color() → equal color(). Similar to CLEVR <ref type="bibr" target="#b17">[18]</ref>, the question programs are automatically generated in a templated manner (Table . 2), making sub-component decomposition (converting questions back to programs) simple (Table <ref type="table" target="#tab_5">. 4</ref>). We use template-based rules by selecting and filling in the arguments in Table . 4 to generate the programs (which is always accurate). While a neural model could also be applied, a learned program generator is not the focus of our work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Navigator</head><p>The navigator executes the nav room() and nav object() programs.</p><p>As shown in Fig. <ref type="figure" target="#fig_6">6</ref>(a), we use an LSTM as our core component. At each time step, the LSTM takes as inputs the current egocentric (firstperson view) image, an encoding of the target phrase (e.g. "bathtub" if the program is nav object(bathtub)), and the previous action, in order to predict the next action.</p><p>The navigator uses a CNN feature extractor that takes a 224x224 RGB image returned from the House3D renderer, and transforms it into a visual feature, which is then fed into the LSTM. Similar to <ref type="bibr" target="#b7">[8]</ref>, the CNN is pre-trained under a multi-task framework consisting of three tasks: RGB-value reconstruction, semantic segmentation, and depth estimation. Thus, the extracted feature contains rich information about the scene's appearance, content, and geometry (objects, color, texture, shape, and depth). In addition to the visual feature, the LSTM is presented with two additional inputs. The first is the target embedding, where we use the average embedding of GloVE vectors <ref type="bibr" target="#b24">[25]</ref> over words describing the target. The second is previous action, which is in the form of a look-up from an action embedding matrix.</p><p>We want to note the different perceptual skills required for room and object navigation: Room navigation relies on understanding the overall scene and finding crossroom paths (entry/exit), while object navigation requires localizing the target object within a room and finding a path to reach it. To capture the difference, we implement two separate navigation modules, nav room() and nav object() respectively. These two modules share same architecture but are trained separately for different targets.</p><p>In MT-EQA, the action space for navigation consists of 3 action types: turning left (30 degrees), turning right (30 degrees), and moving forward. This is almost the same as EQA-v1 <ref type="bibr" target="#b7">[8]</ref>, except we use larger turning angles -as our navigation paths are much longer due to the multi-target setting. We find that this change reduces the number of actions required for navigation, leading to easier training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Controller</head><p>The controller is the central module in our model, as it connects all of the other modules by: 1) creating a plan from the program generator, 2) collecting the necessary observations from the navigator, and 3) invoking the VQA module.</p><p>Fig. <ref type="figure" target="#fig_6">6</ref> (b) shows the controller, whose key component is another LSTM. Consider the question "Does the bathtub have same color as the sink in the bathroom?" with part of its program as examplenav room(bathroom) → nav object(bathtub). The controller starts by calling the room navigator to look for "bathroom". During navigation, the controller keeps track of the first-person views, looking for the target. Particularly, it extracts the features via CNN which are then fused with the target embedding as input to the LSTM. The controller predicts SELECT if the target is found, stopping the current navigator, in our example nav room(bathroom), and starting execution of the next program, nav object(bathtub).</p><p>Finally, after the object target "bathtub" has been found, the next programquery color(), is executed. The controller extracts attribute features from the first-person view containing the target. In all, there are three attribute types in MT-EQA -object's color, object's size, and room's size. Again, we treat object and room differently in our model. For object-specific attributes, we use the hidden state of the controller at the location where SELECT was predicted. This state should contain semantic information for the target, as it is where the controller is confident the target is located. For room-specific attributes, the controller collects a panorama by asking the navigator to rotate 360 degrees (by performing 12 turning-right actions) at the location where SELECT is predicted. The CNN features from this panorama view are concatenated as the representation.</p><p>During program execution by the controller, the extracted cues for all the targets are stored, and in the end they are used by the VQA module to predict the final answer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">VQA Module</head><p>The final task requires comparative reasoning, e.g., object size compare(bigger), equal color(), etc. When the controller has gathered all of the targets for comparison, it invokes the VQA module. As shown in top-right of Fig. <ref type="figure" target="#fig_5">5</ref>, the VQA module embeds the stored features of multiple targets into the question-attribute space, using a FC layer followed by ReLU. The transformed features are then concatenated and fed into another FC+ReLU which is conditioned on the comparison operator (equal, bigger than, smaller than, etc.). The output is a binary prediction (yes/no) for that attribute comparison. We call it composi-tional VQA (cVQA). The cVQA module in Fig. <ref type="figure" target="#fig_5">5</ref> depicts a two-input comparison as an example, but our cVQA module also extends to three inputs, for questions like "Is the refrigerator closer to the coffee machine than the microwave?".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Training</head><p>Training follows a two-stage approach: First, the full model is trained using Imitation Learning (IL); Second, the navigator is further fine-tuned with Reinforcement Learning (RL) using policy gradients.</p><p>First, we jointly train our full model using imitation learning. For imitation learning, we treat the shortest paths and the key positions containing the targets as our groundtruth labels for navigation and for the controller's SELECT classifier, respectively. The objective function consists of a navigation objective and a controller objective at every time step t, and a VQA objective at the final step. For the i-th question, let P nav i,t,a be action a's probability at time t, P sel i,t</p><p>be the controller's SELECT probability at time t, and P vqa i be the answer probability from VQA, then we minimize the combined loss:</p><formula xml:id="formula_1">L = L nav + αL ctrl + βL vqa = - i t a y n i,t,a log P nav i,t,a</formula><p>Cross-entropy on navigator action</p><formula xml:id="formula_2">-α i t (y c i,t log P sel i,t + (1 -y c i,t ) log(1 -P sel i,t ))</formula><p>Binary cross-entropy on controller's SELECT</p><formula xml:id="formula_3">-β i (y v i log P vqa i + (1 -y v i ) log(1 -P vqa i ))</formula><p>Binary cross-entropy on VQA's answer .</p><p>Subsequently, we use RL to fine-tune the room and object navigators.</p><p>We provide two types of reward signals to the navigators. The first is a dense reward, corresponding to the agent's progress toward the goal (positive if moving closer to the target and negative if moving away). This reward is measured by the distance change in the 2D bird-view distance space, clipped to lie within [-1.0, 1.0]. The second is a sparse reward that quantifies whether the agent is looking at the target object when the episode is terminated. For object targets, we compute IOU T between the target's mask and the centered rectangle mask at termination. We use the best IOU score of the target IOU best as reference and compute the ratio IOU T IOU best . If the ratio is greater than 0.5, we set the reward to 1.0 otherwise -1.0. For room targets, we assign reward 0.2 to the agent if it is inside the target room at termination, otherwise -0.2. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>In this section we describe our experimental results. Since MT-EQA is a complex task and our model is modular, we will show both the final results (QA accuracy) and the intermediate performance (for navigation). Specifically, we first describe our evaluation setup and metrics for MT-EQA. Then, we report the comparison of our model against several strong baselines. And finally, we analyze variants of our model and provide ablation results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Evaluation Setup and Metrics</head><p>Spawn Location. MT-EQA questions involve multiple targets (rooms/objects) to be found. To prevent the agent from learning biases due to spawn location, we randomly select one of the mentioned targets as reference and spawn our agent 10 actions (typically 1.9 meters) away. EQA Accuracy. We compute overall accuracy as well as accuracy for each of the 6 types of questions in our dataset. In addition, we also categorize question difficulty level into easy, medium, and hard by binning the ground-truth action length. Easy questions are those with fewer than 25 action steps along the shortest path, medium are those with 25-70 actions, and hard are those with more than 70 actions. We report accuracy for each difficulty, % easy , % medium , % hard , as well as overall, % overall , in Table <ref type="table" target="#tab_6">5</ref>. Navigation Accuracy. We also measure the navigation accuracy for both objects and rooms in MT-EQA. As each question involves several targets, the order of them being navigated matters. We consider the 'ground truth' ordering of targets for navigation as the order in which they are men-tioned in the question, e.g., given "Does the bathtub have same color as the sink?", the agent is trained and evaluated for visiting the "bathtub" first and then the "sink".</p><p>For each mentioned target object, we evaluate the agent's navigation performance by computing the distance to the target object at navigation termination, d T , and change in distance to the target from initial spawned position to terminal position, d ∆ . We also compute the stop ratio %stop o as in EQA-v1 <ref type="bibr" target="#b7">[8]</ref>. Additionally, we propose two new metrics based on the IOU of the target object at its termination. When the navigation is done, we compute the IOU of the target w.r.t a centered rectangular box (see Fig. <ref type="figure" target="#fig_3">3</ref> as example). The first metric is mean IOU ratio IOU r T =</p><formula xml:id="formula_4">1 N i IOU T (oi)</formula><p>IOU best (oi) ) where IOU best (o i ) is the highest IOU score for object o i . The second is hit accuracy h T -we compute the percentage of the ratio IOU T (o i )/IOU best (o i ) greater than 0.5, i.e., h</p><formula xml:id="formula_5">T = 1 N i || IOU T (oi)</formula><p>IOU best (oi) &gt; 0.5||. Both metrics measure to what extent the agent is looking at the target at termination.</p><p>For each mentioned target room, we evaluate the agent's navigation by recording the percentage of agents terminating inside the target room %r T and the stop ratio %stop r . For all the above metrics except for d T , larger is better. Additionally, we report the overall number of action steps (episode length) executed for each question, i.e., ep len.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">EQA Results</head><p>Nav+Ctrl+cVQA is our full model, which is composed of a program generator, a navigator, a controller and a comparative VQA module. Another variant of our model, the REINFORCE fine-tuned model is denoted as Nav(RL)+Ctrl+cVQA. We also train a simplified version of our full model, Nav+cVQA. which does not use a controller. For this model, we let the navigator predict termination whenever a target is detected, then feed its hidden states to the VQA model. The training details are similar to our full model for both IL and RL. We show comparisons of both navigation and EQA accuracy in Table . 5. RL helps both navigation and EQA accuracies. Both object and room navigation performance are improved after RL finetuning. We notice without finetuning d ∆ for both models (Row 1 &amp;3 ) are negative, which means the agent has moved farther away from the target during navigation. After RL finetuning, d ∆ jumps from -0.56 to 0.16 (Row 3 &amp; 4). The hit accuracy also improves from 20% to 33%, indicating that the RL-finetuned agent is more likely to find the target mentioned in the question. Episode lengths from the stronger navigators are shorter, indicating that better navigators find their target more quickly. And, higher EQA accuracy is also achieved with the help of RL finetuning (from 54.44% to 61.45%). After breaking down the EQA into different types, we observe the same trend in Table . 6our full model with RL far outperforms the others. Controller is important. Comparing our full model (Row 4) to the one without a controller (Row 2), we notice that the former outperforms the latter across almost all the metrics. One possible reason is that the VQA task and navigation task are quite different, such that the features (hidden state) from the navigator cannot help improve the VQA module. On the contrary, our controller decouples the two tasks, letting the navigator and VQA module focus on their own roles. Questions with shorter ground-truth path are easier. We observe that our agent is far better at dealing with easy questions than hard ones (72.22% over 54.92% in Table . 5 Row 4). One reason is that the targets mentioned in the easy questions, e.g., sink and toilet in "Does the sink have same color as the toilet in the bathroom?", are typically closer to each other, thus are relatively easier to be explored, whereas questions like "Is the kitchen bigger than the garage?" requires a very long trajectory and the risk of missing one (kitchen or garage) is increased. The same observation is found in Table . 6, where we get higher accuracy for "inroom" questions than "cross-room" ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Oracle Comparisons</head><p>To better understand each module of our model, we run ablation studies. Table. 7 shows EQA accuracy of different approaches given the shortest paths or best-view frames. Our VQA module helps. We first compare the performance of our VQA module against an attention-based VQA. Given the best view of each target, we can directly feed the features from those images to the VQA module, using the CNN features instead of hidden states from con-troller side. The attention-based VQA architecture is similar to <ref type="bibr" target="#b7">[8]</ref>, which uses an LSTM to encode questions and then uses its representation to pool image features with attention. Comparing the two methods in Table . 7, Row 1 &amp; 2, our VQA module achieves 13.64% higher accuracy. The benefit mainly comes from the decomposition of attribute representation and comparison in our VQA module. Controller's features help. We compare the controller's features to raw CNN features for VQA. When given both shortest path and best-view position, we run our full model with these annotations and feed the hidden states from the controller's LSTM to our VQA model. As shown in Table. 7, Row 2 &amp; 3, the controller's features are far better than raw CNN features, especially for object color compare and object size compare question types. Controller's SELECT matters. Our controller predicts SELECT and extracts the features at that moment. One possible question is how important is this moment selection. To demonstrate its advantage, we trained another VQA module which uses a LSTM to encode the whole sequence of frames along the shortest path and uses its final hidden state to predict the answer, denoted as seq-VQA. The hypothesis is that the final hidden state might be able to encode all relevant information, as the LSTM has gone through the whole sequence of frames. Table . 7, Row 4, shows its results, which is nearly random. On the contrary, when controller is used to SELECT frames in Row 5, the results are far better. However, there is still much space for improvement. Comparing Table . 7, Row 3 &amp; 5, the overall accuracy drops 13% when using features from the predicted SELECT instead of oracle moments, and 20% when using additional navigators (comparing Table . 7, Row 3, &amp; Table. 6, Row 4), indicating the necessity of both accurate SELECT and navigation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We proposed MT-EQA, extending the original EQA questions from a limited single-target setting to a more challenging multi-target setting, which requires the agent to perform comparative reasoning before answering questions. We collected a MT-EQA dataset as a test benchmark for the task, and validated its usefulness with simple baselines from just text or prior. We also proposed a new EQA model consisting of four modular components: a program generator, a navigator, a controller, and VQA module for MT-EQA. We experimentally demonstrated that our model significantly outperforms baselines on both question answering and navigation, and conducted detailed ablative analysis for each component in both the embodied and oracle settings.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>What color is the car? MT-EQA: Does the dressing table in the bedroom have same color as the sink in the bathroom?</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Program Generator.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: IOU between the target's mask and the centered rectangle mask. Higher IOU is achieved when the target has larger portion in the center of the view.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Overview of MT-EQA dataset including split statistics and question type distribution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Model architecture: our model is composed of a program generator, a navigator, a controller, and a VQA module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Navigator and Controller.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>What color is the &lt;OBJ&gt; in the &lt;ROOM&gt;?" preposition "What is &lt;on/above/below/next-to&gt; the &lt;OBJ&gt; in the &lt;ROOM&gt;?" Question types and the associated templates used in EQA-v1 and MT-EQA.</figDesc><table><row><cell>Question Type</cell><cell></cell><cell>Template</cell></row><row><cell cols="3">location color color room object color compare inroom object color compare xroom object size compare inroom object size compare xroom "MT-EQA "What room is the &lt;OBJ&gt; located in?" "What color is the &lt;OBJ&gt;?" EQA-v1 "Does &lt;OBJ1&gt; share same color as &lt;OBJ2&gt; in &lt;ROOM&gt;?" "Does &lt;OBJ1&gt; in &lt;ROOM1&gt; share same color as &lt;OBJ2&gt; in &lt;ROOM2&gt;?" "Is &lt;OBJ1&gt; bigger/smaller than &lt;OBJ2&gt; in &lt;ROOM&gt;?" "Is &lt;OBJ1&gt; in &lt;ROOM1&gt; bigger/smaller than &lt;OBJ2&gt; in &lt;ROOM2&gt;?" object dist compare "Is &lt;OBJ1&gt; closer than/farther from &lt;OBJ2&gt; than &lt;OBJ3&gt; in &lt;ROOM&gt;?"</cell></row><row><cell cols="2">room size compare</cell><cell>"Is &lt;ROOM1&gt; bigger/smaller than &lt;ROOM2&gt; in the house?"</cell></row><row><cell>Question Type</cell><cell>Functional Form</cell></row><row><cell>object color compare</cell><cell cols="2">select(rooms) → unique(rooms) → select(objects) → unique(objects) → pair(objects) → query(color compare)</cell></row><row><cell>object size compare</cell><cell cols="2">select(rooms) → unique(rooms) → select(objects) → unique(objects) → pair(objects) →query(size compare)</cell></row><row><cell>object dist compare</cell><cell></cell></row></table><note><p>select(rooms) → unique(rooms) → select(objects) → unique(objects) → triplet(objects) →query(dist compare) room size compare select(rooms) → unique(rooms) → pair(rooms) → query(size compare)</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table><row><cell>Test Acc. (%)</cell><cell>49.44</cell><cell>48.24</cell><cell>53.74</cell><cell>49.22</cell><cell>53.28</cell></row></table><note><p>Functional forms of all question types in the MT-EQA dataset. Note that for each object color/size comparison question type, there exists two modes: inroom and xroom, depending on whether the two objects are in the same room or not. For example, object color compare xroom compares the color of two objects in two different rooms. random q-LSTM q-NN q-BoW "no"</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">room_dist_comp</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>3%</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>obj_dist_comp</cell></row><row><cell cols="2">train 486 Houses</cell><cell cols="2">questions 2,030 14,495 questions Unique Total</cell><cell>10% obj_size_comp_xroom 13%</cell><cell>24% obj_color_comp_inroom</cell></row><row><cell>val</cell><cell>50</cell><cell>938</cell><cell>1,954</cell><cell>obj_size_comp_inroom</cell></row><row><cell>test</cell><cell>52</cell><cell cols="2">1,246 2,838</cell><cell>5%</cell><cell>45% obj_color_comp_xroom</cell></row></table><note><p>EQA (test) accuracy using questions and priors.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>MT-EQA executable programs.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Quantitative evaluation of object/room navigation and EQA accuracy for different approaches.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Object Navigation</cell><cell></cell><cell cols="2">Room Navigation</cell><cell></cell><cell></cell><cell>EQA</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>d T</cell><cell>d ∆</cell><cell>h T</cell><cell>IOU r T</cell><cell>%stopo</cell><cell>%r T</cell><cell>%stopr</cell><cell cols="2">ep len %easy</cell><cell>% medium</cell><cell>% hard</cell><cell>% overall</cell></row><row><cell>1</cell><cell>Nav+cVQA</cell><cell cols="3">5.41 -0.64 0.19</cell><cell>0.15</cell><cell>36</cell><cell>34</cell><cell>60</cell><cell>153.13</cell><cell>58.42</cell><cell>53.29</cell><cell>51.46</cell><cell>53.24</cell></row><row><cell>2</cell><cell>Nav(RL)+cVQA</cell><cell>3.80</cell><cell>0.10</cell><cell>0.33</cell><cell>0.30</cell><cell>46</cell><cell>40</cell><cell>62</cell><cell>144.80</cell><cell>67.57</cell><cell>55.91</cell><cell>53.28</cell><cell>57.40</cell></row><row><cell>3</cell><cell>Nav+Ctrl+cVQA</cell><cell cols="3">5.25 -0.56 0.20</cell><cell>0.18</cell><cell>36</cell><cell>37</cell><cell>70</cell><cell>145.20</cell><cell>59.73</cell><cell>53.48</cell><cell>49.04</cell><cell>54.44</cell></row><row><cell>4</cell><cell cols="2">Nav(RL)+Ctrl+cVQA 3.60</cell><cell>0.16</cell><cell>0.33</cell><cell>0.29</cell><cell>48</cell><cell>43</cell><cell>72</cell><cell>127.71</cell><cell>72.22</cell><cell>59.97</cell><cell>54.92</cell><cell>61.45</cell></row><row><cell></cell><cell></cell><cell cols="11">object color compare object size compare object dist compare room size compare</cell><cell>% overall</cell></row><row><cell></cell><cell></cell><cell cols="2">inroom</cell><cell>xroom</cell><cell></cell><cell>inroom</cell><cell>xroom</cell><cell></cell><cell>inroom</cell><cell></cell><cell>xroom</cell><cell></cell><cell></cell></row><row><cell cols="2">1 Nav+cVQA</cell><cell cols="2">64.15</cell><cell>52.47</cell><cell></cell><cell>57.85</cell><cell>55.68</cell><cell></cell><cell>49.38</cell><cell></cell><cell>48.37</cell><cell></cell><cell>53.24</cell></row><row><cell cols="2">2 Nav(RL)+cVQA</cell><cell cols="2">71.24</cell><cell>53.92</cell><cell></cell><cell>74.38</cell><cell>60.81</cell><cell></cell><cell>51.23</cell><cell></cell><cell>46.66</cell><cell></cell><cell>57.40</cell></row><row><cell cols="2">3 Nav+Ctrl+cVQA</cell><cell cols="2">66.41</cell><cell>52.65</cell><cell></cell><cell>57.85</cell><cell>53.48</cell><cell></cell><cell>49.38</cell><cell></cell><cell>48.37</cell><cell></cell><cell>54.44</cell></row><row><cell cols="2">4 Nav(RL)+Ctrl+cVQA</cell><cell cols="2">72.68</cell><cell>58.19</cell><cell></cell><cell>76.86</cell><cell>63.37</cell><cell></cell><cell>54.94</cell><cell></cell><cell>55.57</cell><cell></cell><cell>61.45</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>EQA accuracy on each question type for different approaches.</figDesc><table><row><cell></cell><cell cols="2">object color compare</cell><cell cols="4">object size compare object dist compare room size compare</cell><cell>% overall</cell></row><row><cell></cell><cell>inroom</cell><cell>xroom</cell><cell>inroom</cell><cell>xroom</cell><cell>inroom</cell><cell>xroom</cell><cell></cell></row><row><cell>1 [BestView] + attn-VQA (cnn)</cell><cell>71.16</cell><cell>59.56</cell><cell>65.29</cell><cell>65.93</cell><cell>58.64</cell><cell>49.74</cell><cell>60.50</cell></row><row><cell>2 [BestView] + cVQA (cnn)</cell><cell>82.92</cell><cell>72.70</cell><cell>80.99</cell><cell>83.88</cell><cell>69.75</cell><cell>64.32</cell><cell>74.14</cell></row><row><cell>3 [ShortestPath+BestView] + Ctrl + cVQA</cell><cell>90.70</cell><cell>85.49</cell><cell>82.64</cell><cell>88.64</cell><cell>68.52</cell><cell>71.87</cell><cell>82.88</cell></row><row><cell>4 [ShortestPath] + seq-VQA</cell><cell>53.32</cell><cell>54.44</cell><cell>51.24</cell><cell>50.55</cell><cell>47.53</cell><cell>49.74</cell><cell>52.36</cell></row><row><cell>5 [ShortestPath] + Ctrl + cVQA</cell><cell>76.09</cell><cell>69.11</cell><cell>75.21</cell><cell>79.49</cell><cell>64.20</cell><cell>61.23</cell><cell>69.77</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 :</head><label>7</label><figDesc>EQA accuracy of different approaches on each question type in oracle setting (given shortest path or best-view images).</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>This is a result of noisy annotations in SUNCG and inaccurate occupancy maps due to the axis-aligned assumption returned by House3D.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>House3D returns the the ground-truth semantic segmentation for each first-person view.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>Rather than 0.5 in<ref type="bibr" target="#b7">[8]</ref>, we set the normalized entropy threshold as 0.9 (maximum is 1) since all of our questions have binary answers.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>The</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="588" xml:id="foot_4"><p>environments are subset of EQA-v1's. Some environments are discarded due to entropy filtering and unavailable paths.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgements: We thank <rs type="person">Abhishek Das</rs>, <rs type="person">Devi Parikh</rs> and <rs type="person">Marcus Rohrbach</rs> for helpful discussions. This work is supported by <rs type="funder">NSF</rs> Awards #<rs type="grantNumber">1633295</rs>, <rs type="grantNumber">1562098</rs>, <rs type="grantNumber">1405822</rs>, and <rs type="funder">Facebook</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_k4p29CA">
					<idno type="grant-number">1633295</idno>
				</org>
				<org type="funding" xml:id="_ef8rcW8">
					<idno type="grant-number">1562098</idno>
				</org>
				<org type="funding" xml:id="_WKEQrm6">
					<idno type="grant-number">1405822</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">Ankesh</forename><surname>Anand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eugene</forename><surname>Belilovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Kastner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><surname>Courville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.05013</idno>
		<title level="m">Blindfold baselines for embodied qa</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Vision-and-language navigation: Interpreting visually-grounded navigation instructions in real environments</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Damien</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jake</forename><surname>Bruce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niko</forename><surname>Sünderhauf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anton</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Modular multitask reinforcement learning with policy sketches</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Neural module networks</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Charles</forename><surname>Beattie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><forename type="middle">Z</forename><surname>Leibo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denis</forename><surname>Teplyashin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcus</forename><surname>Wainwright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heinrich</forename><surname>Küttler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Lefrancq</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Víctor</forename><surname>Valdés</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><surname>Sadik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.03801</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">Deepmind lab. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">Simon</forename><surname>Brodeur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ethan</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankesh</forename><surname>Anand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florian</forename><surname>Golemo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Celotti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florian</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean</forename><surname>Rouat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><surname>Home</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.11017</idno>
		<title level="m">A household multimodal environment</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Gated-attention architectures for taskoriented language grounding</title>
		<author>
			<persName><forename type="first">Devendra</forename><surname>Singh Chaplot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kanthashree</forename><forename type="middle">Mysore</forename><surname>Sathyendra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rama</forename><surname>Kumar Pasumarthi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dheeraj</forename><surname>Rajagopal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Embodied question answering</title>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samyak</forename><surname>Datta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2008">2018. 1, 2, 3, 5, 6, 7, 8</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Modular Control for Embodied Question Answering. CoRL</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Neural modular control for embodied question answering</title>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.11181</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning to fly by crashing</title>
		<author>
			<persName><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dhiraj</forename><surname>Gandhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lerrel</forename><surname>Pinto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IROS</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Temporal modular networks for retrieving complex compositional activities in videos</title>
		<author>
			<persName><forename type="first">Li</forename><surname>Fei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">-</forename><surname>Fei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juan</forename><surname>Carlos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niebles</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Iqa: Visual question answering in interactive environments</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aniruddha</forename><surname>Kembhavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Rastegari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dieter</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Rahul Sukthankar, and Jitendra Malik. Cognitive mapping and planning for visual navigation</title>
		<author>
			<persName><forename type="first">Saurabh</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Davidson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning to reason: End-to-end module networks for visual question answering</title>
		<author>
			<persName><forename type="first">Ronghang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Modeling relationship in referential expressions with compositional modular networks</title>
		<author>
			<persName><forename type="first">Ronghang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcus</forename><surname>Rohrbacnh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Clevr: A diagnostic dataset for compositional language and elementary visual reasoning</title>
		<author>
			<persName><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lawrence Zitnick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Inferring and executing programs for visual reasoning</title>
		<author>
			<persName><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lawrence Zitnick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Vizdoom: A doom-based ai research platform for visual reinforcement learning</title>
		<author>
			<persName><forename type="first">Michał</forename><surname>Kempka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marek</forename><surname>Wydmuch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Grzegorz</forename><surname>Runc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakub</forename><surname>Toczek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Jaśkowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computational Intelligence and Games (CIG), 2016 IEEE Conference on</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Ai2-thor: An interactive 3d environment for visual ai</title>
		<author>
			<persName><forename type="first">Eric</forename><surname>Kolve</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roozbeh</forename><surname>Mottaghi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuke</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.05474</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">End-to-end training of deep visuomotor policies</title>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common objects in context</title>
		<author>
			<persName><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zitnick</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note>ImageNet Large Scale Visual Recognition Challenge</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">CAD2RL: Real single-image flight without a single real image</title>
		<author>
			<persName><forename type="first">Fereshteh</forename><surname>Sadeghi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">RSS</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">MINOS: Multimodal indoor simulator for navigation in complex environments</title>
		<author>
			<persName><forename type="first">Manolis</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angel</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.03931</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Semantic scene completion from a single depth image</title>
		<author>
			<persName><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angel</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manolis</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Funkhouser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">Nantas</forename><surname>Gabriel Synnaeve</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Nardelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumith</forename><surname>Auvolat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothée</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeming</forename><surname>Lacroix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florian</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Richoux</surname></persName>
		</author>
		<author>
			<persName><surname>Usunier</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.00625</idno>
		<title level="m">Torchcraft: a library for machine learning research on real-time strategy games</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Building generalizable agents with a realistic and rich 3d environment. ICLR workshop</title>
		<author>
			<persName><forename type="first">Yi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuandong</forename><surname>Tian</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Gibson env: Real-world perception for embodied agents</title>
		<author>
			<persName><forename type="first">Fei</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyang</forename><surname>Amir R Zamir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jitendra</forename><surname>Sax</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Silvio</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Mattnet: Modular attention network for referring expression comprehension</title>
		<author>
			<persName><forename type="first">Licheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohui</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tamara</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Visual semantic planning using deep successor representations</title>
		<author>
			<persName><forename type="first">Yuke</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Kolve</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dieter</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roozbeh</forename><surname>Mottaghi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Target-driven visual navigation in indoor scenes using deep reinforcement learning</title>
		<author>
			<persName><forename type="first">Yuke</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roozbeh</forename><surname>Mottaghi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Kolve</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><forename type="middle">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICRA</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
