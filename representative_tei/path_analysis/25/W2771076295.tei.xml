<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">An Evaluation of Four Solutions to the Forking Paths Problem: Adjusted Alpha, Preregistration, Sensitivity Analyses, and Abandoning the Neyman-Pearson Approach</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Mark</forename><surname>Rubin</surname></persName>
							<email>mark.rubin@newcastle.edu.au</email>
							<affiliation key="aff0">
								<orgName type="department">School of Psychology</orgName>
								<orgName type="institution">The University of Newcastle</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Behavioural Sciences Building</orgName>
								<orgName type="institution" key="instit2">The University of Newcastle</orgName>
								<address>
									<postCode>2308</postCode>
									<settlement>Callaghan</settlement>
									<region>NSW</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">An Evaluation of Four Solutions to the Forking Paths Problem: Adjusted Alpha, Preregistration, Sensitivity Analyses, and Abandoning the Neyman-Pearson Approach</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1037/gpr0000135</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-21T21:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>forking paths</term>
					<term>null hypothesis significance testing</term>
					<term>preregistration</term>
					<term>replication crisis</term>
					<term>sensitivity analyses</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"> Gelman and Loken (2013, 2014)  <p>proposed that when researchers base their statistical analyses on the idiosyncratic characteristics of a specific sample (e.g., a nonlinear transformation of a variable because it is skewed), they open up alternative analysis paths in potential replications of their study that are based on different samples (i.e., no transformation of the variable because it is not skewed). These alternative analysis paths count as additional (multiple) tests and, consequently, they increase the probability of making a Type I error during hypothesis testing. The present article considers this forking paths problem and evaluates four potential solutions that might be used in psychology and other fields: (a) adjusting the prespecified alpha level, (b) preregistration, (c) sensitivity analyses, and (d) abandoning the Neyman-Pearson approach. It is concluded that although preregistration and sensitivity analyses are effective solutions to p-hacking, they are ineffective against result-neutral forking paths, such as those caused by transforming data. Conversely, although adjusting the alpha level cannot address p-hacking, it can be effective for result-neutral forking paths. Finally, abandoning the Neyman-Pearson approach represents a further solution to the forking paths problem.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Figure <ref type="figure" target="#fig_0">1</ref> provides an example of how three of these forking paths can result in eight potential tests. The green boxes in Figure <ref type="figure" target="#fig_0">1</ref> represent the analysis that a researcher actually conducted in their study. The blue boxes represent other potential analysis paths that the researcher did not follow but that could be followed in any subsequent replications of the same study. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Why are Forking Paths Problematic?</head><p>Forking paths are problematic because they increase the number of potential tests in a study's analysis protocol and, consequently, they increase the probability of making a Type I error. To explain, it is necessary to take a step back and consider the <ref type="bibr" target="#b27">Neyman and Pearson (1933)</ref> approach to null hypothesis significance testing. 1 According to this approach, in order to determine if a p value is significant or nonsignificant, its value needs to be judged against a prespecified threshold value (e.g., .05). This alpha level refers to the probability of making a Type I error in a long run of exact replications of that study, with each replication randomly sampling data from the same population. Following this interpretation, if a p value falls below the prespecified alpha level (i.e., p &lt; .05) then, assuming that the null hypothesis is true, a maximum of 5% of samples in the long run of exact replications will yield a result as extreme or more extreme as that observed in the original study. Importantly, the decision about whether or not to classify a result as being "significant" does not apply to the single result from the original study but rather to the series of results from the long run of exact replications of that study. These exact replications do not need to be actually conducted in order to decide whether or not a result is significant. However, they do need to be taken into account when specifying the probability of making a Type I error regarding this decision.</p><p>Forking paths are problematic for the Neyman-Pearson approach because they imply that different tests will be conducted in different exact replications of the same study. For example, if a researcher transforms self-esteem scores because they are skewed in their original study, then they are making an implicit commitment to follow the same conditional rule in subsequent replications based on different samples (i.e., transform self-esteem scores when they are skewed but not when they are not skewed). In the context of each specific sample, this is a reasonable and justifiable rule to follow. Nonetheless, in the context of a long run of exact replications based on different samples, this conditional rule results in two different tests: one based on transformed scores and the other based on nontransformed scores. Hence, this conditional rule represents a forking path in the analysis protocol because it leads to different tests in different replications of the same study.</p><p>The occurrence of different tests in different replications increases the probability of making a Type I error due to multiple testing <ref type="bibr" target="#b7">(de Groot, 2014;</ref><ref type="bibr" target="#b16">Frane, 2015;</ref><ref type="bibr" target="#b38">Szucs, 2016)</ref>. Note that this multiple testing problem occurs even if a researcher has conducted only a single test in their original study. If the choice of that test is determined by the idiosyncrasies of the sample, then the test may change from one sample to the next in the long run, and it will count as two potential tests in the analysis protocol for exact replications of that study. 2 For example, in Figure <ref type="figure" target="#fig_0">1</ref>, the researcher followed only one analysis path. Nevertheless, the three sample-contingent analysis decisions in their analysis protocol result in eight potential tests in replications of the study, and these multiple tests increase the probability of making a Type I error from 5.00% to 33.66% in a long run of exact replications of that study.</p><p>In summary, the forking paths problem is a special case of the more general multiple testing problem. However, a defining feature of the forking paths problem is that multiple tests do not need to be performed in the original study in order to be problematic. Instead, their potential presence in subsequent replications of the original study leads to an "invisible multiplicity" <ref type="bibr">(Gelman &amp; Loken, 2014, p. 460</ref>) that nonetheless increases the probability of making a Type I error. In Gelman and Loken's words, because the justification for p values lies in what would have happened across multiple data sets, it is relevant to consider whether any choices in analysis and interpretation are data dependent and would have been different given other possible data. If so, even in settings where a single analysis has been carried out on the given data, the issue of multiple comparisons emerges because different choices about combining variables, inclusion and exclusion of cases, transformations of variables, tests for interactions in the absence of main effects, and many other steps in the analysis could well have occurred with different data (p. 460).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Solutions to the Forking Paths Problem</head><p>The forking paths problem represents a common and serious threat to the validity of the Neyman-Pearson approach to null hypothesis significance testing because it increases the chances of making a Type I error. Indeed, the forking paths problem may be a contributor to the current replication crisis in psychology <ref type="bibr" target="#b17">(Gelman &amp; Loken, 2013</ref><ref type="bibr">, 2014;</ref><ref type="bibr">Open Science Collaboration, 2015)</ref>. Consequently, it is important to consider methods for diminishing or avoiding this problem. Below, I consider four potential solutions to the forking paths problem: (a) adjusting the prespecified alpha level, (b) preregistration, (c) sensitivity analyses, and (d) abandoning the Neyman-Pearson approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Adjusting the Alpha Level</head><p>A well-recognised solution to the general problem of multiple testing is to adjust the prespecified alpha level in order to account for the increased probability of making a Type I error (e.g., <ref type="bibr" target="#b16">Frane, 2015;</ref><ref type="bibr" target="#b26">Matsunaga, 2007;</ref><ref type="bibr" target="#b32">Rubin, 2017)</ref>. For example, if a researcher undertakes two tests of the same hypothesis, then they can use a Bonferroni adjustment to reduce the prespecified alpha level from .050 to .025 in order to maintain the actual alpha level for their hypothesis at .050. Hence, in theory, it is possible for researchers to adjust their prespecified alpha level in order to compensate for the increased Type I error probability that results from potential multiple testing in the long run based on forking paths.</p><p>One problem with the alpha adjustment solution is that it increases the Type II error probabilitythe probability of incorrectly accepting the null hypothesis <ref type="bibr" target="#b26">(Matsunaga, 2007;</ref><ref type="bibr">Weber, 2007)</ref>. However, there are several different approaches to adjusting the alpha level (e.g., <ref type="bibr" target="#b26">Matsunaga, 2007;</ref><ref type="bibr" target="#b35">Shaffer, 1995)</ref>, and they vary in the degree to which they (a) compensate for the increased Type I error probability and (b) increase the Type II error probability. Hence, researchers can choose approaches that do not have a large impact on the Type II error probability if this is a concern.</p><p>A potentially bigger problem for the alpha adjustment solution is that, in non-preregistered research studies, it is not possible to exhaustively identify all of the tests that could have been undertaken. The forking paths problem implies that some of these potential tests may be conducted in some samples in the long run of exact replications. Consequently, the forking paths problem leads to an indeterminate number of potential tests in the long run, and this makes it impossible to calculate the extent to which the prespecified alpha level needs to be adjusted (e.g., <ref type="bibr" target="#b7">de Groot, 2014;</ref><ref type="bibr" target="#b15">Forstmeier, Wagenmakers, &amp; Parker, 2016;</ref><ref type="bibr" target="#b17">Gelman &amp; Loken 2013</ref><ref type="bibr">, 2014)</ref>.</p><p>However, indeterminate potential testing is only problematic for certain types of forking path. Below, I consider result-neutral forking paths and two types of result-biased forking pathsfishing and phacking. I show how the alpha adjustment solution is tenable for result-neutral forking paths, potentially unnecessary for fishing, and untenable for p-hacking.</p><p>Result-neutral forking paths. Forking paths can be based on either result-biased analysis rules or result-neutral analysis rules. Result-biased analysis rules specify an iterative form of testing that continues until a significant result is obtained. Consequently, result-biased forking paths produce an indeterminate number of potential tests in the long run because it is unclear how many tests will need to be conducted in subsequent samples in order to obtain a significant result.</p><p>In contrast, result-neutral analysis rules specify a noniterative form of testing that is based on logical and/or conventional analytical principles. Consequently, result-neutral forking paths generate a fixed and knowable number of potential tests in the long run. For example, the analysis rule "if self-esteem scores have ±2.0 skewness, then log10 transform them" generates only two potential tests in the long run: one in which scores are transformed and one in which they are not transformed.</p><p>The alpha adjustment solution cannot be applied to result-biased forking paths because they produce an indeterminate number of potential tests in the long run. However, it can be applied to resultneutral forking paths because they produce a fixed and knowable number of potential tests in the long run. For example, the prespecified alpha level can be lowered to compensate for the two tests that are implied in the long run when a researcher log10 transforms their self-esteem data <ref type="bibr" target="#b32">(Rubin, 2017)</ref>.</p><p>It may be objected that even result-neutral analysis rules can produce an indeterminate number of potential tests in the long run because there are many potential variations to the specifications of each rule, and each variation produces more potential tests. For example, the threshold value for the analysis rule "exclude self-esteem data that exceeds ±3.0 SDs from the sample mean" has the potential to be varied from ±3.0 SDs to ±2.5 SDs, ±2.0 SDs, or any other value. However, these potential variations do not affect the Type I error probability for the original ±3.0 SDs test because they do not represent exact replications of that test. If a researcher actually used a threshold value of ±3.0 SDs in their original test, then a potential variation of that test that used a threshold value of ±2.5 SDs would represent an inexact replication of the original test, and so it would not count as part of the long run of exact replications of that test. Consequently, potential variations of actual analysis rules do not inflate the Type I error probability associated with those rules. If this was not the case, then even preregistered analysis rules would be susceptible to Type I error inflation because a large number of potential variations to those rules remains possible outside of the preregistered testing protocol (e.g., <ref type="bibr" target="#b36">Silberzahn et al., 2017)</ref>.</p><p>It might also be objected that researchers may not report some actual analysis rules. For example, researchers might report that they excluded outliers using the ±3.0 SDs rule but fail to report that they also tried excluding outliers using a ±2.5 SDs rule in order to see how this alternative test affected their results. However, this approach would represent a result-biased analysis rule (p-hacking) rather than a result-neutral analysis rule. I discuss result-biased rules below.</p><p>In summary, the alpha adjustment solution is tenable for result-neutral forking paths, such as nonlinear transformations or outlier exclusions, because they produce a fixed and knowable number of potential tests in the long run. In contrast, result-biased forking paths produce an indeterminate number of potential tests in the long run and, consequently, the alpha adjustment solution is not possible for this type of forking path. However, although alpha adjustment is not possible for result-biased forking paths, it may not always be necessary. In particular, alpha adjustment can be unnecessary in the case of fishing.</p><p>Fishing. Fishing involves testing many different hypotheses in an effort to find a significant result. This result-biased analysis strategy produces an indeterminate number of potential tests in the long run of exact replications because it is unclear how many different hypotheses the researcher would need to test in future samples in order to obtain a significant result. Hence, it is not possible to compute an alpha adjustment to compensate for fishing (e.g., <ref type="bibr" target="#b7">de Groot, 2014;</ref><ref type="bibr" target="#b15">Forstmeier, Wagenmakers, &amp; Parker, 2016;</ref><ref type="bibr" target="#b17">Gelman &amp; Loken 2013</ref><ref type="bibr">, 2014)</ref>. But does fishing always require an alpha adjustment? The answer depends on how researchers operationalize the familywise error rate.</p><p>The familywise error rate is the probability of making at least one Type I error in a series (or "family") of statistical tests. There are two ways of operationalizing the familywise error rate. The first way is in terms of multiple tests of several different hypotheses (e.g., all of the hypotheses in a study; <ref type="bibr" target="#b7">de Groot, 2014)</ref>. In this case, the familywise error rate refers to the probability of making a Type I error when testing a joint null hypothesis that any of the hypothesised effects is zero. The theoretical implications of rejecting this joint null hypothesis are unclear when its constituent individual hypotheses are derived from different theories <ref type="bibr" target="#b32">(Rubin, 2017)</ref>. It is more meaningful to test a joint hypothesis when its constituent hypotheses are derived from the same theory because, in this case, its rejection can be taken as providing support for the theory <ref type="bibr" target="#b20">(Hewes, 2003)</ref>. However, even under these circumstances, it is only necessary to adjust the alpha level for each constituent hypothesis if a significant result (i.e., p &lt; αadjusted) for any one of the constituent hypotheses is regarded as being sufficient to reject the joint null hypothesis as a whole <ref type="bibr" target="#b26">(Matsunaga, 2007;</ref><ref type="bibr" target="#b32">Rubin, 2017;</ref><ref type="bibr">Weber, 2007)</ref>. 3 Although this approach may be appropriate in some fields, psychology researchers do not normally investigate theories (i.e., collections of hypotheses) in this way. Instead, they usually test theories on a hypothesis-by-hypothesis basis in order to understand which hypotheses have been confirmed and which have been disconfirmed and may need revising or rejecting <ref type="bibr" target="#b25">(Lakatos, 1976)</ref>. Hence, even when it is theoretically-meaningful to test a joint null hypothesis, most psychology researchers are likely to test each individual hypothesis separately, at its own individual unadjusted alpha level, rather than jointly at an adjusted alpha level.</p><p>The second way of operationalizing the familywise error rate is in relation to multiple tests of the same individual hypothesis <ref type="bibr" target="#b26">(Matsunaga, 2007;</ref><ref type="bibr" target="#b32">Rubin, 2017)</ref>. In this case, an alpha adjustment is only required to correct for the inflated error rate caused by multiple tests of that hypothesis; not multiple tests of a joint hypothesis that comprises several different hypotheses.</p><p>The way in which the familywise error rate is operationalized has important implications for alpha adjustment in the context of fishing. If familywise error refers to several different hypotheses, then alpha adjustment is necessary in order to compensate for the potential testing of different hypotheses that results from fishing. However, it is not possible to make this adjustment because the number of potential tests is unknown. Consequently, the alpha adjustment solution fails in this case <ref type="bibr" target="#b7">(de Groot, 2014;</ref><ref type="bibr" target="#b17">Gelman &amp; Loken, 2013</ref><ref type="bibr">, 2014)</ref>. In contrast, if familywise error is limited to tests of the same individual hypothesis <ref type="bibr" target="#b26">(Matsunaga, 2007;</ref><ref type="bibr" target="#b32">Rubin, 2017)</ref>, then alpha adjustment is not necessary to compensate for potential tests of different hypotheses in the long run. Consequently, alpha adjustment is not necessary in the context of fishing.</p><p>To illustrate, consider a researcher who follows the fishing strategy by testing Hypothesis 1 first and then only proceeding to test Hypotheses 2 to X (some undetermined number) if they find no significant result in relation to Hypothesis 1. If familywise error refers to several different hypotheses (i.e., Hypotheses 1 to X), then this result-biased analysis strategy represents a problematic forking path in the long run of exact replications because, although Hypotheses 2 to X may not be tested in the original study, one or more of them may be tested in subsequent studies that yield nonsignificant results for Hypothesis 1. Consequently, the familywise error rate is inflated to an unknown extent, and it cannot be adjusted appropriately. In contrast, if the familywise error rate is limited to multiple tests of the same individual hypothesis, then the probability of making a Type I error for Hypothesis 1 is unrelated to the probability of making Type I errors for any of Hypotheses 2 to X <ref type="bibr" target="#b26">(Matsunaga, 2007)</ref>. In this case, each hypothesis is tested at its own individual alpha level rather than in relation to a joint null hypothesis. Consequently, although one or more of Hypotheses 2 to X may be tested in the long run, this analytical variability does not inflate the Type I error probability of the test of Hypothesis 1.</p><p>In summary, if the familywise error rate is limited to multiple tests of the same individual hypothesis, then neither the actual testing nor the potential testing of different hypotheses inflates the Type I error probabilities of each individual hypothesis. Consequently, no alpha adjustment is required is required for fishing. Alpha adjustment is only required for actual or potential multiple tests of the same individual hypothesis. 4 p-hacking. Fishing involves the actual and potential testing of several different hypotheses with the aim of finding a significant result. In contrast, p-hacking involves the actual and potential testing of the same individual hypothesis with the aim of finding a significant result <ref type="bibr" target="#b37">(Simmons, Nelson, &amp; Simonsohn, 2011)</ref>. For example, a researcher might investigate the hypothesis that men have higher self-esteem than women by conducting tests with and without outliers, with and without various transformations, and with and without various covariates. They might find a significant result only when they log10 transform the data and exclude outliers at 2.5 SDs, and so they might only report that result and conceal their other nonsignificant results.</p><p>Like fishing, p-hacking is based on a result-biased analysis rule in which an iterative testing procedure only ceases once the desired result has been obtained. Consequently, like fishing, p-hacking leads to an indeterminate number of potential tests in the long run of exact replications. However, unlike fishing, p-hacking involves conducting multiple tests of the same individual hypothesis, rather than different hypotheses that comprise a joint hypothesis. Consequently, even when familywise error rate is operationalized in terms of the same individual hypothesis, p-hacking results in an indeterminate number of potential tests in the long run of exact replications and an unknown inflation of the Type I error. Consequently, the alpha adjustment solution fails in the presence of p-hacking. 5</p><p>Summary. In summary, the alpha adjustment solution is tenable for result-neutral forking paths (e.g., "exclude self-esteem data if it exceeds ±3.0 SDs from the sample mean"). Furthermore, if familywise error is limited to the same individual hypothesis, then alpha adjustment is not necessary for result-biased forking paths that refer to different hypotheses, such as fishing. However, the alpha adjustment solution is not tenable for result-biased forking paths that refer to the same hypothesis, such as p-hacking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Preregistration</head><p>Gelman and Loken <ref type="bibr">(2013,</ref><ref type="bibr">2014</ref>) suggested preregistration as a potential solution to the forking paths problem (e.g., <ref type="bibr" target="#b28">Nosek, Ebersole, DeHaven, &amp; Mellor, 2017;</ref><ref type="bibr" target="#b29">Nosek &amp; Lakens, 2014;</ref><ref type="bibr" target="#b41">Wagenmakers, Wetzels, Borsboom, van der Maas, &amp; Kievit, 2012</ref>). However, this potential solution suffers from a major drawback. Specifically, if any part of a preregistered protocol describes sample-contingent "if…then" rules for analyses, then it will succumb to the same forking paths problem as non-preregistered research. For example, a preregistered protocol might specify: "If outcome scores are skewed ≥ ±2.0, then log10 transform them. Otherwise, leave them nontransformed." Despite being preregistered, this analysis rule represents a forking path that leads to multiple testing in the long run and increases the Type I error probability. Hence, preregistration per se does not mitigate against the forking paths problem.</p><p>Notably, advocates of preregistration encourage the explicit identification of sample-contingent analysis rules in analysis protocols. For example, in their discussion of a template for preregistered research in social psychology, van't Veer and Giner-Sorolla (2016) suggested that researchers specify contingencies for (a) handling outliers, (b) removing items that reduce reliability, (c) transforming variables to achieve more normal distributions, and (d) statistical corrections for heterogeneity of variance. Similarly, Chambers, Feredoes, Muthukumaraswamy, and Etchells (2014) stated that: pre-registration does not require every step of an analysis to be specified or "hardwired"; instead, in such cases where the analysis decision is contingent on some aspect of the data itself then the pre-registration only requires the decision tree to be specified (e.g. "If A is observed then we will adopt analysis A1 but if B is observed then we will adopt analysis B1") (p. 12, my emphasis).</p><p>However, merely describing forking paths in a decision tree does not do anything to reduce their impact on the Type I error probability <ref type="bibr" target="#b16">(Frane, 2015)</ref>, and it is this issue that needs to be addressed in order to solve the forking paths problem.</p><p>Most recently, <ref type="bibr" target="#b28">Nosek et al. (2017)</ref> summarised four strategies that are intended to address samplecontingent analyses in the preregistration paradigm. The first two strategies blind researchers to the substantive research results. In the first strategy, researchers first preregister their approach to data exclusions, transformations, model assumptions, etc. They then undertake their preliminary preregistered analyses on the data without proceeding to undertake more theoretically-substantive analyses. This preliminary data analysis stage allows researchers to select the most appropriate testing approach for their data. The researchers then specify this testing approach in a second preregistered analysis protocol that also includes more theoretically-substantive analyses. This two-stage sequential preregistration approach allows researchers to make decisions about data exclusions, transformations, model assumptions, etc. on the basis of their observed data without being influenced by their more substantive research results.</p><p>The second strategy is to blind researchers to the identity of different variables (e.g., hide variable names) but to allow researchers to observe the distributional characteristics of those variables (e.g., outliers, skewness). Again, this approach allows researchers to make decisions about outliers, transformations, modelling assumptions, etc. on the basis of their data without allowing them to know how those decisions impact on their more substantive research results.</p><p>Both of the above blinding strategies preclude result-biased forking paths such as p-hacking. However, neither strategy mitigates against result-neutral forking paths (e.g., exclude self-esteem data if it exceeds ±3.0 SDs from the sample mean). In other words, although both strategies prevent analyses from being influenced by the research results, neither strategy prevents analyses from being influenced by the idiosyncrasies of the data. Concealing substantive research results from researchers when they make datacontingent analysis decisions about how to handle outliers, non-normal distributions, and modelling assumptions does not prevent these decisions from creating forking paths in the long run of exact replications. Hence, although these blinding approaches are effective against p-hacking, they are ineffective against other, result-neutral forking paths. <ref type="bibr">Nosek et al.'s (2017)</ref> third and fourth strategies consist of documenting all of the potential forking paths that may result in a sample-contingent analysis, either by preregistering a decision tree for that analysis (see also <ref type="bibr" target="#b6">Chambers et al., 2014)</ref> or by referring to a more general set of standard operating procedures. Again, however, merely identifying and recording forking paths in either preregistered or postregistered documents does not reduce their impact on Type I error probabilities <ref type="bibr" target="#b16">(Frane, 2015)</ref>. Instead, researchers need to adjust their prespecified alpha level in order to reduce the probability of making a Type I error in the long run.</p><p>In order to eliminate forking paths and, consequently, the need to adjust the alpha level, a preregistered analysis protocol must contain a precise description of an analysis that is to be followed regardless of the idiosyncratic characteristics of the observed sample of data. The difficulty with this approach is that any inferential benefits that are gained by eliminating forking paths must be offset against the inferential costs that are associated with an often poor quality analytical approach. In particular, if preregistered analysis protocols prevent researchers from adapting their analyses to the specific characteristics of their data, then researchers will be unable to (a) clean the data (e.g., <ref type="bibr" target="#b31">Osborne, 2012)</ref>, (b) improve the reliability and validity of their measurements (e.g., <ref type="bibr" target="#b33">Schmidt &amp; Hunter, 1999)</ref>, (c) adjust their statistical tests in order to meet test assumptions (e.g., <ref type="bibr" target="#b9">Erceg-Hurn &amp; Mirosevich, 2008)</ref>, and (d) control for relevant covariates (e.g., <ref type="bibr" target="#b4">Bernerth &amp; Aguinis, 2016)</ref>. Hence, an effective preregistration solution is caught between two equally problematic threats to statistical inference: Allowing sample-contingent analysis rules in preregistered research protocols creates forking paths, but disallowing them reduces the quality of the data analyses.</p><p>In summary, although preregistration represents a tenable solution to result-biased forking paths, such as p-hacking, it fails to mitigate against the impact of result-neutral forking paths, such as excluding outliers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sensitivity Analyses</head><p>A third solution to the forking paths issue is to undertake sensitivity analyses (e.g., <ref type="bibr" target="#b39">Thabane et al., 2013;</ref><ref type="bibr">Wasserstein &amp; Lazar, 2016;</ref><ref type="bibr">Wigboldus &amp; Dotsch, 2016)</ref>. This approach requires researchers to be cognizant of the various forking paths that they take during their analyses and, at each fork, to investigate potential discrepancies in their results if they take the alternative path. So, for example, if the data indicate that it is appropriate to transform scores, then researchers should report the results of tests that are conducted when scores are (a) transformed and (b) not transformed. This approach allows researchers and their audience to understand the variability in their results across different analytical approaches.</p><p>Like preregistration, sensitivity analyses provide an effective solution to the p-hacking problem. In particular, they can be used to reveal disconfirming results that would otherwise remain undisclosed. However, sensitivity analyses are not an effective solution to the result-neutral forking paths for three reasons.</p><p>First, like preregistration, sensitivity analyses per se do not lower the prespecified alpha level. Consequently, any result that is classed as "significant" in a sensitivity analysis may be reclassified as "nonsignificant" after an appropriate alpha adjustment has been implemented. For example, a sensitivity analysis of a test with α = .05 may return significant results of p = .003 when scores are transformed and p = .030 when scores are not transformed. Based on these consistently-significant results, the researcher might conclude that their results are robust to variations in their analytical approach. However, the second of these results would be reclassed as being nonsignificant if the alpha level was adjusted to .025 in order to account for multiple testing. Hence, consistency across results cannot be established using sensitivity analyses per se.</p><p>Second, exploring an alternative analytical path in a sample whose characteristics do not warrant that analytical path will result in a suboptimal test, and any results from that test will need to be interpreted with caution. Returning to the previous example, if data are skewed, then only the test that is conducted on the transformed data is valid, and the results of the test that is conducted on the nontransformed data must be interpreted with caution.</p><p>Finally, but most importantly, sensitivity analyses only reveal the influence of different analytical approaches in the original study and not in the long run of exact replications of that study. Consequently, they do not provide any indication of the potential variability in results in the long run.</p><p>In summary, although sensitivity analyses can be used to identify and deter p-hacking, they do not provide an effective solution to result-neutral forking paths because (a) they do not compensate the alpha level for the multiple testing that is associated with forking paths, (b) they include suboptimal tests, and (c) they only reveal variability in results in an original study and not in the long run of exact replications of that study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abandon the Neyman-Pearson Approach</head><p>The final solution to the forking paths problem is to abandon the Neyman-Pearson approach to null hypothesis significance testing. Abandoning the Neyman-Pearson approach solves the forking paths problem because forking paths are only meaningful in the context of a long run of exact replications and, in the absence of the Neyman-Pearson approach, researchers do not need to interpret their results in this context. All that matters are the actual tests that are conducted on the original sample of data, not the potential tests that might be conducted on subsequent samples of data. But, if the Neyman-Pearson approach is abandoned, then what is left? I briefly consider two potential alternatives.</p><p>The first alternative is to continue to use p values but to interpret them in the manner advocated by <ref type="bibr" target="#b12">Fisher (1937</ref><ref type="bibr" target="#b13">Fisher ( , 1955</ref><ref type="bibr" target="#b14">Fisher ( , 1956</ref>) rather than <ref type="bibr" target="#b27">Neyman and Pearson (1933)</ref>. The Fisherian and neo-Fisherian approaches interpret p values in relation to the current sample of data rather than a long run of exact replications <ref type="bibr" target="#b13">(Fisher, 1955;</ref><ref type="bibr" target="#b19">Haig, 2016;</ref><ref type="bibr" target="#b21">Hubbard &amp; Bayarri, 2003;</ref><ref type="bibr" target="#b23">Hurlbert &amp; Lombardi, 2009;</ref><ref type="bibr" target="#b34">Schneider, 2015)</ref>. Consequently, Fisherian p values are more suitable for context-dependent studies that have less potential for exact replication, such as those conducted in psychology and the social sciences. Critically, from a Fisherian perspective, the multiple testing issue only needs to be considered in relation to tests that are actually performed on the current data rather than tests that have the potential to be performed in a series of exact replications. Hence, forking paths are not problematic in the Fisherian paradigm.</p><p>Unlike the Neyman-Pearson approach, Fisherian p values are interpreted on a sliding scale of probability in which the smaller the p value, the less likely it is that the observed data would occur if the null hypothesis was true <ref type="bibr" target="#b0">(Amrhein, Korner-Nievergelt, &amp; Roth, 2017;</ref><ref type="bibr" target="#b2">Biau, Jolles, &amp; Porcher, 2010;</ref><ref type="bibr" target="#b5">Bradley &amp; Brand, 2016;</ref><ref type="bibr" target="#b10">Falissard, 2012;</ref><ref type="bibr" target="#b19">Haig, 2016;</ref><ref type="bibr" target="#b21">Hubbard &amp; Bayarri, 2003;</ref><ref type="bibr">Wasserstein &amp; Lazar, 2016)</ref>. Hence, from a Fisherian perspective, p = .0004 indicates stronger evidence against the null hypothesis than p = .04 <ref type="bibr" target="#b22">(Hubbard &amp; Lindsay, 2008)</ref>.</p><p>Despite this sliding scale interpretation, <ref type="bibr" target="#b11">Fisher (1926)</ref> suggested that p &lt; .05 may sometimes be a useful threshold for determining the significance of a result. However, he stressed that researchers need to make context-based interpretations of specific p values. As he pointed out, "no scientific worker has a fixed level of significance at which from year to year, and in all circumstances, he rejects hypotheses; he rather gives his mind to each particular case in the light of his evidence and his ideas" <ref type="bibr">(Fisher, 1956, p. 42)</ref>. This context-based approach provides a more nuanced and sophisticated approach to significance testing, in which researchers interpret the meaning of p values based on a range of background factors including (a) the size of the p value, (b) the number of tests that have been used to assess the null hypothesis, (c) the theoretical plausibility of the explanation, (d) prior evidence for the effect, (e) the effect size, (f) the variability of the effect, (g) the quality of the research methodology (e.g., sample size, measurement precision, construct validity, etc.), (h) the results of sensitivity analyses, and (i) the results of tests of alternative explanations for the effect (e.g., <ref type="bibr" target="#b0">Amrhein et al., 2017;</ref><ref type="bibr">Wasserstein &amp; Lazar 2016)</ref>. For example, researchers should interpret a p value of .025 differently when it is used to infer the existence of extrasensory perception compared to when it is used to infer the existence of ethnic prejudice.</p><p>The second alternative to the Neyman-Pearson approach is more radical in that it involves abandoning p values altogether. The Bayesian approach replaces p values with Bayes factors that integrate information about the prior probability of a hypothesis based on prior theory and evidence with information about the posterior probability of the hypothesis based on the sampled data. Like the Fisherian approach, the Bayesian approach operates on a sliding scale of probability. For example, <ref type="bibr" target="#b24">Jeffreys (1961)</ref> suggested the following scale for interpreting Bayes factors: "barely worth mentioning," "substantial," "strong," "very strong," and "decisive."</p><p>As with the Fisherian approach, the Bayesian approach only relates to the current sample of data; it is not concerned with a long run of exact replications. Consequently, it is not susceptible to the forking paths problem <ref type="bibr" target="#b8">(Dienes, 2011</ref>; see also <ref type="bibr">Bender &amp; Lange, 2001)</ref>. As <ref type="bibr" target="#b7">Wagenmakers et al. (2014</ref><ref type="bibr" target="#b7">, in de Groot, 2014)</ref> explained, "what is relevant for Bayesian reasoning is not the number of tests that were executed or planned, but rather the prior belief in a particular hypothesis <ref type="bibr">[Scott &amp; Berger, 2010;</ref><ref type="bibr">Stephens &amp; Balding, 2009]" (p. 194)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusions</head><p>The forking paths problem represents a serious threat to the Neyman-Pearson approach of null hypothesis significance testing. Specifically, it can increase the probability of making a Type I error across the long run of exact replications. The present article explored four potential solutions to this problem.</p><p>Preregistration and sensitivity analyses provide effective solutions to result-biased forking paths such as p-hacking. However, they do not mitigate against result-neutral forking paths. If a preregistered analysis protocol includes a result-neutral analysis rule such as "exclude data if it exceeds ±3 SDs from the sample mean," then it will be just as susceptible to the forking paths problem as non-preregistered research. Similarly, although sensitivity analyses may be used to uncover or deter p-hacking, this approach does not reduce the inflated Type I error probability that results from multiple testing in the long run.</p><p>The two best solutions to forking paths relate to the prespecified alpha level. The first solution is to adjust the alpha level in order to compensate for the inflated risk of making a Type I error. This solution can be used for result-neutral forking paths (e.g., "exclude data if it exceeds ±3 SDs from the sample mean") because they produce a fixed and knowable number of potential tests in the long run. Alpha adjustment is not possible for fishing when familywise error refers to different hypotheses. However, it is not necessary when familywise error refers to the same hypothesis. Finally, alpha adjustment is not possible in the context of p-hacking under any circumstances.</p><p>The second solution is to abandon the Neyman-Pearson approach and, consequently, to abandon concerns about the alpha level of a long run of exact replications. Two suitable alternative statistical approaches are the Fisherian and Bayesian approaches.  <ref type="bibr" target="#b0">(Amrhein et al., 2017;</ref><ref type="bibr" target="#b2">Biau et al., 2010;</ref><ref type="bibr" target="#b5">Bradley &amp; Brand, 2016;</ref><ref type="bibr" target="#b21">Hubbard &amp; Bayarri, 2003;</ref><ref type="bibr" target="#b34">Schneider, 2015)</ref>. I discuss the Fisherian approach later on in this article. Hence, to avoid any confusion, I refer to the typical method of null hypothesis significance testing as the Neyman-Pearson approach. 2. Given that larger samples are more representative of the populations from which they are drawn, the characteristics of larger samples are more likely to reflect the characteristics of their population. Hence, if self-esteem scores are skewed beyond some threshold criterion in the first sample that is drawn from a population, then they are likely to be skewed beyond that criterion in most other large samples from that population. Consequently, if researchers use large samples, then their analytical path in exact replications is likely to be the same as that taken in the original study (e.g., self-esteem scores are skewed, and so they are transformed). However, even when using large samples, it remains possible that, in at least one case during a long run of exact replications, the relevant characteristic in the sample will be decisively different from that in the population and therefore require an alternative analysis.</p><p>Based on the Neyman-Pearson approach, this single alternative analysis will then count as an additional test in the analysis protocol, and it will increase the probability of making a Type I error. In other words, although researchers may almost always turn left down a particular forking path, the fact that a right turn is possible must be taken into consideration when interpreting decisions based on null hypothesis significance tests. 3. Based on the multiplication rule for independent events, the familywise error rate is computed using the formula: 1 -(1 -α) k , where α is the prespecified alpha level and k is the number of tests in the family. In order to compute the probability of incorrectly rejecting a joint null hypothesis, this formula multiplies the probability of incorrectly rejecting each constituent null hypothesis by the number of hypotheses in the family. However, this computation is only necessary if researchers intend to reject the joint null hypothesis on the basis of a significant result (i.e., p &lt; αadjusted) in relation to any one of the constituent null hypotheses (i.e., logical disjunction; Weber, 2007). 4. My definition of fishing includes the strategy of testing for interaction effects when main effects are nonsignificant. Contrary to <ref type="bibr">Gelman and</ref><ref type="bibr">Loken (2013, 2014)</ref>, this strategy involves the testing of different hypotheses, rather than the same hypothesis, because different theoretical rationales are required in order to explain main effects and interactions <ref type="bibr" target="#b26">(Matsunaga, 2007)</ref>. For example, the hypothesis that men have higher self-esteem than women (i.e., a main effect of gender on self-esteem) will have a different theoretical explanation to the hypothesis that this gender difference is stronger among young people than among older people (i.e., an interaction between gender and age in predicting self-esteem). 5. It is important to note that even if alpha adjustment was possible to compensate for multiple testing based on p-hacking, p-hacking would remain an invalid and unethical form of data analysis because it precludes reported falsification.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. An illustration of three forking paths in which decisions about removing items, transforming scores, and including covariates are based on the idiosyncrasies of a specific sample of data.</figDesc><graphic coords="3,72.00,135.24,467.91,263.25" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Wasserstein, R. L., &amp; Lazar, N. A. (2016). The ASA's statement on p-values: Context, process, and purpose.AmericanStatistician, 70, 129-133. doi: 10.1080/00031305.2016.1154108  Weber, R. (2007). Responses to Matsunaga: To adjust or not to adjust alpha in multiple testing: That is the question. Guidelines for alpha adjustment as response to O'Keefe's and Matsunaga's critiques.Endnotes 1. The typical method of conducting null hypothesis significance tests often represents a hybrid of the Neyman-Pearson and Fisherian approaches</figDesc><table><row><cell>Communication Methods and Measures, 1, 281-289. doi: 10.1080/19312450701641391</cell></row><row><cell>Wigboldus, D. H., &amp; Dotsch, R. (2016). Encourage playing with data and discourage questionable reporting</cell></row><row><cell>practices. Psychometrika, 81, 27-32. doi: 10.1007/s11336-015-9445-1</cell></row></table></figure>
		</body>
		<back>

			
			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Funding</head><p>The author declares no funding sources.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conflict of Interest</head><p>The author declares no conflict of interest.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<author>
			<persName><forename type="first">V</forename><surname>Amrhein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Korner-Nievergelt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Roth</surname></persName>
		</author>
		<idno type="DOI">10.7717/peerj.3544</idno>
	</analytic>
	<monogr>
		<title level="m">The earth is flat (p &gt; 0.05): Significance thresholds and the crisis of unreplicable research</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Adjusting for multiple testing-when and how</title>
		<author>
			<persName><forename type="first">R</forename><surname>Bender</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lange</surname></persName>
		</author>
		<idno type="DOI">10.1016/S0895-4356(00)00314-0</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Clinical Epidemiology</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page" from="343" to="349" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">P value and the theory of hypothesis testing: an explanation for new researchers</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Biau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M</forename><surname>Jolles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Porcher</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11999-009-1164-4</idno>
	</analytic>
	<monogr>
		<title level="j">Clinical Orthopaedics and Related Research®</title>
		<imprint>
			<biblScope unit="volume">468</biblScope>
			<biblScope unit="page" from="885" to="892" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Adjusting for multiple testing-when and how</title>
		<author>
			<persName><forename type="first">R</forename><surname>Bender</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lange</surname></persName>
		</author>
		<idno type="DOI">10.1016/S0895-4356(00)00314-0</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Clinical Epidemiology</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page" from="343" to="349" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A critical review and best-practice recommendations for control variable usage</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Bernerth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Aguinis</surname></persName>
		</author>
		<idno type="DOI">10.1111/peps.12103</idno>
	</analytic>
	<monogr>
		<title level="j">Personnel Psychology</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="page" from="229" to="283" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Significance testing needs a taxonomy or how the Fisher, Neyman-Pearson controversy resulted in the inferential tail wagging the measurement dog</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Bradley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Brand</surname></persName>
		</author>
		<idno type="DOI">10.1177/0033294116662659</idno>
	</analytic>
	<monogr>
		<title level="j">Psychological Reports</title>
		<imprint>
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="page" from="487" to="504" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Instead of &quot;playing the game&quot; it is time to change the rules: Registered Reports at AIMS Neuroscience and beyond</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Chambers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Feredoes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">D</forename><surname>Muthukumaraswamy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Etchells</surname></persName>
		</author>
		<idno type="DOI">10.3934/Neuroscience2014.1.4</idno>
	</analytic>
	<monogr>
		<title level="j">AIMS Neuroscience</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4" to="17" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The meaning of &quot;significance&quot; for different types of research</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>De Groot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Wagenmakers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Borsboom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Verhagen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kievit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bakker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cramer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">L</forename><surname>…van Der Maas</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.actpsy.2014.02.001</idno>
	</analytic>
	<monogr>
		<title level="j">Acta Psychologica</title>
		<imprint>
			<biblScope unit="volume">148</biblScope>
			<biblScope unit="page" from="188" to="194" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note>Translated</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Bayesian versus orthodox statistics: Which side are you on?</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Dienes</surname></persName>
		</author>
		<idno type="DOI">10.1177/1745691611406920</idno>
	</analytic>
	<monogr>
		<title level="j">Perspectives on Psychological Science</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="274" to="290" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Modern robust statistical methods: An easy way to maximize the accuracy and power of your research</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Erceg-Hurn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">M</forename><surname>Mirosevich</surname></persName>
		</author>
		<idno type="DOI">10.1037/0003-066X.63.7.591</idno>
	</analytic>
	<monogr>
		<title level="j">American Psychologist</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="page" from="591" to="601" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Statistics in brief: When to use and when not to use a threshold p value</title>
		<author>
			<persName><forename type="first">B</forename><surname>Falissard</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11999-011-2090-9</idno>
	</analytic>
	<monogr>
		<title level="j">Clinical Orthopaedics and Related Research</title>
		<imprint>
			<biblScope unit="volume">470</biblScope>
			<biblScope unit="page" from="315" to="316" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Statistical methods for research workers</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Fisher</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1926">1926</date>
			<publisher>Oliver and Boyd</publisher>
			<pubPlace>Edinburgh, UK</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">The design of experiments</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Fisher</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1937">1937</date>
			<publisher>Oliver and Boyd</publisher>
			<pubPlace>Edinburgh, UK</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Statistical methods and scientific induction</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Fisher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society. Series B (Methodological)</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="69" to="78" />
			<date type="published" when="1955">1955</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Statistical methods and scientific inference</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Fisher</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1956">1956</date>
			<publisher>Oliver and Boyd</publisher>
			<pubPlace>Edinburgh, UK</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Detecting and avoiding likely false-positive findings-a practical guide</title>
		<author>
			<persName><forename type="first">W</forename><surname>Forstmeier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Wagenmakers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">H</forename><surname>Parker</surname></persName>
		</author>
		<idno type="DOI">10.1111/brv.12315</idno>
	</analytic>
	<monogr>
		<title level="j">Biological Reviews</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Planned hypothesis tests are not necessarily exempt from multiplicity adjustment</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V</forename><surname>Frane</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Research Practice</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">2</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">The garden of forking paths: Why multiple comparisons can be a problem, even when there is no &quot;fishing expedition&quot; or &quot;p-hacking&quot; and the research hypothesis was posited ahead of time</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gelman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Loken</surname></persName>
		</author>
		<ptr target="http://www.stat.columbia.edu/~gelman/research/unpublished/p_hacking.pdf" />
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
		<respStmt>
			<orgName>Department of Statistics, Columbia University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The statistical crisis in science</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gelman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Loken</surname></persName>
		</author>
		<idno type="DOI">10.1511/2014.111.460</idno>
	</analytic>
	<monogr>
		<title level="j">American Scientist</title>
		<imprint>
			<biblScope unit="volume">102</biblScope>
			<biblScope unit="page">460</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Tests of statistical significance made sound</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">D</forename><surname>Haig</surname></persName>
		</author>
		<idno type="DOI">10.1177/0013164416667981</idno>
	</analytic>
	<monogr>
		<title level="j">Educational and Psychological Measurement</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="page" from="489" to="506" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Methods as tools</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Hewes</surname></persName>
		</author>
		<idno type="DOI">10.1111/j.1468-2958.2003.tb00847.x</idno>
	</analytic>
	<monogr>
		<title level="j">Human Communication Research</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="448" to="454" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Confusion over measures of evidence (p&apos;s) versus errors (α&apos;s) in classical statistical testing</title>
		<author>
			<persName><forename type="first">R</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Bayarri</surname></persName>
		</author>
		<idno type="DOI">10.1198/0003130031856</idno>
	</analytic>
	<monogr>
		<title level="j">The American Statistician</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page" from="171" to="178" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Why p values are not a useful measure of evidence in statistical significance testing</title>
		<author>
			<persName><forename type="first">R</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Lindsay</surname></persName>
		</author>
		<idno type="DOI">10.1177/0959354307086923</idno>
	</analytic>
	<monogr>
		<title level="j">Theory &amp; Psychology</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="69" to="88" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Final collapse of the Neyman-Pearson decision theoretic framework and rise of the neoFisherian</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Hurlbert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Lombardi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annales Zoologici Fennici</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="311" to="349" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Jeffreys</surname></persName>
		</author>
		<title level="m">The theory of probability</title>
		<meeting><address><addrLine>Oxford, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Oxford University Press</publisher>
			<date type="published" when="1961">1961</date>
		</imprint>
	</monogr>
	<note>rd ed.</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Falsification and the methodology of scientific research programmes</title>
		<author>
			<persName><forename type="first">I</forename><surname>Lakatos</surname></persName>
		</author>
		<editor>S. G. Harding</editor>
		<imprint>
			<date type="published" when="1976">1976</date>
			<publisher>Springer</publisher>
			<biblScope unit="page" from="205" to="259" />
			<pubPlace>Netherlands</pubPlace>
		</imprint>
	</monogr>
	<note>Can Theories be Refuted?</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Familywise error in multiple comparisons: Disentangling a knot through a critique of O&apos;Keefe&apos;s arguments against alpha adjustment</title>
		<author>
			<persName><forename type="first">M</forename><surname>Matsunaga</surname></persName>
		</author>
		<idno type="DOI">10.1080/19312450701641409</idno>
	</analytic>
	<monogr>
		<title level="j">Communication Methods and Measures</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="243" to="265" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">The testing of statistical hypotheses in relation to probabilities a priori</title>
		<author>
			<persName><forename type="first">J</forename><surname>Neyman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">S</forename><surname>Pearson</surname></persName>
		</author>
		<idno type="DOI">10.1017/S030500410001152X</idno>
	</analytic>
	<monogr>
		<title level="j">Mathematical Proceedings of the Cambridge Philosophical Society</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="492" to="510" />
			<date type="published" when="1933">1933</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">The preregistration revolution</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Nosek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Ebersole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dehaven</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mellor</surname></persName>
		</author>
		<ptr target="https://osf.io/2dxu5/" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Registered reports: A method to increase the credibility of published results</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Nosek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lakens</surname></persName>
		</author>
		<idno type="DOI">10.1027/1864-9335/a000192</idno>
	</analytic>
	<monogr>
		<title level="j">Social Psychology</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="137" to="141" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Estimating the reproducibility of psychological science</title>
		<author>
			<orgName type="collaboration">Open Science Collaboration</orgName>
		</author>
		<idno type="DOI">10.1126/science.aac4716</idno>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">349</biblScope>
			<biblScope unit="page">4716</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Best practices in data cleaning: A complete guide to everything you need to do before and after collecting your data</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Osborne</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>Sage Publications</publisher>
			<pubPlace>London</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Do p values lose their meaning in exploratory analyses? It depends how you define the familywise error rate</title>
		<author>
			<persName><forename type="first">M</forename><surname>Rubin</surname></persName>
		</author>
		<idno type="DOI">10.1037/gpr0000123</idno>
	</analytic>
	<monogr>
		<title level="j">Review of General Psychology</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="269" to="275" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Theory testing and measurement error</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">L</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Hunter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Intelligence</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="183" to="198" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Null hypothesis significance tests. A mix-up of two different theories: The basis for widespread confusion and numerous misinterpretations</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Schneider</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11192-014-1251-5</idno>
	</analytic>
	<monogr>
		<title level="j">Scientometrics</title>
		<imprint>
			<biblScope unit="volume">102</biblScope>
			<biblScope unit="page" from="411" to="432" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Multiple hypothesis testing</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Shaffer</surname></persName>
		</author>
		<idno type="DOI">10.1146/annurev.ps.46.020195.003021</idno>
	</analytic>
	<monogr>
		<title level="j">Annual Review of Psychology</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="561" to="584" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Many analysts, one dataset: Making transparent how variations in analytical choices affect results</title>
		<author>
			<persName><forename type="first">R</forename><surname>Silberzahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">L</forename><surname>Uhlmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Anselmi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Aust</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">C</forename><surname>Awtrey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Nosek</surname></persName>
		</author>
		<ptr target="https://psyarxiv.com/qkwst/" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">False-positive psychology undisclosed flexibility in data collection and analysis allows presenting anything as significant</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Simmons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">D</forename><surname>Nelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Simonsohn</surname></persName>
		</author>
		<idno type="DOI">10.1177/0956797611417632</idno>
	</analytic>
	<monogr>
		<title level="j">Psychological Science</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="1359" to="1366" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A tutorial on hunting statistical significance by chasing N</title>
		<author>
			<persName><forename type="first">D</forename><surname>Szucs</surname></persName>
		</author>
		<idno type="DOI">10.3389/fpsyg.2016.01444</idno>
	</analytic>
	<monogr>
		<title level="j">Frontiers in Psychology</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">1444</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A tutorial on sensitivity analyses in clinical trials: The what, why, when and how</title>
		<author>
			<persName><forename type="first">L</forename><surname>Thabane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Mbuagbaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Samaan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Marcucci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">.</forename><forename type="middle">.</forename><surname>Debono</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">B</forename></persName>
		</author>
		<idno type="DOI">10.1186/1471-2288-13-92</idno>
	</analytic>
	<monogr>
		<title level="j">BMC Medical Research Methodology</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">92</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Pre-registration in social psychology: A discussion and suggested template</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Van't Veer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Giner-Sorolla</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.jesp.2016.03.004</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Social Psychology</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="page" from="2" to="12" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">An agenda for purely confirmatory research</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Wagenmakers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wetzels</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Borsboom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">L</forename><surname>Van Der Maas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Kievit</surname></persName>
		</author>
		<idno type="DOI">10.1177/1745691612463078</idno>
	</analytic>
	<monogr>
		<title level="j">Perspectives on Psychological Science</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="632" to="638" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
