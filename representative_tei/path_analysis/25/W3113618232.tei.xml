<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DAG Scheduling and Analysis on Multiprocessor Systems: Exploitation of Parallelism and Dependency</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Shuai</forename><surname>Zhao</surname></persName>
							<email>shuai.zhao@york.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of York</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiaotian</forename><surname>Dai</surname></persName>
							<email>xiaotian.dai@york.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of York</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Iain</forename><surname>Bate</surname></persName>
							<email>iain.bate@york.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of York</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Alan</forename><surname>Burns</surname></persName>
							<email>alan.burns@york.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of York</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Wanli</forename><surname>Chang</surname></persName>
							<email>wanli.chang@york.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of York</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">DAG Scheduling and Analysis on Multiprocessor Systems: Exploitation of Parallelism and Dependency</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-21T21:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>With ever more complex functionalities being implemented in emerging real-time applications, multiprocessor systems are demanded for high performance, and directed acyclic graphs (DAGs) are used to model functional dependencies. In this work, we study a single periodic non-preemptive DAG running on a homogeneous multiprocessor platform, which is a common setup in many domains, such as automotive, robotics, and industrial automation. Aiming to reduce the makespan of the DAG and provide a tight yet safe bound, our contributions involve the exploitation of node-level parallelism and inter-node dependency, which are the two key factors of a DAG topology. First, we introduce a concurrent provider and consumer (CPC) model that precisely captures the above two factors, and can be recursively applied when parsing a DAG. Building upon CPC, we propose a novel scheduling method focused on reducing the makespan that orders the nodes in the following sequence: (i) the critical path, (ii) early predecessor paths of the critical path, and (iii) longer paths. Secondly, new response time analysis is presented, which provides a generic bound for any execution order of the non-critical nodes and a specific (tighter) bound for a fixed such order. Comprehensive evaluation demonstrates that our scheduling approach and analysis outperforms the state-ofthe-art methods. ùúÉ ! * ùúÉ # * ùúÉ $ * ùúÉ % * ùêπ(ùúÉ ! * ) ùêπ(ùúÉ # * ) ùêπ(ùúÉ $ * ) (b) ùúÉ ! * ùúÉ # * ùúÉ $ * ùúÉ % * ùê∫(ùúÉ ! * ) ùê∫(ùúÉ # * )</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Driven by the demands of high performance and complex functionalities, multiprocessor systems are increasingly being employed in real-time applications. Directed Acyclic Graphs (DAGs) tasks are used to model functional dependencies <ref type="bibr" target="#b0">[1]</ref>.</p><p>Many existing works use a single recurrent event-or timetriggered DAG task to model the system <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>. For example, a complete automotive task chain from perception to control is described in <ref type="bibr" target="#b1">[2]</ref> and converted to a single periodic DAG task.</p><p>In addition, to avoid migration and cache-related preemption overhead, a non-preemptive global scheduling scheme is often deployed <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b8">[9]</ref>. That is, the nodes of a DAG are scheduled globally on all cores and preemption is not allowed during the execution of a node <ref type="bibr" target="#b9">[10]</ref>.</p><p>Main contributions: In this work, we study a commonly seen setup, where a single periodic non-preemptive DAG runs on a homogeneous multiprocessor platform. By fully exploiting the node-level parallelism and inter-node dependency, which are the essence of the DAG topology, we reduce the makespan (i.e., the time interval between the starting and finishing of the DAG execution) and provide a tight yet safe bound on the makespan. This paper has passed an Artifact Evaluation process.</p><p>The first principal contribution is to introduce a concurrent provider and consumer (CPC) model to precisely capture the two factors: parallelism and dependency. The CPC model describes the critical path (the longest execution sequence) in a DAG as a set of consecutive providers, each of which has a group of non-critical nodes (i.e., consumers) that can 1) execute concurrently with the provider and 2) delay the starting of the next provider. The intuition comes from that the non-critical nodes consume the computation resource (on other cores in parallel) provided when running a critical node. This model can be recursively applied to build nested CPC when parsing a DAG and serves as the foundation of the scheduling and analysis.</p><p>For the second contribution, a novel scheduling method for CPC is proposed that orders the nodes in the following sequence: (i) the critical path (i.e., providers), (ii) early predecessors paths of the critical path (i.e., consumers paths that would otherwise block the following providers), and (iii) longer paths (in a consumer group of a provider). Furthermore, we present new response time analysis that provides two provably safe bounds. One is a generic bound featuring critical-path-first execution with any execution order of the non-critical nodes, accounting for the workload that causes a delay. The other is a specific and tighter bound when the scheduler enforces a fixed order of the non-critical nodes. Comprehensive evaluation shows that our work outperforms the state-of-the-art methods.</p><p>Organisation: The rest of the paper is organised as follows: Section II presents the system and task model. Section III describes the state-of-the-art approaches in DAG scheduling and analysis with a motivational example. The proposed scheduling method is explained in Section IV, with the CPC model given in Section IV-A. Section V provides the new response time analysis. Section VI briefly discusses the extension to multiple DAGs. Finally, the evaluation is reported in Section VII before Section VIII makes the concluding remarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. TASK MODEL AND SCHEDULING PRELIMINARIES</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Task model</head><p>A DAG task œÑ x is defined by {T x , D x , G x = (V x , E x )}, with T x denoting its minimum inter-arrival time, D x gives a constrained relative deadline, i.e., D x ‚â§ T x , and G x is a graph defining the set of activities forming the task. The graph is defined as G x = (V x , E x ) where V x denotes the set of nodes and E x ‚äÜ (V x √óV x ) gives the set of directed edges connecting  any two nodes. Each node v x,j ‚àà V x represents a computation unit that must be executed sequentially and is characterised by its Worst-Case Execution Time (WCET), C x,j . For simplicity, the subscript of the DAG task (i.e., x for œÑ x ) is omitted when the system has only one DAG task.</p><p>For any two nodes v j and v k connected by a directed edge ((v j , v k ) ‚àà E), v k can start execution only if v j has finished its execution. That is, v j is a predecessor of v k , whereas v k is a successor of v j . A node v j has at least one predecessor pre(v j ) and at least one successor suc(v j ), formally defined as pre(</p><formula xml:id="formula_0">v j ) = {v k ‚àà V | (v k , v j ) ‚àà E} and suc(v j ) = {v k ‚àà V | (v j , v k ) ‚àà E}, respectively.</formula><p>Nodes that are either directly or transitively predecessors and successors of a node v j are termed as its ancestors anc(v j ) and descendants des(v j ) respectively. A node v j with pred(v j ) = ‚àÖ or succ(v j ) = ‚àÖ is referred to as the source v src or sink v sink respectively. Without loss of generality, we assume each DAG has one source and one sink node. Nodes that can execute concurrently with v j are given by C(v j ) = {v k |v k / ‚àà (anc(v j ) ‚à™ des(v j )), ‚àÄv k ‚àà V } <ref type="bibr" target="#b10">[11]</ref>. A DAG task has the following fundamental features. First, a path Œª a = {v s , ‚Ä¢ ‚Ä¢ ‚Ä¢ , v e } is a node sequence in V and follows (v k , v k+1 ) ‚àà E, ‚àÄv k ‚àà Œª a \v e . The set of paths in V is defined as Œõ V . A local path is a sub-path within the task and as such does not feature both the source v src and the sink v sink . A complete path features both. Function len(Œª a ) = ‚àÄv k ‚ààŒªa C k gives the length of Œª a . Second, the longest complete path is referred to as the critical path Œª * , and its length is denoted by L, where L = max{len(Œª a ), ‚àÄŒª a ‚àà Œõ V }. Nodes in Œª * are referred to as the critical nodes. Other nodes are referred to as non-critical nodes, denoted as V ¬¨ = V \Œª * . Finally, the workload W is the sum of a task's WCETs, i.e. W = ‚àÄv k ‚ààV C k . The workload of all non-critical nodes is referred to as the non-critical workload.</p><p>Figure <ref type="figure" target="#fig_1">1</ref>(a) shows an example DAG task with eight nodes (i.e., V = {v 1 , v 2 , ..., v 8 }). The number at the top right of each node gives its WCET, e.g., C 2 = 7. Based on the above terminologies, for node v 7 we have pre(</p><formula xml:id="formula_1">v 7 ) = {v 5 , v 6 }, anc(v 7 ) = {v 1 , v 5 , v 6 }, suc(v 7 ) = des(v 7 ) = {v 8 } and C(v j ) = {v 2 , v 3 , v 4 }. For the DAG, we have L = 10, W = 24, with Œª * = {v 1 , v 5 , v 7 , v 8 }, v src = v 1 and v sink = v 8 .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Work-conserving schedule and analysis</head><p>The majority of the existing work on scheduling DAG tasks assumes a work-conserving scheduler <ref type="bibr" target="#b11">[12]</ref>. A scheduling algorithm is said to be work-conserving if it never idles a processor when there exists pending workload. A generic bound that captures the worst-case response time of tasks scheduled globally with any work-conserving method is provided in <ref type="bibr" target="#b12">[13]</ref>. This analysis is later formalised in <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b13">[14]</ref> for DAG tasks, as given in Equation <ref type="formula" target="#formula_2">1</ref>. Notation R x denotes the response time of œÑ x , m denotes the number of processors, I x,y gives the interference from a high priority DAG task œÑ y , and hp(x) gives all high priority tasks of œÑ x .</p><formula xml:id="formula_2">R x = L x + 1 m (W x -L x ) + œÑy‚ààhp(x) I x,y<label>(1)</label></formula><p>In this analysis, the worst-case response time of a DAG task œÑ x is upper bounded by the finish time of the critical path, with interference imposed by all non-critical nodes of œÑ x itself and high priority DAG tasks, i.e., I x,y , ‚àÄœÑ y ‚àà hp(x). Details for bounding I x,y can be found in <ref type="bibr" target="#b11">[12]</ref> and <ref type="bibr" target="#b13">[14]</ref>. However, this analysis assumes a node v j can be delayed by all the concurrent nodes <ref type="bibr" target="#b11">[12]</ref>, which is pessimistic for scheduling methods with an explicit execution order known a priori <ref type="bibr" target="#b10">[11]</ref>.</p><p>Figure <ref type="figure" target="#fig_1">1</ref>(b) provides possible execution scenarios of the example DAG in a dual-core system. With nodes scheduled randomly, a total 240 different execution scenarios are possible, with a makespan ranging from 13 to 17. The analysis described above provides a safe bound with R = L + 1 m (W -L) = 10 + 1 2 (24 -10) = 17. However, there are scheduling orders with a makespan much lower than 17. Based on the work-conserving schedule and the classic analysis, we propose new methods to reduce the run-time makespan and to tighten the analytical bounds of a single recurrent DAG task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. RELATED WORK</head><p>For homogeneous multiprocessors with a global scheme, existing scheduling (and their analysing) methods aim at reducing the makespan and tightening the worst-case analytical bound. They can be classified as either slice-based <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref> or node-based <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b16">[17]</ref>. The slice-based schedule enforces nodelevel preemption and divides each node into a number of small computation units (e.g., units with a WCET of one in <ref type="bibr" target="#b14">[15]</ref>). By doing so, the slice-based methods can improve node-level parallelism but to achieve an improvement the number of preemptions and migrations need to be controlled.</p><p>The node-based methods provide a more generic solution by producing an explicit node execution order, based on heuristics derived from either the spatial (e.g., number of successors of a node <ref type="bibr" target="#b17">[18]</ref> and topological order of nodes <ref type="bibr" target="#b10">[11]</ref>) or the temporal (execution time of nodes <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b18">[19]</ref>) characteristics of the DAG. Below we describe two most recent node-based methods.</p><p>In <ref type="bibr" target="#b16">[17]</ref>, an anomaly-free non-preemptive scheduling method is proposed for a single periodic DAG, which always executes the ready node with the longest WCET to improve parallelism. [17] prevents anomalies occurring when nodes are executing less than their WCETs, which can lead to an execution order different from the schedule. This is achieved by guaranteeing nodes are executed in the same order as the offline simulation. However, without considering inter-node dependencies, this schedule cannot minimise the delay on the completion of DAG. For the example in Figure <ref type="figure" target="#fig_1">1</ref>, this method leads to the scenario with a makespan of 14, in which the non-critical node v 6 delays the DAG completion due to a late start.</p><p>In <ref type="bibr" target="#b10">[11]</ref>, a new response time analysis is presented, which dominates the traditional bound <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b11">[12]</ref> when an explicit node execution order is known a priori. That is, a node v j can only incur delay from the concurrent nodes that are scheduled prior to v j . Then, a scheduling method is proposed that always executes: i) the critical path first; and ii) the immediate interference nodes first (nodes that can cause the most immediate delay on the currently-examined path).</p><p>The novelty in <ref type="bibr" target="#b10">[11]</ref> is considering both topology and path length in a DAG, and provides the state-of-the-art analysis against which our approach is compared. However, He et al. in <ref type="bibr" target="#b10">[11]</ref> schedule concurrent nodes based on the length of their longest complete path (a path from the source to the sink node), i,e, nodes in the longest complete path first. As illustrated in Section IV, this heuristic is not dependencyaware, which can reduce parallelism, and hence, lengthen the final critical path.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. DAG SCHEDULING: A PARALLELISM AND NODE</head><p>DEPENDENCY EXPLOITED METHOD Equation 1 indicates that minimising the delay from noncritical nodes to the critical path (i.e., 1 m (W -L)) effectively reduces makespan of the DAG. Achieving this requires the complete knowledge of the topology (i.e., the dependency and parallelism of each node) of a DAG so that the potential delay of the critical path can be identified. To support this the CPC model is proposed to fully exploit node dependency and parallelism (Section IV-A).</p><p>Based on the CPC model, a scheduling method is then presented to maximise node parallelism. This is achieved by a rule-based priority assignment, in which three rules are developed to statically assign a priority to each node in the DAG. Firstly to always execute the critical path first (Section IV-B), and then two rules (Section IV-C) to maximise parallelism and to minimise the delay to the critical path.</p><p>The entire proposed approach has general applicability to DAGs with any topology (unlike, e.g., <ref type="bibr" target="#b13">[14]</ref>, which assumes nested fork-join DAGs only). It assumes a homogeneous architecture, however, it is not restricted by the number of processors. Table <ref type="table">I</ref> summarises notations introduced by the proposed CPC model and scheduling method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Concurrent provider and consumer model</head><p>The CPC model has two key stages. First, the critical path is divided into a set of consecutive sub-paths based on the potential delay it can incur (Figure <ref type="figure" target="#fig_2">2(a)</ref>). Second, for each sub-path, the CPC model identifies the non-critical nodes that can 1) execute in parallel with the sub-path and 2) delay the start of the next sub-path, based on precedence constraints (Figure <ref type="figure" target="#fig_2">2</ref>(b) and (c)).</p><p>The intuition of the CPC model is: when the critical path is executing, it utilises just one core so that the non-critical ones can execute in parallel on the remaining (m -1) cores. The time allowed for executing non-critical nodes in parallel is termed as the capacity, which is the length of the critical path. Note that non-critical nodes that utilise this capacity to execute cannot cause any delay to the critical path. The subpaths in the critical path are termed capacity providers Œò * and all non-critical nodes are capacity consumers Œò. For each provider Œ∏ * i ‚àà Œò * , it has a set of consumers F (Œ∏ * i ) that can execute using Œ∏ * i 's capacity as well as delay the next provider Œ∏ * i+1 in the critical path. Algorithm 1 presents a two-step process for constructing the CPC model of an input DAG G with its critical path Œª * . Starting from the head node in Œª * , capacity providers are formed by analysing node dependency between the critical path and non-critical nodes (Line 3-9). For a provider Œ∏ * i , its nodes should execute consecutively without delay from noncritical nodes in terms of dependency. That is, each node in Œ∏ * i , other than the head node (Line 5), only has one predecessor which is the previous node in Œ∏ * i , see Figure <ref type="figure" target="#fig_2">2</ref>(b) with four capacity providers identified.</p><p>Then, for each Œ∏ * i ‚àà Œò * , its consumers F (Œ∏ * i ) are identified as the nodes that 1) can execute concurrently with Œ∏ * i , and 2) can delay the start of Œ∏ * i+1 (i.e., anc(Œ∏ * i+1 ) ‚à© V ¬¨ in Line 12). Accordingly, nodes in F (Œ∏ * i ) that finish later than Œ∏ * i will delay the start of Œ∏ * i+1 (if it exists). In Figure <ref type="figure" target="#fig_2">2</ref>  </p><formula xml:id="formula_3">= (V, E)} Outputs : Œò * , F (Œ∏ * i ), G(Œ∏ * i ), ‚àÄŒ∏ * j ‚àà Œò * Parameters: Œª * , V ¬¨ = V \Œª * 1 Œò * = ‚àÖ; 2 / *</formula><formula xml:id="formula_4">F (Œ∏ * i ) = anc(Œ∏ * i+1 ) ‚à© V ¬¨ ; G(Œ∏ * i ) = vj ‚ààF (Œ∏ * i ) {C(v j ) ‚à© V ¬¨ }; V ¬¨ = V ¬¨ \ F (Œ∏ * i ); end return Œò * , F (Œ∏ * i ), G(Œ∏ * i ), ‚àÄŒ∏ * i ‚àà Œò *</formula><p>the CPC model provides detailed knowledge of the potential delay caused by non-critical nodes on the critical path. Furthermore, given an arbitrary DAG structure, a consumer v j ‚àà F (Œ∏ * i ) can start earlier than, synchronous with, or later than the start of Œ∏ * i . For synchronous and late-released consumers, they will only utilise the capacity of Œ∏ * i . However, an early-released consumer can execute concurrently with certain previous providers, and therefore interfere with their consumers and impose an indirect delay to those providers. For a provider Œ∏ * i , G(Œ∏ * i ) (in line 13) denotes the nodes that belong to the consumer groups of later providers, but which can execute in parallel (in terms of topology) with Œ∏ * i . In Figure <ref type="figure" target="#fig_2">2</ref>(c), nodes in G(Œ∏ * 1 ) and G(Œ∏ * 2 ) belong to F (Œ∏ * 2 ) and F (Œ∏ * 3 ), but can execute in parallel with Œ∏ * 1 and Œ∏ * 2 respectively based on the precedence constraints.</p><p>With the CPC model, a DAG is transformed into a set of capacity providers and consumers, with a time complexity of O(|V |+|E|). The CPC model provides complete knowledge of both direct and indirect delays from non-critical nodes on the critical path. For each provider Œ∏ * i , nodes in F (Œ∏ * i ) can utilise a capacity of len(Œ∏ * i ) on each of m -1 cores to execute in parallel while incurring potential delay from G(Œ∏ * i ). Recall the DAG in Figure <ref type="figure" target="#fig_1">1</ref>(a), its critical path forms three providers Œ∏ * 1 = {v 1 , v 5 }, Œ∏ * 2 = {v 7 } and Œ∏ * 3 = {v 8 }, and delay from non-critical nodes only occurs on the head node of the providers. For each provider, we have</p><formula xml:id="formula_5">F (Œ∏ * 1 ) = {v 6 }, F (Œ∏ * 2 ) = {v 2 , v 3 , v 4 } and F (Œ∏ * 3 ) = ‚àÖ. In addition, all nodes in F (Œ∏ * 2 ) = {v 2 , v 3 , v 4 } can start earlier than Œ∏ *</formula><p>2 delaying the execution of F (Œ∏ * 1 ) and subsequently, the start of Œ∏ * 2 . Therefore,</p><formula xml:id="formula_6">G(Œ∏ * 1 ) = {v 2 , v 3 , v 4 } and G(Œ∏ * 2 ) = G(Œ∏ * 3 ) = ‚àÖ.</formula><p>We now formally define the parallel and interfering workload of a capacity provider. Let f (‚Ä¢) denote the finish time of a provider Œ∏ * i or a consumer node v j , L i = len(Œ∏ * i )</p><p>Table <ref type="table">I</ref>: Notations introduced in the proposed CPC model and scheduling method.</p><formula xml:id="formula_7">Notation Description Œò * /Œò</formula><p>The set of capacity providers/consumers. Œ∏ * i A capacity provider with an index i. p j</p><p>The priority of a node v j .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>L i</head><p>The length of provider Œ∏ * i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>W i</head><p>The total workload of all nodes in Œ∏ *</p><formula xml:id="formula_8">i , F (Œ∏ * i ) and G(Œ∏ * i ). Œ± i</formula><p>The workload in</p><formula xml:id="formula_9">F (Œ∏ * i ) and G(Œ∏ * i ) that can execute in parallel with Œ∏ * i . F (Œ∏ * i ) The consumer group of Œ∏ * i . G(Œ∏ * i )</formula><p>Nodes in the consumer groups of later providers that can execute in parallel with</p><formula xml:id="formula_10">Œ∏ * i . f (‚Ä¢)</formula><p>The finish time of a given provider or a consumer node.</p><formula xml:id="formula_11">l j (‚Ä¢)</formula><p>The length of the longest path that includes v j in the set of input nodes.</p><p>gives the length of Œ∏ * i and</p><formula xml:id="formula_12">W i = L i + v k ‚ààF (Œ∏ * i ) {C k } + v k ‚ààG(Œ∏ * i ) {C k } gives the total workload of Œ∏ * i , F (Œ∏ * i ) and G(Œ∏ * i ).</formula><p>We formally define the terms parallel and interfering workload of a provider Œ∏ * i . Note, W ‚â§ Œ∏ * i ‚ààŒò W i as a consumer can be accounted for more than once if it can execute concurrently with multiple providers.</p><formula xml:id="formula_13">Definition 1 (Parallel Workload of Œ∏ * i ). The parallel workload Œ± i of Œ∏ * i is the workload in W i -L i that can execute before the time instant f (Œ∏ * i ). For a node v j in F (Œ∏ * i )‚à™G(Œ∏ * i ), it contributes to Œ± i if either f (v j ) ‚â§ f (Œ∏ * i ) or f (v j ) -C j &lt; f (Œ∏ * i ). The former case (i.e., f (v j ) ‚â§ f (Œ∏ * i )) indicates v j is finished before the finish of Œ∏ *</formula><p>i and cannot cause any delay, whereas f (v j ) -C j &lt; f (Œ∏ * i ) means v j can partially execute in parallel with Œ∏ * i so that its delay on Œ∏ * i+1 is less than C j . In Section V, function f (‚Ä¢) is formulated for both providers and consumers, along with the response time analysis.</p><formula xml:id="formula_14">Definition 2 (Interfering Workload of Œ∏ * i ). The interfering workload of Œ∏ * i is the workload in W i -L i that executes after the time instant f (Œ∏ * i ). For a provider Œ∏ * i , its interfering workload is W i -L i -Œ± i .</formula><p>With Definitions 1 and 2, Lemma 1 follows.</p><p>Lemma 1. For providers Œ∏ * i and Œ∏ * i+1 , the workload in</p><formula xml:id="formula_15">W i that can delay the start of Œ∏ * i+1 is at most W i -L i -Œ± i . Proof.</formula><p>Based on the CPC model, the start of Œ∏ * i+1 depends on the finish of both Œ∏ * i and</p><formula xml:id="formula_16">F (Œ∏ * i ), which is max{f (Œ∏ * i ), max vj ‚ààF (Œ∏ * i ) f (v j )}.</formula><p>By Definition 1, Œ± i will not cause any delay as it always finishes before f (Œ∏ * i ), and hence, the lemma follows. Note that although G(Œ∏ * i ) cannot delay Œ∏ * i+1 directly, it can delay on nodes in F (Œ∏ * i ), and in turn, causes an indirect delay to Œ∏ * i+1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. The "Critical Path First" execution (CPFE)</head><p>In the CPC model, the critical path is conceptually modelled as a set of capacity providers. Arguably, each complete path can be seen as the providers, which offers the time interval of its path length for other nodes to execute in parallel. However, the critical path provides the maximum capacity and hence, enables the maximised total parallel workload (denoted as Œ± = Œ∏ * i ‚ààŒò * Œ± i ). This provides the foundation to minimise the interfering workload on the complete critical path.</p><p>Theorem 1. For a schedule S with CPFE and a schedule S ‚Ä≤ that prioritises a random complete path over the critical path, the total parallel workload of providers in S is always equal to or higher than that of S ‚Ä≤ , i.e., Œ± ‚â• Œ± ‚Ä≤ .</p><p>Proof. The change from S to S ‚Ä≤ leads to two effects: 1) a reduction on the length of the provider path, and 2) an increase on length of one consumer path. Below we prove both effects cannot increase the parallel workload after the change.</p><p>First, suppose the length of provider Œ∏ * i is shortened by ‚àÜ after the change from S to S ‚Ä≤ , the same reduction applies on its finish time, i.e., f</p><formula xml:id="formula_17">‚Ä≤ (Œ∏ * i ) = f (Œ∏ * i ) -‚àÜ. Because nodes in Œ∏ * i are shortened, the finish time f (v j ) of a consumer node v j ‚àà F (Œ∏ * i ) ‚à™ G(Œ∏ * i )</formula><p>can also be reduced by a value from ‚àÜ/m (i.e., a reduction on v j 's interference, if all the shortened nodes in Œ∏ * i belong to C(v j )) to ‚àÜ (if all such nodes belong to pre(v j )) <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b11">[12]</ref>. By definition 1, a consumer</p><formula xml:id="formula_18">v j ‚àà F (Œ∏ * i ) ‚à™ G(Œ∏ * i ) can contribute to the Œ± i if f (v j ) ‚â§ f (Œ∏ * i ) or f (v j ) -C j ‚â§ f (Œ∏ * i ).</formula><p>Therefore, Œ± i cannot increase in S ‚Ä≤ , as the reduction on f (Œ∏ * i ) (i.e., ‚àÜ) is always equal or higher than that of f (v j ) (i.e., ‚àÜ/m or ‚àÜ).</p><p>Second, let L and L ‚Ä≤ denote the length of the provider path under S and S ‚Ä≤ (with L ‚â• L ‚Ä≤ ), respectively. The time for noncritical nodes to execute in parallel with the provider path is L ‚Ä≤ on each of m -1 cores under S ‚Ä≤ . Thus, a consumer path with its length increased from L ‚Ä≤ to L directly leads to an increase of (L -L ‚Ä≤ ) in the interfering workload, as at most L ‚Ä≤ in the consumer can execute in parallel with the provider.</p><p>Therefore, both effects cannot increase the parallel workload after the change from S to S ‚Ä≤ , and hence, Œ± ‚â• Œ± ‚Ä≤ .</p><formula xml:id="formula_19">Rule 1. ‚àÄv j ‚àà Œò * , ‚àÄv k ‚àà Œò ‚áí p j &gt; p k .</formula><p>Theorem 1 leads to the first assignment rule that assigns critical nodes with the highest priority, in which p j denotes the priority of node v j . With Rule 1, the maximum parallel capacity is guaranteed so that an immediate reduction (i.e., Œ±) on the interfering workload of Œª * can be obtained. For the example in Figure <ref type="figure" target="#fig_1">1</ref>, Rule 1 leads to the execution scenarios with a makespan of 16 and 13, and avoids the worst case. In Section V, an analytical bound on Œ± i for each provider Œ∏ * i is presented, with consumers nodes executed either randomly or under an explicit schedule.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Exploiting parallelism and node dependency</head><p>With CPFE, the next objective is to maximise the parallelism of non-critical nodes and reduce the delay on the completion of the critical path. Based on the CPC model, each provider Œ∏ * i is associated with</p><formula xml:id="formula_20">F (Œ∏ * i ) and G(Œ∏ * i ). For v j ‚àà G(Œ∏ * i ), it can execute before F (Œ∏ * i )</formula><p>and use the capacity of Œ∏ * i to execute, if assigned with a high priority. Under this case, v j can 1) delay the finish of F (Œ∏ * i ) and the start of Œ∏ * i+1 , and 2) waste the capacity of its own provider. A similar observation is also obtained in <ref type="bibr" target="#b10">[11]</ref>, which avoids this delay by the heuristic of early interference node first.</p><formula xml:id="formula_21">Rule 2. ‚àÄŒ∏ * i , Œ∏ * l ‚àà Œò * : i &lt; l ‚áí min vj ‚ààF (Œ∏ * i ) p j &gt; max v k ‚ààF (Œ∏ * l ) p k .</formula><p>Therefore, the second assignment rule is derived to specify the priority between consumer groups of each provider. For any two adjacent providers Œ∏ * i and Œ∏ * i+1 , the priority of any consumer in F (Œ∏ * i ) is higher than that of all consumers in F (Œ∏ * i+1 ). With Rule 2, the delay from G(Œ∏ * i ) on F (Œ∏ * i ) (and hence Œ∏ * i+1 ) can be minimised, because all nodes in G(Œ∏ * i ) belong to consumers of following providers and are always assigned with a lower priority than nodes in F (Œ∏ * i ). With Rules 1 and 2 applied to the DAG in Figure <ref type="figure" target="#fig_1">1</ref>, , the delay from v 6 on the critical path can be avoided, by assigning v 6 with a higher priority than that of {v 2 , v 3 , v 4 }.</p><p>We now schedule the consumer nodes in each F (Œ∏ * i ). In <ref type="bibr" target="#b10">[11]</ref>, concurrent nodes with the same earliness (in terms of the time they become ready during the execution of the critical path) are ordered by the length of their longest complete path (i.e., from v src to v sink ). However, based on the CPC model, a complete path can be divided into several local paths, each of these local paths belong to the consumer group of different providers. For local paths in F (Œ∏ * i ), the order of their lengths can be the exact opposite to that of their complete paths. Therefore, this approach can lead to a prolonged finish of F (Œ∏ * i ). In the constructed schedule, we guarantee a longer local path is always assigned with a higher priority in a dependencyaware manner. This derives the final assignment rule, as given below. Notation l j (F (Œ∏ * i )) denotes the length of the longest local path in F (Œ∏ * i ) that includes v j . This length can be computed by traversing anc(v j ) ‚à™ des(v j ) in F (Œ∏ * i ) <ref type="bibr" target="#b10">[11]</ref>. For example, we have l 2 (F (Œ∏ *</p><p>2 )) = 7 and l 3 (F (Œ∏ * 2 )) = l 4 (F (Œ∏ *</p><p>2 )) = 3 for the DAG in Figure <ref type="figure" target="#fig_1">1</ref>, so v 2 is assigned a higher priority than v 3 and v 4 . With Rules 1-3 applied to the example DAG, it finally leads to the best-case schedule with a makespan of 13.</p><formula xml:id="formula_22">Rule 3 ‚ãÜ . v j , v k ‚àà F (Œ∏ * i ) : l j (F (Œ∏ * i )) &gt; l k (F (Œ∏ * i )</formula><p>) ‚áí p j &gt; p k However, simply applying Rule 3 to each F (Œ∏ * i ) is not sufficient. Given a complex DAG structure, every F (Œ∏ * i ) can form a smaller DAG G ‚Ä≤ , and hence, an inner nested CPC model with the longest path in F (Œ∏ * i ) is the provider. Furthermore, this procedure can be recursively applied to keep constructing inner CPC models for each consumer group in a nested CPC model, until all local paths in a consumer group are fully independent. For each inner nested CPC model, Rules 1 and 2 should be applied for maximised capacity and minimised delay of each consumer group, whereas Rule 3 is only applied to independent paths in a consumer group for maximised parallelism (and hence, the star mark on Rule 3). This enables complete awareness of inter-node dependency and guarantees the longest path first in each nested CPC model.</p><p>Algorithm 2 provides the complete approach of the rulebased priority assignment. The method starts from the outermost CPC model (CP C(G, Œª * )), and assigns all provider 9 Œªv e = ve ‚à™ Œªv j , argmax nodes with the highest priority based on Rule 1 (Line 2). By Rule 2, the algorithm starts from the earliest F (Œ∏ * i ) (Line 4) and finds the longest local path Œª ve in F (Œ∏ * i ) (Line 8-9). If there exists dependency between nodes in Œª ve and F (Œ∏ * i )\Œª ve (Line 9), F (Œ∏ * i ) is further constructed as an inner CPC model with the assignment algorithm applied recursively (Line <ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref>. This resolves the detected dependency by dividing Œª ve into a set of providers. Otherwise, Œª ve is an independent local path so that priority is assigned to its nodes based on Rule 3. The algorithm then continues with F (Œ∏ * i )\Œª ve . The process continues until all nodes in V are assigned with a priority.</p><formula xml:id="formula_23">v j {l j (F (Œ∏ * i ))|‚àÄv j ‚àà pre(ve)}; 10 if |pre(v j )|&gt; 1, ‚àÉv j ‚àà Œª ve then 11 {Œò * ‚Ä≤ , Œò ‚Ä≤ } = CP C(F (Œ∏ * i ), Œª ve ); 12 EA(Œò * ‚Ä≤ , Œò ‚Ä≤ );</formula><p>The time complexity of Algorithm 2 is quadratic. At most, |V |+|E| calls to Algorithm 1 are invoked to construct the inner CPC models (Line 11), which examines each node and edge in the DAG. Mutually exclusively, Lines 16-17 assign each node with a priority value. Given that the time complexity of Algorithm 1 is O(|V |+|E|), we have the time complexity O((|V |+|E|) 2 ) for Algorithm 2. Although Algorithm 2 is recursive, this result holds as a node assigned with a priority will be removed from further iterations (Line 17), i.e., each node (edge) is processed only once.</p><p>With the CPC model and the schedule, the complete process for scheduling a DAG consists of three phases: i) transferring the DAG to CPC; ii) statically assigning a priority to each node by the rule-based priority assignment, and iii) executing the DAG by a fixed-priority scheduler. With the input DAG known a priori, phases i) and ii) can be performed offline so that the scheduling cost at run-time is effectively reduced to that of the traditional fixed-priority system.</p><p>V. (Œ±, Œ≤)-PAIR RESPONSE TIME ANALYSIS With the proposed schedule and CPC model, this section presents a new response time analysis that explicitly accounts for the parallel workload (i.e., Œ±) and applies Œ± as a safe reduction on the interfering workload that can delay Œª * . In addition, we highlight that although the proposed schedule assigns explicit node priority, the critical path first execution (i.e., CPFE) is a fundamental property to maximise parallelism (see Theorem 1) and is adopted in many existing methods <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b13">[14]</ref>. For generality, the proposed analysis assumes CPFE and allows any scheduling order for non-critical nodes. That is, compared to the traditional analysis <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, this analysis provides an improved bound for all schedules based on CPFE. The analysis does not assume the explicit execution order is known in advance. In Section V-C, we extend the proposed analysis for scheduling methods with an explicit order known a priori (e.g., <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b10">[11]</ref> and the proposed schedule) with minor modifications. Table <ref type="table" target="#tab_2">II</ref> summarises the notations introduced in the constructed analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. The (Œ±, Œ≤)-pair analysis formulation</head><p>In the CPC model, the critical path of a DAG task is transferred to a set of sequential providers Œò * . A provider Œ∏ * i ‚àà Œò * can start if and only if the previous provider Œ∏ * i-1 and its consumers F (Œ∏ * i-1 ) have finished executions (Figure <ref type="figure" target="#fig_2">2</ref>(b)). In addition, F (Œ∏ * i-1 ) can incur a delay from G(Œ∏ * i-1 ) (i.e., early-released consumers that can execute concurrently with F (Œ∏ * i-1 )), which in turn, delays the start of Œ∏ * i (Figure <ref type="figure" target="#fig_2">2</ref>(c)). Based on Definitions 1 and 2, the parallel workload Œ± i of Œ∏ * i finishes no later than f (Œ∏ * i ) on m -1 cores. After Œ∏ * i completes, the interfering workload (if any) then executes on all m cores, in which the latest-finished node in F (Œ∏ * i ) gives the earliest starting time to the next provider (if it exists). Therefore, bounding this delay requires:</p><p>1) a bound on the parallel workload (i.e., Œ± i );</p><p>2) a bound on the longest execution sequence in F (Œ∏ * i ) that executes later than f (Œ∏ * i ) (i.e., in the interfering workload), denoted as Œ≤ i . With a random execution order, the worst-case finish time of Œ≤ i effectively upper bounds the worst-case finish of workload in F (Œ∏ * i ) that executes later than f (Œ∏ * i ) <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b11">[12]</ref>. With Œ± i and Œ≤ i defined, Lemma 2 gives the bound on the delay Œ∏ * i that can incur due to the consumer nodes in F (Œ∏ * i-1 ). Lemma 2. For two consecutive providers Œ∏ * i-1 and Œ∏ * i , the consumers nodes in F (Œ∏ * i-1 ) can delay Œ∏ * i by at most</p><formula xml:id="formula_24">1 m (W i -L i -Œ± i -Œ≤ i ) + Œ≤ i . Proof. By Definition 2, the interfering workload in F (Œ∏ * i ) ‚à™ G(Œ∏ * i ) that can (directly or transitively) delay Œ∏ * i+1 is at most W i -L i -Œ± i .</formula><p>Given the longest execution sequence in F (Œ∏ * i ) in the interfering workload (i.e., Œ≤ i ), the worst-case finish time of F (Œ∏ * i ) (and also</p><formula xml:id="formula_25">Œ≤ i ) is bounded as 1 m (W i -L i -Œ± i -Œ≤ i ) + Œ≤ i</formula><p>, for a system with m cores. This is proved in <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b11">[12]</ref>. Note, as Œ≤ i is accounted for explicitly, it is removed from the interfering workload to avoid repetition. The set of nodes that forms the longest path in F (Œ∏ * i ) that executes later than f (Œ∏ * i ), with the end node ve. Œõ V Returns all paths of the given input node set V .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>| ‚Ä¢ |</head><p>returns the size of a given input set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I(v j )</head><p>The non-critical nodes that can interfere v j . I e (‚Ä¢)</p><p>The non-critical nodes that can interfere the input node or path with an explicit execution order. I Œªv e ,j</p><p>The actual delay on Œªv e from a node v j that executed in the interfering workload.</p><p>Based on Lemma 2, the response time analysis for a DAG task can be formulated in Equation <ref type="formula" target="#formula_26">2</ref>. As W i -L i -Œ± i starts strictly after f (Œ∏ * i ) (see Definition 1), the finish time of both Œ∏ * i and F (Œ∏ * i ) is bounded by the length of Œ∏ * i (i.e., L i ) and the worst-case finish time of Œ≤ i . In addition, Œ∏ * i+1 can only start after the finish of Œ∏ * i and all nodes in F (Œ∏ * i ). Thus, the final response time of the DAG is bounded by the sum of the finish time of each provider and its consumers.</p><formula xml:id="formula_26">R = Œ∏ * i ‚ààŒò * L i + 1 m (W i -L i -Œ± i -Œ≤ i ) + Œ≤ i<label>(2)</label></formula><p>Compared to the traditional analysis <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b11">[12]</ref>, this analysis can improve the worst-case response time approximations, by tightening the interference on the critical path (i.e., Œ± i ), without undermining the correctness of the analysis (i.e., with Œ≤ i ). In the case of 1 m (W -L) &gt;</p><formula xml:id="formula_27">Œ∏ * i ‚ààŒò 1 m (W i -L i -Œ± i -Œ≤ i ) + Œ≤ i ,</formula><p>a tighter bound can be obtained. That is, the proposed analysis does not always dominate the traditional bound. Therefore, we take min{R, L + 1 m (W -L) } as the final analytical bound.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Bounding Œ± i and Œ≤ i</head><p>Notations Œ± i and Œ≤ i can be bounded by examining</p><formula xml:id="formula_28">f (Œ∏ * i ) and f (v k ), ‚àÄv k ‚àà F (Œ∏ * i ) ‚à™ G(Œ∏ * i )</formula><p>in the scenario that one core is dedicated to Œ∏ * i and (m -1) cores can be used by F (Œ∏ * i ). For a node v j , it can subject to interference (say I j ) from the concurrent nodes upon arrival. Before bounding f (v j ), we first distinguish two special situations in which the interference of a node v j is zero, as given in Lemma 3, with C(v j ) gives v j 's concurrent nodes, Œõ V denotes paths in a given node set V and | ‚Ä¢ | returns the size of a given set. Lemma 3. Under a schedule with CPFE, node v j does not incur any interference from its concurrent nodes</p><formula xml:id="formula_29">C(v j ), if v j ‚àà Œª * ‚à® |Œõ C(vj )\Œª * |&lt; m -1.</formula><p>Proof. First, the interference of v j is zero if v j ‚àà Œª * . This is enforced by CPFE (i.e., Rule 1), where a critical node always starts immediately after all nodes in pre(v j ) have finished their executions.</p><p>Second, a node v j ‚àà V ¬¨ does not incur any interference if |Œõ C(vj )\Œª * |&lt; m-1. The concurrent nodes that can interfere v j on (m -1) cores is C(v j )\Œª * . Given that the number of paths in C(v j )\Œª * is less than m-1, at least one core is idle when v j is ready so that it can start directly with no interference. Followed by Lemma 3, Equation 3 provides the bound on f (v j ), v j ‚àà V . For a node v j , it cannot release until all v k ‚àà pre(v j ) have completed. This is enforced by the precedence constraints from the DAG structure, and hence max v k ‚ààpre(vj ) {f (v k )}. In addition, if v j does not satisfy either case in Lemma 3, v j can incur an worst-case interference of 1 m-1 v k ‚ààI(vj ) C k , in which I(v j ) denotes the non-critical nodes that can interfere v j (see Equation <ref type="formula" target="#formula_31">4</ref>) <ref type="bibr" target="#b10">[11]</ref>. The condition |Œõ C(vj )\Œª * |&lt; m -1 is checked by Line 8-9 in Algorithm 2 with m -1 searches, which identifies a path in the given node set during each search.</p><formula xml:id="formula_30">f (v j ) =C j + max v k ‚ààpre(vj ) f (v k ) + 0, if v j ‚àà Œª * ‚à® |Œõ C(vj )\Œª * |&lt; m -1 1 m-1 √ó ( v k ‚ààI(vj ) C k ) , otherwise<label>(3)</label></formula><p>Equation 3 bounds f (v j ) by recursively computing the finish time of all nodes in anc(v j ). To guarantee each node is taken into account only once when bounding the finish time of v j , I(v j ) only takes the concurrent non-critical nodes that cannot delay anc(v j ) <ref type="bibr" target="#b10">[11]</ref>, as given in Equation <ref type="formula" target="#formula_31">4</ref>. Note that this equation only applies to non-critical nodes v j with |Œõ C(vj )\Œª * |‚â• m -1.</p><formula xml:id="formula_31">I(vj) = {v k |v k / ‚àà Œª * ‚àß v k / ‚àà v l ‚ààanc(v j ) I(v l ), ‚àÄv k ‚àà C(vj)}<label>(4)</label></formula><p>With f (v j ), ‚àÄv j ‚àà V computed, the worst-case finish time of a provider Œ∏ * i and its F (Œ∏ * i ) can be obtained, as given in Equations 5 and 6 respectively.</p><formula xml:id="formula_32">f (Œ∏ * i ) = max ‚àÄ v j ‚ààŒ∏ * i f (v j )<label>(5)</label></formula><formula xml:id="formula_33">f (F (Œ∏ * i )) = max ‚àÄ v j ‚ààF (Œ∏ * i ) f (v j )<label>(6)</label></formula><p>To this end, Œ± i and Œ≤ i can be effectively upper bounded by examining the f (Œ∏ * i ) and f (v j ), ‚àÄv j ‚àà F (Œ∏ * i ) ‚à™ G(Œ∏ * i ). Equation 7 gives the bound on Œ± i .</p><formula xml:id="formula_34">Œ± i = vj ‚ààVa C j + vj ‚ààV b f (Œ∏ * i ) -f (v j ) -C j , ‚àÄv j ‚àà F (Œ∏ * i ) ‚à™ G(Œ∏ * i ), V a = {v j |f (v j ) ‚â§ f (Œ∏ * i )} V b = {v j |f (v j ) &gt; f (Œ∏ * i ) ‚àß f (v j ) -C j &lt; f (Œ∏ * i )}<label>(7)</label></formula><p>This equation is derived from Definition 1. For</p><formula xml:id="formula_35">v j ‚àà F (Œ∏ * i )‚à™ G(Œ∏ * i ), it can contribute to Œ± i if 1) it finishes before Œ∏ * i , i.e., f (v j ) ‚â§ f (Œ∏ * i ), or 2) it finishes after f (Œ∏ * i ) but with a start time earlier than f (Œ∏ * i ), i.e., f (v j ) &gt; f (Œ∏ * i ) ‚àß f (v j ) -C j &lt; f (Œ∏ * i ).</formula><p>The former case gives V a in the equation, with nodes in V a fully contributing to Œ± i by C a . The later case gives the set V b , in which nodes in V b are partially contributing to Œ± i by</p><formula xml:id="formula_36">f (Œ∏ * i ) -(f (v j ) -C j ).</formula><p>Then, Œ≤ i can be decided by the longest path of F (Œ∏ * i ) that executed later than f (Œ∏ * i ), i.e., in the interfering workload. Let Œª ve denote this path ending with node v e , Lemmas 4 and 5 identifies v e and its predecessor node in Œª ve , among all nodes in F (Œ∏ * i ). Lemma 4. For the end node v e in the longest path of F (Œ∏ * i ), f (v e ) = f (F (Œ∏ * i )). Proof. Given two paths Œª a and Œª b with length L a &gt; L b and a total workload of W , it follows f</p><formula xml:id="formula_37">(Œª a ) = L a + 1 m (W -L a ) ‚â• f (Œª b ) = L b + 1 m (W -L b ), as f (Œª a ) -f (Œª b ) = L a -L b + 1 m (L b -L a ) ‚â• 0. Therefore, node v e with f (v e ) = f (F (Œ∏ * i ))</formula><p>gives the end node of the longest path in the interfering workload.</p><p>Lemma 5. The predecessor node of the end node v e in the longest path of F (Œ∏ * i ) is given by <ref type="bibr" target="#b10">[11]</ref>. Therefore, the predecessor node of v e with the latest finish is in the longest path ending with v e in F (Œ∏ * i ).</p><formula xml:id="formula_38">argmax vj {f (v j ) | ‚àÄv j ‚àà pre(v e ) ‚à© F (Œ∏ * i )}. Proof. Given v a , v b ‚àà pre(v c ) with f (v a ) ‚â• f (v b ), we have len(Œª va ‚à™v c ) ‚â• len(Œª v b ‚à™v c )</formula><p>Based on Lemmas 4 and 5, Œª ve is computed recursively by Equation <ref type="formula" target="#formula_39">8</ref>. Starting from v e , Œª ve searches through the predecessor nodes recursively and includes the one with the longest finish time in each recursion, until a complete path is obtained or all predecessors are finished before f (Œ∏ * i ).</p><formula xml:id="formula_39">Œª ve = Œª vj ‚à™ v e : argmax vj f (v j ) ‚àÄv j ‚àà pre(v e ) ‚àß f (v j ) &gt; f (Œ∏ * i ) arg ve f (v e ) = f (F (Œ∏ * i )) , v e , v j ‚àà F (Œ∏ * i )<label>(8)</label></formula><p>With Œª ve obtained, Œ≤ i is computed by Equation <ref type="formula" target="#formula_40">9</ref>, which bounds the workload in Œª ve that is executed later than f (Œ∏ * i ).</p><formula xml:id="formula_40">Œ≤ i = vj ‚ààŒªv e C j , iff (v j ) -C j ‚â• f (Œ∏ * i ) f (v j ) -f (Œ∏ * i ), otherwise<label>(9)</label></formula><p>For the first node in Œª ve (say v s ), two cases can occur based on its worst-case start time</p><formula xml:id="formula_41">f (v s )-C s . First, with f (v s )-C s ‚â• f (Œ∏ * i )</formula><p>, v s starts after the finish of Œ∏ * i and fully contributes to the interfering workload. Otherwise (i.e., f <ref type="formula" target="#formula_34">7</ref>. Thus, by Definitions 1 and 2, it can contribute at most (f (v s ) -f (Œ∏ * i )) to the interfering workload. Note that v s is the only node in</p><formula xml:id="formula_42">(v s ) -C s &lt; f (Œ∏ * i )), v s partially contributes to Œ± i , i.e., v s ‚àà V b in Equation</formula><formula xml:id="formula_43">Œª ve that can have f (v s ) -C s &lt; f (Œ∏ i ).</formula><p>With Œ± i and Œ≤ i computed for each provider Œ∏ * i ‚àà Œò * , the response time analysis for scheduling methods that feature CPFE is complete.</p><p>Sustainability: It is worth noting that this analysis is sustainable <ref type="bibr" target="#b19">[20]</ref>, i.e., provides a safe bound if any node executes less than its WCET. We demonstrate this by reducing the WCET of a randomly node in V ¬¨ and Œª * i respectively.</p><p>First, suppose</p><formula xml:id="formula_44">v j ‚àà F (Œ∏ * i ) ‚à™ G(Œ∏ * i ) executes less than its WCET, denoted as C ‚Ä≤ j &lt; C j . Based on Equation 3, it leads to f ‚Ä≤ (Œ∏ * i ) = f (Œ∏ * i ) as v j / ‚àà pre(v k ), ‚àÄv k ‚àà Œ∏ * i , and f ‚Ä≤ (v k ) ‚â§ f (v k ), ‚àÄv k ‚àà F (Œ∏ * i ) ‚à™ G(Œ∏ * i ).</formula><p>Accordingly, by Definitions 1 and 2 we have Œ± ‚Ä≤ i ‚â• Œ± i and Œ≤ ‚Ä≤ i ‚â§ Œ≤ i and hence, leads to a non-increasing delay on the start of Œ∏ * i+1 based on Equation <ref type="formula" target="#formula_26">2</ref>. Second, if a provider node <ref type="formula" target="#formula_30">3</ref>. That is, the parallel workload obtained by Equation <ref type="formula" target="#formula_34">7</ref>(with full WCET C j ) can still finish before f (Œ∏ * i ) in the case of C ‚Ä≤ j , and subsequently, the interfering workload can start no later than f (Œ∏ * i ) on all m cores. Thus, the finish time of Œ∏ * i and F (Œ∏ * i ) cannot exceed the bound obtained by Equation 2 with full WCET.</p><formula xml:id="formula_45">v j ‚àà Œ∏ * i executes C ‚Ä≤ j &lt; C j , we have f ‚Ä≤ (Œ∏ * i ) &lt; f (Œ∏ * i ) and f ‚Ä≤ (v k ) = f (v k ), ‚àÄv k ‚àà F (Œ∏ * i ) ‚à™ G(Œ∏ * i ) based on Equation</formula><p>Combining both, a decrease in WCET of arbitrary nodes in a DAG leads to a non-increasing bound on its completion. Therefore, the proposed analysis provides a safe worst-case bound as long as each node in the DAG does not exceed its WCET, i.e., sustainable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Supporting explicit execution order</head><p>With an explicit scheduling order for non-critical nodes, a tighter bound can be obtained as each node can only incur interference from concurrent nodes with a higher priority <ref type="bibr" target="#b10">[11]</ref>. Using the proposed schedule as an example, this section illustrates a novel analysis that can support CPFE and explicit execution order for non-critical nodes.</p><p>With node priority, the interfering nodes of v j on m-1 cores can be effectively reduced to 1) nodes in I(v j ) that have a higher priority than p j <ref type="bibr" target="#b10">[11]</ref>, and 2) m -1 nodes in I(v j ) that have a lower priority and the highest WCET due to the nonpreemptive schedule <ref type="bibr" target="#b9">[10]</ref>. Let I e (v j ) denote the nodes that can interfere a non-critical node v j with an explicit order, it is given as Equation <ref type="formula" target="#formula_46">10</ref>, in which argmax m-1 v k returns the first m -1 nodes with the highest value of the given metric (C k in this equation). The correctness of the equation is proven in <ref type="bibr" target="#b10">[11]</ref> and <ref type="bibr" target="#b9">[10]</ref>. For simplicity, we take the (m-1) low priority nodes as a safe upper bound. A finer ILP-based approach is available in <ref type="bibr" target="#b9">[10]</ref> to precisely compute this blocking. In addition, if node-level preemption is allowed,</p><formula xml:id="formula_46">I e (v j ) is further reduced to {v k |p k &gt; p j , v k ‚àà I(v j )}. I e (v j ) = {v k |p k &gt; p j , v k ‚àà I(v j )} ‚à™ m-1 argmax v k {C k |p k &lt; p j , v k ‚àà I(v j )}<label>(10)</label></formula><p>With this schedule, f (v j ), ‚àÄv j ‚àà V can be computed by Equation 3, with I e (v j ) applied to non-critical nodes executing on m-1 cores. Hence, Œ± i and Œ≤ i can be bounded with the updated</p><formula xml:id="formula_47">f (Œ∏ * i ) and f (v j ), ‚àÄv j ‚àà F (Œ∏ * i ) ‚à™ G(Œ∏ * i )</formula><p>, by Equation 7 and 9 respectively. Note that with an explicit schedule, Œª ve computed in Equation 8, it is not necessarily the longest path in F (Œ∏ * i ) that executes in the interfering workload <ref type="bibr" target="#b10">[11]</ref>. Instead, Œª ve in this case gives the path that will always finish last due to the pre-planned node execution order.</p><p>The final bound on the response time of the DAG task is, however, different from the generic case, i.e., Equation <ref type="formula" target="#formula_26">2</ref>. With node priority, it is not necessary that all workload in (W i -L i -Œ± i -Œ≤ i ) can interfere with the execution of Œª ve . Let R e denote the response time of a DAG task with an explicit scheduling order. It is bound in Equation <ref type="formula" target="#formula_48">11</ref>, in which I e (Œª ve ) determines the nodes that can delay Œª ve and I Œªv e ,j gives the actual delay on Œª ve from node v j in the interfering workload.</p><formula xml:id="formula_48">R e = Œ∏ * i ‚ààŒò * L i +Œ≤ i + 0, if |Œõ I e (Œªv e ) |&lt; m 1 m √ó vj ‚ààI e (Œªv e ) I Œªv e ,j , otherwise<label>(11)</label></formula><p>Given the length of Œ∏ * i (L i ) and the worst-case delay on Œª ve (I Œªv e ) in the interfering workload, the worst-case finish time of Œ∏ * i and F (Œ∏ * i ) is upper bounded by</p><formula xml:id="formula_49">L i + Œ≤ i + 1 m √ó vj ‚ààI e (Œªv e )</formula><p>I Œªv e ,j . This is proved in Lemma 2. In addition, if the number of paths in the nodes that can cause I Œªv e is less than m (i.e., |Œõ I e (Œªv e ) |&lt; m), Œª ve executes directly after Œ∏ * i and finishes by L i + Œ≤ i . This is proved in Lemma 3. Note that I Œªv e = 0 if Œ≤ i = 0, as all workload in F (Œ∏ * i ) contributes to Œ± i so that Œ∏ * i+1 (if it exists) can start immediately after Œ∏ * i . The nodes that can interfere with Œª ve (i.e., I e (Œª ve )) are given by Equation <ref type="formula" target="#formula_50">12</ref>, in which I Œªv e ,j gives the actual delay from node v j on Œª ve .</p><formula xml:id="formula_50">I e (Œªv e ) = v k ‚ààŒªv e {vj|f (vj) &gt; f (Œ∏ * i ) ‚àß pj &gt; p k , ‚àÄvj ‚àà I(v k )} ‚à™ v k ‚ààŒªv e 1..m argmax v k {I Œªv e ,j |f (vj) &gt; f (Œ∏ * i ) ‚àß pj &lt; p k , vj ‚àà I(v k )}<label>(12)</label></formula><p>Finally, I Œªv e ,j is bound by Equation <ref type="formula" target="#formula_51">13</ref>, which takes the workload of v j executed after f (Œ∏ * i ) (i.e., in the interfering workload) as the worst-case delay on Œª ve .</p><formula xml:id="formula_51">I Œªv e ,j = C j , if f (v j ) -C j ‚â• f (Œ∏ * i ) f (v j ) -f (Œ∏ * i ), otherwise<label>(13)</label></formula><p>This concludes the analysis for scheduling methods with node execution order known a priori. As with the generic bound, this analysis is sustainable, as a reduction in WCET of any arbitrary node cannot lead to completion later than the worst-case bound (see Section IV-A). Compared to the generic bound for non-critical nodes with random order, this analysis provides tighter results by removing the nodes that cannot cause a delay due to their priority, in which I e (v j ) ‚äÜ I(v j ) and</p><formula xml:id="formula_52">I ve ‚â§ W i -L i -Œ± i -Œ≤ i .</formula><p>In addition, we note that the proposed analysis does not strictly dominate the analysis in <ref type="bibr" target="#b10">[11]</ref> for a particular schedule, but can provide a more accurate result in the general case (see results in Section VII-A). In practice, the bound in <ref type="bibr" target="#b10">[11]</ref> can be used as a safe upper bound for the proposed analysis, to provide the most accurate known worst-case approximations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. EXTENSION TO SUPPORT MULTIPLE DAGS</head><p>In this section, we extend the proposed scheduling and analysing methods to allow the general sporadic task model with n DAG tasks Œì = {œÑ 1 , ..., œÑ n }, in which each task œÑ x is assigned a unique deadline monotonic priority P x .</p><p>With multiple DAG tasks, the schedule follows the principle of highest priority task (P x ) first and then within a task highest priority node (p j ) first, for all DAGs tasks and nodes that are ready to release. With a fully non-preemptive DAGlevel scheduling, the highest priority task in the ready queue is always scheduled to execute after the currently-executing task is finished. That is, task priority is used to select the next task to execute in the ready queue, whereas node priority gives the exact execution order of nodes in the scheduled DAG. We acknowledge this schedule is not work-conserving, and can lead to certain cores being idle with ready tasks await execution. This would lead to a longer worst-case response time. However, it allows the currently executing task to concentrate the available resources on nodes that form the critical path.</p><p>A DAG task œÑ x can be delayed by all jobs of high priority tasks released during œÑ x 's busy period and one job of the low priority task that has the highest completion time. Let R ‚ãÑ</p><p>x denote the worst-case response time of œÑ x in the multi-DAG case. R ‚ãÑ</p><p>x is given by Equation <ref type="formula" target="#formula_53">14</ref>, in which R x gives the worst-case completion time of œÑ x in the single-DAG case (by Equation <ref type="formula" target="#formula_26">2</ref>), lp(x) returns all tasks with a priority lower than P x and hp(x) denotes œÑ x 's higher priority tasks. As a ready task is released after the currently-executing task is completed, the worst-case delay from a job in an interfering task œÑ y is effectively bounded by R y by Equation <ref type="formula" target="#formula_26">2</ref>.</p><formula xml:id="formula_53">R ‚ãÑ x = R x + max œÑy‚ààlp(x) R y + œÑy‚ààhp(x) R ‚ãÑ x T y R y<label>(14)</label></formula><p>Finally, we note that by starting the next task in the ready queue during the "fan-in" phase (in which the parallelism of the DAG decreases monotonically until finished) of the current task, a reduced overall makespan for all tasks can be achieved while not affecting the current task. This is the same principle as used in processors for in order pipeline execution and proven analysis exists for bounding its execution <ref type="bibr" target="#b20">[21]</ref>. This will not jeopardise the analysis as a release earlier than expected cannot cause a node to finish later than the worst-case bound. However, this complicates the scheduling and requires an online analysis that identifies the fan-in phase of each DAG, which may not always be feasible in real-world applications. In addition, analysing this early release can further complicate the (Œ±, Œ≤)-pair analysis, due to extra offsets between the start of the critical path and non-critical nodes of the task. Notably, with <ref type="bibr" target="#b1">[2]</ref>, multi-DAGs with different periods can be described as a single periodic DAG, so the proposed analysis can be directly applied. However, this is out of the scope of this paper and is postponed to future research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. EVALUATIONS</head><p>The objectives of this evaluation are multifold: <ref type="bibr" target="#b0">(1)</ref> to demonstrate the scheduling and analysis (rta-cpf in Section V-A and rta-cpf-eo in Section V-C) improves the worstcase makespan (using the classic bound as reference); (2) to establish the conditions in which the proposed methods lead to an improved makespan; (3) to demonstrate the proposed execution order (EO) improves schedulability and the proposed analysis tightens the worst-case bounds; and (4) to evaluate the improvement through schedulability tests in multi-DAG cases. The proposed node ordering, EO, is compared with He et al. 2019 <ref type="bibr" target="#b10">[11]</ref> (denoted as He2019 hereafter) in which a node priority assignment is proposed alongside the analysis. The experiment is evaluated through randomly generated DAGs. Each DAG task is generated as follows: the generator starts from a source node, and then generates nodes layer by layer. The maximum depth (the number of layers) is randomly chosen from 5 to 8. The number of generated nodes in each layer is uniformly distributed from 2 to the parallelism parameter, p. Open-ended nodes randomly add connections with a probability of p c = 0.5 to join the other nodes in the previous layer. Then, all terminal nodes are connected to a sink node. The source and sink nodes serve the purpose of organising the node graph, they both have a execution time of one unit. Finally the execution times are randomly assigned to nodes given a total workload of W 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Evaluation of the worst-case makespan</head><p>The experiment evaluated the performance scaled with the number of cores (m). For each configuration (task and system setting), 1,000 trials are applied on the compared methods. Each trial generates one DAG task randomly. The normalised worst-case makespan is used as the indicator.</p><p>Observation: Figure <ref type="figure" target="#fig_5">3</ref> presents the worst-case makespan of the existing and the proposed methods with a varied number of cores, on DAGs generated with p = 8. With m ‚â§ 4, the rta-cpf provides similar results to the classic bound, i.e., most of its results are upper bounded by the classic bound. This is because with a small number of cores, the parallelism degree of the DAG is limited so that each non-critical node has a high worst-case finish time (see Equation <ref type="formula" target="#formula_30">3</ref>). This leads to a low Œ± i bound (as well as a high Œ≤ i bound) for each provider and hence a longer worst-case makespan approximation. With m further increased, rta-cpf becomes effective (starting from m = 6) and outperforms the classic bound, e.g., by 15.7% and 16.2% in average (and up to 31.7% and 32.2%), with m = 7 and m = 8 respectively. In this case, more workload 1 The evaluation implementation can be accessed at <ref type="url" target="https://github.com/">https://github.com/</ref> automaticdai/research-dag-scheduling-analysis. can execute in parallel with the critical path, i.e., an increase in Œ± i and a decrease in Œ≤ i . Thus, rta-cpf leads to tighter results by explicitly accounting for such workload, resulting in a safe reduction in interference on the critical path. Similar observations are also obtained in the comparison of rta-cpf-eo and He2019, where rta-cpf-eo provides shorter worst-case makespan approximations with m ‚â• 4, e.g., by up to 11.1% and 12.0% with m = 7 and m = 8 respectively. We note that the node execution order in both methods can also affect the analytical worst-case bounds. In Section VII-C, we compare the scheduling and analysing methods separately. Furthermore, we observe that with m = 7, rta-cpf (with random execution order) provides similar results with He 2019, and outperforms He2019 with m = 8. This observation further demonstrates the effectiveness of the proposed analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Sensitivity of DAG properties on the evaluated methods</head><p>From the result in Section VII-A, it is not straightforward to understand how DAG properties would impact on the worst-case makespan. To accommodate this, this experiment shows how the evaluated analysis is sensitive to certain DAG characteristics. That is to say, by controlling the parameters of the DAGs and evaluating the makespan in normalised values, it can be seen by how much the performance of the analysis changes. This would otherwise not be distinguishable through worst-case makespan or schedulability analysis. Specifically, we consider the following parameters in this experiment (with the number of cores fixed): 1) DAG parallelism (the maximum possible width when generating the randomised DAG), p; and 2) DAG critical path ratio to the total workload, %L, where %L = L/W √ó 100%.</p><p>Observation: Figure <ref type="figure">4</ref> shows the worst-case makespan of the proposed methods with varied values of the parallelism parameter (with m = 4). First, given a fixed number of cores, rta-cpf outperforms the classic bound in general. However, with the increase of p, the difference in performance of both methods becomes less significant. The intuition behind this observation is, with an increased number of concurrent nodes, the interference set of each node also increases (see Equation <ref type="formula" target="#formula_31">4</ref>), which then results in an increased worst-case finish time. This undermines the effectiveness of rta-cpf, which accounts for Œ± i and Œ≤ i based on worst-case finish time.</p><p>However, rta-cpf-eo demonstrates a strong performance and its effectiveness is not affected by the change on p, which consistently outperforms other methods in all system settings. This is because with an explicit execution order, the increase of concurrent nodes cannot impose a significant effect to the finish time of nodes, in which high priority nodes can execute immediately without any delay (see Equation <ref type="formula" target="#formula_50">12</ref>). Therefore, rta-cpf-eo with parallelism DAGs can still account for the actual interfering workload effectively, and provide the lowest worst-case makespan.</p><p>Observations: Figure <ref type="figure">5</ref> evaluates the impact of the length of the critical path on the effectiveness of the proposed methods, with m = 2. The critical path is varied in a range from 60% to 90% of total workload of generated DAGs. In this experiment, the proposed analysis demonstrates the most pronounced performance compared to the existing methods.</p><p>For the proposed methods, the worst-case makespan of rtacpf varies with a small number of %L, due to the varied internal structure of the generated DAGs (e.g., L = 0.6). However, with a further increase of both %L, rta-cpf provides a constant makespan, as all non-critical workload can execute in parallel with the critical path. In this case, the makespan directly equals the length of the critical path. Similar observations are also obtained for rta-cpf-eo, which provides a constant makespan (i.e., the length of critical path) under all experimental settings. Note, with further increases of %L, He2019 is completely dominated by rta-cpf (based on evaluations but not presented due to page limitation).</p><p>Summary: Based on the above experiments, the proposed methods outperform the classic method and the state-of-the-art in a general case. In addition, we observed that all the tested parameters m, p, %L have an impact on the performance of the proposed methods. For rta-cpf, it is sensitive to the relation between m, p, in which a low m or a high p undermines the effectiveness of the method. Both factors have a direct impact to the finish time of all non-critical nodes. In addition, %L can also significantly affects the performance of rta-cpf, in which a long critical path generally leads to more accurate makespan approximations. In a similar fashion to rta-cpf, rtacpf-eo demonstrates better performance with the increase of %L. However, due to its explicit execution order, rta-cpf-eo demonstrates much stronger performance than rta-cpf and is not affected with an impact from parameter p.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Effectiveness of the proposed schedule and analysis</head><p>In this experiment, the proposed priority assignment is compared against the state-of-the-art node-level priority assignment method, i.e., He2019. In addition, we demonstrate the worst-case makespan with the priority assignment considered. The purpose is to demonstrate the improved worst-case scenario achieved by the priority assignment. Overall there are 1000 random tasksets generated under each configuration. Two metrics are compared in this evaluation: (a) the percentage of times that the proposed rta-cpf-eo analysis is better than the compared method, and (b) the reduction in the normalised makespan within the improved cases.</p><p>Observation: Figure <ref type="figure" target="#fig_7">6</ref> reports the comparison of the proposed ordering method and the method in He2019, with a varied number of cores. The term "frequency" indicates the number of cases that the proposed schedule has a shorter (in red) or longer (in blue) makespan than He2019. For fairness, the proposed worst-case makespan analysis for explicit order (Section V-C) is applied for both ordering, so the differences in performance all comes from the ordering policies.</p><p>From the results, the proposed method outperforms He2019 with a higher frequency in general, especially with a small number of cores, e.g., around the frequency of 600 with m = 2 and m = 3. With the increase of m, the difference in frequency of the methods gradually decreases, and becomes difficult to distinguish with m = 7, 8. In these cases, most nodes can execute in parallel so that different execution orders become less significant to affect the final makespan.</p><p>Table <ref type="table" target="#tab_3">III</ref> presents detailed comparison of both methods in their advantage cases, in terms of the percentage of improvements. For EO ‚âª He2019 (i.e., proposed schedule outperforms He2019), we observe an average improvement (in terms of worst-case makespan) higher than 5.4% (up to 7.89%) in all cases. For cases with EO ‚â∫ He2019 (i.e., He2019 performs better), the improvement is consistently lower than the corresponding case with EO ‚âª He2019.</p><p>Table <ref type="table" target="#tab_4">IV</ref> reports the number of advantage cases and the scientific significance of the improvements, in both EO ‚âª He2019 and EO ‚â∫ He2019. The magnitude in Table IV is a categorical value in (negligible effect, small effect, medium effect and large effect) to reflect the scientific significance <ref type="bibr" target="#b21">[22]</ref>. In other words, the scientific significance informs whether any difference is more than random chance and the size of the difference. The column # of data illustrates the number of times one approach has a lower makespan than the other. In all cases our approach outperforms the state of the art, He2019. The Magnitude gives further evidence of the benefits of our approach, e.g. m = 4 the effect size when EO outperforms He2019 is medium versus small for He2019 outperforming EO, and for m = 8 it is small versus negligible even though # of data have similar values. Similarly, we have done a comparison of our analysis and the analysis in He2019 by applying the same ordering on both methods and found consistent results (not presented due to page limitation). Therefore, we conclude that the proposed scheduling and analysing are effective, and outperform the state-of-art techniques in the general case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Schedulability test with multi-DAGs</head><p>To further evaluate priority assignment, we tested the schedulability of random generated multi-DAG tasksets. The experiment is setup as follows: the number of cores is fixed to m = 6. The total utilisation of all DAG tasks (averaged on per core) ranges from 0.1 to 1.0, with a step size of 0.05. The utilisation of a DAG task within a taskset is generated through the UUniFast-discard algorithm <ref type="bibr" target="#b22">[23]</ref>. The utilisation of each DAG should be less than m, otherwise it is discarded.</p><p>A taskset has 10 DAG tasks, and each DAG is generated randomly in the same way as introduced earlier. The periods of DAG tasks are randomly generated for T i ‚àà (1000, 2000), and deadlines are equal to periods. The execution times of the nodes within a DAG task are then generated based on its workload W i = U i /T i . Schedulability for multi-DAGs is tested using Equation 14, in which R i is calculated using rta-cpf-eo analysis and He2019 analysis, respectively. The schedulability of random execution, i.e., without node-level orders is evaluated by the classic response time equation. The priorities are assigned to DAGs based on the deadlinemonotonic policy.</p><p>Observation: As given in Figure <ref type="figure" target="#fig_8">7</ref>, the method random gives a reference bound with nodes in each DAG scheduled  randomly. From the results, the proposed scheduling and analysing methods provides better system schedulability than that of the state-of-the-art in most cases (i.e., for U/m = 0.30 -0.45). The results are consistent with the single DAG case as DAG tasks are executed in a non work-conserving manner (i.e., one task at a time), in the priority order. Table V reports the detailed schedulability results. From this table, the proposed methods outperform the state-of-the-art up to 53.3% when U/m = 0.35. Summary: In these experiments, we have shown that the proposed worst-case makespan analysis and the priority assignment can generally improve the schedulability by tightening the worst-case bound. The effectiveness of the method is also shown by the improved number of schedulable tasks with multi-DAGs compared with the existing approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VIII. CONCLUDING REMARKS</head><p>In this paper, a rule-based scheduling method is proposed which maximises node parallelism to improve the schedulability of single DAG tasks. Based on the rules, response time analysis is developed that provides tighter bounds than existing analysis for 1) any scheduling method that prioritises the critical path, and 2) scheduling methods with explicit execution order known a priori. We demonstrate that the proposed scheduling and analysing methods outperform existing techniques. In future work, we will focus on further optimisations of the proposed method and extensions to fully support workconserving schedules for multiple recurrent DAGs.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Makespan of a DAG task with example execution scenarios and the critical path highlighted.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The CPC model of a DAG. The critical path is highlighted in orange and the non-critical nodes are in blue, with different colour gradation to indicate the earliness they can delay the critical path.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Algorithm 2 : 5 while F (Œ∏ * i ) = ‚àÖ do 6 /</head><label>256</label><figDesc>EA(Œò * , Œò): Priority Assignment Inputs : Œò * , Œò Parameters: p, p max Initialise : p = p max , ‚àÄv j ‚àà Œò * ‚à™ Œò, p j = -1 1 / * Assignment Rule 1. * / 2 ‚àÄv j ‚àà Œò * , p j = p; p = p -1; 3 / * Assignment Rule 2. * / 4 for each Œ∏ * i ‚àà Œò * , in topological order do * Find the longest local path in F (Œ∏ * i ). * / 7 v e , v j ‚àà F (Œ∏ * i ) : 8 ve = argmax ve {le(F (Œ∏ * i ))|suc(ve) = ‚àÖ};</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>15 / 17 F</head><label>1517</label><figDesc>* Assignment Rule 3. * / 16 ‚àÄv j ‚àà Œª ve , p j = p; p = p -1; (Œ∏ * i ) = F (Œ∏ * i ) \ Œª ve ;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: DAG worst-case makespan using analytical methods with varied number of cores (m).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :Figure 5 :</head><label>45</label><figDesc>Figure 4: Sensitivity of parallelism parameter when m = 4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Proposed priority ordering v.s. the ordering in He 2019, grouped by the number of cores (m). p = 8. "‚âª" means outperform and "‚â∫" means the vice versa.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Schedulability v.s. target averaged total utilisation per core for multi-DAGs (when m = 6, p = 8).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Step 1: identifying capacity providers * / 3 for each v j ‚àà Œª * , in topological order do</figDesc><table><row><cell>4</cell><cell>Œ∏  *  i = {v j }; Œª  *  = Œª  *  \v j ;</cell></row><row><cell>5</cell><cell>while pre(v j+1 ) = {v j } do</cell></row><row><cell>6</cell><cell>Œ∏  *  i = Œ∏  *  i ‚à™ {v j+1 }; Œª  *  = Œª  *  \ v j ;</cell></row><row><cell>7</cell><cell>end</cell></row><row><cell>8</cell><cell>Œò  *  = Œò  *  ‚à™ Œ∏  *  i ;</cell></row><row><cell cols="2">9 end</cell></row><row><cell cols="2">10 / * Step 2: identifying capacity consumers * /</cell></row><row><cell></cell><cell>for each Œ∏  *  i ‚àà Œò  *  , in topological order do</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table II :</head><label>II</label><figDesc>Notations introduced in the proposed (Œ±, Œ≤)-pair response time analysis</figDesc><table><row><cell>Notation</cell><cell>Description</cell></row><row><cell>Œ≤ i Œªv e</cell><cell>The length of the longest path in F (Œ∏  *  i ) that executes later than f (Œ∏  *  i ).</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table III :</head><label>III</label><figDesc>Percentage of improvement in advantage cases w.r.t. node ordering policy</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="3">EO ‚âª He2019</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">EO ‚â∫ He2019</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>m=2</cell><cell>m=3</cell><cell>m=4</cell><cell>m=5</cell><cell>m=6</cell><cell>m=7</cell><cell>m=8</cell><cell>m=2</cell><cell>m=3</cell><cell>m=4</cell><cell>m=5</cell><cell>m=6</cell><cell>m=7</cell><cell>m=8</cell></row><row><cell>avg.</cell><cell>7.89</cell><cell>8.05</cell><cell>7.21</cell><cell>6.77</cell><cell>6.18</cell><cell>5.72</cell><cell>5.41</cell><cell>6.47</cell><cell>5.92</cell><cell>4.53</cell><cell>3.24</cell><cell>2.52</cell><cell>1.64</cell><cell>1.65</cell></row><row><cell>max.</cell><cell cols="7">30.63 36.18 33.39 34.17 30.65 27.75 25.27</cell><cell cols="7">30.68 27.19 23.83 21.59 24.09 16.76 19.26</cell></row><row><cell>min.</cell><cell>0.05</cell><cell>0.02</cell><cell>0.02</cell><cell>0.02</cell><cell>0.02</cell><cell>0.02</cell><cell>0.03</cell><cell>0.01</cell><cell>0.04</cell><cell>0.02</cell><cell>0.03</cell><cell>0.03</cell><cell>0.02</cell><cell>0.03</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table IV :</head><label>IV</label><figDesc>Advantage cases numbers and scientific significance in node-level priority assignment -EO and He2019 ordering both implemented in (Œ±, Œ≤) analysis.</figDesc><table><row><cell>m</cell><cell>Dataset</cell><cell># of data</cell><cell>Magnitude</cell></row><row><cell>2</cell><cell>EO ‚âª He2019 He2019 ‚âª EO</cell><cell>668 261</cell><cell>medium medium</cell></row><row><cell>4</cell><cell>EO ‚âª He2019 He2019 ‚âª EO</cell><cell>450 276</cell><cell>medium small</cell></row><row><cell>6</cell><cell>EO ‚âª He2019 He2019 ‚âª EO</cell><cell>298 255</cell><cell>small negligible</cell></row><row><cell>8</cell><cell>EO ‚âª He2019 He2019 ‚âª EO</cell><cell>192 184</cell><cell>small negligible</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table V :</head><label>V</label><figDesc>Schedulable tasksets (%) to the target utilisation U/m (averaged on per core)</figDesc><table><row><cell></cell><cell>U</cell><cell>0.2</cell><cell>0.25</cell><cell>0.3</cell><cell>0.35</cell><cell>0.4</cell><cell>0.45 0.5</cell></row><row><cell cols="2">Random</cell><cell cols="3">100 99.9 87.4</cell><cell>9.0</cell><cell>0.4</cell><cell>0.3</cell><cell>0.2</cell></row><row><cell cols="2">EO</cell><cell cols="5">100 99.9 99.6 80.0 13.0</cell><cell>0.7</cell><cell>0.2</cell></row><row><cell cols="2">He2019</cell><cell cols="4">100 99.9 95.2 26.7</cell><cell>0.7</cell><cell>0.3</cell><cell>0.2</cell></row><row><cell>Schedulable Tasksets (%)</cell><cell>0 20 40 60 80 100</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>random EO He2019</cell></row><row><cell></cell><cell>0.20</cell><cell>0.25</cell><cell>0.30</cell><cell></cell><cell>0.35</cell><cell>0.40</cell><cell>0.45</cell><cell>0.50</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>U/m</cell><cell></cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A generalized parallel task model for recurrent real-time processes</title>
		<author>
			<persName><forename type="first">S</forename><surname>Baruah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Bonifaci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Marchetti-Spaccamela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Stougie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Wiese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Real-Time Systems Symposium</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="63" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Latency-aware generation of single-rate DAGs from multi-rate task sets</title>
		<author>
			<persName><forename type="first">M</forename><surname>Verucchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Theile</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Caccamo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bertogna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Real-Time and Embedded Technology and Applications Symposium</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="226" to="238" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Synthesizing job-level dependencies for automotive multi-rate effect chains</title>
		<author>
			<persName><forename type="first">M</forename><surname>Becker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Dasari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mubeen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Behnam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Nolte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Embedded and Real-Time Computing Systems and Applications</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="159" to="169" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Rosch: realtime scheduling framework for ROS</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Sato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Azumi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Nishio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Embedded and Real-Time Computing Systems and Applications</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="52" to="58" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">HLBS: Heterogeneous laxity-based scheduling algorithm for DAG-based real-time computing</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Suzuki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Azumi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Nobuhiko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Cyber-Physical Systems, Networks, and Applications</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="83" to="88" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Scheduling dependent periodic tasks without synchronization mechanisms</title>
		<author>
			<persName><forename type="first">J</forename><surname>Forget</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Boniol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Grolleau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lesens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Pagetti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Real-Time and Embedded Technology and Applications Symposium</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="301" to="310" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Automatic parallelization of multirate fmi-based co-simulation on multi-core</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Saidi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Pernet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sorel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Symposium on Theory of Modeling and Simulation</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page">Article</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Optimizing end-to-end latencies by adaptation of the activation events in distributed automotive systems</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vincentelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Giusto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Pinello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Natale</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Real Time and Embedded Technology and Applications Symposium</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="293" to="302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Comparative assessment and evaluation of jitter control methods</title>
		<author>
			<persName><forename type="first">G</forename><surname>Buttazzo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cervin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Real-Time and Network Systems</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="163" to="172" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Responsetime analysis of DAG tasks under fixed priority scheduling with limited preemptions</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Serrano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Melani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bertogna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Qui√±ones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Design, Automation &amp; Test in Europe Conference &amp; Exhibition</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1066" to="1071" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Intra-task priority assignment in real-time scheduling of DAG tasks on multi-cores</title>
		<author>
			<persName><forename type="first">Q</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Parallel and Distributed Systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2283" to="2295" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Response-time analysis of conditional DAG tasks in multiprocessor systems</title>
		<author>
			<persName><forename type="first">A</forename><surname>Melani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bertogna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Bonifaci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Marchetti-Spaccamela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">C</forename><surname>Buttazzo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Euromicro Conference on Real-Time Systems</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="211" to="221" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Bounds on multiprocessing timing anomalies</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Graham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal on Applied Mathematics</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="416" to="429" />
			<date type="published" when="1969">1969</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Improved response time analysis of sporadic DAG tasks for global FP scheduling</title>
		<author>
			<persName><forename type="first">J</forename><surname>Fonseca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Nelissen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>N√©lis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Real-Time Networks and Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="28" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Real-time scheduling and analysis of parallel tasks on heterogeneous multi-cores</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Systems Architecture</title>
		<imprint>
			<biblScope unit="volume">105</biblScope>
			<biblScope unit="page">101704</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">DAG-fluid: A real-time scheduling algorithm for DAGs</title>
		<author>
			<persName><forename type="first">F</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computers</title>
		<imprint>
			<biblScope unit="issue">01</biblScope>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Timing-anomaly free dynamic scheduling of conditional DAG tasks on multi-core systems</title>
		<author>
			<persName><forename type="first">P</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Guan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Embedded Computing Systems</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1" to="19" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Degree-of-node task scheduling of fine-grained parallel programs on heterogeneous systems</title>
		<author>
			<persName><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-F</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-N</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>An</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computer Science and Technology</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1096" to="1108" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Performance-effective and low-complexity task scheduling for heterogeneous computing</title>
		<author>
			<persName><forename type="first">H</forename><surname>Topcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hariri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Parallel and Distributed Systems</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="260" to="274" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Sustainability in real-time scheduling</title>
		<author>
			<persName><forename type="first">A</forename><surname>Burns</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Baruah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computing Science and Engineering</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="74" to="97" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Processor pipelines and their properties for static WCET analysis</title>
		<author>
			<persName><forename type="first">J</forename><surname>Engblom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Jonsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Embedded Software</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="334" to="348" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A critique and improvement of the CL common language effect size statistics of McGraw and Wong</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vargha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">D</forename><surname>Delaney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Educational and Behavioral Statistics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="101" to="132" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Techniques for the synthesis of multiprocessor tasksets</title>
		<author>
			<persName><forename type="first">P</forename><surname>Emberson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Stafford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">I</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Analysis Tools and Methodologies for Embedded and Real-time Systems</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="6" to="11" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
