<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Causal Discovery from Poisson Branching Structural Causal Model Using High-Order Cumulant with Path Analysis</title>
				<funder ref="#_VYDFN5W">
					<orgName type="full">National Science Fund for Excellent Young Scholars</orgName>
				</funder>
				<funder ref="#_GRMRWVJ">
					<orgName type="full">National Key R&amp;D Program of China</orgName>
				</funder>
				<funder>
					<orgName type="full">China Scholarship Council</orgName>
					<orgName type="abbreviated">CSC</orgName>
				</funder>
				<funder ref="#_WEjxPfc #_Q3xKEX8">
					<orgName type="full">Natural Science Foundation of China</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jie</forename><surname>Qiao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Guangdong University of Technology</orgName>
								<address>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yu</forename><surname>Xiang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Guangdong University of Technology</orgName>
								<address>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhengming</forename><surname>Chen</surname></persName>
							<email>chenzhengming1103@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Guangdong University of Technology</orgName>
								<address>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ruichu</forename><surname>Cai</surname></persName>
							<email>cairuichu@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Guangdong University of Technology</orgName>
								<address>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Peng Cheng Laboratory</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhifeng</forename><surname>Hao</surname></persName>
							<email>haozhifeng@stu.edu.cn</email>
							<affiliation key="aff2">
								<orgName type="department">College of Science</orgName>
								<orgName type="institution">Shantou University</orgName>
								<address>
									<settlement>Shantou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Causal Discovery from Poisson Branching Structural Causal Model Using High-Order Cumulant with Path Analysis</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-21T21:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Count data naturally arise in many fields, such as finance, neuroscience, and epidemiology, and discovering causal structure among count data is a crucial task in various scientific and industrial scenarios. One of the most common characteristics of count data is the inherent branching structure described by a binomial thinning operator and an independent Poisson distribution that captures both branching and noise. For instance, in a population count scenario, mortality and immigration contribute to the count, where survival follows a Bernoulli distribution, and immigration follows a Poisson distribution. However, causal discovery from such data is challenging due to the non-identifiability issue: a single causal pair is Markov equivalent, i.e., X â†’ Y and Y â†’ X are distributed equivalent. Fortunately, in this work, we found that the causal order from X to its child Y is identifiable if X is a root vertex and has at least two directed paths to Y , or the ancestor of X with the most directed path to X has a directed path to Y without passing X. Specifically, we propose a Poisson Branching Structure Causal Model (PB-SCM) and perform a path analysis on PB-SCM using high-order cumulants. Theoretical results establish the connection between the path and cumulant and demonstrate that the path information can be obtained from the cumulant. With the path information, causal order is identifiable under some graphical conditions. A practical algorithm for learning causal structure under PB-SCM is proposed and the experiments demonstrate and verify the effectiveness of the proposed method.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Causal discovery from observational data especially for count data is a crucial task that arises in numerous applications in biology <ref type="bibr" target="#b20">(Wiuf and Stumpf 2006)</ref>, economic <ref type="bibr" target="#b19">(WeiÃŸ and Kim 2014)</ref>, network operation maintenance <ref type="bibr" target="#b13">(Qiao et al. 2023;</ref><ref type="bibr" target="#b2">Cai et al. 2022)</ref>, etc. In online services, for instance, the reason for the number of product purchases is of particular interest, while finding the underlying causal structure among user behavior from purely observational data is appealing and pivotal for online operation.</p><p>Much effort has been made to address the identification of causal structure from observational data <ref type="bibr">(Spirtes, Glymour,</ref> Figure <ref type="figure" target="#fig_7">1</ref>: Illustration of branching structure causal modeling. and Scheines 2000; <ref type="bibr" target="#b21">Zhang et al. 2018;</ref><ref type="bibr" target="#b7">Glymour, Zhang, and Spirtes 2019;</ref><ref type="bibr" target="#b1">Cai et al. 2018)</ref>. In particular, constraint-based methods <ref type="bibr" target="#b12">(Pearl 2009;</ref><ref type="bibr" target="#b15">Spirtes, Meek, and Richardson 1995)</ref>, score-based methods <ref type="bibr" target="#b3">(Chickering 2002;</ref><ref type="bibr" target="#b17">Tsamardinos, Brown, and Aliferis 2006)</ref> identify the causal structure by exploring the conditional independence relation among variables, but these methods only focus on the category domain and can only identify up to the Markov equivalent class <ref type="bibr" target="#b12">(Pearl 2009)</ref>. Thus, proper count data modeling is required to further identify the causal structure beyond the equivalence class. Recent work by <ref type="bibr" target="#b10">(Park and Raskutti 2015)</ref> introduces a Poisson Bayesian network to model the count data and shows that it is identifiable using the overdispersion properties of Poisson BNs. Subsequently, it has been extended by accommodating a broader spectrum of distributions <ref type="bibr" target="#b11">(Park and Raskutti 2017)</ref>. In addition, the modeling of the zero-inflated Poisson data <ref type="bibr" target="#b4">(Choi, Chapkin, and Ni 2020</ref>) and the ordinal relation data <ref type="bibr" target="#b9">(Ni and Mallick 2022)</ref> and its identifiability of causal structure are investigated. However, the majority of these methods model the count data using Bayesian network ignoring the inherent branching structure among the counting relationship which is frequently encountered <ref type="bibr" target="#b18">(WeiÃŸ 2018)</ref>.</p><p>Take Figure <ref type="figure" target="#fig_7">1</ref> as an example, the cause of the purchasing event can be inherited from some of the searching events, the pop-up ads event, or exogenously occurs. As a result, the causal relationship among counts constitutes a branching structure that can be modeled by a binomial thinning operator 'â€¢' <ref type="bibr" target="#b16">(Steutel and van Harn 1979)</ref> with an additive independent Poisson distribution for innovation. That is, the purchasing count (Y ) is affected by the pop-up ads count (X 2 ) and the searching count (X 1 ) which can be modeled by Y = a 1 â€¢ X 1 + a 2 â€¢ X 2 + Ïµ where a â€¢ X â‰” âˆ‘ n âˆ¼ Bern(a), Ïµ âˆ¼ Pois. Generally speaking, the thinning operator models the branching structure that not every click will lead to purchasing while the additional noise models the general count of exogenous events. That is, a count represents the random size of an imaginary population, and the thinning operation randomly deletes some of the members of this population while concurrently introducing new immigration. This modeling approach finds widespread utility across various domains, notably within the context of the integer-value autoregressive model <ref type="bibr" target="#b18">(WeiÃŸ 2018)</ref>, which is first proposed by <ref type="bibr" target="#b0">Al-Osh and Alzaid (1987)</ref>; <ref type="bibr" target="#b8">McKenzie (1985)</ref>. Despite its extensive used, how to identify the causal structure in such type of model from purely observational data is still unclear.</p><p>To explicitly account for the branching structure, we propose a Poisson Branching Structural Causal Model (PB-SCM). We establish the identifiability theory for the proposed PB-SCM using high-order cumulant with path analysis. Theoretical results suggest that for any adjacent vertex X and Y , the causal order is identifiable if X is a root vertex and has at least two directed paths to Y , or the ancestor of X with the most directed path to X has a directed path to Y without passing X. Based on the results of the causal order we further propose an efficient causal skeleton learning approach featured with FFT acceleration. We demonstrate the effectiveness of the proposed causal discovery method using synthetic data and real data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Poisson Branching Structural Causal Model</head><p>In this section, we first formalize the Poisson branching structural causal model, and then we introduce the preliminary of cumulant and some necessary properties in this model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Problem Formulation</head><p>Our framework is in the causal graphical models. We use P a(i) = {j|j â†’ i}, An(i) = {j|j â† i} denote the set of parents, ancestors of vertex i in a directed acyclic graph (DAG), respectively, and An(i, j) = An(i) âˆ© An(j) denote the set of common ancestors of vertex i and vertex j. Moreover, we define a directed path P = (i 0 , i 1 , ..., i n ) in G is a sequence of vertices of G where there is a directed edge from i j to i j+1 for any 0 â©½ j â©½ n -1 with the coefficient Î± i j ,i j+1 of each edge. The set of vertices can be arranged in causal order, such that no later variable causes any earlier variable. Now, we show the causal relationship in a causal graph can be formalized as the Poisson Branching Structural Causal Model (PB-SCM). Let X = {X 1 , . . . , X |V | } denotes a set of random Poisson counts, of which the causal relationship consist of a causal DAG G(V, E) with the vertex set V = {1, 2, ..., |V |} and edge set E such that each causal relation follows the PB-SCM: Definition 1 (Poisson Branching Structural Causal Model). For each random variable X i âˆˆ X, let Ïµ i âˆ¼ Pois(Âµ i ) be the noise component of X i , then X i is generated by:</p><formula xml:id="formula_0">X i = âˆ‘ jâˆˆP a(i) Î± j,i â€¢ X j + Ïµ i ,<label>(1)</label></formula><p>where Î± j,i âˆˆ (0, 1] is the coefficient from vertex j to i, P a(i) is the parent set of X i in G, and Î± â€¢ X i âˆ¶= âˆ‘</p><formula xml:id="formula_1">X i n=1 Î¾ (Î±)</formula><p>n is a Bi-nomial thinning operation with Î¾ (Î±) n i.i.d.</p><p>âˆ¼ Bern(Î±), Bern(Î±) is the Bernoulli distribution with parameter Î±.</p><p>We further define some graphical concepts. We use</p><formula xml:id="formula_2">P iâ†j = {P iâ†j k } |P iâ†j | k=1</formula><p>denotes the set of all directed paths from vertex i to j, where P iâ†j k = (i, k 1 , k 2 , ..., k p , j), p = |P iâ†j k</p><p>| -2, denote the k-th directed path from vertex i to j. For each directed path P iâ†j k , we use A iâ†j k = (Î± i,k 1 , Î± k 1 ,k 2 , . . . , Î± k p ,j ) denote the corresponding coefficients sequence of path P iâ†j k . We let P iâ†i = {P iâ†i } also be a valid directed path for simplicity. Besides, we use </p><formula xml:id="formula_3">A iâ†j k â€¢ X i â‰” Î± k p ,j â€¢ â‹¯ â€¢ Î± k 1 ,k 2 â€¢ Î± i,k 1 â€¢X i denote</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Preliminary</head><p>To address the identification of PB-SCM, cumulant are used in our work for building a connection to the path, providing a solution to the identifiability issue. Here, we recall the definition of cumulant and some basis properties. Definition 2 (k-th order joint cumulant tensor). The kth order joint cumulant tensor of a random vector X</p><formula xml:id="formula_4">= [X 1 , ..., X n ] T is the k-way tensor T (k) X in R nÃ—â‹¯Ã—n â‰¡ (R n ) k whose entry in (i 1 , ..., i k ) is the joint cumulant: T (k) X i 1 ,...,i k = Îº(X i 1 , . . . , X i k ) âˆ¶= âˆ‘ (B 1 ,...,B L ) (-1) L-1 (L -1)!E[ âˆ jâˆˆB 1 X j ]â‹¯E[ âˆ jâˆˆB L X j ],<label>(2)</label></formula><p>where the sum is taken over all partitions (B 1 , . . . , B L ) of the multiset {i 1 , ..., i k }.</p><p>In this work, we use the following specific cumulant form: Definition 3 (2D slice of joint cumulant tensor). For a random vector X with k-th order joint cumulant tensor</p><formula xml:id="formula_5">T (k) X</formula><p>where k â‰¥ 2, denote its 2D matrix slice of k-th order joint cumulant tensor as C (k) , where</p><formula xml:id="formula_6">C (k) i,j âˆ¶= Îº(X i , X j , â‹¯, X j k-1 times</formula><p>).</p><p>(3)</p><p>Cumulant has the property of multilinearity such that Îº(X + Y, Z 1 , . . . ) = Îº(X, Z 1 , . . . ) + Îº(Y, Z 1 , . . . ). Furthermore, any cumulant involving two (or more) independent random variables equals zero, i.e., Îº(Ïµ i , Ïµ j , . . . ) = 0 if Ïµ i and Ïµ j are independent. More importantly, any two variables in cumulant are exchangeable, e.g., Îº(X, Y, . . . ) = Îº(Y, X, . . . ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Identifiability</head><p>In this section, we deal with the identification problem of causal structure under PB-SCM. Due to our identifiability result benefit from the 'reducibility' of cumulant in Poisson Figure <ref type="figure" target="#fig_8">2</ref>: Triangular structure. For simplicity, we denote di-</p><formula xml:id="formula_7">rected path P 1 âˆ¶ X 1 a -â†’ X 2 and P 2 âˆ¶ X 1 b 1 -â†’ X 3 b 2 -â†’ X 2 with sequence of path coefficients A 1 = (a) and A 2 = (b 1 , b 2 ).</formula><p>distribution, we first characterize such property in Theorem 1. After which, an example is provided to reveal the intrinsic relation between the cumulant and the path in a causal graph under PB-SCM. Based on such connection, we complete the identifiability results that are divided into the case when the cause variable is root (Theorem 3) and the case when the cause variable is not root (Theorem 6).</p><p>We first introduce a fundamental property of cumulant in PB-SCM that the cumulant is reducible: Theorem 1 (Reducibility). Given a Poisson random variable Ïµ and n distinct sequences of coefficients A 1 , ..., A n , we have</p><formula xml:id="formula_8">Îº(A 1 â€¢ Ïµ, ..., A 1 â€¢ Ïµ k 1 times , ..., A n â€¢ Ïµ, ..., A n â€¢ Ïµ k n times ) = Îº(A 1 â€¢ Ïµ, . . . , A n â€¢ Ïµ) (4)</formula><p>where each A i â€¢ Ïµ repeats k i â‰¥ 1 times in the original cumulant and only appears once in the reduced cumulant.</p><p>Such a result is a generalization of the property of the Poisson distribution since the cumulant of the Poisson distribution is identical in every order.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Motivating Example</head><p>Before describing our theoretical results, we use a motivating example to show the challenges of the non-identifiability issues and then introduce the basic intuition regarding in what case and how can we identify the PB-SCM.</p><p>To see the non-identifiability issue, we can show that a reversed model always exists in a two-variable system. Remark 1. For any two variables causal graph, the causal direction of PB-SCM is not identifiable and a distributed equivalent reversed model exists.</p><p>For instance, consider X 1 â†’ X 3 in Fig. <ref type="figure" target="#fig_8">2</ref>, the distributed equivalent reverse model satisfies X</p><formula xml:id="formula_9">1 = b1 â€¢ X 3 + Îµ1 , where b1 = b 1 Âµ 1 /(b 1 Âµ 1 + Âµ 3 ) and Îµ1 âˆ¼ Pois(Âµ 1 -b 1 Âµ 1 ) such that this direction is not identifiable.</formula><p>Fortunately, we find that the causal direction is still possible to identify in a more general structure. Considering the causal relationship between X 1 and X 2 in Fig. <ref type="figure" target="#fig_8">2</ref>, here we provide an intuitive example to show how to identify such causal direction by utilizing the relationship between cumulant and path. Considering the cumulant C 1,2 with different orders, we can observe different behaviors of cumulant in the causal direction and the reverse direction. Thanks to the reducibility in Theorem 1, e.g., Îº(A 1 â€¢ Ïµ 1 , Ïµ 1 ) = Îº(A 1 â€¢ Ïµ 1 , Ïµ 1 , Ïµ 1 ), the cumulants with different orders for X 1 and X 2 is shown in Fig. <ref type="figure">3</ref>(a) and <ref type="bibr">Fig. 4(a)</ref>. Interestingly, we have C</p><p>(2)</p><formula xml:id="formula_10">2,1 = C (3) 2,1 in the reverse direction (Fig. 4(a)) but C (2) 1,2 â‰  C (3)</formula><p>1,2 in the causal direction (Fig. <ref type="figure">3(a</ref>)), i.e., there exists an asymmetry in the inequality relations of cumulants. Such asymmetry intriguing possibility to identify the causal order between two variables using the cumulant.</p><p>To understand how this asymmetry occurs and hence use it to identify the causal relations. We first discuss the identification in the simple scenario that the cause variable is a root vertex in G, and then we generalize such results into the scenario that the cause variable is not root.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Identification When Cause Variable Is Root</head><p>We start with the case that the cause variable is root vertex, in which our goal is to identify causal direction even though we do not know it is a root vertex. Recall the previous example, the key of identification is the inequality</p><formula xml:id="formula_11">C (2) 1,2 â‰  C (3) 1,2</formula><p>rendering an asymmetry for a causal pair. To understand how it occurs, we seek to character and leverage such inequality constraints of cumulants in a causal graph to infer the causal order (Theorem 4).</p><p>Here, we begin with two basic observations, which illustrate that inequality constraints of cumulants are driven by the number of paths between two variables. As shown in Fig. <ref type="figure">3</ref>(a), one may see that (i) the decomposition of C 1,2 is composed by a series of cumulants of the common noise (Ïµ 1 in this example) between X 1 and X 2 , which is due to the fact that any cumulant involving two (or more) independent random variables equals zero; (ii) moreover, such decomposition relates to the number of paths between X 1 and X 2 since</p><formula xml:id="formula_12">X 2 = A 1 â€¢ Ïµ 1 + A 2 â€¢ Ïµ 1 + b 2 â€¢ Ïµ 3 + Ïµ 2</formula><p>and by multilinearity, the cumulant will be split exponentially as the order of cumulant increase. With these observations, the reason why C</p><p>(2)</p><formula xml:id="formula_13">1,2 â‰  C (3)</formula><p>1,2 is that there exists more than one path in the causal direction while zero path in the reverse direction, i.e.,</p><formula xml:id="formula_14">|P 1â†2 | = 2, |P 2â†1 | = 0. As a result, C (2) 2,1 = C (k)</formula><p>2,1 for all k â‰¥ 2 order cumulant in the reverse direction.</p><p>In the following, we articulate the underlying law of the cumulant in PB-SCM and propose a closed-form solution to it. The first important observation is that due to the reducibility and the exchangeability of cumulant, the</p><formula xml:id="formula_15">C (k) 1,2</formula><p>for k â‰¥ 3 is only composed by three distinct cumulants:</p><formula xml:id="formula_16">Îº(Ïµ 1 , A 1 â€¢ Ïµ 1 ), Îº(Ïµ 1 , A 2 â€¢ Ïµ 1 ), and Îº(Ïµ 1 , A 1 â€¢ Ïµ 1 , A 2 â€¢ Ïµ 1 )</formula><p>with varying number of these cumulants. In particular, if we define the summation of cumulants that only contains one path as</p><formula xml:id="formula_17">Î› 1â†2 1 (Ïµ 1 â† X 2 ) â‰” Îº(Ïµ 1 , A 1 â€¢ Ïµ 1 ) + Îº(Ïµ 1 , A 2 â€¢ Ïµ 1 )</formula><p>and the summation of cumulants that contains two paths as</p><formula xml:id="formula_18">Î› 1â†2 2 (Ïµ 1 â† X 2 ) â‰” Îº(Ïµ 1 , A 1 â€¢ Ïµ 1 , A 2 â€¢ Ïµ 1</formula><p>), we will have the following closed-form solution:</p><formula xml:id="formula_19">C (4) 1,2 = Î› 1â†2 1 (Ïµ 1 â† X 2 ) + âˆ‘ m 1 +m 2 =3 m 1 ,m 2 &gt;0 ( 3 m 1 m 2 )Î› 1â†2 2 (Ïµ 1 â† X 2 ) (5)</formula><p>where</p><formula xml:id="formula_20">( 3 m 1 m 2</formula><p>) is the multinomial coefficient, indicating the number of ways of placing 3 distinct objects into 2 distinct bins with m 1 objects in the first bin, m 2 objects in the second</p><formula xml:id="formula_21">ğœ…ğœ… Ïµ 1 , A 1 âˆ˜ ğœ–ğœ– 1 + ğœ…ğœ… Ïµ 1 , A 2 âˆ˜ ğœ–ğœ– 1 ğ¶ğ¶ 1,2 (2) = ğœ…ğœ… ğ‘‹ğ‘‹ 1 , ğ‘‹ğ‘‹ 2 = 1, A 1 , A 1 + 1, A 1 , A 2 + 1, A 2 , A 1 + 1, A 2 , A 2 ğ¶ğ¶ 1,2 (4) = 1,A 1 ,A 1 ,A 1 + 1,A 1 ,A 1 ,A 2 â‹¯+ 1,A 2 ,A 2 ,A 1 + 1,A 2 ,A 2 ,A 2 â‰” (1, ğ´ğ´ 1 ) â‰” (1, ğ´ğ´ 2 ) ğœ…ğœ… Ïµ 1 , A 1 âˆ˜ ğœ–ğœ– 1 , ğ‘‹ğ‘‹ 2 + ğœ…ğœ… Ïµ 1 , A 2 âˆ˜ ğœ–ğœ– 1 , ğ‘‹ğ‘‹ 2 2 Ã— Î› 2 1â†2 (1 âˆ˜ ğœ–ğœ– 1 â† ğ‘‹ğ‘‹ 2 ) 6 Ã— Î› 2 1â†2 (1 âˆ˜ ğœ–ğœ– 1 â† ğ‘‹ğ‘‹ 2 ) ğ¶ğ¶ 1,2 (3) = split over X 2 + (a) Cumulant decomposition of the causal pair X 1 â† X 2 where X 1 is root. ğœ…ğœ… Ïµ 3 , ğ‘ğ‘ 2 âˆ˜ ğœ–ğœ– 3 , ğ‘ğ‘ 2 âˆ˜ ğœ–ğœ– 3 + ğ‘ğ‘ 1 ,A 1 ,A 1 + ğ‘ğ‘ 1 ,A 1 ,A 2 + ğ‘ğ‘ 1 ,A 2 ,A 1 + ğ‘ğ‘ 1 ,A 2 ,A 2 ğœ…ğœ… Ïµ 3 , ğ‘ğ‘ 2 âˆ˜ ğœ–ğœ– 3 + ğœ…ğœ… ğ‘ğ‘ 1 âˆ˜ Ïµ 1 , A 1 âˆ˜ ğœ–ğœ– 1 + ğœ…ğœ… ğ‘ğ‘ 1 âˆ˜ Ïµ 1 , A 2 âˆ˜ ğœ–ğœ– 1 â‰” (ğ‘ğ‘ 1 , ğ´ğ´ 1 ) â‰” (ğ‘ğ‘ 1 , ğ´ğ´ 2 ) ğ¶ğ¶ 3,2 (2) =ğœ…ğœ…(ğ‘‹ğ‘‹ 3 , ğ‘‹ğ‘‹ 2 )= ğœ…ğœ… Ïµ 3 , ğ‘ğ‘ 2 âˆ˜ ğœ–ğœ– 3 , ğ‘‹ğ‘‹ 2 + ğœ…ğœ… ğ‘ğ‘ 1 âˆ˜ Ïµ 1 , A 1 âˆ˜ ğœ–ğœ– 1 , ğ‘‹ğ‘‹ 2 + ğœ…ğœ… ğ‘ğ‘ 1 âˆ˜ Ïµ 1 , A 2 âˆ˜ ğœ–ğœ– 1 , ğ‘‹ğ‘‹ 2 2Ã—Î› 2 1â†2 (ğ‘ğ‘ âˆ˜ğœ–ğœ– 1 â†ğ‘‹ğ‘‹ 2 ) 6 Ã— Î› 2 1â†2 (ğ‘ğ‘ âˆ˜ ğœ–ğœ– 1 â† ğ‘‹ğ‘‹ 2 ) ğ¶ğ¶ 3,2 (3) = split over X 2</formula><p>From ğ‘‹ğ‘‹ 3 to ğ‘‹ğ‘‹ 2</p><p>From the common ancestor</p><formula xml:id="formula_22">ğ‘‹ğ‘‹ 1 to ğ‘‹ğ‘‹ 2 =ğœ…ğœ… Ïµ 3 , ğ‘ğ‘ 2 âˆ˜ğœ–ğœ– 3 , ğ‘ğ‘ 2 âˆ˜ğœ–ğœ– 3 , ğ‘ğ‘ 2 âˆ˜ğœ–ğœ– 3 + ğ‘ğ‘ 1 ,A 1 ,A 1 ,A 1 + ğ‘ğ‘ 1 ,A 1 ,A 1 ,A 2 + â‹¯ + ğ‘ğ‘ 1 ,A 2 ,A 2 ,A 2 ğ¶ğ¶ 3,2 (4)</formula><p>split over X 2 (b) Cumulant decomposition of the causal pair X 3 â† X 2 where X 3 is not root.</p><p>Figure <ref type="figure">3</ref>: Illustration of decomposing the cumulant of causal direction, C 1,2 and C 3,2 , in triangular structure (Fig. <ref type="figure" target="#fig_8">2</ref>). For simplicity, we denote Îº(Ïµ</p><formula xml:id="formula_23">i , A i â€¢ Ïµ i , ..., A j â€¢ Ïµ i ) by (1, A i , ..., A j ) and denote Îº(b 1 â€¢ Ïµ i , A i â€¢ Ïµ i , ..., A j â€¢ Ïµ i ) by (b 1 , A i , ..., A j ). ğœ…ğœ… ğ´ğ´ 1 ğœ–ğœ– 1 ,ğœ–ğœ– 1 ğœ…ğœ…(ğ´ğ´ 2 ğœ–ğœ– 1 ,ğœ–ğœ– 1 ) (ğ´ğ´ 1 , 1) (ğ´ğ´ 2 , 1) ğ´ğ´ 1 , 1, 1 + (ğ´ğ´ 2 , 1, 1) ğ¶ğ¶ 2,1 (3) ğ´ğ´ 1 ,1,1,1 + (ğ´ğ´ 2 ,1,1,1) ğ¶ğ¶ 2,1 (2) =ğ¶ğ¶ 2, 1 (3) =ğ¶ğ¶ 2,1 (4) =ğœ…ğœ… ğ‘‹ğ‘‹ 2 ,ğ‘‹ğ‘‹ 1 ğ¶ğ¶ 2,1 (4) ğ¶ğ¶ 2,1 (2) = = = âˆ˜ âˆ˜ â‰” â‰” + (a) Cumulant decompo- sition of X 2 â†X 1 . ğœ…ğœ… ğ‘ğ‘ 2 ğœ–ğœ– 3 ,ğœ–ğœ– 3 ,ğœ–ğœ– 3 ğ´ğ´ 1 ,ğ‘ğ‘ 1 ,ğ‘ğ‘ 1 (ğ´ğ´ 2 ,ğ‘ğ‘ 1 ,ğ‘ğ‘ 1 ) + âˆ˜ + ğœ…ğœ… ğ‘ğ‘ 2 ğœ–ğœ– 3 ,ğœ–ğœ– 3 ,ğœ–ğœ– 3 ,ğœ–ğœ– 3 ğ´ğ´ 1 ,ğ‘ğ‘ 1 ,ğ‘ğ‘ 1 ,ğ‘ğ‘ 1 (ğ´ğ´ 2 ,ğ‘ğ‘ 1 ,ğ‘ğ‘ 1 ,ğ‘ğ‘ 1 ) ğœ…ğœ… ğ‘ğ‘ 2 ğœ–ğœ– 3 ,ğœ–ğœ– 3 ğœ…ğœ… ğ´ğ´ 1 ğœ–ğœ– 1 ,ğ‘ğ‘ 1 ğœ–ğœ– 1 ğœ…ğœ…(ğ´ğ´ 2 ğœ–ğœ– 1 ,ğ‘ğ‘ 1 ğœ–ğœ– 1 ) â‰” (ğ´ğ´ 1 , ğ‘ğ‘ 1 ) â‰” (ğ´ğ´ 2 , ğ‘ğ‘ 1 ) ğ¶ğ¶ 2,3<label>(3)</label></formula><formula xml:id="formula_24">ğ¶ğ¶ 2,3<label>(4)</label></formula><p>From ğ‘‹ğ‘‹ 3 to ğ‘‹ğ‘‹ 3</p><p>From ğ‘‹ğ‘‹ 1 to ğ‘‹ğ‘‹ 3</p><formula xml:id="formula_25">= = ğ¶ğ¶ 2,3 (2) = ğ¶ğ¶ 2,3 (3) = ğ¶ğ¶ 2,3<label>(4)</label></formula><p>= ğœ…ğœ… ğ‘‹ğ‘‹ 2 , ğ‘‹ğ‘‹ 3</p><formula xml:id="formula_26">+ + + + = ğ¶ğ¶ 2,3 (2) âˆ˜ âˆ˜ âˆ˜ âˆ˜ (b) Cumulant decomposition of X 2 â† X 3 .</formula><p>Figure <ref type="figure">4</ref>: Illustration of decomposing the cumulant of reverse direction, C 2,1 and C 2,3 , in triangular structure (Fig. <ref type="figure" target="#fig_8">2</ref>).</p><p>bin. As a result, we will eventually have 6 Ã— Î› 1â†2 2</p><p>(Ïµ 1 â† X 2 ) as shown in Fig. <ref type="figure">3(a)</ref>. Generally, we define Î› iâ†j k (A â€¢ Ïµ i â† X j ) as the summation of cumulants that contain k paths from root vertex i to j: Definition 4 (k-path cumulants summation for root vertex). Given two vertices i and j, for k â©½ |P iâ†j |, the k-path cumulants summation from vertex i to j is given by:</p><formula xml:id="formula_27">Î› iâ†j k (A â€¢ Ïµ i â† X j ) = âˆ‘ 1â‰¤l 1 &lt;l 2 &lt;...&lt;l k â‰¤|P iâ†j | Îº(A â€¢ Ïµ i , A iâ†j l 1 â€¢ Ïµ i , ..., A iâ†j l k â€¢ Ïµ i ),<label>(6)</label></formula><p>where l 1 , . . . , l k âˆˆ Z + , A is an arbitrary sequence of co-</p><formula xml:id="formula_28">efficients. For k &gt; |P iâ†j |, Î› iâ†j k â‰¡ 0 and for k = 1, Î› iâ†i 1 (A â€¢ Ïµ i â† X i ) = Îº(A â€¢ Ïµ i , Ïµ i ), and k &gt; 1, Î› iâ†i k â‰¡ 0.</formula><p>Intuitively, Eq. ( <ref type="formula" target="#formula_27">6</ref>) is a summation of all cumulants that contain k paths information from vertex i to j , and Î› iâ†i 1 denotes the relation from the noise to itself. Based on the k-path cumulants summation, C (n) i,j can be decomposed as follows: Theorem 2. For any two vertices i and j where i is root vertex, i.e., vertex i has an empty parent set, the 2D slice of joint cumulant C (n) i,j satisfies:</p><formula xml:id="formula_29">C (n) i,j = âˆ‘ n-1 k=1 âˆ‘ m 1 +â‹¯+m k =n-1 m l &gt;0 ( n -1 m 1 m 2 â‹¯m k )Î› iâ†j k (1â€¢Ïµ i â† X j ).<label>(7)</label></formula><p>where</p><formula xml:id="formula_30">( n-1 m 1 m 2 â‹¯m k ) = (n-1)! m 1 !m 2 !â‹¯m k ! is the multinomial coeffi- cients.</formula><p>Theorem 2 plays an important role in the identification of the causal order as it introduces the connection between the joint cumulant and path information. Moreover, since every order of the 2D slice joint cumulant can be obtained by Eq. ( <ref type="formula" target="#formula_23">3</ref>), and thus every order of Î› k can also be obtained by solving the equation in Eq. (C.1). By using Î› k we are able to understand the identifiability in the following theorem:</p><p>Theorem 3 (Identifiability for root vertex). For any vertex i and j, where i is the root vertex in graph G, if</p><formula xml:id="formula_31">C (3) i,j -C (2) i,j â‰  0, then C (3) j,i -C (2) j,i = 0 and X i is the ancestor of X j .</formula><p>Intuitively, based on Theorem 2, we have <ref type="table"></ref>and<ref type="table">thus C</ref> (3) i,j -C</p><formula xml:id="formula_32">C (3) i,j -C (2) i,j = Î› iâ†j 2 (1 â€¢ Ïµ i â†X j ),</formula><p>(2) i,j â‰  0 indicates that there exists more than one path from i to j than the reverse direction. That is, the causal direction for root vertex is identifiable if there are at least two directed paths:</p><p>Theorem 4 (Graphical Implication of Identifiability for Root Vertex). For a pair of vertices i and j in graph G, if vertex i is a root vertex and exists at least two directed paths from i to j, i.e., |P iâ†j | â‰¥ 2, then the causal order between i and j is identifiable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Identification When Cause Variable Is Not Root</head><p>In this section, we aim to generalize the identification result from the root vertex to the non-root vertex.</p><p>When vertex i is not root, the main difference is that there might exist more than one common noise between two variables due to the possible common ancestor. Therefore, one may extend the result from the root vertex by considering each noise term as the separated root vertex. We present a general version of k-path cumulants summation as follows, which can be expressed as the aggregation of the k-path cumulants summations for the root vertices.</p><p>Definition 5 (k-path cumulants summation). The k-path cumulants summation from vertex i to vertex j is given by:</p><formula xml:id="formula_33">Î›k (X i â† X j ) =Î› iâ†j k (1â€¢Ïµ i â† X j ) + âˆ‘ mâˆˆAn(i,j)âˆª{j} |P mâ†i | âˆ‘ h=1 Î› mâ†j k (A mâ†i h â€¢Ïµ m â† X j ) .</formula><p>(8) where Î› k is the k-path cumulants summation for root vertex, |P mâ†i | is the number of directed paths from m to i.</p><p>With the general k-path cumulants summation, the general joint cumulant can be decomposed as follows: Theorem 5. For any two vertices i and j, the 2D slice of joint cumulant C (n) i,j satisfies:</p><formula xml:id="formula_34">C (n) i,j = âˆ‘ n-1 k=1 âˆ‘ m 1 +â‹¯+m k =n-1 m l &gt;0 ( n -1 m 1 m 2 â‹¯m k ) Î›k (X i â† X j ),<label>(9)</label></formula><p>where</p><formula xml:id="formula_35">( n-1 m 1 m 2 â‹¯m k ) = (n-1)! m 1 !m 2 !â‹¯m k ! is the multinomial coeffi- cients.</formula><p>To see the connection with the case of root vertex, we take X 3 â†’ X 2 in Fig. <ref type="figure" target="#fig_8">2</ref> as example. Since X 3 can be expressed as</p><formula xml:id="formula_36">X 3 = b 1 â€¢ Ïµ 1 + Ïµ 3 , as shown in Fig. 3(b), we can separate the cumulant into two parts Îº(Ïµ 3 , X 2 ), Îº(b 1 â€¢Ïµ 1 , X 2</formula><p>), which can be considered as the cumulant starting from vertex X 3 to X 2 and X 1 to X 2 , respectively. As a result, the general k-path cumulants summation can be expressed as the aggregate of all different Î› k starting with the corresponding noise terms. For instance, for X 3 â†’ X 2 in Fig. <ref type="figure" target="#fig_8">2</ref>, we have:</p><formula xml:id="formula_37">Î›2 (X 3 â† X 2 ) = Î› 3â†2 2 (1 â€¢ Ïµ 3 â† X 2 ) =0 + Î› 1â†2 2 (b 1 â€¢ Ïµ 1 â† X 2 ) =Îº(b 1 â€¢Ïµ 1 ,A 1 â€¢Ïµ 1 ,A 2 â€¢Ïµ 1 ) â‰  0, (10)</formula><p>where Eq. ( <ref type="formula">10</ref>) contains two different terms starting from Ïµ 3 and Ïµ 1 , respectively. In particular, since there only exists one directed path from X 3 to X 2 , Î› 3â†2 2 is zero while X 1 to X 2 has two paths and thus Î› 1â†2 2 is not zero. Similarly, for the reverse direction, we have</p><formula xml:id="formula_38">Î›2 (X 2 â†X 3 ) = Î› 2â†3 2 (1 â€¢ Ïµ 2 â†X 3 ) =0 + Î› 3â†3 2 (b 2 â€¢ Ïµ 3 â†X 3 ) =0 + Î› 1â†3 2 (A 1 â€¢ Ïµ 1 â†X 3 ) =0 + Î› 1â†3 2 (A 2 â€¢ Ïµ 1 â†X 3 ) =0 = 0, (<label>11</label></formula><formula xml:id="formula_39">)</formula><p>where Î›2 is zero since there are 0 directed path from X 2 to X 3 and only 1 directed path from X 1 or Ïµ 3 to X 3 . Intuitively, the general k-path cumulants summation Î›(X i â† X j ) captures the number of directed paths from the common ancestor to j. Moreover, for any two adjacency vertex i â†’ j and their common ancestor m, the number of directed paths from m to j is greater or equal to that from m to i, and thus, the causal order can be identified using the following strategy:</p><p>Theorem 6 (Identification of PB-SCM). If there exist k âˆˆ Z + such that Î›k (X i â† X j ) â‰  0 and Î›k (X j â† X i ) = 0 for any two adjacency vertex i and j, then X i is the parent of X j .  In addition, the k-path cumulants summation Î›k (X i â† X j ) will be 'dominated' by the variables (might be the common ancestor or i itself) that has the most paths to j since it is the aggregation of all the directed paths from both common ancestor and i. Therefore, for a non-root vertex, it is possible to be non-identifiable by Theorem 3 if the dominant variable is the common ancestor. Specifically, we provide the graphical implication of such identifiability given as follows: Theorem 7 (Graphical Implication of Identifiability). For a pair of causal relationship i â†’ j. The causal order of i, j is identifiable by Theorem 6, if (i) vertex i is a root vertex and |P iâ†j | â‰¥ 2; or (ii) there exists a common ances-</p><formula xml:id="formula_40">tor k âˆˆ arg max l {|P lâ†i ||l âˆˆ An(i, j)} has a directed path from k to j without passing i in G.</formula><p>One of the examples is given in Fig. <ref type="figure" target="#fig_3">5</ref>, in which Fig. <ref type="figure" target="#fig_3">5</ref>(a) is not identifiable but Fig. <ref type="figure" target="#fig_3">5(b</ref>) is identifiable. The reason is that Z is the dominant common ancestor of X, Y , and all directed paths from Z to Y will pass X making it unidentifiable based on Theorem 7. In contrast, Fig. <ref type="figure" target="#fig_3">5</ref>(b) includes an additional directed path Z â†’ C â†’ Y without passing X making X â†’ Y identifiable. This intriguingly implies that a denser structure would facilitate the effectiveness of our method.</p><p>Generally speaking, once the causal order is identified, one may identify the complete causal structure by orienting edges based on the causal order in the causal skeleton. Such implementation will be provided in the next section. By this, the identifiability of causal structure under PB-SCM is answered.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Learning Casual Structure For PB-SCM</head><p>In this section, we propose a causal structure learning algorithm for PB-SCM. Our method involves two steps: learning the skeleton of DAG G and inferring the causal direction using the results developed in Theorem 6.</p><p>Learning Causal Skeleton To learn the causal skeleton, instead of using the constraint-based method, we propose a likelihood-based method. This boosts sample efficiency as the likelihood of PB-SCM captures its branching structure but the constraint-based method does not.</p><p>Given a set of count data D and model parameters</p><formula xml:id="formula_41">Î˜ = {A = [Î± i,j ] âˆˆ [0, 1] |V |Ã—|V | , Âµ = [Âµ i ] âˆˆ R |V | â‰¥0 }, the log-likelihood is Markov respect to G, that is L(G, Î˜; D) = âˆ‘ |D| j=1 âˆ‘ |V | i=1 log P Î˜ (X i = x (j) i |X P a(i) = x (j)</formula><p>P a(i) ). However, calculating the likelihood directly using the probability mass function is costly. Therefore, we propose to calculate the probability mass function by using the probability-generating function (PGF). In detail, for each conditional distribution of X i , the likelihood can be calculated as follows:</p><p>Theorem 8. Let G X i |X P a(i) (s) be the PGF of random variable X i given its parents variable X P a(i) , we have:</p><formula xml:id="formula_42">P (X i = k|X P a(i) = x P a(i) ) = 1 k! âˆ‚ k G X i |X P a(i) (s) (âˆ‚s) k s=0 = âˆ‘ t i + âˆ‘ jâˆˆP a(i) t j =k Âµ t i i exp(-Âµ i ) t i ! âˆ jâˆˆP a(i) (x j ) t j Î± t j j,i (1 -Î± j,i ) x j -t j t j ! , (<label>12</label></formula><formula xml:id="formula_43">)</formula><p>where t j â‰¤ x j , (x j )</p><formula xml:id="formula_44">t j â‰” x j ! (x j -t j )!</formula><p>is the falling factorial,</p><formula xml:id="formula_45">Âµ i = E[Ïµ i ],</formula><p>and Ïµ i is the noise component of X i .</p><p>The result of Eq. (G.1) can be converted to a polynomial coefficient after taking polynomial multiplication, which can be accelerated via Fast Fourier Transform (FFT) <ref type="bibr" target="#b5">(Cormen et al. 2022)</ref>. A detailed discussion is given in the supplement.</p><p>Generally, the likelihood-based method will tend to produce excessive redundant causal edges. Such effect can be alleviated by introducing the Bayesian Information Criterion (BIC) penalty d log(m)/2 into the L(G, Î˜; D), where d is the number of edge of G and m is the size of dataset D. The penalized objective function is updated as follows: * into a skeleton (Line 6). The correctness of such a procedure can be guaranteed by the consistent property of BIC which is discussed in <ref type="bibr" target="#b3">(Chickering 2002)</ref>.</p><formula xml:id="formula_46">L p (G, Î˜; D) = L(G, Î˜; D) -d log(m)/2<label>(</label></formula><p>Learning Causal Direction Given the learned skeleton, we orient each undirected edge using the k-path cumulants summation, according to Theorem 6. In detail, for each undirected edge (i, j) âˆˆ E, we calculate Î›k (X i â† X j ) and Î›k (X j â† X i ) for k = 1, . . . , K until one of them being zero or k reaches the upper limit K. We then orient the direction based on Theorem 6 (Lines 11-14).</p><p>To assess whether Î›k is equal to 0, a bootstrap hypothesis test is conducted <ref type="bibr" target="#b6">(Efron and Tibshirani 1994)</ref> while a threshold can be used for orientation once such testing fails. In detail, we calculate the statistic</p><formula xml:id="formula_47">Î›+ k from N resampling dataset D + âˆˆ {D + i |D + i=1,..,N âŠ‚ D, }.</formula><p>Then, we estimate the distribution P ( Î›+ k ) by kernel density estimator and centralize it to mean zero. Finally, the p-value of Î›k from the original dataset can be obtained. Complexity Analysis We provide the complexity of calculating likelihood in the worst cases-when graph is complete. Specifically, the complexity of</p><formula xml:id="formula_48">Algorithm 1: Causal Discovery for PB-SCM Input: Data set D, Max order K Output: Learning Causal Graph G 1 G â€² â† empty graph, L * p â† -âˆ; // Learning Causal Skeleton 2 while L * p (G * , Î˜ * ; D) &lt; L â€² p (G â€² , Î˜ â€² ; D) do 3 G * â† G â€² with largest L â€² p (G â€² , Î˜ â€² ; D) 4 for every G â€² âˆˆ V(G * ) do 5 Estimate Î˜ â€² and record score L â€² p (G â€² , Î˜ â€² ; D) 6 G â† Transfer G * to a skeleton // Learning Causal Direction 7 for each pair X i -X j âˆˆ G do 8 for k â† 1 âˆ¶ K do 9</formula><p>Obtain Î›k at each side by solving Eq. (E.1)</p><formula xml:id="formula_49">10</formula><p>Test whether Î›k equal to 0 for each side</p><formula xml:id="formula_50">11 if Î›k (X i â†X j ) â‰  0 âˆ§ Î›k (X j â†X i ) = 0 then 12 Orient "X i â†’ X j " in G 13 if Î›k (X i â†X j ) = 0 âˆ§ Î›k (X j â†X i ) â‰  0 then 14 Orient "X i â† X j " in G 15 Return G Eq. (13) is O(âˆ‘ m j=1 âˆ‘ |V | i=1 (|V |+x (j) i -i)! (|V |-i)!x (j) i !</formula><p>), by using FFT acceleration, this complexity can be reduced to</p><formula xml:id="formula_51">O(âˆ‘ m j=1 âˆ‘ |V | i=1 (|V | -i + 1) 2 x (j) i log(|V | -i + 1) 2 x (j) i )</formula><p>, where m is the sample size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiment Synthetic Experiments</head><p>In this section, we test the proposed PB-SCM on synthetic data. We design control experiments using synthetic data to test the sensitivity of sample size, number of vertices, and different indegree rate. The baseline methods include OCD <ref type="bibr" target="#b9">(Ni and Mallick 2022)</ref>, PC <ref type="bibr" target="#b14">(Spirtes, Glymour, and Scheines 2000)</ref>, GES <ref type="bibr" target="#b3">(Chickering 2002)</ref>. We further provide the results using the true skeleton as prior knowledge (PB-SCM-P) to demonstrate the effectiveness of learning causal direction.</p><p>In the sensitivity experiment, we synthesize data with fixed parameters while traversing the target parameter as shown in Fig. <ref type="figure" target="#fig_5">6</ref>. The default settings are as follows, sample size=30000, number of vertices=10, indegree rate=3.0, range of causal coefficient Î± i,j âˆˆ [0.1, 0.5], range of the mean of Poisson noise Âµ i âˆˆ [1.0, 3.0], the max order of cumulant K = 4. Each simulation is repeated 30 times.</p><p>As shown in Fig. <ref type="figure" target="#fig_5">6</ref>, we conduct three different control experiments for PB-SCM. Overall, our method outperforms all the baseline methods in all three control experiments.</p><p>In the control experiments of the indegree rate given in Fig. <ref type="figure" target="#fig_5">6</ref>(a), as the indegree rate controls the sparse of causal structure, the higher the indegree rate, the less sparse in causal structure leading to a decrease of performance of the baseline methods. In contrast, PB-SCM keeps giving the best results in all indegree rates. The reason is that our method benefits from the sparsity of the graph and the denser structure would result in more causal order being identified which verified the theoretical result in our work.</p><p>In the control experiments of the number of vertices given in Fig. <ref type="figure" target="#fig_5">6(b)</ref>. Our method outperforms all the baseline methods, showing a slight decrease as the number of nodes increases, yet still demonstrating reasonable performance. The reason might be that with an increasing number of vertices, the number of paths for both directions also increases, which requires a higher-order cumulant to obtain the asymmetry. However, estimating high-order cumulant is difficult and has a large variance which leads to a decrease in performance.</p><p>In the control experiments of sample size shown in Fig. <ref type="figure" target="#fig_5">6</ref>(c), as the sample size increases, our method's performance continues to improve and outperforms all the baseline methods. This suggests a sufficient sample size is beneficial for estimating accurate cumulant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Real World Experiments</head><p>We also test the proposed PB-SCM on a real-world football events dataset 1 , which contains 941,009 events from 9,074 football games across Europe. For this experiment, we focus on the causal relation in the following count of events: Foul, Yellow card, Second yellow card (abbreviated as 2nd Y. card), Red card, and Substitution. These events possess clear causal relationships according to the rules of the football game. Our goal is to identify the causal relationship from the observed count data while reasoning the possible number of paths between two events as a byproduct of our method.</p><p>In detail, we employ the bootstrap hypothesis test with 0.05 significance level to test whether Î›k is equal to zero. The result is shown in Table <ref type="table" target="#tab_1">1</ref>. The column of X â†’ Y shows the highest order of cumulants summation Î›k (X â† Y ) that is not equal to zero while the column of Y â†’ X shows the lowest order of cumulants summation that equals zero.</p><p>The results are given in Fig. <ref type="figure" target="#fig_6">7</ref>(b). Generally, PB-SCM successfully identifies five cause-effect pairs, except for Foul â†’ Red card. The possible reason might be attributed to the weak causal influence since only a few serious fouls will result in a red card. Interestingly, We find Î›2 (Foul â†’ Yellow card) â‰  0 , indicating two paths from F or its ancestor to Yellow card.</p><p>1 <ref type="url" target="https://www.kaggle.com/datasets/secareanualin/football-">https://www.kaggle.com/datasets/secareanualin/football-</ref>events Cause (X)</p><formula xml:id="formula_52">Effect (Y ) X â†’ Y Y â†’ X Foul Yellow card Î›k=2 â‰  0 Î›k=2 = 0 2nd Y. card Î›k=3 â‰  0 Î›k=2 = 0 Red card Î›k=1 = 0 Î›k=1 = 0 Yellow card 2nd Y. card Î›k=3 â‰  0 Î›k=2 = 0 Substitution Î›k=2 â‰  0 Î›k=2 = 0 2nd Y. card Red card Î›k=2 â‰  0 Î›k=2 = 0 Table 1:</formula><p>The result of real-world dataset experiment. This suggests a hidden confounder between Foul and Yellow card, possibly related to the football team's style which also coincides with other path findings. Moreover, the causal direction between Yellow card and Substitution is identified suggesting a hidden confounder or indirect relation exists. This result suggests the effectiveness of our method when dealing with complex real-world scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>In this work, we study the identification of the Poisson branching structural causal model using high-order cumulant. We establish a link between cumulants and paths in the causal graph under PB-SCM, showing that cumulants encompass information about the number of paths between two vertices, which is retrievable. By leveraging this link, we propose the identifiability of the causal order of PB-SCM and its graphical implication. With the identifiability result, we propose a causal structure learning algorithm for PB-SCM consisting of learning causal skeleton and learning causal direction. Our theoretical results and the practical algorithm will hopefully further inspire a series of future methods to deal with count data and move the research of causal discovery further toward achieving real-world impacts in different respects.</p><p>Supplementary Material of "Causal Discovery from Poisson Branching Structural Causal Model Using High-Order Cumulant with Path Analysis"</p><p>A Proof of Theorem 1</p><p>Theorem 1 (Reducibility). Given a Poisson random variable Ïµ and n distinct sequences of coefficients A 1 , ..., A n , we have</p><formula xml:id="formula_53">Îº(A 1 â€¢ Ïµ, ..., A 1 â€¢ Ïµ k 1 times , ..., A n â€¢ Ïµ, ..., A n â€¢ Ïµ k n times ) = Îº(A 1 â€¢ Ïµ, . . . , A n â€¢ Ïµ) (A.1)</formula><p>where each A i â€¢ Ïµ repeats k i times in the original cumulant and only contains one time in the reduced cumulant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Outline of Proof</head><p>First of all, we introduce the moment-generating function (MGF) and cumulant-generating function (CGF).</p><p>Definition 6 (Moment-generating function). For X = [X 1 , ..., X n ] T , an n-dimensional random vector, the moment-generating function of X is given by:</p><formula xml:id="formula_54">M X (t) âˆ¶= E [e t T X ] = E [e t 1 X 1 +t 2 X 2 +â‹¯+t n X n ] (A.2)</formula><p>where t = [t 1 , ..., t n ] is a fixed vector.</p><p>Definition 7 (Cumulant-generating function). For X = [X 1 , ..., X n ] T , an n-dimensional random vector, the cumulantgenerating function of X is given by:</p><formula xml:id="formula_55">K X (t) = ln M X (t) (A.3)</formula><p>where M X (t) is the moment-generating function of X.</p><p>With CGF, we can calculate the joint cumulant of a given random vector X by taking deviate of CGF:</p><formula xml:id="formula_56">Îº(X 1 , X 2 , ..., X n ) = âˆ‚ n K X (t 1 , t 2 ..., t n ) âˆ‚t 1 âˆ‚t 2 â‹¯âˆ‚t n t 1 =0,...,t n =0</formula><p>(A.4) Furthermore, if each X i in the random vector X repeated k i times, then we only need to take k i times of derivatives of the CGF with respect to the corresponding t i , i.e., Îº(X 1 , ..., X 1</p><formula xml:id="formula_57">k 1 times , X 2 , ..., X 2 k 2 times , ..., X n , ..., X n k n times ) = âˆ‚ k 1 +k 2 +â‹¯+k n K X (t 1 , t 2 ..., t n ) âˆ‚t k 1 1 âˆ‚t k 2 2 â‹¯âˆ‚t k n n t 1 =0,...,t n =0 (A.5)</formula><p>Therefore, Theorem 1 is equivalent to show the following equality hold:</p><formula xml:id="formula_58">âˆ‚ n K X (t 1 , t 2 ..., t n ) âˆ‚t 1 âˆ‚t 2 â‹¯âˆ‚t n t 1 =0,...,t n =0 = âˆ‚ k 1 +k 2 +â‹¯+k n K X (t 1 , t 2 ..., t n ) âˆ‚t k 1 1 âˆ‚t k 2 2 â‹¯âˆ‚t k n n t 1 =0,...,t n =0 . (A.6)</formula><p>To do show, we will prove that the</p><formula xml:id="formula_59">âˆ‚ n K X (t 1 ,t 2 ...,t n ) âˆ‚t 1 âˆ‚t 2 â‹¯âˆ‚t n</formula><p>has the form of exponential function, i.e.</p><formula xml:id="formula_60">âˆ‚ n K X (t 1 , t 2 ..., t n ) âˆ‚t 1 âˆ‚t 2 â‹¯âˆ‚t n = Î²e t 1 +t 2 +â‹¯t n , (A.7)</formula><p>which is a function that remains unchanged when taking derivatives with respect to any t i and thus the Eq. (A.6) holds. Following this outline, consider a random vector R</p><formula xml:id="formula_61">= [A 1 â€¢ Ïµ, A 2 â€¢ Ïµ, . . . , A n â€¢ Ïµ] T , A i â‰  A j</formula><p>, where Ïµ represents the Poisson noise component of a vertex X in graph G, and A i is a sequence of path coefficients corresponding to a direct path from X to one of its descendant vertices. Then according to the definition of MGF, we have:</p><formula xml:id="formula_62">M R (t) = E [e t T R ] = E [e t 1 Ã—A 1 â€¢Ïµ+â‹¯+t n Ã—A n â€¢Ïµ ] (A.8)</formula><p>where t = [t 1 , t 2 , ..., t n ] T is a fixed vector.</p><p>Following the outline, we first provide an intuition of proof through a specific case that each vertex are conditional independce by the root vertex Ïµ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proof of Specific Case</head><p>Given a Poisson random variable Ïµ âˆ¼ P ois(Âµ) and n distinct sequences of coefficients A 1 , ..., A n , in which</p><formula xml:id="formula_63">A i = (a (i) k ) |A i | k=1 where a (i) k is the k-th coefficients of A i .</formula><p>Assume there exist no k = 1, 2, ... min(|A i |, |A j |) between any two A i and A j such that (a</p><formula xml:id="formula_64">(i) l ) k l=1 = (a (j) l ) k l=1</formula><p>, which means that there exist no two paths P i and P j sharing the same part from the source point.</p><p>We consider the random vector R = (A 1 â€¢ Ïµ, A 2 â€¢ Ïµ, ..., A n â€¢ Ïµ), where each random variable A i â€¢ X appears uniquely. The moment generating function (MGF) of R is:</p><formula xml:id="formula_65">M R (t) = E [e t 1 Ã—A 1 â€¢Ïµ+â‹¯+t n Ã—A n â€¢Ïµ ] . (A.9)</formula><p>According to the law of total expectation, we have:</p><formula xml:id="formula_66">M R (t) = E [E [e t 1 Ã—A 1 â€¢Ïµ+â‹¯+t n Ã—A n â€¢Ïµ |Ïµ]] = E [ n âˆ i=1 E [e t i Ã—A i â€¢Ïµ |Ïµ]] , (A.10) since A i â€¢ Ïµ|Ïµ âŠ¥ âŠ¥ A j â€¢ Ïµ|Ïµ for all i â‰  j.</formula><p>Next, according to the property of thinning operation, we have </p><formula xml:id="formula_67">A i â€¢ Ïµ|Ïµ d = Binorm (n = Ïµ, p = âˆ |A i | j=1 a (i) j ),</formula><formula xml:id="formula_68">E [exp(t i Ã— A i â€¢ Ïµ) Ïµ] = â› âœ â 1 - |A i | âˆ j=1 a (i) j + |A i | âˆ j=1 a (i) j e t i â âŸ â  Ïµ . (A.11)</formula><p>Substituting equation (A.11) into equation (A.10), we have:</p><formula xml:id="formula_69">M R (t) = E â¡ â¢ â¢ â¢ â¢ â¢ â¢ â¢ â£ n âˆ i=1 â› âœ â 1 - |A i | âˆ j=1 a (i) j + |A i | âˆ j=1 a (i) j e t i â âŸ â  Ïµ â¤ â¥ â¥ â¥ â¥ â¥ â¥ â¥ â¦ = +âˆ âˆ‘ k=0 â¡ â¢ â¢ â¢ â¢ â¢ â¢ â¢ â¢ â£ P (Ïµ = k) n âˆ i=1 â› âœ â 1 - |A i | âˆ j=1 a (i) j + |A i | âˆ j=1 a (i) j e t i â âŸ â  k â¤ â¥ â¥ â¥ â¥ â¥ â¥ â¥ â¥ â¦ = +âˆ âˆ‘ k=0 â¡ â¢ â¢ â¢ â¢ â¢ â¢ â¢ â¢ â£ Âµ k e -Âµ k! n âˆ i=1 â› âœ â 1 - |A i | âˆ j=1 a (i) j + |A i | âˆ j=1 a (i) j e t i â âŸ â  k â¤ â¥ â¥ â¥ â¥ â¥ â¥ â¥ â¥ â¦ = exp(-Âµ) +âˆ âˆ‘ k=0 [Âµ âˆ n i=1 (1 -âˆ |A i | j=1 a (i) j + âˆ |A i | j=1 a (i) j e t i )] k k! .</formula><p>(A.12)</p><p>According to the power series expansion for the exponential function, i.e. exp x = âˆ‘ +âˆ x=0</p><p>x n n! , we have:</p><formula xml:id="formula_70">M R (t) = exp(-Âµ) exp â¡ â¢ â¢ â¢ â¢ â¢ â¢ â¢ â£ Âµ n âˆ i=1 â› âœ â 1 - |A i | âˆ j=1 a (i) j + |A i | âˆ j=1 a (i) j e t i â âŸ â  â¤ â¥ â¥ â¥ â¥ â¥ â¥ â¥ â¦ , (A.13)</formula><p>then the cumulant-generating function (CGF) of R is given by:</p><formula xml:id="formula_71">K R (t) = log M R (t) = Âµ n âˆ i=1 â› âœ â 1 - |A i | âˆ j=1 a (i) j + |A i | âˆ j=1 a (i) j e t i â âŸ â  -Âµ, (A.14)</formula><p>We obtain the cumulant by the partial derivatives of the cumulant generating function:</p><formula xml:id="formula_72">âˆ‚ n K R (t) âˆ‚t 1 âˆ‚t 2 â‹¯âˆ‚t n = Âµ n âˆ i=1 |A i | âˆ j=1 a (i) j e t i . (A.15) Since âˆ‚ n K R (t)</formula><p>âˆ‚t 1 âˆ‚t 2 â‹¯âˆ‚t n has the form of the exponential function, further partial derivatives of it will also retain the same form:</p><formula xml:id="formula_73">âˆ‚ k 1 +k 2 +â‹¯k n K R (t) âˆ‚t k 1 1 âˆ‚t k 2 2 â‹¯âˆ‚t k n n = âˆ‚ n K R (t) âˆ‚t 1 âˆ‚t 2 â‹¯âˆ‚t n = Âµ n âˆ i=1 |A i | âˆ j=1 a (i) j e t i . (A.16)</formula><p>Therefore, we have:</p><formula xml:id="formula_74">Îº(A 1 â€¢ Ïµ, A 2 â€¢ Ïµ, ..., A n â€¢ Ïµ) = âˆ‚ n K R (t) âˆ‚t 1 âˆ‚t 2 â‹¯âˆ‚t n t 1 =0,...,t n =0 = Âµ n âˆ i=1 |A i | âˆ j=1 a (i) j , Îº(A 1 â€¢ Ïµ, ..., A 1 â€¢ Ïµ k 1 times , ..., A n â€¢ Ïµ, ..., A n â€¢ Ïµ k n times ) = âˆ‚ k 1 +k 2 +â‹¯k n K R (t) âˆ‚t k 1 1 âˆ‚t k 2 2 â‹¯âˆ‚t k n n t 1 =0,...,t n =0 = Âµ n âˆ i=1 |A i | âˆ j=1 a (i) j , (A.17)</formula><p>which finishes the proof:</p><formula xml:id="formula_75">Îº(A 1 â€¢ Ïµ, ..., A 1 â€¢ Ïµ k 1 times , ..., A n â€¢ Ïµ, ..., A n â€¢ Ïµ k n times ) = Îº(A 1 â€¢ Ïµ, A 2 â€¢ Ïµ, ..., A n â€¢ Ïµ) (A.18)</formula><p>The above proof of specific case highlights that the way to calculate the MGF involves decomposing the expectation E [e</p><formula xml:id="formula_76">t T R ]</formula><p>using the law of total expectation to establish conditional independence. In the specific case, the conditional independence can be simply established by condition on the Ïµ since there is no common sub-sequence between any A i and A j . However, given Ïµ is not enough to build the conditional independence if A i and A j share the same sub-sequence, i.e. there exist a k such that</p><formula xml:id="formula_77">(A i ) 1âˆ¶k = (A j ) 1âˆ¶k .</formula><p>Before going into the formal proof, we provide an example to illustrate why simply conditioning on Ïµ cannot establish conditional independence. Subsequently, we further show how to establish conditional independence in the presence of a common sub-sequence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>An Example when Common sub-sequence Exists</head><p>Consider random variables:</p><formula xml:id="formula_78">A 1 â€¢ Ïµ = b â€¢ a â€¢ Ïµ, A 2 â€¢ Ïµ = c â€¢ a â€¢ Ïµ and A 3 â€¢ Ïµ = c â€¢ d â€¢ Ïµ, where A 1 = (a, b), A 2 = (a, c), A 3 = (d, c</formula><p>) and A 1 and A 2 has the common sub-sequence (a). The generating process can be represented through a tree structure, as shown in Fig. <ref type="figure" target="#fig_7">1</ref>. Now, if we condition on Ïµ, we obtain conditional independence</p><formula xml:id="formula_79">A 1 â€¢ Ïµ|Ïµ âŠ¥ âŠ¥ A 3 â€¢ Ïµ|Ïµ and A 2 â€¢ Ïµ|Ïµ âŠ¥ âŠ¥ A 3 â€¢ Ïµ|Ïµ, how- ever, A 1 â€¢ Ïµ|Ïµ / âŠ¥ âŠ¥ A 2</formula><p>â€¢ Ïµ|Ïµ. This is because both A 1 â€¢ Ïµ|Ïµ and A 2 â€¢ Ïµ|Ïµ are dependent on the binomial random variable a â€¢ Ïµ|Ïµ d = B(n = Ïµ, p = Î±) generated by the common sub-sequence (Î±). Such dependence occurs due to performing the thinning operation â€¢ on a random variable, resulting in the creation of a new random variable, distinct from the straightforward linear operations involving mere coefficient multiplication.</p><p>Therefore, to build conditional independence between A 1 â€¢ Ïµ|Ïµ and A 2 â€¢ Ïµ|Ïµ, we need to further condition on a â€¢ Ïµ|Ïµ. Such a process of establishing conditional independence step by step can be represented through the tree structure in Fig. <ref type="figure" target="#fig_7">1</ref>, as shown in Fig. <ref type="figure" target="#fig_8">2</ref>.</p><p>Specifically, the MGF of R <ref type="bibr">A.19)</ref> where â€¢dâ€¢Ïµ |Ïµ] correspond to the blue box and the green box in Fig. <ref type="figure" target="#fig_8">2</ref>, respectively.</p><formula xml:id="formula_80">= [A 1 â€¢ Ïµ, A 2 â€¢ Ïµ, A 3 â€¢ Ïµ] T is given by M R (t 1 , t 2 , t 3 ) = E [e t 1 Ã—A 1 â€¢Ïµ e t 2 Ã—A 2 â€¢Ïµ e t 3 Ã—A 3 â€¢Ïµ ] = E [e t 1 Ã—bâ€¢aâ€¢Ïµ e t 2 Ã—câ€¢aâ€¢Ïµ e t 3 Ã—câ€¢dâ€¢Ïµ ] = E Ïµ [E [e t 1 Ã—bâ€¢aâ€¢Ïµ e t 2 Ã—câ€¢aâ€¢Ïµ e t 3 Ã—câ€¢dâ€¢Ïµ |Ïµ]] = E Ïµ [E [e t 1 Ã—bâ€¢aâ€¢Ïµ e t 2 Ã—câ€¢aâ€¢Ïµ |Ïµ] E [e t 3 Ã—câ€¢dâ€¢Ïµ |Ïµ]] ,<label>(</label></formula><formula xml:id="formula_81">E [e t 1 bâ€¢aâ€¢Ïµ e t 2 câ€¢aâ€¢Ïµ |Ïµ] and E [e t 3 c</formula><p>The next step is to establish conditional independence and separate</p><formula xml:id="formula_82">E [e t 1 bâ€¢aâ€¢Ïµ e t 2 câ€¢aâ€¢Ïµ |Ïµ].</formula><p>By applying the law of total expectation to it, we obtain </p><formula xml:id="formula_83">E [e t 1 bâ€¢aâ€¢Ïµ e t 2 câ€¢aâ€¢Ïµ |Ïµ] = E [E [e t 1 bâ€¢aâ€¢Ïµ |a â€¢ Ïµ] E [e t 2 câ€¢aâ€¢Ïµ |a â€¢ Ïµ] |Ïµ] ,<label>(</label></formula><formula xml:id="formula_84">which is calculable since E [e t 1 bâ€¢aâ€¢Ïµ |a â€¢ Ïµ] is the MGF of Binorm(n = a â€¢ Ïµ, p = b) and the same for E [e t 2 câ€¢aâ€¢Ïµ |a â€¢ Ïµ].</formula><p>Motivated by the above example, when conditioning on a vertex in a tree, conditional independence is established among the random variables corresponding to each subtree of that vertex (if the subtree exists), enabling the separation of expectations.</p><p>Therefore, one can calculate the MGF by conditioning the random variables layer by layer according to the hierarchical structure of the tree in the generating process.</p><p>Here, to formalize the computation of the MGF, we introduce the following definition of a tree to model the generating process of the random vector R</p><formula xml:id="formula_85">= (A 1 â€¢ Ïµ, A 2 â€¢ Ïµ, ..., A n â€¢ Ïµ).</formula><p>Definition 8 (Tree representation of the generating process of random vector in PB-SCM). For a given random vector</p><formula xml:id="formula_86">R = [A 1 â€¢ Ïµ, A 2 â€¢ Ïµ, . . . , A n â€¢ Ïµ]</formula><p>T , âˆ€ i,j A i â‰  A j , the generating process of each random variable in R can be summarized by a tree T R . Let {T 0 , T 1 , T 2 , ...} denote all the vertices of T R , where T 0 = Ïµ is the root vertex of the tree and</p><formula xml:id="formula_87">T j = Î± iâ†’j â€¢ T i . Let L = {L 1 , L 2 , ..., L n } with index i = 1, 2, ..., n denote the leaf vertices in the tree, such that L i = A i â€¢ Ïµ Moreover, let A j i denotes the sub-sequence of A i that start from T j . For example, A i = {Î± 0â†’1 , Î± 1â†’2 , Î± 2â†’3 }, then A 0 i = A i and A 1 i = {Î± 1â†’2 , Î± 2â†’3 }. Let L(T i ) = {k|T k is leaf âˆ§ k âˆˆ L(T i )} denotes the set of leaf vertex in the tree.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The recursive relation of MGF</head><p>Based on this definition, several lemmas are introduced to establish the recursive relation for the MGF of</p><formula xml:id="formula_88">R = [A 1 â€¢ Ïµ, A 2 â€¢ Ïµ, . . . , A n â€¢ Ïµ] T .</formula><p>Lemma 1 (Start from root vertex T 0 ). For a given random vector R</p><formula xml:id="formula_89">= [A 1 â€¢ Ïµ, A 2 â€¢ Ïµ, . . . , A n â€¢ Ïµ]</formula><p>T and its tree representation</p><formula xml:id="formula_90">T R , A i â‰  A j , the MGF of R satisfy M R (t) = E [e âˆ‘ n i=1 t i Ã—A i â€¢Ïµ ] = E T 0 â¡ â¢ â¢ â¢ â¢ â¢ â¢ â¢ â£ âˆ jâˆˆCh(T 0 ) E [e âˆ‘ iâˆˆL(T j ) t i Ã—A j i â€¢T j |T 0 ] â¤ â¥ â¥ â¥ â¥ â¥ â¥ â¥ â¦ (A.21)</formula><p>Proof. The result is straightforward since T 0 the the root of the tree, then given the condition of T 0 each child of T 0 will be conditional independence:</p><formula xml:id="formula_91">M R (t) = E [e âˆ‘ n i=1 t i Ã—A i â€¢Ïµ ] = E [e âˆ‘ n i=1 t i Ã—A i â€¢T 0 ] = E â¡ â¢ â¢ â¢ â¢ â¢ â¢ â¢ â£ âˆ jâˆˆCh(T 0 ) E [e âˆ‘ iâˆˆL(T j ) t i Ã—A j i â€¢Î± 0â†’j â€¢T 0 |T 0 ] â¤ â¥ â¥ â¥ â¥ â¥ â¥ â¥ â¦ = E â¡ â¢ â¢ â¢ â¢ â¢ â¢ â¢ â£ âˆ jâˆˆCh(T 0 ) E [e âˆ‘ iâˆˆL(T j ) t i Ã—A j i â€¢T j |T 0 ] â¤ â¥ â¥ â¥ â¥ â¥ â¥ â¥ â¦ (A.22)</formula><p>By Lemma 1, MGF can be decomposed into separated conditional expectation in the first level of the tree. Next, we will investigate how such conditional expectation can be further decomposed.</p><p>Lemma 2 (From vertex T j to T k ). For a given random vector R</p><formula xml:id="formula_92">= [A 1 â€¢ Ïµ, A 2 â€¢ Ïµ, . . . , A n â€¢ Ïµ]</formula><p>T and its tree representation T R . Let T j be a node in a level that decomposed the conditional expectation into the product of its child. Then, one of such decomposed expectation of its child T k , can be further decomposed if</p><formula xml:id="formula_93">T k is not leaf, E [e âˆ‘ iâˆˆL(T k ) t i Ã—A k i â€¢T k |T j ] = E â¡ â¢ â¢ â¢ â¢ â¢ â¢ â¢ â£ âˆ lâˆˆCh(k) E [e âˆ‘ iâˆˆL(T l ) t i Ã—A l i â€¢T l |T k ] |T j â¤ â¥ â¥ â¥ â¥ â¥ â¥ â¥ â¦ (A.23) and if T k is leaf, E [e âˆ‘ iâˆˆL(T k ) t i Ã—A k i â€¢T k |T j ] = [M B(Î± jâ†’k ) (t L(T k ) )] T j . (A.24)</formula><p>Proof. If T k is not leaf, we can separate the expectation according its child:</p><formula xml:id="formula_94">E [e âˆ‘ iâˆˆL(T k ) t i Ã—A k i â€¢T k |T j ] = E [e âˆ‘ lâˆˆCh(T k ) âˆ‘ iâˆˆL(T l ) t i Ã—A l i â€¢Î± kâ†’l â€¢T k |T j ] = E â¡ â¢ â¢ â¢ â¢ â¢ â¢ â¢ â£ âˆ lâˆˆCh(k) e âˆ‘ iâˆˆL(T l ) t i Ã—A l i â€¢Î± kâ†’l â€¢T k |T j â¤ â¥ â¥ â¥ â¥ â¥ â¥ â¥ â¦ . (A.25)</formula><p>Then, according to the law of total expectation, we have</p><formula xml:id="formula_95">E â¡ â¢ â¢ â¢ â¢ â¢ â¢ â¢ â£ âˆ lâˆˆCh(k) e âˆ‘ iâˆˆL(T l ) t i Ã—A l i â€¢Î± kâ†’l â€¢T k |T j â¤ â¥ â¥ â¥ â¥ â¥ â¥ â¥ â¦ = E â¡ â¢ â¢ â¢ â¢ â¢ â¢ â¢ â£ âˆ lâˆˆCh(k) E [e âˆ‘ iâˆˆL(T l ) t i Ã—A l i â€¢Î± kâ†’l â€¢T k |T k ] |T j â¤ â¥ â¥ â¥ â¥ â¥ â¥ â¥ â¦ = E â¡ â¢ â¢ â¢ â¢ â¢ â¢ â¢ â£ âˆ lâˆˆCh(k) E [e âˆ‘ iâˆˆL(T l ) t i Ã—A l i â€¢T l |T k ] |T j â¤ â¥ â¥ â¥ â¥ â¥ â¥ â¥ â¦ (A.26) If T k is leaf, which means i = L(T k )</formula><p>is the exactly index of the leaf vertex and A k i is empty, and then we have:</p><formula xml:id="formula_96">E [e âˆ‘ iâˆˆL(T k ) t i Ã—A k i â€¢T k |T j ] = E [e t L(T k ) Ã—T k |T j ] = E [e t L(T k ) Ã—Î± jâ†’k â€¢T j |T j ] . (A.27)</formula><p>According to the definition of thin operator, we have</p><formula xml:id="formula_97">Î± jâ†’k â€¢ T j = âˆ‘ T j l=1 Î¾ (Î± jâ†’k ) l with Î¾ (Î± jâ†’k ) l i.i.d.</formula><p>âˆ¼ B(Î± jâ†’k ), where B(Î± jâ†’k ) is Bernoulli distribution with parameter Î± jâ†’k . Thus,</p><formula xml:id="formula_98">E [e t L(T k ) Ã—Î± jâ†’k â€¢T j |T j ] = E [e t L(T k ) Ã—âˆ‘ T j l=1 Î¾ (Î± jâ†’k ) l |T j ] = E [âˆ T j l=1 e t L(T k ) Ã—Î¾ (Î± jâ†’k ) l |T j ] = âˆ T j l=1 E [e t L(T k ) Ã—Î¾ (Î± jâ†’k ) l ] = E [e t L(T k ) Ã—Î¾ (Î± jâ†’k ) l ] T j . (A.28) Note that E [e t L(T k ) Ã—Î¾ (Î± jâ†’k ) l ] is the MGF of Î¾ (Î± jâ†’k ) l</formula><p>. In the end, we obtain:</p><formula xml:id="formula_99">E [e t L(T k ) Ã—Î¾ (Î± jâ†’k ) l ] T j = [M B(Î± jâ†’k ) (t L(T k ) )] T j . (A.29)</formula><p>To represent the recursive relation, we now introduce the probability-generating function (PGF):</p><p>Definition 9 (Probability-generating function). For X = [X 1 , ..., X n ] T , where each X i is a discrete random variable, the probability-generating function of X is given by: G</p><formula xml:id="formula_100">X (z) âˆ¶= E[z X 1 1 z X 2 2 â‹¯z X n n ], where z = [z 1 , ..., z n ]. Then, following lemma disclose the recursive relation of MGF. Lemma 3. For a given random vector R = [A 1 â€¢ Ïµ, A 2 â€¢ Ïµ, . . . , A n â€¢ Ïµ]</formula><p>T and its tree representation</p><formula xml:id="formula_101">T R . Let M j,k (t) âˆ¶= E [e âˆ‘ iâˆˆL(T k ) t i Ã—A k i â€¢T k |T j ] and Mj,k (t L(T k ) ) = [M j,k (t L(T k ) )] 1 T j , where t L(T k ) = {t i |i âˆˆ L(T k )}.</formula><p>The joint MGF can be expressed as follows:</p><formula xml:id="formula_102">M R (t) = G T 0 â› âœ â âˆ jâˆˆCh(T 0 ) M0,j (t L(T j ) ) â âŸ â  , (A.30)</formula><p>where</p><formula xml:id="formula_103">Mj,k (t L(T k ) ) = { G B(Î± jâ†’k ) (âˆ lâˆˆCh(k) Mk,l (t L(T k ) )) if T k is not leaf vertex M B(Î± jâ†’k ) (t L(T k ) ) otherwise . (A.31)</formula><p>Proof. First, by Lemma 2, we have the following recursive formula</p><formula xml:id="formula_104">M j,k (t L(T k ) ) = { E [âˆ lâˆˆCh(k) M k,l (t L(T l ) )|T j ] if T k is not leaf vertex [M B(Î± jâ†’k ) (t i )] T j otherwise (A.32) and thus M R (t) = E T 0 â¡ â¢ â¢ â¢ â¢ â¢ â¢ â¢ â£ âˆ jâˆˆCh(T 0 ) M 0,j (t L(T j ) ) â¤ â¥ â¥ â¥ â¥ â¥ â¥ â¥ â¦ . (A.33) Then, since Mj,k (t L(T k ) ) = [M j,k (t L(T k ) )]</formula><p>1 T j , based on the recursive formula in Eq. (A.32), we have</p><formula xml:id="formula_105">Mj,k (t L(T k ) ) = M j,k (t L(T k ) ) 1 T j = { (E [âˆ lâˆˆCh(k) Mk,l (t L(T l ) ) T k |T j ]) 1 T j if T k is not leaf vertex M B(Î± jâ†’k ) (t i ) otherwise = â§ âª âª âª â¨ âª âª âª â© (E [(âˆ lâˆˆCh(k) Mk,l (t L(T l ) )) Î± jâ†’k â€¢T j |T j ]) 1 T j if T k is not leaf vertex M B(Î± jâ†’k ) (t L(T k ) ) otherwise (A.34)</formula><p>Consider the expectation in Eq. (A.34) when T k is not leaf vertex. Since T j is conditioned such that Î± jâ†’k â€¢ T j follows the distribution Binorm(n = T j , p = Î± jâ†’k ), then by the probability generating function of Binomial distribution, we have <ref type="bibr">.35)</ref> where G B(Î± jâ†’k ) (â‹…) is the probability generating function of Bernoulli distribution according to the relation between Bernoulli and Binomial distribution. Substituting Eq. (A.35) into Eq. (A.34) we have</p><formula xml:id="formula_106">E â¡ â¢ â¢ â¢ â¢ â¢ â¢ â¢ â¢ â£ â› âœ â âˆ lâˆˆCh(k) Mk,l (t L(T l ) ) â âŸ â  Î± jâ†’k â€¢T j |T j â¤ â¥ â¥ â¥ â¥ â¥ â¥ â¥ â¥ â¦ = G B(Î± jâ†’k ) â› âœ â âˆ lâˆˆCh(k) Mk,l (t L(T l ) ) â âŸ â  T j . (<label>A</label></formula><formula xml:id="formula_107">Mj,k (t L(T k ) ) = { G B(Î± jâ†’k ) (âˆ lâˆˆCh(k) Mk,l (t L(T l ) )) if T k is not leaf vertex M B(Î± jâ†’k ) (t i ) otherwise . (A.36)</formula><p>As for the joint MGF, similarly, we have</p><formula xml:id="formula_108">M R (t) = E T 0 â¡ â¢ â¢ â¢ â¢ â¢ â¢ â¢ â£ âˆ jâˆˆCh(T 0 ) M 0,j (t L(T j ) ) â¤ â¥ â¥ â¥ â¥ â¥ â¥ â¥ â¦ = E T 0 â¡ â¢ â¢ â¢ â¢ â¢ â¢ â¢ â¢ â£ â› âœ â âˆ jâˆˆCh(T 0 ) M0,j (t L(T j ) ) â âŸ â  T 0 â¤ â¥ â¥ â¥ â¥ â¥ â¥ â¥ â¥ â¦ = G T 0 â› âœ â âˆ jâˆˆCh(T 0 ) M0,j (t L(T j ) ) â âŸ â  , (A.37)</formula><p>which finishes the proof.</p><p>After deriving the recursive relation, we now step into the formal proof of Theorem 1 following the proof outline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Formal Proof of Theorem 1</head><p>According to the Lemma 3, for a given random vector R</p><formula xml:id="formula_109">= [A 1 â€¢ Ïµ, A 2 â€¢ Ïµ, . . . , A n â€¢ Ïµ]</formula><p>T and its tree representation T R , the</p><formula xml:id="formula_110">MGF of it is M R (t) = G T 0 (âˆ jâˆˆCh(T 0 ) M0,j (t L(T j ) )</formula><p>), where T 0 = Ïµ âˆ¼ Pois(Âµ) is a Poisson random variable. The cumulant generating function of R is given by:</p><formula xml:id="formula_111">K R (t) = log M R (t) = log exp [Âµ (âˆ jâˆˆCh(T 0 ) M0,j (t L(T j ) ) -1)] = Âµ (âˆ jâˆˆCh(T 0 ) M0,j (t L(T j ) ) -1) . (A.38)</formula><p>Our goal is to show the derivative of K R (t) is of the exponential form:</p><formula xml:id="formula_112">âˆ‚ n K R (t) âˆ‚t 1 âˆ‚t 1 â‹¯âˆ‚t n = Î²e t 1 +t 2 +â‹¯+t n . (A.39)</formula><p>We start with</p><formula xml:id="formula_113">âˆ‚ n K R (t) âˆ‚t 1 âˆ‚t 1 â‹¯âˆ‚t n = âˆ‚ n âˆ‚t 1 âˆ‚t 1 â‹¯âˆ‚t n Âµ â› âœ â âˆ jâˆˆCh(T 0 ) M0,j (t L(T j ) ) -1 â âŸ â  = Âµ âˆ‚ n âˆ‚t 1 âˆ‚t 1 â‹¯âˆ‚t n âˆ jâˆˆCh(T 0 ) M0,j (t L(T j ) ), (A.40)</formula><p>where M0,j (t L(T j ) ) is a function involving only {t i |i âˆˆ L(T j )}, we then have:</p><formula xml:id="formula_114">Âµ âˆ‚ n âˆ‚t 1 âˆ‚t 1 â‹¯âˆ‚t n âˆ jâˆˆCh(T 0 ) M0,j (t L(T j ) ) = Âµ âˆ jâˆˆCh(T 0 ) âˆ‚ |L(T j )| âˆ iâˆˆL(T j ) âˆ‚t i M0,j (t L(T j ) ). (A.41)</formula><p>We then introduce the recursive representation of âˆ‚ |L(T j )| âˆ iâˆˆL(T j ) âˆ‚t i M0,j (t L(T j ) ).</p><p>Lemma 4. The higher order partial derivative of Mj,k (t L(T k ) ) can be given by:</p><formula xml:id="formula_115">âˆ‚ |L(T k )| Mj,k (t L(T k ) ) âˆ iâˆˆL(T k ) âˆ‚t i = â§ âª âª âª â¨ âª âª âª â© Î± jâ†’k âˆ lâˆˆCh(k) âˆ‚ |L(T l )| Mk,l (t L(T l ) ) âˆ t i âˆˆL(T l ) âˆ‚t i , if T k is not leaf vertex,</formula><p>Î± jâ†’k e t L(T k ) , otherwise.</p><p>(A.42)</p><p>Proof. When T k is not a leaf vertex, according to the Lemma 3, we have:</p><formula xml:id="formula_116">âˆ‚ |L(T k )| Mj,k (t L(T k ) ) âˆ iâˆˆL(T k ) âˆ‚t i = âˆ‚ |L(T k )| âˆ iâˆˆL(T k ) âˆ‚t i G B(Î± jâ†’k ) â› âœ â âˆ lâˆˆCh(k) Mk,l (t L(T l ) ) â âŸ â  = âˆ‚ |L(T k )| âˆ iâˆˆL(T k ) âˆ‚t i â› âœ â 1 -Î± jâ†’k + Î± jâ†’k âˆ lâˆˆCh(k) Mk,l (t L(T l ) ) â âŸ â  = Î± jâ†’k âˆ‚ |L(T k )| âˆ iâˆˆL(T k ) âˆ‚t i âˆ lâˆˆCh(k)</formula><p>Mk,l (t L(T l ) ).</p><formula xml:id="formula_117">(A.43) Since Mk,l (t L(T l )</formula><p>) is a function involving only {t i |i âˆˆ L(T l )}, we have:</p><formula xml:id="formula_118">Î± jâ†’k âˆ‚ |L(T k )| âˆ iâˆˆL(T k ) âˆ‚t i âˆ lâˆˆCh(k) Mk,l (t L(T l ) ) = Î± jâ†’k âˆ lâˆˆCh(k) âˆ‚ |L(T l )| Mk,l (t L(T l ) ) âˆ iâˆˆL(T k ) âˆ‚t i . (A.44)</formula><p>Otherwise, when T k is a leaf vertex, we have:</p><formula xml:id="formula_119">âˆ‚ |L(T k )| Mj,k (t L(T k ) ) âˆ iâˆˆL(T k ) âˆ‚t i = âˆ‚M B(Î± jâ†’k ) (t L(T k ) ) âˆ‚t L(T k ) = âˆ‚ (1 -Î± jâ†’k + Î± jâ†’k e t L(T k ) ) âˆ‚t L(T k ) = Î± jâ†’k e t L(T k ) , (A.45)</formula><p>which finishes the proof.</p><p>According to Lemma 4, as the recursion terminates with an exponential function upon reaching the leaf vertex, we can deduce that the expansion of</p><formula xml:id="formula_120">âˆ‚ n K R (t) âˆ‚t 1 âˆ‚t 1 â‹¯âˆ‚t n</formula><p>results in the product of e t i for all i âˆˆ [n], along with a series of corresponding path coefficients. Moreover, our focus does not lie in the specific form of these coefficients, and thus we denote the coefficient as Î².</p><p>We conclude:</p><formula xml:id="formula_121">âˆ‚ n K R (t) âˆ‚t 1 âˆ‚t 1 â‹¯âˆ‚t n = Î²e t 1 +t 2 +â‹¯+t n . (A.46)</formula><p>Finally, we obtain:</p><formula xml:id="formula_122">Îº(A 1 â€¢ Ïµ, A 2 â€¢ Ïµ, ..., A n â€¢ Ïµ) = âˆ‚ n K R (t) âˆ‚t 1 âˆ‚t 2 â‹¯âˆ‚t n t 1 =0,...,t n =0 = Î², Îº(A 1 â€¢ Ïµ, ..., A 1 â€¢ Ïµ k 1 times , ..., A n â€¢ Ïµ, ..., A n â€¢ Ïµ k n times ) = âˆ‚ k 1 +k 2 +â‹¯k n K R (t) âˆ‚t k 1 1 âˆ‚t k 2 2 â‹¯âˆ‚t k n n t 1 =0,...,t n =0 = Î², (A.47)</formula><p>which finishes the proof:</p><formula xml:id="formula_123">Îº(A 1 â€¢ Ïµ, ..., A 1 â€¢ Ïµ k 1 times , ..., A n â€¢ Ïµ, ..., A n â€¢ Ïµ k n times ) = Îº(A 1 â€¢ Ïµ, A 2 â€¢ Ïµ, ..., A n â€¢ Ïµ). (A.48) B Proof of Remark 1</formula><p>Remark 1. For any two variables causal graph, the causal direction of PB-SCM is not identifiable and a distributed equivalent reversed model exists.</p><p>Proof. We prove by the equality of PGFs of both directions. Given a two variables causal graph</p><formula xml:id="formula_124">X Î± -â†’ Y , where X = Ïµ X , Y = Î± â€¢ X + Ïµ Y , Ïµ i âˆ¼ Pois(Âµ i ),</formula><p>and we denote the reverse model by Y Î± -â†’ X, where Y = ÎµY , X = Î± â€¢ Y + Îµ X . We now show the solution of Î±, Îµ X and ÎµY .</p><p>For the causal direction, the PGF is given by:</p><formula xml:id="formula_125">G X,Y (z 1 , s 2 ) = E [z X 1 z Î±â€¢X+Ïµ Y 2 ] = E [z Ïµ X 1 z Î±â€¢Ïµ X 2 ] E [z Ïµ Y 2 ] = G Ïµ X (z 1 G B(Î±) (z 2 ))G Ïµ Y (z 2 ) = G Ïµ X (z 1 (1 -Î± + Î±z 2 ))G Ïµ Y (z 2 ) = e Âµ X (z 1 (1-Î±+Î±z 2 )-1) e Âµ Y (z 2 -1) . (B.1)</formula><p>For the reverse direction, the PGF is given by: ÄœX</p><formula xml:id="formula_126">,Y (z 1 , z 2 ) = E [z Î±â€¢Y +Îµ X 1 z Y 2 ] = E [z Î±â€¢Y 1 z Y 2 ] E [z ÎµX 1 ] = G Y (z 2 G B( Î±) (z 1 ))G ÎµX (z 1 ) = G Y (z 2 (1 -Î± + Î±z 1 ))G ÎµX (z 1 ) = e E[Y ](z 2 (1-Î±+ Î±z 1 )-1) e Î¼x (z 1 -1) . (B.2) If these two models are equivalent, we have G X,Y (z 1 , z 2 ) = ÄœX,Y (z 1 , z 2 ), i.e. Âµ X (z 1 (1 -Î± + Î±z 2 ) -1) + Âµ Y (z 2 -1) = E[Y ](z 2 (1 -Î± + Î±z 1 ) -1) + Î¼X (z 1 -1). (B.3)</formula><p>As Y is a root vertex in the reverse model, we have Ïµ Y âˆ¼ Pois(E[Y ]) = Pois(Î±Âµ X + Âµ Y ). Then we have:</p><formula xml:id="formula_127">Âµ X (z 1 (1 -Î± + Î±z 2 ) -1) + Âµ Y (z 2 -1) = (Î±Âµ x + Âµ y )(z 2 (1 -Î± + Î±z 1 ) -1) + Î¼X (z 1 -1). (B.4)</formula><p>Expanding the expression and simplifying, we obtain</p><formula xml:id="formula_128">Î±Âµ X z 1 z 2 + Âµ X (1 -Î±)z 1 + Âµ Y z 2 -Âµ X -Âµ Y = (Î±Âµ X + Âµ Y )Î±z 1 z 2 + Î¼X z 1 + (Î±Âµ X + Âµ Y )(1 -Î±)z 2 -(Î±Âµ X + Âµ Y ) -Î¼X (B.5)</formula><p>To ensure the equality holds, we equate the coefficients, resulting in following system of equations:</p><formula xml:id="formula_129">Î±Âµ X = (Î±Âµ X + Âµ Y )Î± Âµ X (1 -Î±) = Î¼X Âµ Y = (Î±Âµ X + Âµ Y )(1 -Î±) Âµ X + Âµ Y = (Î±Âµ X + Âµ Y ) + Î¼X , (B.6)</formula><p>where the solution of it is Î± = Î±Âµ X /(Î±Âµ X + Âµ Y ), Î¼X = Âµ X (1 -Î±). This completes the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Proof of Theorem 2</head><p>Theorem 2. For any two vertex i and j where i is root vertex, i.e., vertex i has empty parent set, the 2D slice of joint cumulant C (n) i,j satisfies:</p><formula xml:id="formula_130">C (n) i,j = âˆ‘ n-1 k=1 âˆ‘ m 1 +â‹¯+m k =n-1 m l &gt;0 ( n -1 m 1 m 2 â‹¯m k )Î› iâ†j k (1â€¢X i â† X j ). (C.1)</formula><p>where</p><formula xml:id="formula_131">( n-1 m 1 m 2 â‹¯m k ) = (n-1)! m 1 !m 2 !â‹¯m k ! is the multinomial coefficients.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proof</head><p>For any two vertex i and j, where i is root vertex, let P iâ†j = {P i,j , we have:</p><formula xml:id="formula_132">C (n) i,j = Îº â› âœ âœ â X i , X j , . . . , X j n-1times â âŸ âŸ â  = Îº â› âœ âœ â Ïµ i , X j , . . . , X j n-1times â âŸ âŸ â  (C.2)</formula><p>then we expand X j according to the structural equation of X j :</p><formula xml:id="formula_133">C (n) i,j = Îº(Ïµ i , A iâ†j 1 â€¢ Ïµ i + â‹¯ + A iâ†j |P iâ†j | â€¢ Ïµ i |P iâ†j |times , . . . , A iâ†j 1 â€¢ Ïµ i + â‹¯ + A iâ†j |P iâ†j | â€¢ Ïµ i n-1times</formula><p>).</p><p>(C.3)</p><p>By applying the multilinearity of cumulant, we obtain the following decomposition: To establish the connection between the cumulant and the k-path summation, we first recall the definition of</p><formula xml:id="formula_134">C (n) i,j = |P iâ†j | âˆ‘ l 1 =1 â‹¯ |P iâ†j | âˆ‘ l n-1 =1 Îº (Ïµ i , A iâ†j l 1 â€¢ Ïµ i , A iâ†j l 2 â€¢ Ïµ i , . . . , A iâ†j l n-1 â€¢ Ïµ i ) ,<label>(</label></formula><formula xml:id="formula_135">Î› iâ†j k (1 â€¢ X i â† X j ): Î› iâ†j k (1 â€¢ Ïµ i â† X j ) = âˆ‘ 1â‰¤l 1 &lt;l 2 &lt;...&lt;l k â‰¤|P iâ†j | Îº(1 â€¢ Ïµ i , A iâ†j l 1 â€¢ Ïµ i , ..., A iâ†j l k â€¢ Ïµ i ), (C.5)</formula><p>which is the sum of cumulants and each cumulant involve k distinct A iâ†j l . Note that due to the reducibility, the C (n) i,j can be reduced to several distinct cumulants. In particular, the k-path summation contains all the distinct cumulants of</p><formula xml:id="formula_136">C (n) i,j which involve k distinct A iâ†j l</formula><p>. Therefore, the connection between Eq. (C.5) and Eq.</p><p>(C.4) can be formulated as how many numbers for each distinct A iâ†j l occurs after the reducing. For each distinct cumulant in Î› iâ†j k (1 â€¢ Ïµ i â† X j ), the number of occurrences is the same because each cumulant is constructed by k different paths sharing the same property in term of number.</p><p>Thus, we only need to count the number of each distinct cumulant for some specific k paths. Without loss of generality, consider the cumulant with k path information: Îº(1</p><formula xml:id="formula_137">â€¢ Ïµ i , A iâ†j 1 â€¢ Ïµ i , ..., A iâ†j k â€¢ Ïµ i ).</formula><p>Since before the reducing step, there are n -1 positions for each A, and the count can be formulated by counting the number of ways to place k distinguishable A into n -1 indistinguishable boxes with replacement such that each ball must appear at least once. Such a number can be calculated by âˆ‘</p><formula xml:id="formula_138">m 1 +â‹¯+m k =n-1 m l &gt;0 ( n-1 m 1 m 2 â‹¯m k ), which is the coefficient of Î› iâ†j k (1 â€¢ Ïµ i â† X j )</formula><p>. By combining each order of k, we can obtain the close-form solution of C</p><p>(n) i,j in Eq. (C.3). This completes the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Proof of Theorem 3 and Theorem 4</head><p>Theorem 3 (Identifiability for root vertex). For any vertex i and j, where i is the root vertex in graph G, if C</p><p>(3)</p><formula xml:id="formula_139">i,j -C (2) i,j â‰  0, then C (3) j,i -C (2)</formula><p>j,i = 0 and X i is the ancestor of X j . Proof. For the reverse direction, since X i is a root vertex, we have:</p><formula xml:id="formula_140">C (2) j,i = Îº(X j , X i ) = Îº â› âœ âœ â |P iâ†j | âˆ‘ l=1 A iâ†j l â€¢ Ïµ i , Ïµ i â âŸ âŸ â  , C<label>(3)</label></formula><formula xml:id="formula_141">j,i = Îº(X j , X i , X i ) = Îº â› âœ âœ â |P iâ†j | âˆ‘ l=1 A iâ†j l â€¢ Ïµ i , Ïµ i , Ïµ i â âŸ âŸ â  . (D.1)</formula><p>Based on Theorem 1, Eq. (D.1) can be reduced as follow:</p><formula xml:id="formula_142">C (3) j,i = Îº â› âœ âœ â |P iâ†j | âˆ‘ l=1 A iâ†j l â€¢ Ïµ i , Ïµ i , Ïµ i â âŸ âŸ â  = Îº â› âœ âœ â |P iâ†j | âˆ‘ l=1 A iâ†j l â€¢ Ïµ i , Ïµ i â âŸ âŸ â  = C (2) j,i (D.2) thus C (3) j,i -C (2)</formula><p>j,i = 0. For the causal direction, based on Theorem 2, we have:</p><formula xml:id="formula_143">C (2) i,j = Î› iâ†j 1 (X i â† X j ) C (3) i,j = Î› iâ†j 1 (X i â† X j ) + 2Î› iâ†j 2 (X i â† X j ) (D.3) Then we have C (3) i,j -C (2)</formula><p>i,j = 2Î› iâ†j 2 (X i â† X j ) â‰  0 which means that there are more than one path from i to j, i.e. |P iâ†j | â‰¥ 2. Therefore, X i is the ancestor of X j . This completes the proof.</p><p>Theorem 4 (Graphical Implication of Identifiability for Root Vertex). For a pair of vertices i and j in graph G, if vertex i is a root vertex and exists at least two directed paths from i to j, i.e., |P iâ†j | â‰¥ 2 then the causal order between i and j is identifiable.</p><p>Proof. Suppose the causal order between i and j can not be identified by Theorem 3. Then there must be in the following cases: (i) C</p><p>(3)</p><formula xml:id="formula_144">i,j -C (2) i,j = 0; (ii) C (3) j,i -C (2) j,i â‰  0. For (i), since Î› iâ†j 2 (1 â€¢ Ïµ i â† X j ) = C (3) i,j -C (2)</formula><p>i,j = 0 indicating that there exists zero or one path from i to j which contradict to the fact that |P iâ†j | â‰¥ 2.</p><p>For (ii), C</p><formula xml:id="formula_145">j,i -C (2) j,i â‰  0 is contradicted to Theorem 3 that C (3) j,i -C (2) j,i = 0 when C (3) i,j -C<label>(3)</label></formula><p>i,j â‰  0. By combining these two cases, we complete the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Proof of Theorem 5</head><p>Theorem 5. For any two vertex i and j, the 2D slice of joint cumulant C (n) i,j satisfies:</p><formula xml:id="formula_147">C (n) i,j = âˆ‘ n-1 k=1 âˆ‘ m 1 +â‹¯+m k =n-1 m l &gt;0 ( n -1 m 1 m 2 â‹¯m k ) Î›k (X i â† X j ). (E.1)</formula><p>where</p><formula xml:id="formula_148">( n-1 m 1 m 2 â‹¯m k ) = (n-1)! m 1 !m 2 !â‹¯m k ! is the multinomial coefficients.</formula><p>Proof. Since i is not a root vertex, the structural equation of X i is</p><formula xml:id="formula_149">X i = âˆ‘ mâˆˆAn(i) âˆ‘ |P mâ†i | h=1 A mâ†i h â€¢ Ïµ m + Ïµ i , where A mâ†i h</formula><p>is the sequence of coefficients corresponding to the h-th path from m, one of the ancestor of i ,to i.</p><p>According the structural equation of X i , we have:</p><formula xml:id="formula_150">C (n) i,j = Îº â› âœ âœ â X i , X j , . . . , X j n-1times â âŸ âŸ â  = Îº â› âœ âœ â âˆ‘ mâˆˆAn(i) |P mâ†i | âˆ‘ h=1 A mâ†i h â€¢ Ïµ m + Ïµ i , X j , . . . , X j n-1times â âŸ âŸ â  (E.2)</formula><p>then we decompose C</p><p>(n) i,j according to the structural equation of X i , we have:</p><formula xml:id="formula_151">C (n) i,j = âˆ‘ mâˆˆAn(i) |P mâ†i | âˆ‘ h=1 Îº â› âœ âœ â A mâ†i h â€¢ Ïµ m , X j , . . . , X j n-1times â âŸ âŸ â  + Îº â› âœ âœ â Ïµ i , X j , . . . , X j n-1times â âŸ âŸ â  = âˆ‘ mâˆˆAn(i,j) |P mâ†i | âˆ‘ h=1 Îº â› âœ âœ â A mâ†i h â€¢ Ïµ m , X j , . . . , X j n-1times â âŸ âŸ â  + Îº â› âœ âœ â Ïµ i , X j , . . . , X j n-1times â âŸ âŸ â  (E.3)</formula><p>As those cumulants involve independent noise components equal to zero, m is the common ancestor of i and j in the Eq. (E.3). Now, we consider the decomposition of cumulant: (i) Îº (A mâ†i h</p><p>â€¢ Ïµ m , X j , . . . , X j ) and (ii) Îº(Ïµ i , X j , . . . , X j ). For (i), the term Îº (A mâ†i h</p><p>â€¢ Ïµ m , X j , . . . , X j ) has the similar form as the cumulant we proved in Theorem 2, i.e.,</p><formula xml:id="formula_152">Îº (A mâ†i h â€¢ Ïµ m , X j , . . . , X j ) = |P mâ†j | âˆ‘ l 1 =1 â‹¯ |P mâ†j | âˆ‘ l n-1 =1 Îº (A mâ†i h â€¢ Ïµ m , A mâ†j l 1 â€¢ Ïµ m , A mâ†j l 2 â€¢ Ïµ m , . . . , A mâ†j l n-1 â€¢ Ïµ m ) (E.4)</formula><p>where the only difference is the first noise component, which is A mâ†i h</p><p>â€¢ Ïµ m instead of Ïµ m . This variation does not impact the result in Theorem 2 leading to</p><formula xml:id="formula_153">Îº (A mâ†i h â€¢ Ïµ m , X j , . . . , X j ) = n-1 âˆ‘ k=1 âˆ‘ m 1 +â‹¯+m k =n-1 m l &gt;0 ( n -1 m 1 m 2 â‹¯m k )Î› mâ†j k (A mâ†i h â€¢ Ïµ m â† X j ) (E.5)</formula><p>For (ii), Îº(Ïµ i , X j , . . . , X j ) has the same form as the cumulant we proved in Theorem 2, we have:</p><formula xml:id="formula_154">Îº(Ïµ i , X j , . . . , X j ) = âˆ‘ n-1 k=1 âˆ‘ m 1 +â‹¯+m k =n-1 m l &gt;0 ( n -1 m 1 m 2 â‹¯m k )Î› iâ†j k (1 â€¢ X i â† X j ). (E.6)</formula><p>Substituting Eq. (E.5) and Eq. (E.6) into Eq. (E.3), we have</p><formula xml:id="formula_155">C (n) i,j = âˆ‘ mâˆˆAn(i,j) |P mâ†i | âˆ‘ h=1 n-1 âˆ‘ k=1 âˆ‘ m 1 +â‹¯+m k =n-1 m l &gt;0 ( n -1 m 1 m 2 â‹¯m k )Î› mâ†j k (A mâ†i h â€¢ Ïµ m â† X j ) + âˆ‘ n-1 k=1 âˆ‘ m 1 +â‹¯+m k =n-1 m l &gt;0 ( n -1 m 1 m 2 â‹¯m k )Î› iâ†j k (1 â€¢ X i â† X j ).</formula><p>(E.7)</p><p>By rewriting Eq. (E.7), we have</p><formula xml:id="formula_156">C (n) i,j = n-1 âˆ‘ k=1 âˆ‘ m 1 +â‹¯+m k =n-1 m l &gt;0 ( n -1 m 1 m 2 â‹¯m k ) â¡ â¢ â¢ â¢ â¢ â¢ â¢ â¢ â¢ â£ âˆ‘ mâˆˆAn(i,j) |P mâ†i | âˆ‘ h=1 Î› mâ†j k (A mâ†i h â€¢ Ïµ m â† X j ) + Î› iâ†j k (1 â€¢ X i â† X j ) â¤ â¥ â¥ â¥ â¥ â¥ â¥ â¥ â¥ â¦ = n-1 âˆ‘ k=1 âˆ‘ m 1 +â‹¯+m k =n-1 m l &gt;0 ( n -1 m 1 m 2 â‹¯m k ) Î›k (X i â† X j ). (E.8)</formula><p>This completes the proof.</p><p>F Proof of Theorem 6 and Theorem 7</p><p>Theorem 6 (Identification of PB-SCM). If there exist k âˆˆ Z + such that Î›k (X i â† X j ) â‰  0 and Î›k (X j â† X i ) = 0 for any two adjacency vertex i and j, then X i is the ancestor of X j Proof. For the case that X i is a root vertex. Suppose X i is not the ancestor of X j , then X i and X j are independent since X i is the root vertex and X j is not the ancestor of X i . In this case, the Î›k (X i â† X j ) = 0 for each k since X i and X j are independent.</p><p>For the case that X i is not a root vertex. Suppose X i is not the ancestor of X j , then there must exist k path from the common ancestor to X i since Î›k (X i â† X j ) â‰  0. However, this contradicts the condition Î›k (X j â† X i ) = 0 as it indicates that there not exist k paths from the common ancestor to X i . Hence, we conclude that X i is the ancestor of X j .</p><p>Theorem 7 (Graphical Implication of Identifiability). For a pair of vertices i and j, if i is an ancestor of j. The causal order of i, j is identifiable by Theorem 6, if (i) vertex i is a root vertex and |P iâ†j | â‰¥ 2; or (ii) there exists a common ancestor</p><formula xml:id="formula_157">k âˆˆ arg max l {|P lâ†i ||l âˆˆ An(i, j)} has a directed path from k to j without passing i in G. Proof. (i) If vertex i is a root vertex and |P iâ†j | â‰¥ 2, we have Î›2 (X i â† X j ) = Î› iâ†j 2 (1 â€¢ Ïµ i â† X j ) â‰  0 Î›2 (X j â† X i ) = |P jâ†i | âˆ‘ h=1 Î› jâ†i 2 (A jâ†i h â€¢ Ïµ i â† X i ) = 0 (F.1)</formula><p>Since there are â©¾ 2 paths from i to j and no two paths from i to i (from noise component of i to i), we have Î›2 (X i â† X j ) â‰  0 and Î›2 (X j â† X i ) = 0. Based on Theorem 6, i is the ancestor of j.</p><p>(ii) According to the acyclic constraints, the number of paths from their common ancestors to j is either equal or more than the number of paths from their common ancestors to i, since i is an ancestor of j and those paths to i must reach j.</p><p>If there exists a common ancestor k âˆˆ arg max l {|P lâ†i ||l âˆˆ An(i, j)} has a directed path from k to j without passing i in G, it implies that the number of paths from k to j greater than that to i, Consequently, there must exist a value n = |P kâ†j | such that Î›n (X i â† X j ) â‰  0 and Î›n (X j â† X i ) = 0 , and the causal order of i, j is identifiable based on Theorem 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G Proof of Theorem 8</head><p>Theorem 8. Let G X i |X P a(i) (s) be the PGF of random variable X i given its parents variable X P a(i) , we have:</p><formula xml:id="formula_158">P (X i = k|X P a(i) = x P a(i) ) = 1 k! âˆ‚ k G X i |X P a(i) (s) (âˆ‚s) k s=0 = âˆ‘ t i + âˆ‘ jâˆˆP a(i) t j =k Âµ t i i exp(-Âµ i ) t i ! âˆ jâˆˆP a(i)</formula><p>(x j ) t j Î± t j j,i (1 -Î± j,i )</p><p>x j -t j t j ! , (G.1) where t j â‰¤ x j , (x j ) t j =</p><p>x j ! (x j -t j )! is the falling factorial, Âµ i = E[Ïµ i ], and Ïµ i is the noise component of X i .</p><p>Proof. For probability mass function P (X i |X P a(i) ), which can be decomposed as follow:</p><p>P (X i |X P a(i) ) = P (Ïµ i ) âˆ jâˆˆP a(i)</p><formula xml:id="formula_159">P (Î± j,i â€¢ X j |X j ). (G.2)</formula><p>Let G Ïµ i (s) represent the probability generating function (PGF) of P (Ïµ i ), which is the noise component of X i , and G Î± j,i â€¢X j |X j (s) denote the PGF of P (Î± j,i â€¢ X j |X j ), we have:</p><formula xml:id="formula_160">G X i |X P a(i) (s) = G Ïµ i (s) âˆ jâˆˆP a(i) G Î± j,i â€¢X j |X j (s), (G.3)</formula><p>where G Ïµ i (s) = exp[Âµ i (s -1)] and G Î± j,i â€¢X j |X j (s) = (1 -Î± j,i + Î± j,i s) X j .</p><p>According to the property of PGF, we can calculate the probability mass function by taking derivatives of G X i |X P a(i) (s), and the derivative is expressed as:</p><formula xml:id="formula_161">âˆ‚ k G X i |X P a(i) (s) (âˆ‚s) k = âˆ‚ k (G Ïµ i (s) âˆ jâˆˆP a(i) G Î± j,i â€¢X j |X j (s)) (âˆ‚s) k (G.4)</formula><p>According to the product rule of higher derivatives, i.e.</p><formula xml:id="formula_162">( n âˆ i=1 f i ) (k) = âˆ‘ t 1 +t 2 +â‹¯+t n =k ( k t 1 , t 2 , . . . , t n ) n âˆ i=1 f (t i ) i = âˆ‘ t 1 +t 2 +â‹¯+t n =k k! t 1 !t 2 !â‹¯t n ! n âˆ i=1 f (t i ) i = k! âˆ‘ t 1 +t 2 +â‹¯+t n =k n âˆ i=1 f (t i ) i t i ! , (G.5)</formula><p>we have:</p><formula xml:id="formula_163">âˆ‚ k G X i |X P a(i) (s) (âˆ‚s) k = k! âˆ‘ t i + âˆ‘ jâˆˆP a(i) t j =k G (t i ) Ïµ i (s) t i ! âˆ jâˆˆP a(i) G (t j ) Î± j,i â€¢X j |X j (s) t j ! . (G.6)</formula><p>Furthermore, we have</p><formula xml:id="formula_164">G (t i ) Ïµ i (s) = Âµ t i i exp(Âµ i (s -1)), G (t j )</formula><p>Î± j,i â€¢X j |X j (s) = { (X j ) t j Î± t j j,i (1 -Î± j,i )</p><p>X j -t j t j â©½ X j 0 t j &gt; X j .</p><p>(G.7) Given X i = x i , X P a(i) = x P a(i) , along with the model parameter</p><formula xml:id="formula_165">Î˜ = {A = [Î± i,j ] âˆˆ [0, 1] |V |Ã—|V | , Âµ = [Âµ i ] âˆˆ R |V |</formula><p>â‰¥0 }, we can compute the probability mass function P (X i |X P a(i) ) as follow:</p><formula xml:id="formula_166">P (X i = k|X P a(i) = x P a(i) ) = 1 k! âˆ‚ k G X i |X P a(i) =x P a(i) (s) (âˆ‚s) k s=0 = 1 k! âˆ‘ t i + âˆ‘ jâˆˆP a(i) t j =k G (t i ) Ïµ i (0) t i ! âˆ jâˆˆP a(i) G (t j )</formula><p>Î± j,i â€¢X j |X j =x j (0)</p><formula xml:id="formula_167">t j ! = 1 k! âˆ‘ t i + âˆ‘ jâˆˆP a(i) t j =k G (t i ) Ïµ i (0) t i ! âˆ jâˆˆP a(i) G (t j )</formula><p>Î± j,i â€¢X j |X j =x j (0)</p><formula xml:id="formula_168">t j ! = âˆ‘ t i + âˆ‘ jâˆˆP a(i) t j =k, Âµ t i i exp(-Âµ i ) t i ! âˆ</formula><p>jâˆˆP a(i) (x j ) t j Î± t j j,i (1 -Î± j,i )</p><p>x j -t j t j ! , (G.8) where t j â©½ x j . This completes the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Accelerating Likelihood Computation Using FFT</head><p>To compute the likelihood function, we have to calculate the Eq. (G.8). However, this task remains computationally intensive due to the numerous parameter combinations satisfying the specific summation condition t i + âˆ‘ jâˆˆP a(i)</p><p>t j = k and t j â©½ x j .</p><p>To address this issue, we show that the likelihood in Eq. (G.1) can be formulated as the problem of obtaining the coefficient of a polynomial product.</p><p>Specifically, the production of polynomials can be constructed as follows:</p><formula xml:id="formula_169">F (y) = ( Âµ 0 i exp(-Âµ i ) 0! + Âµ 1 i exp(-Âµ i ) 1! y + â‹¯ + Âµ k i exp(-Âµ i ) k! y k ) Ã— âˆ</formula><p>jâˆˆP a(i) ( (x j ) 0 Î± j,i (1 -Î± j,i )</p><p>x j 0! + (x j ) 1 Î± j,i (1 -Î± j,i )</p><p>x j -1 1! y + â‹¯ + (x j ) x j Î± j,i (1 -Î± j,i )</p><p>x j -x j</p><p>x j ! y x j ) (G.9)</p><p>Then the likelihood in Eq. (G.8) is exactly the coefficient of y k after the production. To obtain the coefficient of such production, we can employ the Fast Fourier Transform (FFT). In detail, we can create a series of vectors of the coefficient of each polynomial in (G.9), and pad the list with 0 since the highest power of x is k Ã— |P a(i)|: a j p = â¡ â¢ â¢ â¢ â¢ â¢ â¢ â¢ â¢ â¢ â¢ â£ (x j p ) 0 Î± j p ,i (1 -Î± j p ,i )</p><formula xml:id="formula_170">a 0 = â¡ â¢ â¢ â¢ â¢ â¢ â¢ â¢ â¢ â¢ â£ Âµ 0 i exp(-Âµ i ) 0! , Âµ 1 i exp(-Âµ i )<label>1</label></formula><p>x jp 0! , (x j p ) 1 Î± j p ,i (1 -Î± j p ,i )</p><p>x jp -1 1! , ..., (x j p ) x jp Î± j p ,i (1 -Î± j p ,i )</p><p>x jp -x jp</p><p>x j p ! , 0, ..., 0 kÃ—|P a(i)|-x jp +1 times â¤ â¥ â¥ â¥ â¥ â¥ â¥ â¥ â¥ â¥ â¥ â¦ (G.10) where j p âˆˆ P a(i) and p = 1, 2, ..., |P a(i)|. Then the coefficient vector of the expansion of Eq. (G.9) is given by: Ã¢ = IFFT (FFT(a 0 ) âŠ™ FFT(a j 1 ) âŠ™ â‹¯ âŠ™ FFT(a j |P a(i)| )) (G.11)</p><p>Here, âŠ™ is the element-wise multiplication, FFT(â‹…) and IFFT(â‹…) is the implementation of fast Fourier transform and Inverse fast Fourier transform respectively. Consequently, the k + 1-th element in the vector Ã¢ is the coefficient of y k in the expansion of Eq. (G.9), which is the likelihood given Eq. (G.8).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H Additional Experiments</head><p>The main paper has shown the F1 scores and other baselines in synthetic data experiments. Here, we further provide the Precision, Recall, and Structural Hamming Distance (SHD) in these experiments, as shown in Fig. <ref type="figure">3</ref>  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>to perform a consecutive thinning operation on X i based on the path sequence. Goal: Given i.i.d. samples D = {x joint distribution P (X), our goal is to identify the unknown causal structure G from D, assuming the data generative mechanism follows PB-SCM.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Illustration of the identifiability of X â†’ Y .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>13) We maximum the objective function L p (G, Î˜; D) by using a Hill-Climbing-based algorithm as shown in Lines 2-6 of Algorithm 1. It mainly consists of two phases. First, we perform a structure searching scheme by taking one step adding, deleting, and reversing the graph G * in the last iteration, i.e., in Line 4, V (G * ) represents a collection of the one-step modified graph of G * . Second, by fixing the graph G â€² , we estimate the parameter Î˜ â€² of the model via optimizer with initial values from approximated covariance estimates and then calculate the L â€² p (G â€² , Î˜ â€² ; D) in Lines 5. Iterating the two steps above until the likelihood no longer increases. In the end, we transform G</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: F1 in the Sensitivity Experiments</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Football Dataset Result (F :Foul, Y 1 : Yellow card, Y 2 : Second yellow card, R: Red card, S: Substitution).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Generating process of A 1 â€¢ Ïµ, A 2 â€¢ Ïµ, A 3 â€¢ Ïµ, each leaf node corresponds to an original random variable.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Obtain conditional independence according to the hierarchical structure of the tree</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>|P iâ†j | } be the set of paths from vertex i to j with the corresponding set of sequences of coefficientsA iâ†j =</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>C.4) which yields |P iâ†j | Ã— (n -1) cumulant term where each term correspondent to a different combination of coefficient A iâ†j l . To characterize the combinations of A iâ†j l within each cumulant in Eq. (C.3), we can conceptualize that choose different A iâ†j l into n -1 box from |P iâ†j | number of different coefficient, , i.e., we can select a A iâ†j l from A iâ†j = {A iâ†j 1 , A iâ†j 2 , ..., A iâ†j |P iâ†j | } for each position in the decomposed cumulant.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>, Fig. 4 and Fig. 5.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Sensitivity to the max order of cumulant K Â± 0.04 0.56 Â± 0.05 0.90 Â± 0.05 23.38 Â± 3.88 3 0.82 Â± 0.06 0.82 Â± 0.08 0.83 Â± 0.06 9.53 Â± 2.71 4 0.82 Â± 0.06 0.82 Â± 0.07 0.83 Â± 0.06 9.47 Â± 2.78 5 0.83 Â± 0.05 0.82 Â± 0.07 0.84 Â± 0.05 9.47 Â± 2.68</figDesc><table><row><cell>3UHFLVLRQ</cell><cell>$OJRULWKP 3%6&amp;03 3%6&amp;0</cell><cell>3UHFLVLRQ</cell><cell></cell><cell>$OJRULWKP 3%6&amp;03 3%6&amp;0</cell><cell>3UHFLVLRQ</cell><cell>$OJRULWKP 3%6&amp;03 3%6&amp;0</cell></row><row><cell></cell><cell>3&amp;</cell><cell></cell><cell></cell><cell>3&amp;</cell><cell></cell><cell>3&amp;</cell></row><row><cell></cell><cell>*(6</cell><cell></cell><cell></cell><cell>*(6</cell><cell></cell><cell>*(6</cell></row><row><cell></cell><cell>2&amp;'</cell><cell></cell><cell></cell><cell>2&amp;'</cell><cell></cell><cell>2&amp;'</cell></row><row><cell>$YJ,QGHJUHH5DWH</cell><cell></cell><cell></cell><cell cols="2">1XPEHURIYHUWLFHV</cell><cell>6DPSOH6L]H</cell></row><row><cell cols="2">(a) Sensitivity to Avg. Indegree Rate</cell><cell></cell><cell cols="2">(b) Sensitivity to Number of vertices</cell><cell cols="2">(c) Sensitivity to Sample Size</cell></row><row><cell></cell><cell cols="5">Figure 3: Precision in the Sensitivity Experiments</cell></row><row><cell>5HFDOO</cell><cell>$OJRULWKP 3%6&amp;03</cell><cell>5HFDOO</cell><cell></cell><cell>$OJRULWKP 3%6&amp;03</cell><cell>5HFDOO</cell><cell>$OJRULWKP 3%6&amp;03</cell></row><row><cell></cell><cell>3%6&amp;0</cell><cell></cell><cell></cell><cell>3%6&amp;0</cell><cell></cell><cell>3%6&amp;0</cell></row><row><cell></cell><cell>3&amp;</cell><cell></cell><cell></cell><cell>3&amp;</cell><cell></cell><cell>3&amp;</cell></row><row><cell></cell><cell>*(6</cell><cell></cell><cell></cell><cell>*(6</cell><cell></cell><cell>*(6</cell></row><row><cell></cell><cell>2&amp;'</cell><cell></cell><cell></cell><cell>2&amp;'</cell><cell></cell><cell>2&amp;'</cell></row><row><cell>$YJ,QGHJUHH5DWH</cell><cell></cell><cell></cell><cell cols="2">1XPEHURIYHUWLFHV</cell><cell>6DPSOH6L]H</cell></row><row><cell cols="2">(a) Sensitivity to Avg. Indegree Rate</cell><cell></cell><cell cols="2">(b) Sensitivity to Number of vertices</cell><cell cols="2">(c) Sensitivity to Sample Size</cell></row><row><cell></cell><cell cols="5">Figure 4: Recall in the Sensitivity Experiments</cell></row><row><cell>6+'</cell><cell>$OJRULWKP 3%6&amp;03</cell><cell>6+'</cell><cell></cell><cell>$OJRULWKP 3%6&amp;03</cell><cell>6+'</cell><cell>$OJRULWKP 3%6&amp;03</cell></row><row><cell></cell><cell>3%6&amp;0</cell><cell></cell><cell></cell><cell>3%6&amp;0</cell><cell></cell><cell>3%6&amp;0</cell></row><row><cell></cell><cell>3&amp;</cell><cell></cell><cell></cell><cell>3&amp;</cell><cell></cell><cell>3&amp;</cell></row><row><cell></cell><cell>*(6</cell><cell></cell><cell></cell><cell>*(6</cell><cell></cell><cell>*(6</cell></row><row><cell></cell><cell>2&amp;'</cell><cell></cell><cell></cell><cell>2&amp;'</cell><cell></cell><cell>2&amp;'</cell></row><row><cell>$YJ,QGHJUHH5DWH</cell><cell></cell><cell></cell><cell cols="2">1XPEHURIYHUWLFHV</cell><cell>6DPSOH6L]H</cell></row><row><cell cols="2">(a) Sensitivity to Avg. Indegree Rate</cell><cell></cell><cell cols="2">(b) Sensitivity to Number of vertices</cell><cell cols="2">(c) Sensitivity to Sample Size</cell></row><row><cell></cell><cell cols="5">Figure 5: SHD in the Sensitivity Experiments</cell></row><row><cell></cell><cell>K</cell><cell></cell><cell cols="2">Score type</cell><cell></cell></row><row><cell></cell><cell></cell><cell>F1</cell><cell>Precision</cell><cell>Recall</cell><cell>SHD</cell></row><row><cell></cell><cell>2 0.69</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This research was supported in part by <rs type="funder">National Key R&amp;D Program of China</rs> (<rs type="grantNumber">2021ZD0111501</rs>), <rs type="funder">National Science Fund for Excellent Young Scholars</rs> (<rs type="grantNumber">62122022</rs>), <rs type="funder">Natural Science Foundation of China</rs> (<rs type="grantNumber">61876043</rs>, <rs type="grantNumber">61976052</rs>), the major key project of PCL (PCL2021A12). ZM's research was supported by the <rs type="funder">China Scholarship Council (CSC)</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_GRMRWVJ">
					<idno type="grant-number">2021ZD0111501</idno>
				</org>
				<org type="funding" xml:id="_VYDFN5W">
					<idno type="grant-number">62122022</idno>
				</org>
				<org type="funding" xml:id="_WEjxPfc">
					<idno type="grant-number">61876043</idno>
				</org>
				<org type="funding" xml:id="_Q3xKEX8">
					<idno type="grant-number">61976052</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">First-order integervalued autoregressive (INAR (1)) process</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Al-Osh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Alzaid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Time Series Analysis</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="261" to="275" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Self: structural equational likelihood framework for causal discovery</title>
		<author>
			<persName><forename type="first">R</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Hao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<title level="m">THPs: Topological Hawkes Processes for Learning Causal Structure on Event Sequences. IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Optimal Structure Identification with Greedy Search</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Chickering</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="507" to="554" />
			<date type="published" when="2002-11">2002. Nov</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Bayesian causal structural learning with zero-inflated poisson bayesian networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chapkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="5887" to="5897" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">H</forename><surname>Cormen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Leiserson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Rivest</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Stein</surname></persName>
		</author>
		<title level="m">Introduction to algorithms</title>
		<imprint>
			<publisher>MIT press</publisher>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">An introduction to the bootstrap</title>
		<author>
			<persName><forename type="first">B</forename><surname>Efron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Tibshirani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994">1994</date>
			<publisher>CRC press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Review of causal discovery methods based on graphical models. Frontiers in genetics</title>
		<author>
			<persName><forename type="first">C</forename><surname>Glymour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Spirtes</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">524</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Some simple models for discrete variate time series 1</title>
		<author>
			<persName><forename type="first">E</forename><surname>Mckenzie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JAWRA Journal of the American Water Resources Association</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="645" to="650" />
			<date type="published" when="1985">1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Ordinal causal discovery</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mallick</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Uncertainty in Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1530" to="1540" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning large-scale poisson dag models based on overdispersion scoring</title>
		<author>
			<persName><forename type="first">G</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Raskutti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page">28</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning Quadratic Variance Function (QVF) DAG Models via OverDispersion Scoring (ODS)</title>
		<author>
			<persName><forename type="first">G</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Raskutti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">224</biblScope>
			<biblScope unit="page" from="1" to="44" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Causality</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>Cambridge university press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Structural Hawkes Processes for Learning Causal Structure from Discrete-Time Event Sequences</title>
		<author>
			<persName><forename type="first">J</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Hao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence, IJCAI-23</title>
		<meeting>the Thirty-Second International Joint Conference on Artificial Intelligence, IJCAI-23</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="5702" to="5710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Spirtes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">N</forename><surname>Glymour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Scheines</surname></persName>
		</author>
		<title level="m">Causation, prediction, and search</title>
		<imprint>
			<publisher>MIT press</publisher>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Causal inference in the presence of latent variables and selection bias</title>
		<author>
			<persName><forename type="first">P</forename><surname>Spirtes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Meek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Richardson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh conference on Uncertainty in artificial intelligence</title>
		<meeting>the Eleventh conference on Uncertainty in artificial intelligence</meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="499" to="506" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Discrete analogues of self-decomposability and stability. The Annals of Probability</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">W</forename><surname>Steutel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Van Harn</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1979">1979</date>
			<biblScope unit="page" from="893" to="899" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The max-min hill-climbing Bayesian network structure learning algorithm</title>
		<author>
			<persName><forename type="first">I</forename><surname>Tsamardinos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">E</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">F</forename><surname>Aliferis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="page" from="31" to="78" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">An introduction to discrete-valued time series</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>WeiÃŸ</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>John Wiley &amp; Sons</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Diagnosing and modeling extra-binomial variation for time-dependent counts</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>WeiÃŸ</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-Y</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Stochastic Models in Business and Industry</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="588" to="608" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Binomial subsampling</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wiuf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Stumpf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences</title>
		<imprint>
			<biblScope unit="volume">462</biblScope>
			<biblScope unit="page" from="1181" to="1195" />
			<date type="published" when="2006">2006. 2068</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning causality and causality-related learning: some recent progress</title>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>SchÃ¶lkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Spirtes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Glymour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">National science review</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="26" to="29" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
