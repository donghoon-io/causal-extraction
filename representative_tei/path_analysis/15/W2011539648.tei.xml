<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Crawling AJAX-based Web Applications through Dynamic Analysis of User Interface State Changes</title>
				<funder ref="#_qjW8jFG">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Ali</forename><surname>Mesbah</surname></persName>
							<email>amesbah@ece.ubc.ca</email>
							<affiliation key="aff0">
								<orgName type="laboratory">ARIE VAN DEURSEN</orgName>
								<orgName type="institution" key="instit1">University of British Columbia</orgName>
								<orgName type="institution" key="instit2">Delft University of Technology</orgName>
								<orgName type="institution" key="instit3">Delft University of Technology</orgName>
								<orgName type="institution" key="instit4">University of British Columbia</orgName>
								<address>
									<addrLine>2332 Main Mall</addrLine>
									<postCode>V6T1Z4</postCode>
									<settlement>Vancouver</settlement>
									<region>BC</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">ACM, Inc</orgName>
								<address>
									<addrLine>Suite 701</addrLine>
									<settlement>Penn Plaza New York</settlement>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Stefan</forename><surname>Lenselink</surname></persName>
							<email>ands.r.lenselink@student.tudelft.nl</email>
							<affiliation key="aff1">
								<orgName type="department">from Publications Dept</orgName>
								<orgName type="institution">Delft University of Technology</orgName>
								<address>
									<addrLine>Mekelweg 4</addrLine>
									<postCode>2628CD</postCode>
									<settlement>Delft</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Crawling AJAX-based Web Applications through Dynamic Analysis of User Interface State Changes</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/0000000.0000000</idno>
					<note type="submission">Received 2010; revised ; accepted ACM Transactions on the</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-21T21:15+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.5.4 [Information Interfaces and Presentation]: Hypertext/Hypermedia-Navigation</term>
					<term>H.3.3 [Information Search and Retrieval]: Search process</term>
					<term>D.2.2 [Software Engineering]: Design Tools and Techniques Design, Algorithms, Experimentation Crawling, ajax, web 2.0, hidden web, dynamic analysis, dom crawling</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Using JavaScript and dynamic DOM manipulation on the client-side of web applications is becoming a widespread approach for achieving rich interactivity and responsiveness in modern web applications. At the same time, such techniques, collectively known as Ajax, shatter the metaphor of web 'pages' with unique URLs, on which traditional web crawlers are based. This paper describes a novel technique for crawling Ajax-based applications through automatic dynamic analysis of user interface state changes in web browsers. Our algorithm scans the DOM-tree, spots candidate elements that are capable of changing the state, fires events on those candidate elements, and incrementally infers a state machine modelling the various navigational paths and states within an Ajax application. This inferred model can be used, for instance, in program comprehension, analysis and testing of dynamic web states, or for generating a static version of the application. In this paper, we discuss our sequential and concurrent Ajax crawling algorithms.</p><p>We present our open source tool called Crawljax, which implements the concepts and algorithms discussed in this paper. Additionally, we report a number of empirical studies in which we apply our approach to a number of open-source and industrial web applications and elaborate on the obtained results.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>The web as we know it is undergoing a significant change. A technology that has gained a prominent position lately, under the umbrella of Web 2.0, is AJAX (Asynchronous JAVASCRIPT and XML) <ref type="bibr" target="#b19">[Garrett 2005</ref>], in which the combination of JAVASCRIPT and Document Object Model (DOM) manipulation, along with asynchronous server com- With this new change in developing web applications comes a whole set of new challenges, mainly due to the fact that AJAX shatters the metaphor of a web 'page' upon which many web technologies are based. Among these challenges are the following:</p><p>Searchability. ensuring that AJAX sites are crawled and indexed by the general search engines, instead of (as is currently often the case) being ignored by them because of the use of client-side scripting and dynamic state changes in the DOM; Testability. systematically exercising dynamic user interface (UI) elements and analyzing AJAX states to find abnormalities and errors;</p><p>One way to address these challenges is through the use of a crawler that can automatically walk through different states of a highly dynamic AJAX site and create a model of the navigational paths and states.</p><p>General web search engines, such as Google and Bing, cover only a portion of the web called the publicly indexable web that consists of the set of web pages reachable purely by following hypertext links, ignoring forms <ref type="bibr" target="#b5">[Barbosa and Freire 2007]</ref> and clientside scripting. The web content behind forms and client-side scripting is referred to as the hidden-web, which is estimated to comprise several millions of pages <ref type="bibr" target="#b5">[Barbosa and Freire 2007]</ref>. With the wide adoption of AJAX techniques that we are witnessing today this figure will only increase. Although there has been extensive research on crawling and exposing the data behind forms <ref type="bibr" target="#b5">[Barbosa and Freire 2007;</ref><ref type="bibr" target="#b14">de Carvalho and Silva 2004;</ref><ref type="bibr" target="#b22">Lage et al. 2004;</ref><ref type="bibr" target="#b35">Ntoulas et al. 2005;</ref><ref type="bibr" target="#b38">Raghavan and Garcia-Molina 2001]</ref>, crawling the hidden-web induced as a result of client-side scripting has gained very little attention so far.</p><p>Crawling AJAX-based applications is fundamentally more difficult than crawling classical multi-page web applications. In traditional web applications, states are explicit, and correspond to pages that have a unique URL assigned to them. In AJAX applications, however, the state of the user interface is determined dynamically, through changes in the DOM that are only visible after executing the corresponding JAVASCRIPT code.</p><p>In this paper, we propose an approach to analyze these user interface states automatically. Our approach is based on a crawler that can exercise client-side code and identify clickable elements (which may change with every click) that change the state within the browser's dynamically built DOM. From these state changes, we infer a state-flow graph, which captures the states of the user interface and the possible transitions between them. The underlying ideas have been implemented in an open source tool called CRAWLJAX. 1 To the best of our knowledge, CRAWLJAX is the first and currently the only available tool that can detect dynamic contents of AJAX-based web applications automatically without requiring specific URLs for each web state.</p><p>The inferred model can be used, for instance, to expose AJAX sites to general search engines or to examine the accessibility <ref type="bibr" target="#b4">[Atterer and Schmidt 2005]</ref> of different dynamic states. The ability to automatically exercise all the executable elements of an AJAX site gives us a powerful test mechanism. CRAWLJAX has successfully been used for conducting automated model-based and invariant-based testing <ref type="bibr" target="#b34">[Mesbah and van Deursen 2009]</ref>, security testing <ref type="bibr" target="#b6">[Bezemer et al. 2009</ref>], regression testing <ref type="bibr" target="#b39">[Roest et al. 2010]</ref>, and cross-browser compatibility testing <ref type="bibr" target="#b32">[Mesbah and Prasad 2011]</ref> of AJAX web applications.</p><p>We have performed a number of empirical studies to analyze the overall performance of our approach. We evaluate the effectiveness in retrieving relevant clickables and as-sess the quality and correctness of the detected states and edges. We also examine the performance of our crawling tool as well as the scalability in crawling AJAX applications with a large number of dynamic states and clickables. The experimental benchmarks span from open source to industrial web applications.</p><p>This paper is a revised and extended version of our original paper in 2008 <ref type="bibr">[Mesbah et al. 2008]</ref>. The extensions in this paper are based on three years of tool usage and refinements. In addition, we report on our new multi-threaded, multi-browser crawling approach as well as a new (industrial) empirical study, evaluating its influence on the runtime performance. The results of our study show that by using 5 browsers instead of 1, we can achieve a decrease of up to 65% in crawling runtime.</p><p>The paper is further structured as follows. We start out, in Section 2 by exploring the difficulties of crawling AJAX. In Section 3 we present a detailed discussion of our AJAX crawling algorithm and technique. In Section 4, we extend our sequential crawling approach to a concurrent multiple-browser crawling algorithm. Section 5 discusses the implementation of our tool CRAWLJAX. In Section 6, the results of applying our techniques to a number of AJAX applications are shown, after which Section 7 discusses the findings and open issues. We conclude with a survey of related work, a summary of our key contributions and suggestions for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">CHALLENGES OF CRAWLING AJAX</head><p>AJAX-based web applications have a number of properties that make them very challenging to crawl automatically.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Client-side Execution</head><p>The common ground for all AJAX applications is a JAVASCRIPT engine, which operates between the browser and the web server <ref type="bibr">[Mesbah and van Deursen 2008]</ref>. This engine typically deals with server communication and user interface modifications. Any search engine willing to approach such an application must have support for the execution of the scripting language. Equipping a crawler with the necessary environment complicates its design and implementation considerably. The major search engines such as Google and Bing currently have little or no support for executing scripts and thus ignore content produced by JAVASCRIPT, 2 due to scalability and security issues.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">State Changes and Navigation</head><p>Traditional web applications are based on the multi-page interface paradigm consisting of multiple pages each having a unique URL. In AJAX applications, not every state change necessarily has an associated REST-based <ref type="bibr" target="#b17">[Fielding and Taylor 2002]</ref> URI. Ultimately, an AJAX application could consist of a single-page with a single URL <ref type="bibr" target="#b31">[Mesbah and van Deursen 2007]</ref>. This characteristic makes it difficult for a search engine to index and point to a specific state in an AJAX application. Crawling traditional web pages constitutes extracting and following the hypertext links (the src attribute of anchor tags) on each page. In AJAX, hypertext links can be replaced by events which are handled by JAVASCRIPT; i.e., it is not possible any longer to navigate the application by simply extracting and retrieving the internal hypertext links.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Dynamic Document Object Model (DOM)</head><p>Crawling and indexing traditional web applications consists of following links, retrieving and saving the HTML source code of each page. The state changes in AJAX applications are dynamically represented through the run-time changes on the DOM-tree in the browser. This means that the initial HTML source code retrieved from the server 0:4 Mesbah et al.</p><p>1 &lt;A href = " javascript : OpenNewsPage (); " &gt; ... 2 &lt;A href = " # " onClick = " OpenNewsPage (); " &gt; ... 3 &lt; DIV onClick = " OpenNewsPage (); " &gt; ... 5 &lt; DIV class = " news " / &gt; 6 &lt; SPAN id = " content " / &gt; 7 &lt;! --jQuery f u n c t i o n a t t a c h i n g events to e l e m e n t s having a t t r i b u t e class = " news " .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>8</head><p>The news c o nt e n t s are i n j e c t e d into the SPAN element --&gt; 9 &lt; script &gt; $ ( " . news " ). click ( function () { $ ( " # content " ). load ( " news . html " ); }); &lt;/ script &gt; Fig. <ref type="figure">1</ref>: Different ways of attaching events to elements.</p><p>does not represent the state changes. An AJAX crawler will need to have access to this run-time dynamic document object model of the application.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Delta-communication</head><p>AJAX applications rely on a delta-communication <ref type="bibr">[Mesbah and van Deursen 2008]</ref> style of interaction in which merely the state changes are exchanged asynchronously between the client and the server, as opposed to the full-page retrieval approach in traditional web applications. Retrieving and indexing the data served by the server, for instance, through a proxy between the client and the server, could have the side-effect of losing the context and actual meaning of the changes because most of such updates become meaningful after they have been processed by the JAVASCRIPT engine on the client and injected into the runtime DOM-tree.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.">Clickable Elements Changing the Internal State</head><p>To illustrate the difficulties involved in crawling AJAX, consider Figure <ref type="figure">1</ref>. It is a highly simplified example, showing different ways in which a news page can be opened. The example code shows how in AJAX, it is not just the hypertext link element that forms the doorway to the next state. Note the way events (e.g., onClick, onMouseOver) can be attached to DOM elements at run-time. As can be seen, a DIV element (line 3) can have an onclick event attached to it so that it becomes a clickable element capable of changing the internal DOM state of the application when clicked.</p><p>Event handlers can also be dynamically registered using JAVASCRIPT. The jQuery<ref type="foot" target="#foot_0">foot_0</ref> code (lines 5-13) attaches a function to the onClick event listener of the element with class attribute news. When this element is clicked, the news content is retrieved from the server and injected into the SPAN element with ID content.</p><p>There are different ways to attach event listeners to DOM elements. For instance, if we have the following handler:</p><formula xml:id="formula_0">var handler = function () { alert ( ' Element clicked ! ') };</formula><p>we can attach it to an onClick listener of a DOM element e in the following ways:</p><p>(1) e . onclick = handler ;</p><p>( The first case presents the traditional way of attaching handlers to DOM elements. In this case, we can examine the DOM element at runtime and find out that it has the handler attached to its onClick attribute. The second case show how handlers could be attached to DOM elements through the DOM Level 2 API. In this case, however, by examining the DOM element it is not possible to find information about the handler, since the event model (DOM Level 2 Events <ref type="bibr" target="#b37">[Pixley 2000]</ref>) maintaining the handler registration information is separated from the DOM core model itself. Hence, automatically finding these clickable elements at runtime is another non-trivial task for an AJAX crawler.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">A METHOD FOR CRAWLING AJAX</head><p>The challenges discussed in the previous section should make it clear that crawling AJAX is more demanding than crawling the classical web. In this section, we propose a dynamic analysis approach, in which we open the AJAX application in a browser, scan the DOM-tree for candidate elements that are capable of changing the state, fire events on those elements, and analyze the effects on the DOM-tree. Based on our analysis, we infer a state-flow graph representing the user interface states and possible transitions between them.</p><p>In this section, we first present the terminology used in this paper followed by a discussion of the most important components of our crawling technique, as depicted in Figure <ref type="figure" target="#fig_3">3</ref>. In traditional multi-page web applications, each state is represented by a URL and the corresponding web page. In AJAX however, it is the internal structure of the DOM-tree of the (single-page) user interface that represents a state. Therefore, to adopt a generic approach for all AJAX sites, we define a state change as a change on the DOM tree caused by (1) either client-side events handled by the AJAX engine; (2) or server-side state changes propagated to the client.</p><p>3.1.2. State Transition and Clickable. On the browser, the end-user can interact with the web application through the user interface: click on an element, bring the mousepointer over an element, and so on. These actions can cause events that, as described above, can potentially change the state of the application. We call all DOM elements that have event-listeners attached to them and can cause a state transition, clickable elements. For the sake of simplicity, we use the click event type to present our approach, note, however, that other event types can be used just as well to analyze the effects on the DOM in the same manner.</p><p>3.1.3. The State-flow Graph. To be able to navigate an AJAX-based web application, the application can be modelled by recording the click trails to the various user interface state changes. To record the states and transitions between them, we define a stateflow graph as follows: DEFINITION 1. A state-flow graph G for an AJAX site A is a labeled, directed graph, denoted by a 4 tuple &lt; r, V , E, L &gt; where:</p><p>(1) r is the root node (called Index) representing the initial state after A has been fully loaded into the browser.</p><p>(2) V is a set of vertices representing the states. Each v ∈ V represents a runtime DOM state in A.</p><p>(3) E is a set of (directed) edges between vertices.  (4) L is a labelling function that assigns a label, from a set of event types and DOM element properties, to each edge. (5) G can have multi-edges and be cyclic.</p><p>As an example of a state-flow graph, Figure <ref type="figure" target="#fig_2">2</ref> depicts the visualization of the stateflow graph of a simple AJAX site. The edges between states are labeled with an identification (either via its ID-attribute or via an XPath expression) of the clickable. Thus, clicking on the //DIV[1]/ <ref type="bibr">SPAN[4]</ref> element in the Index state leads to the S 1 state, from which two states are directly reachable namely S 3 and S 4.</p><p>The state-flow graph is created incrementally. Initially, it only contains the root state and new states are created and added as the application is crawled and state changes are analyzed.</p><p>The following components, also shown in Figure <ref type="figure" target="#fig_3">3</ref>, participate in the construction of the state-flow graph:</p><p>-Embedded Browser: The embedded browser provides a common interface for accessing the underlying engines and runtime objects, such as the DOM and JAVASCRIPT. -Robot: A robot is used to simulate user actions (e.g., click, mouseOver, text input) on the embedded browser. -Controller: The controller has access to the embedded browser's DOM. It also controls the Robot's actions and is responsible for updating the state machine when relevant changes occur on the DOM. -DOM Analyzer: The analyzer is used to check whether the DOM-tree is changed after an event has been fired by the robot. In addition, it is used to compare DOMtrees when searching for duplicate-states in the state machine.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Inferring the State Machine</head><p>The algorithm used by these components to actually infer the state machine is shown in Algorithm 1. The main procedure (lines 1-5) takes care of initializing the various components and processes involved. The actual, recursive, crawl procedure starts at line 6. The main steps of the crawl procedure are explained below.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Detecting Clickables</head><p>There is no feasible way of automatically obtaining a list of all clickable elements on a DOM-tree, due to the reasons explained in Section 2. Therefore, our algorithm makes use of a set of candidate elements, which are all exposed to an event type (e.g., click, mouseOver). Each element on the DOM-tree that meets the labelling requirements is selected as a candidate element.</p><p>In automated mode, the candidate clickables are labeled as such based on their HTML tag element name. In our implementation, all elements with a tag &lt;A&gt;, &lt;BUTTON&gt;, or &lt;INPUT type='submit'&gt; are considered as candidate clickables, by default. The selection of candidate clickables can be relaxed or constrained by the user as well, by defining element properties such as the XPATH position on the DOM-tree, attributes and their values, and text values. For instance, the user could be merely interested in examining DIV elements with attribute class='article'. It is also possible to exclude certain elements from the crawling process.</p><p>Based on the given definition of the candidate clickables, our algorithm scans the DOM-tree and extracts all the DOM elements that meet the requirements of the definition (line 12). For each extracted candidate element, the crawler then instructs the robot to fill in the detected data entry points (line 14) and fire an event (line 15) on the element in the browser. The robot is currently capable of using either self-generated random values, or custom values provided by the user to fill in the forms (for more details on the form filling capabilities see <ref type="bibr" target="#b34">[Mesbah and van Deursen 2009]</ref>). DEFINITION 2. Let D 1 be the DOM-tree of the state before firing an event e on a candidate clickable cc and D 2 the DOM-tree after e is fired, then cc is a clickable if and only if D 1 differs from D 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">State Comparison</head><p>After firing an event on a candidate clickable, the algorithm compares the resulting DOM-tree with the DOM-tree as it was just before the event fired, in order to determine whether the event results in a state change (line 17).</p><p>To detect a state change, the DOM-trees need to be compared. One way of comparing them is by calculating the edit distance between two DOM-trees is calculated, using the <ref type="bibr" target="#b23">Levenshtein [1996]</ref> method. A similarity threshold τ is used under which two DOM trees are considered clones. This threshold (0.0-1.0) can be given as input. A threshold of 0 means two DOM states are seen as clones if they are exactly the same in terms of structure and content. Any change is, therefore, seen as a state change.</p><p>Another way of comparing the states we have proposed recently <ref type="bibr" target="#b39">[Roest et al. 2010</ref>], is the use of a series of comparators, each capable of focusing on and comparing specific aspects of two DOM-trees. In this technique, each comparator filters out specific parts of the DOM-tree and passes the output to the next comparator. For instance, a Datetime comparator looks for any date/time patterns and filters those. This way, two states containing different timestamps can be marked similar automatically.</p><p>If a state change is detected, according to our comparison heuristics, we create a new state and add it to the state-flow graph of the state machine (line 19).</p><p>The ADDSTATE call works as follows: In order to recognize an already met state, we compare every new state to the list of already visited states on the state-flow graph. If we recognize an identical or similar state in the state machine (based on the same similarity notion used for detecting a new state after an event) that state is used for adding a new edge, otherwise a new state is created and added to the graph.</p><p>As an example, Figure <ref type="figure" target="#fig_6">4a</ref> shows the full state space of a simple application, before any similarity comparison heuristics. In Figure <ref type="figure" target="#fig_6">4c</ref>, the states that are identical (S 4) TUD-SERG-2011-033</p><p>Crawling AJAX-based Web Applications through Dynamic Analysis of User Interface State Changes0:9</p><formula xml:id="formula_1">Index S_2 S_3' S_3 S_4 S_5 S_4 S_5</formula><p>(a) Full state machine.  are merged and Figure <ref type="figure" target="#fig_6">4c</ref> presents the state space after similar states (S 3 and S 3 ) have been merged.</p><p>For every detected state, a new edge is created on the graph between the state before the event and the current state (line 20). Using properties such as the XPATH expression, the clickable causing the state transition is also added as part of the new edge (line 18).</p><p>Moreover, the current state pointer of the state machine is updated to this newly added state at that moment (line 21).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Processing Document Tree Deltas</head><p>After a clickable has been identified, and its corresponding state transition created, the CRAWL procedure is recursively called (line 23) to find possible states reachable from the newly detected state.</p><p>Upon every new (recursive) entry into the CRAWL procedure, the first action taken (line 10) is computing the differences between the previous document tree and the current one, by means of an enhanced Diff algorithm <ref type="bibr" target="#b10">[Chawathe et al. 1996;</ref><ref type="bibr" target="#b31">Mesbah and van Deursen 2007]</ref>. The resulting differences are used to find new candidate clickables, which are then further processed by the crawler. Such "delta updates" may be due, for example, to a server request call that injects new elements into the DOM.</p><p>It is worth mentioning that in order to avoid a loop, a list of visited elements is maintained to exclude already checked elements in the recursive algorithm. We use the tag name, the list of attribute names and values, and the XPath expression of each element to conduct the comparison. Additionally, a depth-level number can be defined to constrain the depth level of the recursive function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.">Backtracking to the Previous State</head><p>Upon completion of the recursive call, the browser should be put back into the state it was in before the call, at least if there are still unexamined clickable elements left on that state.</p><p>Unfortunately, navigating (back and forth) through an AJAX site is not as easy as navigating a classical multi-page one. A dynamically changed DOM state does not reg-ACM Transactions on the Web, Vol. 0, No. 0, Article 0, Publication date: 2011. ister itself with the browser history engine automatically, so triggering the 'Back' function of the browser usually does not bring us to the previous state. Saving the whole browser state is also not feasible due to many technical difficulties. This complicates traversing the application when crawling AJAX. Algorithm 2 shows our backtracking procedure.</p><p>The backtracking procedure is called once the crawler is done with a certain state (line 25 in Algorithm 1). Algorithm 2 first tries to find the relevant previous state that still has unexamined candidate clickables (lines 4-18 in Algorithm 2).</p><p>If a relevant previous state is found to backtrack to (line 6 in Algorithm 2), then we distinguish between two situations: Browser History Support It is possible to programmatically register each state change with the browser history through frameworks such as the jQuery history/remote plugin 4 , the Really Simple History library, 5 or the recently proposed HTML5 history manipulation API (e.g., history.pushState() and history.replaceState(). 6  If an AJAX application has support for the browser history (line 7), then for changing the state in the browser, we can simply use the built-in history back functionality to move backwards (line 8 in Algorithm 2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Clicking Through From Initial State</head><p>In case the browser history is not supported, which is the case with many AJAX applications currently, the approach we propose to get to a previous state is by saving information about the clickable elements, the event type (e.g., click), and the order in which the events fired on the elements results in reaching to a particular state. Once we possess such information, we can reload the application (line 10 in Algorithm 2) and fire events on the clickable elements from the initial state to the desired state, using the exact path taken to reach that state (line 11 in Algorithm 2). However, as an optimization step, it is also possible to use Dijkstra's shortest path algorithm <ref type="bibr" target="#b15">[Dijkstra 1959</ref>] to find the shortest element/event path on the graph to a certain state. For every element along the path, we first check whether the element can be found on the current DOM-tree and try to resolve it using heuristics to find the best match possible (line 13 in Algorithm 2). Then, after filling in the related input fields (line 14 in Algorithm 2), we fire an event on the element (line 15 in Algorithm 2). We adopt XPath along with its attributes to provide a better, more reliable, and persistent element identification mechanism. For each clickable, we reverse engineer the XPath expression of that element, which gives us its exact location on the DOM (line 18 in Algorithm 1). We save this expression in the state machine (line 20 in Algorithm 1) and use it to find the element after a reload, persistently (line 13 in Algorithm 2). Figure <ref type="figure" target="#fig_8">5</ref> shows an example of how our backtracking mechanism operates. Lets assume that we have taken the (E 1, E 3, E 4, E 5) path and have landed on state S 5. From S 5, our algorithm knows that there are no candidate clickables left in states S 4 and S 3 by keeping the track of examined elements. S 1, however, does contain an unexamined clickable element. The dotted blue line annotated with 1 shows our desired path for backtracking to this relevant previous state. To go from S 5 to S 1, the algorithm reloads the browser so that it lands on the index state, and from there it fires an event on the clickable E 1. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">CONCURRENT AJAX CRAWLING</head><p>The algorithm and its implementation for crawling AJAX as just described is sequential, depth-first, and single-threaded. Since we crawl the web application dynamically, the crawling runtime is determined by:</p><p>(1) the speed at which the web server responds to HTTP requests;</p><p>(2) network latency;</p><p>(3) the crawler's internal processes (e.g., analyzing the DOM, firing events, updating the state machine); (4) the speed of the browser in handling the events and request/response pairs, modifying the DOM, and rendering the user interface,</p><p>We have no influence on the first two factors and we already have many optimization heuristics for the third step (See Section 3). Therefore, in this section we focus on the last factor, the browser. Since the algorithm has to wait some considerable amount of time for the browser to finish its tasks after each event, our hypothesis is that we can decrease the total runtime by adopting concurrent crawling through multiple browsers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Multi-threaded, Multi-Browser Crawling</head><p>Figure <ref type="figure">6</ref> shows the processing view of our concurrent crawling. The idea is to maintain a single state machine and split the original controller into a new controller and multiple crawling nodes. The controller is the single main thread monitoring the total crawl procedure. In this new setting, each crawling node is responsible for deriving its corresponding robot and browser instances to crawl a specific path.</p><p>Compared with Figure <ref type="figure" target="#fig_3">3</ref>, the new architecture is capable of having multiple crawler instances, running from a single controller. All the crawlers share the same state machine. The state machine makes sure every crawler can read and update the state machine in a synchronized way. This way, the operation of discovering new states can be executed in parallel. TUD-SERG-2011-033</p><p>Crawling AJAX-based Web Applications through Dynamic Analysis of User Interface State Changes0:13 Fig. <ref type="figure" target="#fig_9">7</ref>: Partitioning the state space for multi-threaded crawling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Partition Function</head><p>To divide the work over the crawlers in a multi-threaded manner, a partition function must be designed. The performance of a concurrent approach is determined by the quality of its partition function <ref type="bibr" target="#b18">[Garavel et al. 2001]</ref>. A partition function can be either static or dynamic. With a static partition function the division of work is known in advance, before executing the code. When a dynamic partition function is used, the decision of which thread will execute a given node is made at runtime. Our algorithm infers the state-flow graph of an AJAX application dynamically and incrementally. Thus, due to this dynamic nature we adopt for a dynamic partition function.</p><p>The task of our dynamic partition function is to distribute the work equally over all the participating crawling nodes. While crawling an AJAX application, we define work as: Bringing the browser back into a given state and exploring the first unexplored candidate state from that state. Our proposed partition function operates as follows: After the discovery of a new state, if there are still unexplored candidate clickables left in the previous state, that state is assigned to another thread for further exploration. The processor chosen will be the one with the least amount of work left. S 2, S 3, S 4 and finishes in S 5 in a depth-first manner. Simultaneously, a new thread is branched off to explore state S 11. This new thread (thread #2) first reloads the browser to Index and after that goes into S 11. In state S 2 and S 6 this same branching mechanism happens, which results in a total of 5 threads. Now that the partition function has been introduced, the original sequential crawling algorithm (Algorithm 1) can be changed into a concurrent version.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">The Concurrent Crawling Algorithm</head><p>The concurrent crawling approach is shown in Algorithm 3. Here we briefly explain the main differences with respect to the original sequential crawling algorithm, as presented in Algorithm 1 and discussed in Section 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Global State-flow Graph</head><p>The first change is the separation of the state-flow graph from the state machine. The graph is defined in a global scope (line 3), so that it can be centralized and used by all concurrent nodes. Upon the start of the crawling process, an initial crawling node is created (line 5) and its RUN procedure is called (line 6).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Browser Pool</head><p>The robot and state machine are created for each crawling node. Thus, they are placed in the local scope of the RUN procedure (lines 10-11).</p><p>Generally, each node needs to acquire a browser instance and after the process is finished, the browser is killed. Creating new browser instances is a process-intensive and time-consuming operation. To optimize, a new structure is introduced: the BrowserPool (line 4), which creates and maintains browsers in a pool of browsers to be re-used by the crawling nodes. This reduces start-up and shut-down costs. The BrowserPool can be queried for a browser instance (line 9), and when a node is finished working, the browser used is released back to the pool.</p><p>In addition, the algorithm now takes the desired number of browsers as input. Increasing the number of browsers used can decrease the crawling runtime, but it also comes with some limitations and trade-offs that we will discuss in Section 6.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Forward-tracking</head><p>In the sequential algorithm, after finishing a crawl path, we need to bring the crawler to the previous (relevant) state. In the concurrent algorithm, however, we create a new crawling node for each path to be examined (see Figure <ref type="figure" target="#fig_9">7</ref>). Thus, instead of bringing the crawler back to the desired state (backtracking) we must take the new node forward to the desired state, hence, forward-tracking. This is done after the browser is pointed to the URL (line 12). The first time the RUN procedure is executed, there is no forward-tracking taking place, since the eventpath (i.e., the list of clickable items resulting to the desired state) is empty, so the initial crawler starts from the Index state. However, if the event-path is not empty, the clickables are used to take the browser forward to the desired state (lines 13-16). At that point, the CRAWL procedure is called (line 17).</p><p>Crawling Procedure The first part of the CRAWL procedure is unchanged (lines 21-24). To enable concurrent nodes accessing the candidate clickables in a thread-safe manner, the body of the for loop is synchronized around the candidate element to be examined (line 26). To avoid examining a candidate element multiple times by multiple nodes, each node first checks the examined state of the candidate element (line 28). If the element has not been examined previously, the robot executes an event on the element in the browser and sets its state as examined (line 31). If the state is changed, before going into the recursive CRAWL call, the PARTITION procedure is called (line 38). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Partition</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>46</head><p>). The new crawlers are initialized with two parameters, namely, (1) the current state cs (2) the execution path from the initial Index state to this state. Every new node is distributed to the work queue participating in the concurrent crawling (line 47). When a crawling node is chosen from the work queue, its corresponding RUN procedure is called in order to spawn a new crawling thread.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">TOOL IMPLEMENTATION</head><p>We have implemented the crawling concepts in a tool called CRAWLJAX. The development of CRAWLJAX originally started in 2007. There have been many extension and improvement iterations since the first release in 2008. CRAWLJAX has been used by various users and applied to a range of industrial case studies. It is released under the Apache open source license and is available for download. In 2010 alone, the tool was downloaded more than 1000 times. More information about the tool can be found on <ref type="url" target="http://crawljax.com">http://crawljax.com</ref>.</p><p>CRAWLJAX is implemented in Java. We have engineered a variety of software libraries and web tools to build and run CRAWLJAX. Here we briefly mention the main modules and libraries.</p><p>The embedded browser interface supports three browsers currently (IE, Chrome, Firefox) and has been implemented on top of the Selenium 2.0 (WebDriver) APIs. <ref type="foot" target="#foot_3">7</ref> The state-flow graph is based on the JGrapht<ref type="foot" target="#foot_4">foot_4</ref> library.</p><p>CRAWLJAX has a Plugin-based architecture. There are various extension points for different phases of the crawling process. The main interface is Plugin, which is extended by the various types of plugin available. Each plugin interface serves as an extension point that is called in a different phase of the crawling execution, e.g., preCrawlingPlugin runs before the crawling starts, OnNewStatePlugin runs when a new state is found during crawling, PostCrawlingPlugin runs after the crawling is finished. More details of the plugin extension points can be found on the project homepage.<ref type="foot" target="#foot_5">foot_5</ref> There is a growing list of plugins available for CRAWLJAX,<ref type="foot" target="#foot_6">foot_6</ref> examples of which include a static mirror generator, a test suite generator, a crawl overview generator for visualization of the crawled states, a proxy to intercept communication between client/server while crawling, and a cross-browser compatibility tester.</p><p>Through an API (CrawljaxConfiguration), the user is able to configure many crawling options such as the elements that should be examined (e.g., clicked on) during crawling, elements that should be ignored (e.g., logout), crawling depth and time, the maximum number of states to examine, the state comparison method, the plugins to be used, and the number of desired browsers that should be used during crawling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">EVALUATION</head><p>Since 2008, we and others have used CRAWLJAX for a series of crawling tasks on different types of systems. In this section, we provide an empirical assessment of some of the key properties of our crawling technique. In particular, we address the accuracy (are the results correct?), scalability (can we deal with realistic sites?), and performance, focusing in particular on the performance gains resulting from concurrent crawling.</p><p>We first present our findings concerning accuracy and scalability, for which we study six systems, described next (Section 6.1). For analyzing the performance gains from concurrent crawling, we apply CRAWLJAX to Google's ADSENSE application (Section 6.5).  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Applying CRAWLJAX</head><p>The results of applying CRAWLJAX to C1-C6 are displayed in Table <ref type="table" target="#tab_5">II</ref>. The table lists key characteristics of the sites under study, such as the average DOM size and the total number of candidate clickables. Furthermore, it lists the key configuration parameters set, most notably the tags used to identify candidate clickables, and the maximum crawling depth.</p><p>The performance measurements were obtained on a a laptop with Intel Pentium M 765 processor 1.73GHz, with 1GB RAM and Windows XP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Accuracy</head><p>6.3.1. Experimental Setup. Assessing the correctness of the crawling process is challenging for two reasons. First, there is no strict notion of "correctness" with respect to state equivalence. The state comparison operator part of our algorithm (see Section 3.4) can be implemented in different ways: the more states it considers equal, the smaller and the more abstract the resulting state-flow graph is. The desirable level of abstraction depends on the intended use of the crawler (regression testing, program comprehension, security testing, to name a few) and the characteristics of the system that is being crawled.</p><p>Second, no other crawlers for AJAX are available, making it impossible to compare our results to a "gold standard". Consequently, an assessment in terms of precision (percentage of correct states) and recall (percentage of states recovered) is impossible to give.</p><p>To address these concerns, we proceed as follows. For the cases where we have full control, C1 and C2, we inject specific clickable elements:</p><p>-For C1, 16 elements were injected, out of which 10 were on the top-level index page.</p><p>Furthermore, to evaluate the state comparison procedure, we intentionally introduced a number of identical (clone) states. -For C2, we focused on two product categories, CATS and DOGS, from the five available categories. We annotated 36 elements (product items) by modifying the JAVASCRIPT method which turns the items retrieved from the server into clickables on the interface.</p><p>Subsequently, we manually create a reference model, to which we compare the derived state-flow graph. To assess the four external sites C3-C6, we inspect a selection of the states. For each site, we randomly select 10 clickables in advance, by noting their tag names, attributes, and XPath expressions. After crawling of each site, we check the presence of these 10 elements among the list of detected clickables.</p><p>In order to do the manual inspection of the results, we run CRAWLJAX with the Mirror plugin enabled. This post-crawling plugin creates a static mirror based on the derived state-flow graph, by writing all DOM states to file, and replacing edges with appropriate hyperlinks.</p><p>6.3.2. Findings. Our results are as follows:</p><p>-For C1, all 16 expected clickables were correctly identified, leading to a precision and recall of 100% for this case. Furthermore, the clone states introduced were correctly identified as such. -For C2, 33 elements were detected correctly from the annotated 36. The three element that were not detected turn out to be invisible elements requiring multiple clicks on a scroll bar to appear. Since our default implementation avoids clicking the same element multiple times (see Section 3.5), these invisible elements cannot become visible. Hence they cannot be clicked in order to produce the required state change when the default settings are used. Note that multiple events on the same element is an option supported in the latest version of CRAWLJAX. -For C3-C6, 38 out of the 4 * 10 = 40, corresponding to 95% of the states were correctly identified. The reasons for not creating the missing two states is similar to the C2case: the automatically derived navigational flow did not permit reaching the two elements that had to be clicked in order to generate the required states.</p><p>Based on these findings, we conclude that (1) states detected by CRAWLJAX are correct; (2) duplicate states are correctly identified as such; but that (3) not all states are necessarily reached.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.">Scalability</head><p>6.4.1. Experimental Setup. In order to obtain an understanding of the scalability of our approach, we measure the time needed to crawl, as well as a number of site characteristics that will affect the time needed. We expect the crawling performance to be directly proportional to the input size, which is composed of (1) the average DOM string size, (2) number of candidate elements, and (3) number of detected clickables and states, which are the characteristics that we measure for the six cases.</p><p>To test the capability of our method in crawling real sites and coping with unknown environments, we run CRAWLJAX on four external cases C3-C6. We run CRAWLJAX with depth level 2 on C3 and C5 each having a huge state space to examine the scalability of our approach in analyzing tens of thousands of candidate clickables and finding clickables.</p><p>6.4.2. Findings. Concerning the time needed to crawl the internal sites, we see that it takes CRAWLJAX 14 and 26 seconds to crawl C1 and C2 respectively. The average DOM size in C2 is 5 times and the number of candidate elements is 3 times higher.</p><p>In addition to this increase in DOM size and in the number of candidate elements, the C2 site does not support the browser's built-in Back method. Thus, as discussed in Section 3.6, for every state change on the browser CRAWLJAX has to reload the application and click through to the previous state to go further. This reloading and clicking through naturally has a negative effect on the performance.</p><p>Note that the performance is also dependent on the CPU and memory of the machine CRAWLJAX is running on, as well as the speed of the server and network properties  of the case site. C6, for instance, is slow in reloading and retrieving updates from its server, which increases the performance measurement numbers in our experiment.</p><p>CRAWLJAX was able to run smoothly on the external sites. Except a few minor adjustments (see Section 7) we did not witness any difficulties. C3 with depth level 2 was crawled successfully in 83 minutes resulting in 19247 examined candidate elements, 1101 detected clickables, and 1071 detected states. For C5, CRAWLJAX was able to finish the crawl process in 107 minutes on 32365 candidate elements, resulting in 1554 detected clickables and 1234 states. As expected, in both cases, increasing the depth level from 1 to 2 expands the state space greatly. Section 6.5 presents our case study conducted on Google ADSENSE, which shows the scalability of the approach on a real-world industrial web application.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5.">Concurrent Crawling</head><p>In our final experiment, the main goal is to assess the influence of the concurrent crawling algorithm on the crawling runtime.</p><p>6.5.1. Experimental Object. Our experimental object for this study is Google ADSENSE , <ref type="foot" target="#foot_9">12</ref> an AJAX application developed by Google, which empowers online publishers to earn revenue by displaying relevant ads on their web content. The ADSENSE interface is built using GWT (Google Web Toolkit) components and is written in Java.</p><p>Figure <ref type="figure" target="#fig_13">8</ref> shows the index page of ADSENSE. On the top, there are four main tabs (Home, My ads, Allow &amp; block ads, Performance reports). On the top-left side, there is a box holding the anchors for the current selected tab. Underneath the left-menu box, there is a box holding links to help related pages. On the right of the left-menu we can see the main contents, which are loaded by AJAX calls.</p><p>6.5.2. Experimental Design. Our research questions can be presented as follows:</p><p>RQ1. Does our concurrent crawling approach positively influence the performance? RQ2. Is there a limit on the number of browsers that can be used to reduce the runtime?</p><p>Based on these two research questions we formulate our two null hypotheses as follows:</p><p>H1 0 . The availability of more browsers does not impact the time needed to crawl a given AJAX application. H2 0 . There is no limit on the number of browsers that can be added to reduce the runtime.</p><p>The alternative hypotheses that we use in the experiment are the following:</p><p>H1. The availability of more browsers reduces the time needed to crawl a given AJAX application. H2. There is a limit on the number of browsers that can be added to reduce the runtime.</p><p>Infrastructure To derive our experimental data we use the Google infrastructure, which offers the possibility to run our experiments either on a local workstation or on a distributed testing cluster.</p><p>On the distributed testing cluster, the number of cores varies between clusters. Newer clusters are supplied with 6 or 8 core CPU's, while older clusters include 2 or 4 cores. The job-distributor ensures a minimum of 2 Gb of memory per job at minimum. The cluster is shared between all development teams, so our experiment data were gathered while other teams were also using the cluster. To prevent starvation on the distributed testing cluster, a maximum runtime of one hour is specified, i.e., any test running longer than an hour is killed automatically.</p><p>To achieve a repeatable experiment, we initiate a new Adsense front-end with a clean database server for every experimental test. The test-data is loaded into the database during the initialization phase of the Adsense front-end.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Tool Configuration</head><p>To crawl ADSENSE, we configured CRAWLJAX 2.0 to click on all anchor-tags and fill in form inputs with custom data.</p><p>To inform the user that the interface is being updated, ADSENSE displays a loadingicon. While crawling, to determine whether the interface was finished with loading the content after each fired event, CRAWLJAX analyzed the DOM-tree to check for this icon.</p><p>Due to the infrastructure we were restricted to use Linux as our operating system and we chose Firefox 3.5 as our embedded browser.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Variables and Analysis</head><p>The independent variable in our experiment is the number of browsers used for crawling. We use the same crawl configuration for all the experiments. The only property that changes is the number of browsers used during crawling: 1-10.</p><p>The dependent variable that we measure is the time needed to crawl the given crawl specification, calculated from the start of the crawling until the (last) browser finishes. To compare, we also measure the actual number of examined clickables, crawled states, edges, and paths.</p><p>We run every experiment multiple times and take the average of the runtime. On the distributed cluster all resources are shared. Hence, to get reliable data we executed every experiment 300 times.</p><p>Since we have 10 independent samples of data with 2953 (see Table <ref type="table" target="#tab_7">III</ref>) data points, we use the One-Way ANOVA statistical method to test the first hypothesis (H1 0 ). Levene's test is used for checking the homogeneity of variances. Welch and Brown-Forsythe are used as tests of equality of means <ref type="bibr" target="#b27">[Maxwell and Delaney 2004]</ref>.</p><p>If H1 turns out to be true, we proceed with our second hypothesis. To test H2 0 , we need to compare the categories to find out which are responsible for runtime differences. Thus, we use the Post Hoc Tukey test if the population variances are equal, or ACM Transactions on the Web, Vol. 0, No. 0, Article 0, Publication date: 2011. q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q 1 2 3 4 5 the Games-Howell test if that does not turn out to be the case. We use SPSS 13 for the statistical analysis and R 14 for plotting the graphs.</p><p>6.5.3. Results and Evaluation. We present our data and analysis on the data from the distributed infrastructure. We obtained similar results with different configurations. Our experimental data can be found on the following link. 15  Figures 9-11 depict boxplots of the detected states, detected edges, and runtime (minutes) respectively versus the number of browsers used during crawling, on the distributed infrastructure. The number of detected states and edges is constant, which means our multi-browser crawling and state exploration is stable.</p><p>13 <ref type="url" target="http://www.spss.com">http://www.spss.com</ref> 14 <ref type="url" target="http://www.r-project.org">http://www.r-project.org</ref>  Crawling AJAX-based Web Applications through Dynamic Analysis of User Interface State Changes0:23 q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q 1 2 3  Figure <ref type="figure" target="#fig_19">11</ref> shows that there is a decrease in the runtime when the number of browsers is increased.  q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q 1 2 3 Table <ref type="table" target="#tab_9">IV</ref> shows the main ANOVA result. The significance value comparing the groups is &lt; .05. The significance value for homogeneity of variances, as shown in Table <ref type="table" target="#tab_10">V</ref>, is &lt; .05, which means the variances are significantly different. The Welch and Brown-Forsythe are both 0, so we can reject the first null hypothesis. Thus, we can conclude that our concurrent crawling approach positively influences the performance.</p><p>To test for the second hypothesis, we need to compare the groups to find out if the differences between them is significant. Table <ref type="table" target="#tab_11">VI</ref> shows our post hoc test results. a * means that the difference in runtime is significant. It is evident that there is a limit on the number of browsers that can significantly decrease the runtime. The optimal number for our ADSENSE study is 5 browsers. By increasing the number of browsers from 1 to 5, we can achieve a decrease of up to 65% in runtime. Increasing the number of browsers beyond 5 has no significant influence on the runtime in our current impementation.  As far as the repeatability of the studies is concerned, CRAWLJAX is open source and publicly available for download. The experimental applications in Section 6.1 are composed of open source and public domain websites. In the concurrent crawling experiment (Section 6.5), the study was done at Google using Google's ADSENSE, which is also publicly accessible. More case studies are required to generalize the findings on correctness and scalability. One concern with using public domain web applications as benchmarks is that they can change and evolve over time, making the results of the study irreproducible in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">DISCUSSION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.">Detecting DOM Changes</head><p>An interesting observation in C2 in the beginning of the experiment was that every examined candidate element was detected as a clickable. Further investigation revealed that this phenomenon was caused by a piece of JAVASCRIPT code (banner), which constantly changed the DOM-tree with textual notifications. Hence, every time a DOM comparison was done, a change was detected. We had to use a higher similarity threshold so that the textual changes caused by the banner were not seen as a relevant state change for detecting clickables. In CRAWLJAX, it is also possible to ignore certain parts of the DOM-tree through, for instance, regular expressions that capture the recurring patterns. How the notion of a dynamic state change is defined can poten- tially influence the crawling behaviour. The automatic crawler ignores subtle changes in the DOM that we believe are not of significant importance (such as case sensitivity and timestamps). We also provide the user with different mechanisms to define their own notion of state similarity. Push-based techniques such as Comet <ref type="bibr" target="#b40">[Russell 2006</ref>] in which data is constantly pushed from the server could also cause state comparison challenges. Such push-based updates are usually confined to a specific part of the DOM tree and hence can be controlled using custom DOM change filters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.">Back and forward tracking</head><p>Because of side effects of back-end state changes, there is no guarantee that we reach the exact same state when we traverse a click path a second time. This nondeterminism characteristic is inherent in dynamic web applications. Our crawler uses the notion of state similarity, thus as long as the revisited state is similar to the state visited before, the crawling process continues without side-effects. In our experiments, we did not encounter any problems with this non-deterministic behaviour.</p><p>When the crawling approach is used for testing web applications, one way to ensure that a state revisited is the same as the state previously visited (e.g., for regression testing <ref type="bibr" target="#b39">[Roest et al. 2010]</ref>), is by bringing the server-side state to the previous state as well, which could be challenging. More research is needed to adopt ways of synchronizing the client and server side state during testing.</p><p>Cookies can also cause some problems in crawling AJAX applications. C3 uses Cookies to store the state of the application on the client. With Cookies enabled, when CRAWLJAX reloads the application to navigate to a previous state, the application does not start in the expected initial state. In this case, we had to disable Cookies to perform a correct crawling process. The new features of HTML5 such as web storage <ref type="bibr" target="#b21">[Hickson 2011</ref>] could possibly cause the same problems by making parts of the client-side state persistent between backtracking sessions. It would be interesting future work to explore ways to get around these issues.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3.">State Space</head><p>The set of found states and the inferred state machine is not complete i.e., CRAWLJAX creates an instance of the state machine of the AJAX application but not necessarily the instance. Any crawler can only crawl and index a snapshot instance of a dynamic web application in a given point of time. The order in which clickables are chosen could generate different states. Even executing the same clickable twice from an state could theoretically produce two different DOM states depending on, for instance, server-side factors.</p><p>The number of possible states in the state space of almost any realistic web application is huge and can cause the well-know state explosion problem <ref type="bibr" target="#b41">[Valmari 1998</ref>]. Just as a traditional web crawler, CRAWLJAX provides the user with a set of configurable options to constrain the state space such as the maximum search depth level, the similarity threshold, maximum number of states per domain, maximum crawling time, and the option of ignoring external links (i.e., different domains) and links that match some predefined set of regular expressions, e.g., mail:*, *.ps, *.pdf.</p><p>The current implementation of CRAWLJAX keeps the DOM states in the memory. As an optimization step, next to the multi-browser crawling, we intend to abstract and serialize the DOM state into the file system and only keep a reference in the memory. This saves much space in the memory and enables us to handle much more states. With a cache mechanism, the essential states for analysis can be kept in the memory while the other ones can be retrieved from the file system when needed in a later stage. Determining when a DOM is fully loaded into the browser after a request or an event is a very difficult task. Partially loaded DOM states can adversely influence the state exploration accuracy during crawling. The asynchronous nature of AJAX calls and the dynamic DOM updates make the problem even more challenging to handle. Since the major browsers currently do not provide APIs to determine when a DOM-tree is fully loaded, we wait a specific amount of time after each event or page reload. This waiting time can be adjusted by the user through the CRAWLJAX configuration API. By choosing a high enough waiting time, we can be certain that the DOM is fully settled in the browser. As a side effect, a too high waiting period, can make the crawling process slow.</p><p>An alternative way that is more reliable is when the web application provides a completion flag in the form of a DOM element either visible or invisible to the end user. Before continuing with its crawling operations, CRAWLJAX can be configured to wait for that specific DOM flag to appear after each state transition. This flag-based waiting approach is in fact what we used during our experiments on ADSENSE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.5.">Applications of CRAWLJAX</head><p>As mentioned in the introduction, we believe that the crawling and generating capabilities of our approach have many applications for modern web applications.</p><p>We believe that the crawling techniques that are part of our solution can serve as a starting point and be adopted by general search engines to expose the hidden-web content induced by JAVASCRIPT in general and AJAX in particular.</p><p>In their proposal for making AJAX applications crawlable, 16 Google proposes using URLs containing a special hash fragment, i.e., #!, for identifying dynamic content. Google then uses this hash fragment to send a request to the server. The server has to treat this request in a special way and send a HTML snapshot of the dynamic content, which is then processed by Google's crawler. In the same proposal, they suggest using CRAWLJAX for creating a static snapshot for this purpose. Web developers can use the model inferred by CRAWLJAX to automatically generate a static HTML snapshot of their dynamic content, which then can be served to Google for indexing.</p><p>The ability to automatically detect and exercise the executable elements of an AJAX site and navigate between the various dynamic states gives us a powerful web analysis and test automation mechanism. In the recent past, we have applied CRAWLJAX in the following web testing domains: (1) invariant-based testing of AJAX user interfaces <ref type="bibr" target="#b34">[Mesbah and van Deursen 2009]</ref>, (2) spotting security violations in web widget interactions <ref type="bibr" target="#b6">[Bezemer et al. 2009]</ref> (3) regression testing of dynamic and non-deterministic web interfaces <ref type="bibr" target="#b39">[Roest et al. 2010]</ref>, and (4) automated cross-browser compatibility testing <ref type="bibr" target="#b32">[Mesbah and Prasad 2011]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">RELATED WORK</head><p>Web crawlers, also known as web spiders and (ro)bots, have been studied since the advent of the web itself <ref type="bibr" target="#b36">[Pinkerton 1994;</ref><ref type="bibr" target="#b20">Heydon and Najork 1999;</ref><ref type="bibr" target="#b12">Cho et al. 1998;</ref><ref type="bibr" target="#b8">Brin and Page 1998;</ref><ref type="bibr" target="#b9">Burner 1997</ref>].</p><p>More recently, there has been extensive research, on the hidden-web behind forms <ref type="bibr" target="#b38">[Raghavan and Garcia-Molina 2001;</ref><ref type="bibr" target="#b14">de Carvalho and Silva 2004;</ref><ref type="bibr" target="#b22">Lage et al. 2004;</ref><ref type="bibr" target="#b35">Ntoulas et al. 2005;</ref><ref type="bibr" target="#b5">Barbosa and Freire 2007;</ref><ref type="bibr" target="#b13">Dasgupta et al. 2007;</ref><ref type="bibr" target="#b24">Madhavan et al. 2008]</ref>. The main focus in this research area is to detect ways of accessing the web content behind data entry points. On the contrary, the hidden-web induced as a result of client-side scripting in general and AJAX in particular has gained very little attention so far. <ref type="bibr">Alvares et al. [2004;</ref><ref type="bibr" target="#b40">2006]</ref> discuss some challenges of crawling hidden content generated with JAVASCRIPT, but focus on hypertext links.</p><p>To the best of our knowledge, our initial work on CRAWLJAX <ref type="bibr">[Mesbah et al. 2008</ref>] in 2008 was the first academic research work proposing a solution to the problem of crawling AJAX, in the form of algorithms and an open source tool that automatically crawls and creates a finite state-machine of the states and transitions.</p><p>In 2009, <ref type="bibr" target="#b16">Duda et al. [2009]</ref> discussed how AJAX states could be indexed. The authors present a crawling and indexing algorithm. Their approach also builds finite state models of AJAX applications, however, there is no accompanying tool available for comparison. The main difference between their algorithm and ours seems to be in the way clickable elements are detected, which is through JAVASCRIPT analysis.</p><p>The work of <ref type="bibr" target="#b29">Memon et al. [2001;</ref><ref type="bibr" target="#b28">2003]</ref> on GUI Ripping for testing purposes is related to our work in terms of how they reverse engineer an event-flow graph of desktop GUI applications by applying dynamic analysis techniques.</p><p>There also exists a large body of knowledge targeting challenges in parallel and distributed computing. Specifically for the web, <ref type="bibr" target="#b11">Cho and Garcia-Molina [2002]</ref> discuss the challenges of parallel crawling and propose an architecture for parallel crawling the classical web. <ref type="bibr" target="#b7">Boldi et al. [2004]</ref> present the design and implementation of UbiCrawler, a distributed web crawling tool. Note that these works are URL-based and as such not capable of targeting event-based AJAX applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.">CONCLUDING REMARKS</head><p>Crawling modern AJAX-based web systems requires a different approach than the traditional way of extracting hypertext links from web pages and sending requests to the server.</p><p>This paper proposes an automated crawling technique for AJAX-based web applications, which is based on dynamic analysis of the client-side web user interface in embedded browsers. The main contributions of our work are:</p><p>-An analysis of the key challenges involved in crawling AJAX-based applications; -A systematic process and algorithm to drive an AJAX application and infer a state machine from the detected state changes and transitions. Challenges addressed include the identification of clickable elements, the detection of DOM changes, and the construction of the state machine; -A concurrent multi-browser crawling algorithm to improve the runtime performance; -The open source tool called CRAWLJAX, which implements the crawling algorithms; -Two studies, including seven AJAX applications, used to evaluate the effectiveness, correctness, performance, and scalability of the proposed approach.</p><p>Although we have been focusing on AJAX in this paper, we believe that the approach could be applied to any DOM-based web application.</p><p>The fact that the tool is freely available for download will help to identify exciting case studies. Furthermore, strengthening the tool by extending its functionality, improving the accuracy, performance, and the state explosion algorithms are directions we foresee for future work. We will conduct controlled experiments to systematically analyze and find new ways of optimizing the back tracking algorithm and implementation. Many AJAX applications use hash fragments in URLs nowadays. Investigating how such hash fragments can be utilized during crawling is another interesting direction. Exploring the hidden-web induced by client-site JAVASCRIPT using CRAWLJAX</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>to achieve a high level of user interactivity. Highly visible examples include Gmail and Google Docs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>1. User Interface State and State Changes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 :</head><label>2</label><figDesc>Fig. 2: The state-flow graph visualization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 :</head><label>3</label><figDesc>Fig. 3: Processing view of the crawling architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 4 :</head><label>4</label><figDesc>Fig. 4: State Machine Optimization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>E ← sm.GETPATHTO(ps) for e ∈ E do re ← RESOLVEELEMENT(e) robot.ENTERFORMVALUES(re) robot.FIREEVENT(re) return else cs ← ps</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>4Fig. 5 :</head><label>5</label><figDesc>Fig. 5: Backtracking to the previous relevant state.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 7</head><label>7</label><figDesc>visualizes our partition function for concurrent crawling of a simple web application. In the Index state, two candidate clickables are detected that can lead to: S 1 and S 11. The initial thread continues with the exploration of the states S 1, ACM Transactions on the Web, Vol. 0, No. 0, Article 0, Publication date: 2011. SERG Mesbah et. al. -Crawling Ajax-based Web Applications TUD-SERG-2011-033 0:14 Mesbah et al.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>Procedure The partition procedure, called on a particular state cs (line 44), creates a new crawling node for every unexamined candidate clickable in cs (line TUD-SERG-2011-033 Crawling AJAX-based Web Applications through Dynamic Analysis of User Interface State Changes0:15 ALGORITHM 3: Concurrent AJAX Crawling input : URL, tags, browserType, nrOfBrowsers Procedure MAIN() begin global sf g ← INITSTATEFLOWGRAPH() global browserP ool ← INITBROWSERPOOL(nrOf Browsers, browserT ype) crawlingN ode ← CRAWLINGNODE() crawlingN ode.RUN(null, null) Procedure RUN(State s, EventPath ep) begin browser ← browserP ool.GETEMBEDDEDBROWSER() robot ← INITROBOT() sm ← INITSTATEMACHINE(sf g) browser.GOTO(U RL) for e ∈ ep do re ← RESOLVEELEMENT(e) robot.ENTERFORMVALUES(re) robot.FIREEVENT(re) CRAWL(s, browser, robot, sm) Procedure CRAWL(State ps, EmbeddedBrowser browser, Robot robot, StateMachine sm) begin cs ← sm.GETCURRENTSTATE() ∆update ← DIFF(ps, cs) f ← ANALYSEFORMS(∆update) Set C ← GETCANDIDATECLICKABLES(∆update, tags, f ) for c ∈ C do SYNCH(c) begin if cs.NOTEXAMINED(c) then robot.ENTERFORMVALUES(c) robot.FIREEVENT(c) cs.EXAMINED(c) dom ← browser.GETDOM() if STATECHANGED(cs.GETDOM(), dom) then xe ← GETXPATHEXPR(c) ns ← sm.ADDSTATE(dom) sm.ADDEDGE(cs, ns, EVENT(c, xe)) sm.CHANGETOSTATE(ns) PARTITION(cs) if STATEALLOWEDTOBECRAWLED(ns) then CRAWL(cs) sm.CHANGETOSTATE(cs) Procedure PARTITION(State cs) begin while SIZEOF(cs.NOTEXAMINEDCLICKABLES()) &gt; 0 do crawlingN ode ← CRAWLINGNODE(cs, GETEXACTPATH()) DISTRIBUTEPARTITION(crawlingN ode) ACM Transactions on the Web, Vol. 0, No. 0, Article 0, Publication date: 2011.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 8 :</head><label>8</label><figDesc>Fig. 8: Google ADSENSE.</figDesc><graphic coords="22,123.24,150.29,368.57,120.06" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Fig. 9 :</head><label>9</label><figDesc>Fig.9: Boxplots of detected states versus the number of browsers. Distributed setting (300 measurements for each category of browsers).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head></head><label></label><figDesc>15 http://tinyurl.com/3d5km3b ACM Transactions on the Web, Vol. 0, No. 0, Article 0, Publication date: 2011. Mesbah et. al. -Crawling Ajax-based Web Applications SERG 22 TUD-SERG-2011-033</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Fig. 10 :</head><label>10</label><figDesc>Fig. 10: Boxplots of detected edges versus the number of browsers. Distributed setting (300 measurements for each category of browsers).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Fig. 11 :</head><label>11</label><figDesc>Fig. 11: Boxplots of runtime versus the number of browsers. Distributed setting (300 measurements for each category of browsers).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head></head><label></label><figDesc>16 http://code.google.com/web/ajaxcrawling/docs/getting-started.html ACM Transactions on the Web, Vol. 0, No. 0, Article 0, Publication date: 2011. SERG Mesbah et. al. -Crawling Ajax-based Web Applications TUD</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Crawling AJAX-based Web Applications through Dynamic Analysis of User Interface State Changes0:5</figDesc><table /><note><p>2) if ( e . addEventListener ) { e . addEventListener ( ' click ' , handler , false ) } else if ( e . attachEvent ) { // IE e . attachEvent ( ' onclick ' , handler ) }</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Each (v 1 , v 2 ) ∈ Erepresents a clickable c connecting two states if and only if state v 2 is reached by executing c in state v 1 .</figDesc><table><row><cell>0:6</cell><cell></cell><cell></cell><cell>Mesbah et al.</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Index</cell></row><row><cell cols="2">&lt;click, xpath://DIV[1]/SPAN[4]&gt;</cell><cell cols="2">&lt;mouseover, id:c_9&gt;</cell><cell>&lt;click, xpath://A[2]&gt;</cell></row><row><cell>S_1</cell><cell></cell><cell></cell><cell>S_2</cell><cell>&lt;click, xpath://DIV[3]/IMG[1]&gt;</cell></row><row><cell>&lt;click, id:c_3&gt;</cell><cell cols="3">&lt;mouseover, xpath://SPAN[2]/A[2]&gt;</cell></row><row><cell>S_4</cell><cell></cell><cell></cell><cell>S_3</cell></row><row><cell cols="3">&lt;click, xpath://SPAN[2]/A[3]&gt;</cell><cell>&lt;click, xpath://SPAN[2]/A[3]&gt;</cell></row><row><cell>S_5</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>-Finite State Machine: The finite state machine is a data component maintaining the state-flow graph, as well as a pointer to the state being currently crawled.</figDesc><table><row><cell cols="2">Mesbah et. al. -Crawling Ajax-based Web Applications</cell><cell></cell><cell></cell><cell>SERG</cell></row><row><cell cols="5">Crawling AJAX-based Web Applications through Dynamic Analysis of User Interface State Changes0:7</cell></row><row><cell>Robot</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>UI</cell><cell></cell><cell></cell></row><row><cell></cell><cell>event</cell><cell></cell><cell cols="2">Embedded</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Browser</cell></row><row><cell>generate event</cell><cell>update</cell><cell>event</cell><cell></cell></row><row><cell>Crawljax Controller</cell><cell cols="2">DOM</cell><cell>update event</cell><cell>Engine Ajax</cell></row><row><cell>Analyze Dom</cell><cell></cell><cell></cell><cell></cell><cell>Legend Access</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Control flow</cell></row><row><cell>DOM Analyzer</cell><cell>update</cell><cell>State Machine</cell><cell></cell><cell>Data component Event invocation</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Processing component</cell></row><row><cell>6</cell><cell></cell><cell></cell><cell></cell><cell>TUD-SERG-2011-033</cell></row></table><note><p>ACM Transactions on the Web, Vol. 0, No. 0, Article 0, Publication date: 2011.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>4 global robot ← INITROBOT() 5 global sm ← INITSTATEMACHINE() 6 CRAWL(null) 7 Procedure CRAWL(State ps) 8 begin 9 cs ← sm.GETCURRENTSTATE() 10 ∆update ← DIFF(ps, cs)</head><label></label><figDesc></figDesc><table><row><cell cols="2">ALGORITHM 1: Crawling AJAX</cell></row><row><cell cols="2">input : URL, tags, browserType 1 Procedure MAIN() 2 begin 3 global browser ← INITEMBEDDEDBROWSER(U RL, browserT ype)</cell></row><row><cell>11 12 13 14</cell><cell>f ← ANALYSEFORMS(∆update) Set C ← GETCANDIDATECLICKABLES(∆update, tags, f ) for c ∈ C do robot.ENTERFORMVALUES(c)</cell></row><row><cell>15</cell><cell>robot.FIREEVENT(c)</cell></row><row><cell>16 17 18 19 20</cell><cell>dom ← browser.GETDOM() if STATECHANGED(cs.GETDOM(), dom) then xe ← GETXPATHEXPR(c) ns ← sm.ADDSTATE(dom) sm.ADDEDGE(cs, ns, EVENT(c, xe))</cell></row><row><cell>21 22 23</cell><cell>sm.CHANGETOSTATE(ns) if STATEALLOWEDTOBECRAWLED(ns) then CRAWL(cs)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table II :</head><label>II</label><figDesc>Results of running CRAWLJAX on 6 AJAX applications.</figDesc><table><row><cell>Case</cell><cell>DOM string size (byte)</cell><cell>Candidate Clickables</cell><cell>Detected Clickables</cell><cell>Detected States</cell><cell>Crawl Time (s)</cell><cell>Depth</cell><cell>Tags</cell></row><row><cell>C1 C2 C3 C4 C5 C6</cell><cell>4590 24636 262505 40282 165411 134404</cell><cell cols="2">540 1813 150 19247 1101 16 33 148 3808 55 267 267 32365 1554 6972 83</cell><cell cols="2">16 34 148 1071 5012 14 26 498 56 77 145 806 1234 6436 79 701</cell><cell cols="2">3 A, DIV, SPAN, IMG 2 A, IMG 1 A 2 A, TD 2 A, DIV, INPUT, IMG 1 A 2 A, DIV 1 A, DIV</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Crawling AJAX-based Web Applications through Dynamic Analysis of User Interface State Changes0:19</figDesc><table /><note><p>TUD-SERG-2011-033</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table III :</head><label>III</label><figDesc>Descriptive statistics of the runtime (in minutes) for the 10 categories of browsers. * 95% Confidence Interval for Mean.</figDesc><table><row><cell></cell><cell>N</cell><cell>Mean</cell><cell>Std. Dev.</cell><cell>Std. Err.</cell><cell>Lower Bound*</cell><cell>Upper Bound*</cell><cell>Min</cell><cell>Max</cell></row><row><cell>1 2 3 4 5 6 7 8 9 10 Total</cell><cell>2953 299 295 297 291 290 294 297 292 299 299</cell><cell>7.4871 5.4721 5.5521 5.6404 5.6744 5.6920 5.9806 6.2338 7.4651 9.9243 17.0613</cell><cell>3.70372 1.14160 1.26716 1.11101 1.15004 .99718 1.05696 1.03936 1.04823 1.28598 2.31540</cell><cell>.06816 .06602 .07378 .06447 .06742 .05856 .06164 .06031 .06134 .07437 .13390</cell><cell>7.3535 5.3422 5.4069 5.5135 5.5417 5.5767 5.8593 6.1151 7.3444 9.7779 16.7978</cell><cell>7.6207 5.6020 5.6973 5.7673 5.8070 5.8072 6.1020 6.3525 7.5858 10.0707 17.3249</cell><cell>2.33 2.33 3.38 3.78 3.69 4.20 4.13 4.74 5.35 7.05 10.17</cell><cell>22.15 10.55 11.83 11.12 10.75 11.01 10.72 10.32 10.56 13.37 22.15</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>Table III presents the descriptive statistics of the runtime for the 10 categories of browsers. ACM Transactions on the Web, Vol. 0, No. 0, Article 0, Publication date: 2011.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table IV :</head><label>IV</label><figDesc>One-way ANOVA.</figDesc><table><row><cell>Between Groups Within Groups Total</cell><cell>Sum of Squares 35540.225 4953.987 2943 df Mean Square 9 3948.914 2345.919 000 F Sig. 1.683 40494.212 2952</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table V :</head><label>V</label><figDesc>Tests for Homogeneity of Variances and Equality of Means.</figDesc><table><row><cell>Method Levene Welch Brown-Forsythe 2355.528 Statistic df1 55.884 9 1060.017 9 1198.048 0.000 df2 Sig. 2943 0.000 9 1914.845 0.000</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table VI :</head><label>VI</label><figDesc>Post Hoc Games-Howell multiple comparisons. * indicates the mean difference is significant at the 0.05 level.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head></head><label></label><figDesc>TUD-SERG-2011-033 Crawling AJAX-based Web Applications through Dynamic Analysis of User Interface State Changes0:27</figDesc><table><row><cell>7.4. DOM Settling</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_0"><p>http://jquery.com ACM Transactions on the Web, Vol. 0, No. 0, Article 0, Publication date: 2011.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1"><p>ACM Transactions on the Web, Vol. 0, No. 0, Article 0, Publication date: 2011.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_2"><p>Mesbah et. al. -Crawling Ajax-based Web Applications TUD-SERG-2011-033</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_3"><p>http://code.google.com/p/selenium/wiki/GettingStarted</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_4"><p>http://jgrapht.sourceforge.net</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_5"><p>http://crawljax.com/documentation/writing-plugins/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_6"><p>http://crawljax.com/plugins/ ACM Transactions on the Web, Vol. 0, No. 0, Article 0, Publication date: 2011.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11" xml:id="foot_7"><p>http://java.sun.com/developer/releases/petstore/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_8"><p>ACM Transactions on the Web, Vol. 0, No. 0, Article 0, Publication date: 2011. SERG Mesbah et. al. -Crawling Ajax-based Web Applications TUD-SERG-2011-033</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12" xml:id="foot_9"><p>https://www.google.com/adsense/ ACM Transactions on the Web, Vol. 0, No. 0, Article 0, Publication date: 2011.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>TUD-SERG-2011-033 <rs type="projectName">Crawling AJAX-based Web Applications through Dynamic Analysis of User Interface State Changes0:29</rs></p><p>and continuing with automated web analysis and testing are other application domains we will be working on.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_qjW8jFG">
					<orgName type="project" subtype="full">Crawling AJAX-based Web Applications through Dynamic Analysis of User Interface State Changes0:29</orgName>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Crawling AJAX-based Web Applications through Dynamic Analysis of User Interface State Changes0:17 blindtextgenerator. com &lt;input type="radio" value="7" name="radioTextname" class="js-textname iradio" id="idRadioTextname-EN-li-europan"/&gt; &lt;a id="idSelectAllText" title="Select all" href="#"&gt; C5 site.snc.tudelft.nl &lt;div class="itemtitlelevel1 itemtitle" id="menuitem 189 e"&gt;organisatie&lt;/div&gt; &lt;a href="#" onclick="ajaxNews('524')"&gt;...&lt;/a&gt; C6 www.gucci.com &lt;a onclick="Shop.selectSort(this); return false" class="booties" href="#"&gt;booties&lt;/a&gt; &lt;div class="darkening"&gt;...&lt;/div&gt;</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Subject Systems</head><p>In order to assess the accuracy (Section 6.3) and scalability (Section 6.4), we study the six systems C1-C6 listed in Table <ref type="table">I</ref>. For each case, we show the site under study, as well as a selection of typical clickable elements. We selected these sites because they adopt AJAX to change the state of the application, using JAVASCRIPT, assigning events to HTML elements, asynchronously retrieving delta updates from the server, and performing partial updates on the DOM-tree.</p><p>The first site C1 in our case study is an AJAX test site developed internally by our group using the jQuery AJAX library. Although the site is small, it is a case where we are in full control of the AJAX features used, allowing us to introduce different types of dynamically set clickables as shown in Figure <ref type="figure">1</ref> and<ref type="figure">Table I</ref>.</p><p>Our second case, C2, is Sun's Ajaxified PETSTORE 2.0 11 which is built on Java ServerFaces and the Dojo AJAX toolkit. This open-source web application is designed to illustrate how the Java EE Platform can be used to develop an AJAX-enabled Web 2.0 application and adopts many advanced rich AJAX components.</p><p>The other four cases are all real-world external public AJAX applications. Thus, we have no access to their source-code. C4 is an AJAX-based application that can function as a tool for comparing the visual impression of different typefaces. C3 (online shop), C5 (sport center), and C6 (Gucci) are all single-page commercial applications with numerous clickables and dynamic states.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Crawling AJAX-based Web Applications through Dynamic Analysis of User Interface State</title>
		<idno>Changes0:21 TUD-SERG-2011-033</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Crawling AJAX-based Web Applications through Dynamic Analysis of User Interface State Changes0:25 REFERENCES</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Crawling web pages with support for clientside dynamism</title>
		<author>
			<persName><forename type="first">M</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Raposo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hidalgo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Web-Age Information Management</title>
		<title level="s">Lecture Notes in Computer Science Series</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">4016</biblScope>
			<biblScope unit="page" from="252" to="262" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Client-side deep web data extraction</title>
		<author>
			<persName><forename type="first">M</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Raposo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vina</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CEC-EAST &apos;04: Proceedings of the IEEE International Conference on E-Commerce Technology for Dynamic E-Business</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="158" to="161" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Adding usability to web engineering models and tools</title>
		<author>
			<persName><forename type="first">R</forename><surname>Atterer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Schmidt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th International Conferencee on Web Engineering (ICWE&apos;05)</title>
		<meeting>the 5th International Conferencee on Web Engineering (ICWE&apos;05)</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="36" to="41" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">An adaptive crawler for locating hidden-web entry points</title>
		<author>
			<persName><forename type="first">L</forename><surname>Barbosa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Freire</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW &apos;07: Proceedings of the 16th international conference on World Wide Web</title>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="441" to="450" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Automated security testing of web widget interactions</title>
		<author>
			<persName><forename type="first">C.-P</forename><surname>Bezemer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mesbah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Van Deursen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th joint meeting of the European Software Engineering Conference and the ACM SIGSOFT symposium on the Foundations of Software Engineering (ESEC-FSE&apos;09)</title>
		<meeting>the 7th joint meeting of the European Software Engineering Conference and the ACM SIGSOFT symposium on the Foundations of Software Engineering (ESEC-FSE&apos;09)</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="81" to="91" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Ubicrawler: A scalable fully distributed web crawler</title>
		<author>
			<persName><forename type="first">P</forename><surname>Boldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Codenotti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Santini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vigna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Software: Practice and Experience</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="711" to="726" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The anatomy of a large-scale hypertextual Web search engine</title>
		<author>
			<persName><forename type="first">S</forename><surname>Brin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Page</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Netw. ISDN Syst</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="107" to="117" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Crawling towards eternity: Building an archive of the world wide web</title>
		<author>
			<persName><forename type="first">M</forename><surname>Burner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Web Techniques Magazine</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="37" to="40" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Change detection in hierarchically structured information</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Chawathe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rajaraman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Garcia-Molina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Widom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD &apos;96: Proceedings of the 1996 ACM SIGMOD international conference on Management of data</title>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="493" to="504" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Parallel crawlers</title>
		<author>
			<persName><forename type="first">J</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Garcia-Molina</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th international conference on World Wide Web</title>
		<meeting>the 11th international conference on World Wide Web</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="124" to="135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Efficient crawling through URL ordering. Computer Networks and ISDN Systems</title>
		<author>
			<persName><forename type="first">J</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Garcia-Molina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Page</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="161" to="172" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The discoverability of the web</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dasgupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Olston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pandey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tomkins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW &apos;07: Proceedings of the 16th international conference on World Wide Web</title>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="421" to="430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Smartcrawl: a new strategy for the exploration of the hidden web</title>
		<author>
			<persName><surname>De</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Carvalho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">S</forename><surname>Silva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WIDM &apos;04: Proceedings of the 6th annual ACM international workshop on Web information and data management</title>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="9" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A note on two problems in connexion with graphs</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">W</forename><surname>Dijkstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Numerische Mathematik</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="269" to="271" />
			<date type="published" when="1959">1959</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Ajax crawl: making Ajax applications searchable</title>
		<author>
			<persName><forename type="first">C</forename><surname>Duda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Frey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kossmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Matter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">25th International Conference on Data Engineering (ICDE&apos;09)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="78" to="89" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Principled design of the modern Web architecture</title>
		<author>
			<persName><forename type="first">R</forename><surname>Fielding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">N</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Inter. Tech. (TOIT)</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="115" to="150" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Parallel state space construction for modelchecking. Model Checking Software 2057</title>
		<author>
			<persName><forename type="first">H</forename><surname>Garavel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mateescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Smarandache</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="217" to="234" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Garrett</surname></persName>
		</author>
		<ptr target="http://www.adaptivepath.com/publications/essays/archives/000385.php" />
		<title level="m">Ajax: A new approach to web applications. Adaptive path</title>
		<imprint>
			<date type="published" when="2005-02">February 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Mercator: A scalable, extensible web crawler</title>
		<author>
			<persName><forename type="first">A</forename><surname>Heydon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Najork</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">World Wide Web</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="219" to="229" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">I</forename><surname>Hickson</surname></persName>
		</author>
		<ptr target="http://dev.w3.org/html5/webstorage/" />
		<title level="m">W3C Web Storage</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Automatic generation of agents for collecting hidden web pages for data extraction</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Lage</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">B</forename><surname>Golgher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H F</forename><surname>Laender</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Knowl. Eng</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="page" from="177" to="196" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">L</forename><surname>Levenshtein</surname></persName>
		</author>
		<title level="m">Binary codes capable of correcting deletions, insertions, and reversals. Cybernetics and Control Theory</title>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="707" to="710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Google&apos;s deep web crawl</title>
		<author>
			<persName><forename type="first">J</forename><surname>Madhavan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ganapathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rasmussen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Halevy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. VLDB Endow</title>
		<meeting>VLDB Endow</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1241" to="1252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on the Web</title>
		<imprint>
			<biblScope unit="volume">0</biblScope>
			<biblScope unit="issue">0</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
	<note>Publication date</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Crawling Ajax-based Web Applications</title>
		<author>
			<persName><surname>Serg Mesbah</surname></persName>
		</author>
		<idno>TUD-SERG-2011-033 0:30</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Designing experiments and analyzing data: A model comparison perspective</title>
		<author>
			<persName><forename type="first">S</forename><surname>Maxwell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Delaney</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
			<publisher>Lawrence Erlbaum</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">GUI ripping: Reverse engineering of graphical user interfaces for testing</title>
		<author>
			<persName><forename type="first">A</forename><surname>Memon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nagarajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">10th Working Conference on Reverse Engineering (WCRE&apos;03)</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="260" to="269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Coverage criteria for GUI testing</title>
		<author>
			<persName><forename type="first">A</forename><surname>Memon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Soffa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Pollack</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings ESEC/FSE&apos;01</title>
		<meeting>ESEC/FSE&apos;01</meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="256" to="267" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Crawling Ajax by inferring user interface state changes</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mesbah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Bozdag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Van Deursen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International Conference on Web Engineering (ICWE&apos;08)</title>
		<meeting>the 8th International Conference on Web Engineering (ICWE&apos;08)</meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="122" to="134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Migrating multi-page web applications to single-page Ajax interfaces</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mesbah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Van Deursen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 11th Eur. Conf. on Sw. Maintenance and Reengineering (CSMR&apos;07)</title>
		<meeting>11th Eur. Conf. on Sw. Maintenance and Reengineering (CSMR&apos;07)</meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="181" to="190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Automated cross-browser compatibility testing</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mesbah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Prasad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd ACM/IEEE International Conference on Software Engineering (ICSE&apos;11)</title>
		<meeting>the 33rd ACM/IEEE International Conference on Software Engineering (ICSE&apos;11)</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="561" to="570" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A component-and push-based architectural style for Ajax applications</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mesbah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Van Deursen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Systems and Software</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="page" from="2194" to="2209" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Invariant-based automatic testing of Ajax user interfaces</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mesbah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Van Deursen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Software Engineering (ICSE&apos;09)</title>
		<meeting>the 31st International Conference on Software Engineering (ICSE&apos;09)</meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="210" to="220" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Downloading textual hidden web content through keyword queries</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ntoulas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zerfos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">JCDL &apos;05: Proceedings of the 5th ACM/IEEE-CS joint conference on Digital libraries</title>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="100" to="109" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Finding what people want: Experiences with the web crawler</title>
		<author>
			<persName><forename type="first">B</forename><surname>Pinkerton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second International World Wide Web Conference</title>
		<meeting>the Second International World Wide Web Conference</meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="volume">94</biblScope>
			<biblScope unit="page" from="17" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Pixley</surname></persName>
		</author>
		<ptr target="http://www.w3.org/TR/DOM-Level-2-Events/" />
		<title level="m">W3C Document Object Model (DOM) Level 2 Events Specification</title>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Crawling the hidden web</title>
		<author>
			<persName><forename type="first">S</forename><surname>Raghavan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Garcia-Molina</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">VLDB &apos;01: Proceedings of the 27th International Conference on Very Large Data Bases</title>
		<imprint>
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="129" to="138" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Regression testing Ajax applications: Coping with dynamism</title>
		<author>
			<persName><forename type="first">D</forename><surname>Roest</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mesbah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Van Deursen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd International Conference on Software Testing, Verification and Validation</title>
		<meeting>the 3rd International Conference on Software Testing, Verification and Validation</meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="128" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Comet: Low latency data for the browser</title>
		<author>
			<persName><forename type="first">A</forename><surname>Russell</surname></persName>
		</author>
		<ptr target="http://alex.dojotoolkit.org/?p=545" />
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">The state explosion problem</title>
		<author>
			<persName><forename type="first">A</forename><surname>Valmari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LNCS: Lectures on Petri Nets I, Basic Models, Advances in Petri Nets</title>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="429" to="528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title/>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
