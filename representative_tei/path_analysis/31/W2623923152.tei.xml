<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Autonomous UAV with vision based on-board decision making for remote sensing and precision agriculture</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">In</forename><surname>Mattingly</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Bilal</forename><surname>Hazim</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Younus</forename><surname>Alsalam</surname></persName>
							<email>bilalhazimyounus.alsalam@hdr.qut.edu.au</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Electrical and Electronics Engineers</orgName>
								<orgName type="institution">Queensland University of Technology</orgName>
								<address>
									<postCode>4001</postCode>
									<settlement>Brisbane</settlement>
									<region>QLD</region>
									<country>United States of America, pp Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kye</forename><surname>Morton</surname></persName>
							<email>kye.morton@hdr.qut.edu.au</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Electrical and Electronics Engineers</orgName>
								<orgName type="institution">Queensland University of Technology</orgName>
								<address>
									<postCode>4001</postCode>
									<settlement>Brisbane</settlement>
									<region>QLD</region>
									<country>United States of America, pp Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Duncan</forename><surname>Campbell</surname></persName>
							<email>da.campbell@qut.edu.au</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Electrical and Electronics Engineers</orgName>
								<orgName type="institution">Queensland University of Technology</orgName>
								<address>
									<postCode>4001</postCode>
									<settlement>Brisbane</settlement>
									<region>QLD</region>
									<country>United States of America, pp Australia</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Australian Research Centre for Aerospace Automation (ARCAA)</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Felipe</forename><surname>Gonzalez</surname></persName>
							<email>felipe.gonzalez@qut.edu.au</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Electrical and Electronics Engineers</orgName>
								<orgName type="institution">Queensland University of Technology</orgName>
								<address>
									<postCode>4001</postCode>
									<settlement>Brisbane</settlement>
									<region>QLD</region>
									<country>United States of America, pp Australia</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Australian Research Centre for Aerospace Automation (ARCAA)</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Autonomous UAV with vision based on-board decision making for remote sensing and precision agriculture</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1109/AERO.2017.7943593</idno>
					<note type="submission">This may be the author&apos;s version of a work that was submitted/accepted for publication in the following source: mitted for peer review or as Accepted for publication after peer review</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-29T01:39+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In recent years, a phenomenal increase in the development of Unmanned Aerial Vehicles (UAVs) has been observed in a broad range of applications in various fields of study. Precision agriculture has emerged as a major field of interest, integrating unmanned monitoring of crop health into general agricultural practices for researchers are utilizing UAV to collect data for post-analysis. This paper describes a modular and generic system that is able to control the UAV using computer vision. A configuration approach similar to the Observation, Orientation, Decision and Action (OODA) loop has been implemented to allow the system to perform on-board decision making. The detection of an object of interest is performed by computer vision functionality. This allows the UAV to change its planned path accordingly and approach the target in order to perform a close inspection, or conduct a manoeuvres such as the application of herbicide or collection of higher resolution agricultural images.</p><p>The results show the ability of the developed system to dynamically change its current goal and implement an inspection manoeuvre to perform necessary actions after detecting the target. The vision based navigation system and on-board decision making were demonstrated in three types of tests: ArUco Marker detection, colour detection and weed detection. The results are measured based on the sensitivity and the selectivity of the algorithm. The sensitivity is the ability of the algorithm to identify and detect the true positive target while the selectivity is the capability of the algorithm to filter out the false negatives for detection targets. Results indicate that the system is capable of detecting ArUco Markers with 99% sensitivity and 100% selectivity at 5 m above the ground level. The system is also capable of detecting a red target with 96% sensitivity and 99% selectivity at the same height during a test height at 5 metres. This system has potential applicability in the field of precision agriculture such as, crop health monitoring, pest plant detection which causes detrimental financial damage to crop yields if not noticed at an early stage. TABLE OF CONTENTS 1. INTRODUCTION .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Unmanned Aerial Vehicle (UAV) applications in path planning, design, search and rescue, ecology, wildlife and precision agriculture is an active field of research <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref>. Remote sensing using UAVs in precision agriculture can assist farmers to assess plant yield, plant health and disease <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref>.</p><p>Computer vision and image processing have also been applied to remotely sensed images to make decisions in agricultural applications. These decisions could be carried out off-board after collecting and processing the data or UAV on-board while the UAV is flying <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b13">14]</ref>. There are cases where it is desirable to make an on-board decision to minimise the amount of data that is stored on-board. As an example, the UAV could be flying at a specific height, decide on a potential issue, descend and capture a high resolution image, or perform a closer inspection or apply herbicide.</p><p>Precision agriculture is an approach to farm management that uses information technology from satellite airplane or UAVs to observe measure and ensure that the crops and soil receive their need for optimum health and productivity <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16]</ref>. Precision agriculture and remote sensing using UAVs are a growing field of research that can assist farmers to assess plant health <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b17">17]</ref>. UAVs can fly in a controlled autonomous path at very low heights and produce images at relatively high spatial resolutions (&lt;2 cm) <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b18">18]</ref>. Remote sensing using UAVs can provide a low-cost option to deal with the basic prerequisites of spatial and dynamic resolutions in contrast to satellite and manned aircraft <ref type="bibr" target="#b19">[19]</ref><ref type="bibr" target="#b20">[20]</ref><ref type="bibr" target="#b21">[21]</ref>. A considerable number of studies have been carried out either with direct or indirect UAV application of remote sensing and computer vision in the field of precision agriculture. As a result, unmanned aerial platforms represent a remarkable opportunity for weed detection and mapping. Remote sensing techniques using thermal and multispectral imaging sensors at different heights were discussed by Han <ref type="bibr" target="#b18">[18]</ref>, Gonzalez-Dugo et al. <ref type="bibr" target="#b9">[10]</ref>, and Salami et al. <ref type="bibr" target="#b22">[22]</ref>. Their research concentrates on filling the gap between the application prerequisites and the qualities of the various selected tools, payloads and platforms. A digital camera using a paraglider UAV was used by Dunford et al. <ref type="bibr" target="#b23">[23]</ref> to oversee the evaluation of a riparian landscape and vegetation units, and furthermore, to distinguish standing dead wood. The results demonstrated an estimation of standing dead wood units and an average precision with omission and commission errors of 80% and 65%, respectively. Mcfadyen et al. <ref type="bibr" target="#b10">[11]</ref> evaluated UAVs for use in plant biosecurity. The research provided recommendations for the applicability of UAVs and onboard sensor technology for plant biosecurity and pest detection. Nebiker et al. <ref type="bibr" target="#b24">[24]</ref> illustrated the gap between satellite-based remote sensing and ground based sensing. Their study showed the benefits of remote sensing applications using a very high resolution micro UAV platform.</p><p>The motivation for using UAVs with the structure of the Observation, Orientation, Decision and Action (OODA) loop framework is to detect invasive weed and implement an action such as applying herbicide. Autonomous on-board detection and action will target the specific location of the weed or pest for herbicide application instead of applying it on the whole field.</p><p>The rest of this paper is organised as follows: Section II describes system architecture (Hardware and Software); section III describes the image test results. Section IV will present the discussion. Final section presents conclusion and future works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">SYSTEM ARCHITECTURE</head><p>The system architecture is divided into two parts: Hardware and Software.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A) Hardware System Design</head><p>The hardware system architecture consists of several components. Figure <ref type="figure" target="#fig_0">1</ref> shows the diagram for the entire hardware system while Figure <ref type="figure" target="#fig_1">2</ref> shows the physical equipment for the hardware system. The on-board system consists of the a 3DR-IRIS UAV frame, an AC2830-358 850 KV motor and 10x4.7 propellers, a Pixhawk Autopilot, a GPS Compass Module, a 4 in 1 ESC/Power Module, a 5000 mAh 3S 30C Lipo Pack Battery that gives 25 minutes' flight time without payload and 15-17 minutes with full payload. In our experiments the UAV flown at 0.5 m/sec vertical flight speed and 1 m/sec horizontal flight speed, and attached with a FrSky-DF Radio Control (Tx/Rx), a WiFi connection, a UBEC, a GSC, an ultrasonic sensor (HC-SR04), a HD Webcam Logitech C270, a relay (SRD-05VDC), a spraying pump and a liquid tank, an Odroid U3 + and a micro Arduino. The HC-SR04 ultrasonic sensor is controlled by a micro Arduino which is connected via a USB cable to the Odroid U3 + . The micro Arduino has C++ code to run the ultrasonic measurements. The Odroid U3 + receives the measurements through the USB cable and uses these measurements to control the UAV flight height through a node in ROS by sending these measurements to the Pixhawk Autopilot. The micro Arduino is also responsible for controlling both the ultrasonic sensor and the relay which controls the motor for the spraying system. The Pixhawk is connected to the Odroid U3 + through a serial cable called an FTDI cable. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Electrical Integration 1. Pixhawk -Odroid U3 + Serial Interface</head><p>The Pixhawk is connected to the Odroid U3 + using an FTDI cable as shown in Figure <ref type="figure" target="#fig_2">3</ref>. A 6-Postion DF13 plug is soldered to the FTDI cable. The benefit of using the FTDI cable is to make the connection between the microcomputer and the autopilot faster than the serial connection with the GPIO.  Figure <ref type="figure" target="#fig_3">4</ref> shows the physical connection of the ultrasonic sensor (HC-SR04) to the micro Arduino and also the connection of the motor to the relay (SRD-05VDC) to micro Arduino. The micro Arduino uses C++ which controls both the motor and the ultrasonic sensor. The micro Arduino is powered and connected to the Odroid U3 + through a USB connection. A ROS node was also created for the ultrasonic module to receive measurement data from the ultrasonic sensor and to check the UAV flight height.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Ultrasonic Sensor &amp; Relay -Micro Arduino Serial Interface</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B-Software System Design</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Detection and On-board Decision Making Approach</head><p>The OODA loop is a closed loop control method used as a framework for decision making <ref type="bibr" target="#b25">[25,</ref><ref type="bibr" target="#b26">26]</ref>. Figure <ref type="figure" target="#fig_5">5</ref> shows the flowchart system in the concept of OODA loop for on-board decision making developed in this work. The observation is based on using the sensors such as ultrasonic sensors and a camera. The ground station receives messages from the onboard computer to check the status of the mission while the UAV is flying. After sending the command to start the mission, the different ROS nodes are executed as shown in Figure <ref type="figure" target="#fig_4">6</ref>. While the UAV is flying, the detection algorithm in the Odroid U3 + checks if the target is in the frame or not.</p><p>Once the target is detected, the UAV is automatically commanded to fly to the target by sending the target location in X, Y, Z to the Pixhawk autopilot using ROS nodes. The on-board decision making focuses on the decision and the action part of the OODA loop. After the UAV reaches the new location (above the target), the action will be for the UAV first: to descend to a lower height just above the ground (e.g. 45 cm) and then: to run the spray pump to spray the target. When the spraying is complete, the UAV either goes to the next waypoint looking for new targets or flies home and land.</p><p>The Odroid U3 + microcomputer is used to process the image on-board the UAV as well as for navigation. The pixhawk autopilot is used to guide the UAV to the new location which is sent from the navigation node in the Odroid U3+ as shown in Figure <ref type="figure" target="#fig_4">6</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Robotic Operating System (ROS)</head><p>ROS is open source software which provides libraries and tools and an easy environment to develop robotic applications <ref type="bibr" target="#b27">[27]</ref>. ROS provides an operating system service with common functionality, including message transformations between processes, and package management <ref type="bibr" target="#b27">[27]</ref>. ROS packages consist of several nodes. A node is basically executable when it is called with rosrun or roslaunch. In addition, ROS supports simulations to represent a graph architecture platform to run the processes in nodes that receive information from multiplex sensor, control, and other messages and post information as well. ROS also support Pixhawk autopilot firmware.</p><p>An on-board downward facing camera attached to the 3D IRIS quad-rotor and connected to the Odroid U3+ microcomputer is used for detecting the target and translating the position of the target in the image plane to the navigation command for the UAV. Several nodes were  developed including a navigation node, a camera node, a detection node (to detect features such as ArUco Markers, colours and specific types of weed), a rotation matrix node and a transfer node. All the nodes connect together (see Figure <ref type="figure" target="#fig_4">6</ref>). The camera is connected to the Odroid U3 via a USB cable and the micro Arduino is connected through another USB cable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Target Detection Software</head><p>The camera node is continuously capturing frames and passing them to the detection node through a ROS node. The detection algorithm is progressively checked to see if the target is in the frame or not. If the target is within frame, the target location in pixels (u, v) will be passed to the transfer node in order to change the pixels (u, v) location to the target location in metres (x, y). In order to have the camera frame in the same direction as the body frame, a rotation matrix node was used to correct the direction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ultrasonic Sensor Software</head><p>The ultrasonic sensor (HC-SR04) and spraying system are connected to the micro Arduino as described in Figure <ref type="figure" target="#fig_3">4</ref>.</p><p>The ultrasonic data is received by the Odroid U3 + via a USB serial connection. The ultrasonic data is used to correct the height (z) of the UAV using a Python node.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Navigation Software</head><p>Once the navigation node receives the location of the target from the rotation matrix node (x, y) and the height from the ultrasonic node (z), the new local position (x, y, z) is sent to the autopilot (Pixhawk) through the navigation node to direct the UAV to go to the new location. When the new location is reached, the navigation node sends a new message to the autopilot to hover above the target at, for example, a height of 45 cm. Once the target reaches that height (e.g. 45 cm), the micro Arduino sends a message to the motor to start spraying on the target for a period of x seconds (e.g. 3 seconds). After completing the task, the UAV resumes flight and continues to fly to the next waypoint or returns to the starting location.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">TEST RESULTS</head><p>Several flight missions were considered. The flight mission can be modified through the navigation node by adding or deleting waypoints. The mission for this test was taken to fly between three points: Home, A and B. The microcomputer is connected to the ground-station through a WiFi network in order to send commands through the ROS environment. The image processing method is programmed to process a captured frame in order to search for a target of interest. When the target is detected, the UAV will fly to new location and hover above the target at a preset altitude of 45 cm and do an action such as spraying the target with herbicide. The UAV will subsequently climb back to the planned flight height and fly back to Home and land. Three different cases were considered to demonstrate and validate the on-board decision making system: ArUco Markers, colour detection and invasive weed detection and spraying.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Test Case 1: ArUco marker</head><p>An ArUco marker is a synthetic square marker composed of a wide black border and an inner binary matrix which makes up its identifier. The on-board camera detects the ''ArUco'' markers arranged in a square pattern as shown in Figure <ref type="figure" target="#fig_7">7B</ref> and transmits their position to the Pixhawk autopilot through the navigation node. The camera is connected to the Odroid U3 + , which runs a ROS node designed to search for the marker while the UAV is flying and also calculate the marker position and send it in metres to the Pixhawk as local position (x, y) <ref type="bibr" target="#b28">[28]</ref> through the navigation node. This type of marker is supported by the OpenCV library <ref type="bibr" target="#b29">[29]</ref>.</p><p>ArUco markers are implemented in the first test to ensure that the vision based control is working without any issues.  The benefits of using the ArUco marker is that the ROS messages (geometry_msgs.msg/PoseStamp) are sent directly to the navigation node in metres so that there is no need to use a rotation matrix node. Figure <ref type="figure" target="#fig_7">7C</ref> shows the UAV hovering above the target at a preset flight height of 45 cm and detecting the target. The ArUco marker detection and navigation was conducted 10 times. The test produced onboard detection results with 99% sensitivity and 100% selectivity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Test Case 2: Coulor</head><p>A ROS node for red colour detection was implemented and run on-board the UAV with the help of the on-board computer (Odroid U3 + ). The position and orientation of the target are provided through rosbag recording data. The algorithm uses image processing using the OpenCV library to scan for the colour red <ref type="bibr" target="#b30">[30]</ref>. The camera node publishes image frames through the CV Bridge in ROS <ref type="bibr">[31]</ref>. A combination of HSV colour and circle detection were also used to search for red circular object consistently to verify the vision based navigation. The pseudo code for position estimation needed for the vision based navigation is as follows:</p><p>1. Split each image into R, G, B channels and then convert these to H, S, V channels.</p><p>2. Threshold the HSV image and keep only the red pixels. The test for on-board decision making for colour detection was conducted eight times. Results shows that the UAV flew from home towards point A and then towards B. While moving towards B the UAV detects the target colour and descends and hovers 45 cm above the red circle. Figure <ref type="figure" target="#fig_10">8A</ref> and Figure <ref type="figure" target="#fig_10">8B</ref> shows the UAV is detecting the red circle, and Figure <ref type="figure" target="#fig_10">8C</ref> shows the UAV hovering above the target and spraying it for 3 seconds. The test provides on-board detection and action results with 96% sensitivity and 99% selectivity.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Test Case 3: Weed Detection and Spraying</head><p>An invasive weed detection algorithm using an Object Based Image Analysis (OBIA) algorithm and a threshold selection method was developed. Initially, several images of the target (i.e. spear thistle weed) of different sizes and/or stages of growth were collected. Each image is analysed using an OBIA algorithm and a threshold selection method with the true colour and texture image data using RGB triplet. Each RGB triplet defines a colour for one pixel of the image in three layers. The first layer of the 3D array (colours band) contains red components, the second layer represents green components and the third layer consist of blue components. In order to detect the weed, the threshold number must be chosen for each colour component and a set of colour thresholds is generated.</p><p>Figure <ref type="figure" target="#fig_12">9</ref> shows a mission example of on-board invasive weed detection and action; Figure <ref type="figure" target="#fig_12">9A</ref> shows the UAV above the weed when the weed is detected; Figure <ref type="figure" target="#fig_12">9B</ref> shows a green boxes which represented that weed was detected. Figure <ref type="figure" target="#fig_12">9C</ref> shows the UAV hovering above the weed at 45 cm and spraying on the target.</p><p>A demonstration for invasive weed detection and action has been applied on-board. The system demonstrate action based on every successfully detection weed.   Figure <ref type="figure" target="#fig_13">10</ref> shows the reference UAV trajectory, the actual flight trajectory and the targets. Once the target (e.g. ArUco, colour or weed) is detected the control transforms image feature (pixels) to the target location in metres and directs the UAV to fly to that new location. The flight test results presented in (Figure <ref type="figure" target="#fig_13">10</ref>) demonstrate the capacity of the system for on-board decision making using the concept of OODA loop.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">DISCUSSION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Root Mean Square Error (RMSE)</head><p>Table <ref type="table" target="#tab_0">1</ref> shows the RMSE for the simulation and the actual test flight. The RMSE for the actual test flight is, however, different from the RMSE for the simulation and that is due to the light conditions and the weather during the outdoor flight test where the wind speed was in avarage15 km/hour (8 knots). The RMSE for the actual flight test overall is 58 cm which is small enough to perform the on-board decision making and action. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">CONCLUSION AND FUTURE WORK</head><p>This paper described the application of the system architecture for on-board vision based navigation and decision making and action. The feedback from the image features enabled the quad-rotor 3D IRIS to move to a desired position based on the OODA loop concept for feedback control. The vision based navigation system and on-board decision making were illustrated in three types of tests: ArUco Marker detection, colour detection and weed detection. Results demonstrated that the system is capable of detecting ArUco Markers to 99% sensitivity and 100% selectivity at 5 m above the ground level. The system is also capable of detecting a red target to 96% sensitivity and 99% selectivity at the same height during a test height at 5 metres. A demonstration of invasive weed on-board detection and action has been given. The system demonstrate action based on every successfully detection weed.</p><p>There are a few items that should be considered when applying the on-board system with the concept of vision based navigation for on-board decision making in precision agriculture applications.</p><p>The weed detection method is based on specific characteristics of the weed, which can be affected by the sun angle and seasonal variability. The existing detection method can be modified to ensure the weed can be detected accurately at different times of the day and at different stages of growth. Suggestions to improve the detection method include:</p><p>1. Using micro multispectral or micro hyperspectral camera spectral bands of near infrared ranges to detect weed.</p><p>2. Using machine learning methods such as deep learning based on extensive data collection to train the algorithm for more precise weed detection.</p><p>A video of our flight tests can be found in link below <ref type="url" target="https://www.youtube.com/watch?v=P8YH9cllrcE">https://www.youtube.com/watch?v=P8YH9cllrcE</ref> </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: System architecture for on-board decision making using an Odroid U3 + and ROS.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: On-board system Hardware architecture.</figDesc><graphic coords="4,54.00,98.50,242.90,160.35" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Pixhawk Autopilot Connection to Odroid U3 + through USB serial.</figDesc><graphic coords="4,54.00,405.85,242.50,243.95" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Interface between ultrasonic sensor (HC-SR04), relay (SRD-05VDC) and micro Arduino.</figDesc><graphic coords="4,315.00,71.30,242.75,143.85" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Robotic Operating System (ROS) nodes and Hardware system for onboard decision making and action control system.</figDesc><graphic coords="5,66.45,456.80,470.05,257.15" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Observation, Orientation, Decision and Action (OODA) loop flowchart for on-board decision making.</figDesc><graphic coords="5,54.25,37.35,503.40,240.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: A: The ArUco marker target is detected. B: Image showing the result of the pole of the detecting target.</figDesc><graphic coords="6,315.00,289.45,242.40,389.15" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: C: Hovering above the target after detection.</figDesc><graphic coords="7,54.00,54.00,242.40,197.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>3 .</head><label>3</label><figDesc>Normalise each channel and convert to greyscale images. 4. Use the Hough transform to detect red circles in the threshold image. 5. Find the image feature representation of the centre using the image centre and focal length. 6. Send the image feature in pixels to the transfer node in order to convert from pixels to metres.7. Send the target location in metres to the rotation matrix node to orient the UAV body frame in same direction as the world frame. 8. Send the target location from the rotation matrix node to the navigation node to direct the UAV to the desired target.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8:A: Colour target is detected. B: Image processing result after detecting the target.</figDesc><graphic coords="7,315.00,300.70,242.40,361.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: C: Hovering above the target at 45 cm.</figDesc><graphic coords="8,54.00,54.00,242.40,184.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>Simulation and Actual Flight Test Results and AnalysisNumerical experiments were simulated in the proposed model to verify the performance of the vision based navigation for on-board decision making. After recording the flight test data from the simulation test and the actual flight test, the flight trajectories are plotted in order to compare them and analyse the data from the simulation and the actual flight test. The quad-copter flight path starts from the initial position (0,0) home and progresses to waypoint A (5,2) then to waypoint B (-5,2) in the simulation flight test.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: A: weed target is detected. B: Image processing result green boxes for the weed after detecting the target. C: Hovering and spraying on the target at 45cm height.</figDesc><graphic coords="8,315.10,437.40,242.65,184.70" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: A: Reference and Actual flight indicating the target trajectory with on-board decision making in 3D, The UAV is programmed to fly from Home-A-B. It continuously looks for a target and when found modify its trajectory, B: Top view of reference and for actual flight trajectory.</figDesc><graphic coords="9,126.05,358.35,363.70,318.10" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 . RMSE for simulation flight and actual flight in metres.</head><label>1</label><figDesc></figDesc><table><row><cell></cell><cell>Simulation</cell><cell>Actual</cell></row><row><cell>RMS</cell><cell>Flight Test</cell><cell>Flight Test</cell></row><row><cell></cell><cell>(m)</cell><cell>(m)</cell></row><row><cell>RMS_x</cell><cell>0.0818</cell><cell>0.4481</cell></row><row><cell>RMS_y</cell><cell>0.0796</cell><cell>0.1571</cell></row><row><cell>RMS_z</cell><cell>0.0646</cell><cell>0.3404</cell></row><row><cell>RMS_Euclidean</cell><cell>0.13115</cell><cell>0.5842</cell></row><row><cell>distance</cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGEMENTS</head><p>The authors would like to thank the pilot <rs type="person">Steven Bulmer</rs> for his very useful help during the fieldwork and <rs type="person">Mr Michael Cahill</rs> the owner of the farm for his assistance while collecting the data and allowing the flight test at his farm. Also we would like to thanks <rs type="person">Dr Aaron Mcfadyen</rs> and <rs type="person">Chandrama Sarker</rs> for your support during writing the paper.</p></div>
			</div>			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>He has 25 years of research leadership in control and automation. Duncan serves as Treasurer on the Board of the Australian Association for Unmanned Systems (AAUS) and was the IEEE Queensland Section Chapter Chair of the Control Systems/Robotics and Automation Society Joint Chapter (2008/2009). He is the Chair of CDIO Australian and New Zealand regional group of the global CDIO collaboration in engineering education, and was President of the Australasian Association for Engineering Education in 2011.</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Associate</head><p>Professor Felipe Gonzalez is with the School of Electrical Engineering and Computer Science (ECCS), Science and Engineering Faculty with a passion for innovation in the fields of aerial robotics and automation. He creates aerial robots that possess a high level of cognition using efficient on-board computer algorithms using advanced optimization and game theory approaches. These robots assist us to understand and improve our physical and natural world. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Open Source Computer-Vision Based Guidance System for UAVs On-Board Decision Making</title>
		<author>
			<persName><forename type="first">H</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Geeves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Alsalam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Gonzalez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Aerospace conferece</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<pubPlace>Big sky, Montana</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Autonomous UAVs Wildlife Monitoring and Tracking Using Thermal Imaging and Computer vision</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hensler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">H Y</forename><surname>Alsalam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">F</forename><surname>Gonzalez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Aerospace conferece</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<pubPlace>Big sky, Montana</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Hybrid-Game Strategies for multiobjective design optimization in engineering</title>
		<author>
			<persName><forename type="first">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">F</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Periaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Onate</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers &amp; Fluids</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="189" to="204" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Wind-energy based path planning for unmanned aerial vehicles using Markov decision processes</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">H</forename><surname>Al-Sabban</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">F</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">N</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Robotics and Automation (ICRA)</title>
		<imprint>
			<date type="published" when="2013">2013. 2013</date>
			<biblScope unit="page" from="784" to="789" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">UAS mission path planning system (MPPS) using hybrid-game coupled to multi-objective optimiser</title>
		<author>
			<persName><forename type="first">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Periaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">F</forename><surname>Gonzalez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ASME 2009 International Design Engineering Technical Conferences and Computers and Information in Engineering Conference</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1111" to="1120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Robust evolutionary methods for multi-objective and multdisciplinary design optimisation in aeronautics</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">F</forename><surname>Gonzalez</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">High-Precision Positioning And Real-Time Data Processing Of Uav-Systems</title>
		<author>
			<persName><forename type="first">M</forename><surname>Rieke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Foerster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Geipel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Prinz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Archives of Photogrammetry, Remote Sensing and Spatial Information Sciences</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="1" to="C22" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Multispectral Aerial Imaging Of Pasture Quality And Biomass Using Unmanned Aerial Vehicles (UAV)</title>
		<author>
			<persName><forename type="first">S</forename><surname>Von Bueren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Yule</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Accurate and Efficient Use of Nutrients on Farms</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">Report</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The application of small unmanned aerial systems for precision agriculture: a review</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Kovacs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Precision agriculture</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="693" to="712" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Using high resolution UAV thermal imagery to assess the variability in the water status of five fruit tree species within a commercial orchard</title>
		<author>
			<persName><forename type="first">V</forename><surname>Gonzalez-Dugo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zarco-Tejada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Nicolas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Nortes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Alarcon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Intrigliolo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Precision Agriculture</title>
		<imprint>
			<date type="published" when="2013-12">Dec 2013</date>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="660" to="678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Evaluating unmanned aircraft systems for deployment in plant biosecurity</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mcfadyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Eagling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<pubPlace>Canberra, Australia</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Tech. Report</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Method for automatic georeferencing aerial remote sensing (RS) images from an unmanned aerial vehicle (UAV) platform</title>
		<author>
			<persName><forename type="first">H</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biosystems Engineering</title>
		<imprint>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="page" from="104" to="113" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Evaluation of digital photography from model aircraft for remote sensing of crop biomass and nitrogen status</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">R</forename><surname>Hunt</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cavigelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">S</forename><surname>Daughtry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Mcmurtrey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iii</forename></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Walthall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Precision Agriculture</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="359" to="378" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Assessment of unmanned aerial vehicles imagery for quantitative monitoring of wheat crop in small plots</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C D</forename><surname>Lelong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Burger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Jubelin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Labbe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Baret</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="3557" to="3585" />
			<date type="published" when="2008-05">May 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Precision agriculture and sustainability</title>
		<author>
			<persName><forename type="first">R</forename><surname>Bongiovanni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lowenberg-Deboer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="359" to="387" />
		</imprint>
	</monogr>
	<note>Precision Agriculture</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Twenty five years of remote sensing in precision agriculture: Key advances and remaining knowledge gaps</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Mulla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biosystems</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title/>
		<author>
			<persName><surname>Engineering</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">114</biblScope>
			<biblScope unit="page" from="358" to="371" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Acquisition of NIR-Green-Blue Digital Photographs from Unmanned Aircraft for Crop Monitoring</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">R</forename><surname>Hunt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">D</forename><surname>Hively</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Fujikawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Linden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">S T</forename><surname>Daughtry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">W</forename><surname>Mccarty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="290" to="305" />
			<date type="published" when="2010-01">Jan 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">An autonomous unmanned aerial vehiclebased imagery system development and remote sensing images classification for agricultural applications</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Han</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>Graduate Theses and Dissertations</publisher>
			<biblScope unit="page">513</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Remote sensing of vegetation from UAV platforms using lightweight multispectral and thermal imaging sensors</title>
		<author>
			<persName><forename type="first">J</forename><surname>Berni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zarco-Tejada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Surez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Gonzlez-Dugo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Fereres</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. Arch. Photogramm. Remote Sens. Spatial Inform. Sci</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Feature Learning Based Approach for Weed Classification Using High Resolution Aerial Images from a Digital Camera Mounted on a UAV</title>
		<author>
			<persName><forename type="first">C</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sukkarieh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="12037" to="12054" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Processing and Assessment of Spectrometric, Stereoscopic Imagery Collected Using a Lightweight UAV Spectral Camera for Precision Agriculture</title>
		<author>
			<persName><forename type="first">E</forename><surname>Honkavaara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Saari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kaivosoja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Polonen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hakala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Litkey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="5006" to="5039" />
			<date type="published" when="2013-10">Oct 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">UAV Flight Experiments Applied to the Remote Sensing of Vegetated Areas</title>
		<author>
			<persName><forename type="first">E</forename><surname>Salami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Barrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Pastor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="11051" to="11081" />
			<date type="published" when="2014-11">Nov 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Potential and constraints of Unmanned Aerial Vehicle technology for the characterization of Mediterranean riparian forest</title>
		<author>
			<persName><forename type="first">R</forename><surname>Dunford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gagnage</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Piegay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Tremelo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="4915" to="4935" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">A light-weight multispectral sensor for micro UAV-Opportunities for very high resolution airborne remote sensing</title>
		<author>
			<persName><forename type="first">S</forename><surname>Nebiker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Annen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Scherrer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Oesch</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="1193" to="1200" />
		</imprint>
	</monogr>
	<note>The international archives of the photogrammetry, remote sensing and spatial information sciences</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Orientation: key to the OODA loop-the culture factor</title>
		<author>
			<persName><forename type="first">D</forename><surname>Maccuish</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Defense Resources Management (JoDRM)</title>
		<imprint>
			<biblScope unit="page" from="67" to="74" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">The committee to abolish hell: Strategic culture, OODA loops, and decisionmaking by the US national security council during the Bosnian war</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Pullen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
		<respStmt>
			<orgName>The University of North Carolina at Chapel Hill</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title/>
		<author>
			<persName><surname>Von</surname></persName>
		</author>
		<idno>15/06/2016</idno>
		<ptr target="http://wiki.ros.org/mavros" />
	</analytic>
	<monogr>
		<title level="j">Ros.org&apos;</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">MPC controlled multirotor with suspended slung load: System architecture and visual load detection</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zrn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mcfadyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Notter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Heckmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Morton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">F</forename><surname>Gonzalez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Aerospace conferece</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<pubPlace>Big sky, Montana</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title/>
		<author>
			<persName><surname>Opencv</surname></persName>
		</author>
		<author>
			<persName><surname>Org</surname></persName>
		</author>
		<ptr target="http://opencv.org/" />
		<imprint>
			<date type="published" when="2015-03">2015. March 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">The OpenCV library</title>
		<author>
			<persName><forename type="first">G</forename><surname>Bradski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Dr. Dobb&apos;s Journal</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="120" to="125" />
			<date type="published" when="2000-11">Nov 2000, 2014-05-18 2000</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
