<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Concolic Testing for Deep Neural Networks *</title>
				<funder ref="#_EYXnaFQ">
					<orgName type="full">EPSRC</orgName>
				</funder>
				<funder>
					<orgName type="full">CSC-PAG Oxford Scholarship</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Youcheng</forename><surname>Sun</surname></persName>
							<email>youcheng.sun@cs.ox.ac.uk</email>
						</author>
						<author>
							<persName><forename type="first">Min</forename><surname>Wu</surname></persName>
							<email>min.wu@cs.ox.ac.uk</email>
						</author>
						<author>
							<persName><forename type="first">Wenjie</forename><surname>Ruan</surname></persName>
							<email>wenjie.ruan@cs.ox.ac.uk</email>
						</author>
						<author>
							<persName><forename type="first">Xiaowei</forename><surname>Huang</surname></persName>
							<email>xiaowei.huang@liverpool.ac.uk</email>
						</author>
						<author>
							<persName><forename type="first">Marta</forename><surname>Kwiatkowska</surname></persName>
							<email>marta.kwiatkowska@cs.ox.ac.uk</email>
						</author>
						<author>
							<persName><forename type="first">Daniel</forename><forename type="middle">2018</forename><surname>Kroening</surname></persName>
							<email>kroening@cs.ox.ac.uk</email>
						</author>
						<author>
							<persName><surname>Concolic</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">University of Oxford</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">University of Oxford</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">University of Oxford</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">University of Liverpool</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">University of Oxford</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="institution">University of Oxford</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Concolic Testing for Deep Neural Networks *</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-21T21:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>neural networks</term>
					<term>symbolic execution</term>
					<term>concolic testing</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Concolic testing combines program execution and symbolic analysis to explore the execution paths of a software program. This paper presents the first concolic testing approach for Deep Neural Networks (DNNs). More specifically, we formalise coverage criteria for DNNs that have been studied in the literature, and then develop a coherent method for performing concolic testing to increase test coverage. Our experimental results show the effectiveness of the concolic testing approach in both achieving high coverage and finding adversarial examples.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Deep neural networks (DNNs) have been instrumental in solving a range of hard problems in AI, e.g., the ancient game of Go, image classification, and natural language processing. As a result, many potential applications are envisaged. However, major concerns have been raised about the suitability of this technique for safety-and security-critical systems, where faulty behaviour carries the risk of endangering human lives or financial damage. To address these concerns, a (safety or security) critical system comprising DNNbased components needs to be validated thoroughly.</p><p>The software industry relies on testing as a primary means to provide stakeholders with information about the quality of the software product or service under test <ref type="bibr" target="#b11">[12]</ref>. So far, there have been only few attempts to test DNNs systematically <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b29">30]</ref>. These are either based on concrete execution, e.g., Monte Carlo tree search <ref type="bibr" target="#b29">[30]</ref> or gradient-based search <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b24">25]</ref>, or symbolic execution in combination with solvers for linear arithmetic <ref type="bibr" target="#b22">[23]</ref>. Together with these test-input generation algorithms, several test coverage criteria have been presented, including neuron coverage <ref type="bibr" target="#b17">[18]</ref>, a criterion that is inspired by MC/DC <ref type="bibr" target="#b22">[23]</ref>, and criteria to capture particular neuron activation values to identify corner cases <ref type="bibr" target="#b14">[15]</ref>. None of these approaches implement concolic testing <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b21">22]</ref>, which combines concrete execution and symbolic analysis to explore the execution paths of a program that are hard to cover by techniques such as random testing.</p><p>We hypothesise that concolic testing is particularly well-suited for DNNs. The input space of a DNN is usually high dimensional, which makes random testing difficult. For instance, a DNN for image classification takes tens of thousands of pixels as input. Moreover, owing to the widespread use of the ReLU activation function for hidden neurons, the number of "execution paths" in a DNN is simply too large to be completely covered by symbolic execution. Concolic testing can mitigate this complexity by directing the symbolic analysis to particular execution paths, through concretely evaluating given properties of the DNN.</p><p>In this paper, we present the first concolic testing method for DNNs. The method is parameterised using a set of coverage requirements, which we express using Quantified Linear Arithmetic over Rationals (QLAR). For a given set R of coverage requirements, we incrementally generate a set of test inputs to improve coverage by alternating between concrete execution and symbolic analysis. Given an unsatisfied test requirement r , we identify a test input t within our current test suite such that t is close to satisfying r according to an evaluation based on concrete execution. After that, symbolic analysis is applied to obtain a new test input t ′ that satisfies r . The test input t ′ is then added to the test suite. This process is iterated until we reach a satisfactory level of coverage.</p><p>Finally, the generated test suite is passed to a robustness oracle, which determines whether the test suite includes adversarial examples <ref type="bibr" target="#b23">[24]</ref>, i.e., pairs of test cases that disagree on their classification labels when close to each other with respect to a given distance metric. The lack of robustness has been viewed as a major weakness of DNNs, and the discovery of adversarial examples and the robustness problem are studied actively in several domains, including machine learning, automated verification, cyber security, and software testing. Overall, the main contributions of this paper are threefold: <ref type="bibr" target="#b0">(1)</ref> We develop the first concolic testing method for DNNs.</p><p>(2) We evaluate the method with a broad range of test coverage requirements, including Lipschitz continuity <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30]</ref> and several structural coverage metrics <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b22">23]</ref>. We show experimentally that our new algorithm supports this broad range of properties in a coherent way. (3) We implement the concolic testing method in the software tool DeepConcolic 1 . Experimental results show that Deep-Concolic achieves high coverage and that it is able to discover a significant number of adversarial examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>We briefly review existing efforts for assessing the robustness of DNNs and the state of the art in concolic testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Robustness of DNNs</head><p>Current work on the robustness of DNNs can be categorised as offensive or defensive. Offensive approaches focus on heuristic search algorithms (mainly guided by the forward gradient or cost gradient of the DNN) to find adversarial examples that are as close as possible to a correctly classified input. On the other hand, the goal of defensive work is to increase the robustness of DNNs. There is an arms race between offensive and defensive techniques.</p><p>In this paper we focus on defensive methods. A promising approach is automated verification, which aims to provide robustness guarantees for DNNs. The main relevant techniques include a layer-by-layer exhaustive search <ref type="bibr" target="#b10">[11]</ref>, methods that use constraint solvers <ref type="bibr" target="#b13">[14]</ref>, global optimisation approaches <ref type="bibr" target="#b19">[20]</ref> and abstract interpretation <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b15">16]</ref> to over-approximate a DNN's behavior. Exhaustive search suffers from the state-space explosion problem, which can be alleviated by Monte Carlo tree search <ref type="bibr" target="#b29">[30]</ref>. Constraint-based approaches are limited to small DNNs with hundreds of neurons. Global optimisation improves over constraint-based approaches through its ability to work with large DNNs, but its capacity is sensitive to the number of input dimensions that need to be perturbed. The results of over-approximating analyses can be pessimistic because of false alarms.</p><p>The application of traditional testing techniques to DNNs is difficult, and work that attempts to do so is more recent, e.g., <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b29">30]</ref>. Methods inspired by software testing methodologies typically employ coverage criteria to guide the generation of test cases; the resulting test suite is then searched for adversarial examples by querying an oracle. The coverage criteria considered include neuron coverage <ref type="bibr" target="#b17">[18]</ref>, which resembles traditional statement coverage. A set of criteria inspired by MD/DC coverage <ref type="bibr" target="#b9">[10]</ref> is used in <ref type="bibr" target="#b22">[23]</ref>; Ma et al. <ref type="bibr" target="#b14">[15]</ref> present criteria that are designed to capture particular values of neuron activations. Tian et al. <ref type="bibr" target="#b24">[25]</ref> study the utility of neuron coverage for detecting adversarial examples in DNNs for the Udacity-Didi Self-Driving Car Challenge.</p><p>We now discuss algorithms for test input generation. Wicker et al. <ref type="bibr" target="#b29">[30]</ref> aim to cover the input space by exhaustive mutation testing that has theoretical guarantees, while in <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b24">25]</ref> gradient-based search algorithms are applied to solve optimisation problems, and Sun et al. <ref type="bibr" target="#b22">[23]</ref> apply linear programming. None of these consider concolic testing and a general means for modeling test coverage requirements as we do in this paper. 1 <ref type="url" target="https://github.com/TrustAI/DeepConcolic">https://github.com/TrustAI/DeepConcolic</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Concolic Testing</head><p>By concretely executing the program with particular inputs, which includes random testing, a large number of inputs can be tested at low cost. However, without guidance, the generated test cases may be restricted to a subset of the execution paths of the program and the probability of exploring execution paths that contain bugs can be extremely low. In symbolic execution <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b31">32]</ref>, an execution path is encoded symbolically. Modern constraint solvers can determine feasibility of the encoding effectively, although performance still degrades as the size of the symbolic representation increases. Concolic testing <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b21">22]</ref> is an effective approach to automated test input generation. It is a hybrid software testing technique that alternates between concrete execution, i.e., testing on particular inputs, and symbolic execution, a classical technique that treats program variables as symbolic values <ref type="bibr" target="#b12">[13]</ref>.</p><p>Concolic testing has been applied routinely in software testing, and a wide range of tools is available, e.g., <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b21">22]</ref>. It starts by executing the program with a concrete input. At the end of the concrete run, another execution path must be selected heuristically. This new execution path is then encoded symbolically and the resulting formula is solved by a constraint solver, to yield a new concrete input. The concrete execution and the symbolic analysis alternate until a desired level of structural coverage is reached.</p><p>The key factor that affects the performance of concolic testing is the heuristics used to select the next execution path. While there are simple approaches such as random search and depth-first search, more carefully designed heuristics can achieve better coverage <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b8">9]</ref>. Automated generation of search heuristics for concolic testing is an active area of research <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b26">27]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Comparison with Related Work</head><p>We briefly summarise the similarities and differences between our concolic testing method, named DeepConcolic, and other existing coverage-driven DNN testing methods: DeepXplore <ref type="bibr" target="#b17">[18]</ref>, DeepTest <ref type="bibr" target="#b24">[25]</ref>, DeepCover <ref type="bibr" target="#b22">[23]</ref>, and DeepGauge <ref type="bibr" target="#b14">[15]</ref>. The details are presented in Table <ref type="table" target="#tab_0">1</ref>, where NC, SSC, and NBC are short for Neuron Coverage, SS Coverage, and Neuron Boundary Coverage, respectively. In addition to the concolic nature of DeepConcolic, we observe the following differences.</p><p>• DeepConcolic is generic, and is able to take coverage requirements as input; the other methods are ad hoc, and are tailored to specific requirements.  <ref type="bibr" target="#b16">[17]</ref>. For convenience, we explicitly denote the activation value before the ReLU as u k,l such that</p><formula xml:id="formula_0">v k,l = ReLU (u k,l ) = u k,l if u k,l ≥ 0 0 otherwise<label>(1)</label></formula><p>ReLU is the most popular activation function for neural networks. Except for inputs, every neuron is connected to neurons in the preceding layer by pre-defined weights such that</p><formula xml:id="formula_1">∀1 &lt; k ≤ K, ∀1 ≤ l ≤ s k , u k,l = 1≤h ≤s k -1 {w k -1,h,l • v k -1,h } + b k,l<label>(2)</label></formula><p>where w k -1,h,l is the pre-trained weight for the connection between n k-1,h (i.e., the h-th neuron of layer k -1) and n k,l (i.e., the l-th neuron of layer k), and b k,l is the bias.</p><p>Finally, for any input, the neural network assigns a label, that is, the index of the neuron of the output layer that has the largest value, i.e., label = argmax 1≤l ≤s K {v K,l }.</p><p>Due to the existence of ReLU, the neural network is a highly non-linear function. In this paper, we use variable x to range over all possible inputs in the input domain D L 1 and use t, t 1 , t 2 , ... to denote concrete inputs. Given a particular input t, we say that the DNN N is instantiated and we use N [t] to denote this instance of the network. We remark that, while for simplicity the definition focuses on DNNs with fully connected and convolutional layers, as shown in the experiments (Section 10) our method also applies to other popular layers, e.g., maxpooling, used in state-of-the-art DNNs.</p><formula xml:id="formula_2">• Given a network instance N [t],</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">TEST COVERAGE FOR DNNS 4.1 Activation Patterns</head><p>A software program has a set of concrete execution paths. Similarly, a DNN has a set of linear behaviours called activation patterns <ref type="bibr" target="#b22">[23]</ref>.</p><p>Definition 4.1 (Activation Pattern). Given a network N and an input t, the activation pattern of N [t] is a function ap[N, t] that maps the set of hidden neurons to {true, false}. We write ap[t] for ap[N, t] if N is clear from the context. For an activation pattern ap[t], we use ap[t] k,i to denote whether the ReLU operator of the neuron n k,i is activated or not. Formally,</p><formula xml:id="formula_3">ap[t] k,l = false ≡ u[t] k,l &lt; v[t] k,l ap[t] k,l = true ≡ u[t] k,l = v[t] k,l<label>(3)</label></formula><p>Intuitively, ap[t] k,l = true if the ReLU of the neuron n k,l is activated, and ap[t] k,l = false otherwise.</p><p>Given a DNN instance N [t], each ReLU operator's behaviour (i.e., each ap[t] k,l ) is fixed and this results in the particular activation pattern ap[t], which can be encoded by using a Linear Programming (LP) model <ref type="bibr" target="#b22">[23]</ref>.</p><p>Computing a test suite that covers all activation patterns of a DNN is intractable owing to the large number of neurons in pratically-relevant DNNs. Therefore, we identify a subset of the activation patterns according to certain coverage criteria, and then generate test inputs that cover these activation patterns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Formalizing Test Coverage Criteria</head><p>We use a specific fragment of Quantified Linear Arithmetic over Rationals (QLAR) to express the coverage requirements on the test suite for a given DNN. This enables us to give a single test input generation algorithm (Section 8) for a variety of coverage criteria. We denote the set of formulas in our fragment by DR. Definition 4.2. Given a network N , we write IV = {x, x 1 , x 2 , ...} for a set of variables that range over the all inputs D L 1 of the network. We define</p><formula xml:id="formula_4">V = {u[x] k,l , v[x] k,l | 1 ≤ k ≤ K, 1 ≤ l ≤ s k , x ∈ IV }</formula><p>to be a set of variables that range over the rationals. We fix the following syntax for DR formulas:</p><formula xml:id="formula_5">r ::= Qx .e | Qx 1 , x 2 .e e ::= a ▷◁ 0 | e ∧ e | ¬e | |{e 1 , ..., e m }| ▷◁ q a ::= w | c • w | p | a + a | a -a (4)</formula><p>where Q ∈ {∃, ∀}, w ∈ V , c, p ∈ R, q ∈ N, ▷◁∈ {≤, &lt;, =, &gt;, ≥}, and x, x 1 , x 2 ∈ IV . We call r a coverage requirement, e a Boolean formula, and a an arithmetic formula. We call the logic DR + if the negation operator ¬ is not allowed. We use R to denote a set of coverage requirement formulas.</p><p>The formula ∃x .e expresses that there exists an input x such that e is true, while ∀x .e expresses that e is true for all inputs x. The formulas ∃x 1 , x 2 .e and ∀x 1 , x 2 .e have similar meaning, except that they quantify over two inputs x 1 and x 2 . The Boolean expression |{e 1 , ..., e m }| ▷◁ q is true if the number of true Boolean expressions in the set {e 1 , ..., e m } is in relation ▷◁ with q. The other operators in Boolean and arithmetic formulas have their standard meaning.</p><p>Although V does not include variables to specify an activation pattern ap[x], we may write</p><formula xml:id="formula_6">ap[x 1 ] k,l = ap[x 2 ] k,l and ap[x 1 ] k,l ap[x 2 ] k,l<label>(5)</label></formula><p>to require that x 1 and x 2 have, respectively, the same and different activation behaviours on neuron n k,l . These conditions can be expressed in the syntax above using the expressions in Equation ( <ref type="formula" target="#formula_3">3</ref>). Moreover, some norm-based distances between two inputs can be expressed using our syntax. For example, we can use the set of constraints</p><formula xml:id="formula_7">{x 1 (i) -x 2 (i) ≤ q, x 2 (i) -x 1 (i) ≤ q | i ∈ {1, . . . , s 1 }}<label>(6)</label></formula><p>to express ||x 1 -x 2 || ∞ ≤ q, i.e., we can constrain the Chebyshev distance L ∞ between two inputs x 1 and x 2 , where x(i) is the i-th dimension of the input vector x.</p><p>Semantics. We define the satisfiability of a coverage requirement r by a test suite T .</p><p>Definition 4.3. Given a set T of test inputs and a coverage requirement r , the satisfiability relation T |= r is defined as follows.</p><p>• </p><formula xml:id="formula_8">T |= e[x 1 → t 1 ][x 2 → t 2 ]</formula><p>The cases for ∀ formulas are similar. For the evaluation of Boolean expression e over an input t, we have </p><formula xml:id="formula_9">• T |= a ▷◁ 0 if a ▷◁ 0 • T |= e 1 ∧</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Test Coverage Metrics</head><p>Now we can define test coverage criteria by providing a set of requirements on the test suite. The coverage metric is defined in the standard way as the percentage of the test requirements that are satisfied by the test cases in the test suite T . Definition 4.4 (Coverage Metric). Given a network N , a set R of test coverage requirements expressed as DR formulas, and a test suite T , the test coverage metric M(R, T ) is as follows:</p><formula xml:id="formula_10">M(R, T ) = |{r ∈ R | T |= r }| |R|<label>(7)</label></formula><p>The coverage is used as a proxy metric for the confidence in the safety of the DNN under test.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">SPECIFIC COVERAGE REQUIREMENTS</head><p>In this section, we give DR + formulas for several important coverage criteria for DNNs, including Lipschitz continuity <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30]</ref> and test coverage criteria from the literature <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b22">23]</ref>. The criteria we consider have syntactical similarity with structural test coverage criteria in conventional software testing. Lipschitz continuity is semantic, specific to DNNs, and has been shown to be closely related to the theoretical understanding of convolutional DNNs <ref type="bibr" target="#b28">[29]</ref> and the robustness of both DNNs <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b29">30]</ref> and Generative Adversarial Networks <ref type="bibr" target="#b0">[1]</ref>. These criteria have been studied in the literature using a variety of formalisms and approaches.</p><p>Each test coverage criterion gives rise to a set of test coverage requirements. In the following, we discuss the three coverage criteria from <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b22">23]</ref>, respectively. We use ||t 1 -t 2 || q to denote the distance between two inputs t 1 and t 2 with respect to a given distance metric || • || q . The metric || • || q can be, e.g., a norm-based metric such as the L 0 -norm (the Hamming distance), the L 2 -norm (the Euclidean distance), or the L ∞ -norm (the Chebyshev distance), or a structural similarity distance, such as SSIM <ref type="bibr" target="#b27">[28]</ref>. In the following, we fix a distance metric and simply write ||t 1 -t 2 ||. Section 10 elaborates on the particular metrics we use for our experiments.</p><p>We may consider requirements for a set of input subspaces. Given a real number b, we can generate a finite set S(D L 1 , b) of subspaces of D L 1 such that for all inputs x 1 ,</p><formula xml:id="formula_11">x 2 ∈ D L 1 , if ||x 1 -x 2 || ≤ b, then there exists a subspace X ∈ S(D L 1 , b) such that x 1 , x 2 ∈ X .</formula><p>The subspaces can be overlapping. Usually, every subspace X ∈ S(D L 1 , b) can be represented with a box constraint, e.g., X = [l, u] s 1 , and therefore t ∈ X can be expressed with a Boolean expression as follows.</p><formula xml:id="formula_12">s 1 i=1 x(i) -u ≤ 0 ∧ x(i) -l ≥ 0 (8)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Lipschitz Continuity</head><p>In <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b23">24]</ref>, Lipschitz continuity has been shown to hold for a large class of DNNs, including DNNs for image classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition 5.1 (Lipschitz Continuity).</head><p>A network N is said to be Lipschitz continuous if there exists a real constant c ≥ 0 such that, for all x 1 , x 2 ∈ D L 1 :</p><formula xml:id="formula_13">||v[x 1 ] 1 -v[x 2 ] 1 || ≤ c • ||x 1 -x 2 ||<label>(9)</label></formula><p>Recall that v[x] 1 denotes the vector of activation values of the neurons in the input layer. The value c is called the Lipschitz constant, and the smallest such c is called the best Lipschitz constant, denoted as c best .</p><p>Since the computation of c best is an NP-hard problem and a smaller c can significantly improve the performance of verification algorithms <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b30">31]</ref>, it is interesting to determine whether a given c is a Lipschitz constant, either for the entire input space D L 1 or for some subspace. Testing for Lipschitz continuity can be guided using the following requirements. Definition 5.2 (Lipschitz Coverage). Given a real c &gt; 0 and an integer b &gt; 0, the set R Lip (b, c) of requirements for Lipschitz coverage is</p><formula xml:id="formula_14">{∃x 1 , x 2 .(||v[x 1 ] 1 -v[x 2 ] 1 || -c • ||x 1 -x 2 || &gt; 0) ∧x 1 , x 2 ∈ X | X ∈ S(D L 1 , b)}<label>(10)</label></formula><p>where the S(D L 1 , b) are given input subspaces.</p><p>Intuitively, for each X ∈ S(D L 1 , b), this requirement expresses the existence of two inputs x 1 and x 2 that refute that c is a Lipschitz constant for N . It is typically impossible to obtain full Lipschitz coverage, because there may exist inconsistent r ∈ R Lip (b, c). Thus, the goal for a test case generation algorithm is to produce a test suite T that satisfies the criterion as much as possible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Neuron Coverage</head><p>Neuron Coverage (NC) <ref type="bibr" target="#b17">[18]</ref> is an adaptation of statement coverage in conventional software testing to DNNs. It is defined as follows.</p><p>Definition 5.3. Neuron coverage for a DNN N requires a test suite T such that, for any hidden neuron n k,i , there exists test case t ∈ T such that ap[t] k,i = true. This is formalised with the following requirements R N C , each of which expresses that there is a test with an input x that activates the neuron n k,i , i.e., ap[x] k,i = true. </p><formula xml:id="formula_15">{∃x .ap[x] k,i = true | 2 ≤ k ≤ K -1, 1 ≤ i ≤ s k }<label>(11)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Modified Condition/Decision (MC/DC) Coverage</head><p>In <ref type="bibr" target="#b22">[23]</ref>, a family of four test criteria is proposed, inspired by MC/DC coverage in conventional software testing. We will restrict the discussion here to Sign-Sign Coverage (SSC). According to <ref type="bibr" target="#b22">[23]</ref>, each neuron n k +1, j can be seen as a decision where the neurons in the previous layer (i.e., the k-th layer) are conditions that define its activation value, as in Equation ( <ref type="formula" target="#formula_1">2</ref>). Adapting MC/DC to DNNs, we must show that all condition neurons can determine the outcome of the decision neuron independently. In the case of SSC coverage we say that the value of a decision or condition neuron changes if the sign of its activation function changes. Consequently, the requirements for SSC coverage are defined as follows.</p><p>Definition 5.5 (SSC Requirements). For SCC coverage, we first define a requirement R S SC (α) for a pair of neurons α = (n k,i , n k +1, j ):</p><formula xml:id="formula_16">{∃x 1 , x 2 . ap[x 1 ] k,i ap[x 2 ] k,i ∧ ap[x 1 ] k +1, j ap[x 2 ] k +1, j ∧ 1≤l ≤s k ,l i ap[x 1 ] k,l -ap[x 2 ] k,l = 0}</formula><p>(12) and we get</p><formula xml:id="formula_17">R S SC = 2≤k ≤K -2,1≤i ≤s k ,1≤j ≤s k +1 R S SC ((n k,i , n k +1, j ))<label>(13)</label></formula><p>That is, for each pair (n k,i , n k+1, j ) of neurons in two adjacent layers k and k + 1, we need two inputs x 1 and x 2 such that the sign change of n k,i independently affects the sign change of n k +1, j . Other neurons at layer k are required to maintain their signs between x 1 and x 2 to ensure that the change is independent. The idea of SS Coverage (and all other criteria in <ref type="bibr" target="#b22">[23]</ref>) is to ensure that not only the existence of a feature needs to be tested but also the effects of less complex features on a more complex feature must be tested.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Neuron Boundary Coverage</head><p>Neuron Boundary Coverage (NBC) <ref type="bibr" target="#b14">[15]</ref> aims at covering neuron activation values that exceed a given bound. It can be formulated as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition 5.6 (Neuron Boundary Coverage Requirements). Given two sets of bounds</head><formula xml:id="formula_18">h = {h k,i |2 ≤ k ≤ K -1, 1 ≤ i ≤ s k } and l = {l k,i |2 ≤ k ≤ K -1, 1 ≤ i ≤ s k }, the requirements R NBC (h, l) are {∃x . u[x] k,i -h k,i &gt; 0, ∃x . u[x] k,i -l k,i &lt; 0 | 2 ≤ k ≤ K -1, 1 ≤ i ≤ s k }<label>(14)</label></formula><p>where h k,i and l k,i are the upper and lower bounds on the activation value of a neuron n k,i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">OVERVIEW OF OUR APPROACH</head><p>This section gives an overview of our method for generating a test suite for a given DNN. Our method alternates between concrete evaluation of the activation patterns of the DNN and symbolic generation of new inputs. The pseudocode for our method is given as Algorithm 1. It is visualised in Figure <ref type="figure" target="#fig_1">1</ref>. Algorithm 1 takes as inputs a DNN N , an input t 0 for the DNN, a heuristic δ , and a set R of coverage requirements, and produces a test suite T as output. The test suite T initially only contains the given test input t 0 . The algorithm removes a requirement r ∈ R from R once it is satisfied by T , i.e., T |= r .</p><p>The function requirement_evaluation (Line 7), whose details are given in Section 7, looks for a pair (t, r )<ref type="foot" target="#foot_1">foot_1</ref> of input and requirement that, according to our concrete evaluation, is the most promising candidate for a new test case t ′ that satisfies the requirement r . The heuristic δ is a transformation function that maps a formula r with operator ∃ to an optimisation problem. This step relies on concrete execution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Concolic Testing for DNNs</head><formula xml:id="formula_19">INPUT: N, R, δ, t 0 OUTPUT: T 1: T ← {t 0 } and F = {} 2: t ← t 0 3: while R \ S ∅ do 4: for each r ∈ R do 5: if T |= r then R ← R \ {r } 6:</formula><p>while true do 7:</p><p>t, r ← requirement_evaluation(T , δ (R))</p><formula xml:id="formula_20">8:</formula><p>t ′ ← symbolic_analysis(t, r ) F ← F ∪ {r } 14: break 15: return T After obtaining (t, r ), symbolic_analysis (Line 8), whose details are in Section 8, is applied to obtain a new concrete input t ′ . Then a function validity_check (Line 9), whose details are given in Section 9, is applied to check whether the new input is valid or not. If so, the test is added to the test suite. Otherwise, ranking and symbolic input generation are repeated until a given computational cost is exceeded, after which test generation for the requirement is deemed to have failed. This is recorded in the set F .</p><p>The algorithm terminates when either all test requirements have been satisfied, i.e., R = ∅, or no further requirement in R can be satisfied, i.e., F = R. It then returns the current test suite T .</p><p>Finally, as illustrated in Figure <ref type="figure" target="#fig_1">1</ref>, the test suite T generated by Algorithm 1, is passed to an oracle in order to evaluate the robustness of the DNN. The details of the oracle are in Section 9.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">RANKING COVERAGE REQUIREMENTS</head><p>This section presents our approach for Line 7 of Algorithm 1. Given a set of requirements R that have not yet been satisfied, a heuristic δ , and the current set T of test inputs, the goal is to select a concrete input t ∈ T together with a requirement r ∈ R, both of which will be used later in a symbolic approach to compute the next concrete input t ′ (to be given in Section 8). The selection of t and r is done by means of a series of concrete executions.</p><p>The general idea is as follows. For all requirements r ∈ R, we transform r into δ (r ) by utilising operators arg opt for opt ∈ {max, min} that will be evaluated by concretely executing tests in T . As R may contain more than one requirement, we return the pair (t, r ) such that</p><formula xml:id="formula_21">r = arg max r {val(t, δ (r )) | r ∈ R}. (<label>15</label></formula><formula xml:id="formula_22">)</formula><p>Note that, when evaluating arg opt formulas (e.g., arg min x a : e), if an input t ∈ T is returned, we may need the value (min x a : e) as well. We use val(t, δ (r )) to denote such a value for the returned input t and the requirement formula r . The formula δ (r ) is an optimisation objective together with a set of constraints. We will give several examples later in Section 7.1. In the following, we extend the semantics in Definition 4.3 to work with formulas with arg opt operators for opt ∈ {max, min}, including arg opt x a : e and arg opt x 1 ,x 2 a : e. Intuitively, arg max x a : e (arg min x a : e, resp.) determines the input x among those satisfying the Boolean formula e that maximises (minimises) the value of the arithmetic formula a. Formally,</p><p>• the evaluation of arg min x a : e on T returns an input t ∈ T such that, T |= e[x → t] and for all t ′ ∈ T such that</p><formula xml:id="formula_23">T |= e[x → t ′ ] we have a[x → t] ≤ a[x → t ′ ]. • the evaluation of T |= arg min x 1 ,x 2 a : e on T returns two inputs t 1 , t 1 ∈ T such that, T |= e[x 1 → t 1 ][x 2 → t 2 ] and for all t ′ 1 , t ′ 2 ∈ T such that T |= e[x 1 → t ′ 1 ][x 2 → t ′ 2 ] we have a[x 1 → t 1 ][x 2 → t 2 ] ≤ a[x 1 → t ′ 1 ][x 2 → t ′ 2 ]</formula><p>. The cases for arg max formulas are similar to those for arg min, by replacing ≤ with ≥. Similarly to Definition 4.3, the semantics is for a set T of test cases and we can adapt it to a continuous input subspace X ⊆ D L 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Heuristics</head><p>We present the heuristics δ we use the coverage requirements discussed in Section 5. We remark that, since δ is a heuristic, there exist alternatives. The following definitions work well in our experiments.</p><p>7.1.1 Lipschitz Continuity. When a Lipschitz coverage requirement r as in Equation ( <ref type="formula" target="#formula_14">10</ref>) is not satisfied by T , we transform it into δ (r ) as follows: arg max</p><formula xml:id="formula_24">x 1 ,x 2 .||v[x 1 ] 1 -v[x 2 ] 1 || -c • ||x 1 -x 2 || : x 1 , x 2 ∈ X<label>(16)</label></formula><p>I.e., the aim is to find the best t 1 and t 2 in T to make</p><formula xml:id="formula_25">||v[t 1 ] 1 - v[t 2 ] 1 || -c • ||t 1 -t 2 ||</formula><p>as large as possible. As described, we also need to compute val(t</p><formula xml:id="formula_26">1 , t 2 , r ) = ||v[t 1 ] 1 -v[t 2 ] 1 || -c • ||t 1 -t 2 ||.</formula><p>7.1.2 Neuron Coverage. When a requirement r as in Equation ( <ref type="formula" target="#formula_15">11</ref>) is not satisfied by T , we transform it into the following requirement:</p><formula xml:id="formula_27">arg max x c k • u k,i [x] : true<label>(17)</label></formula><p>We obtain the input t ∈ T that has the maximal value for</p><formula xml:id="formula_28">c k •u k,i [x].</formula><p>The coefficient c k is a per-layer constant. It motivated by the following observation. Given the way signals propagate in the DNN, activation values in each layer can be of different magnitudes. For example, if the minimum activation value of neurons at layer k and k + 1 are -10 and -100, respectively, then even when a neuron u</p><formula xml:id="formula_29">[x] k,i = -1 &gt; -2 = u[x] k +1, j ,</formula><p>we may still regard n k+1, j as closer to be activated than u k,i . Consequently, for each layer we define a separate factor c k that normalises the average activation valuations of neurons at different layers to the same magnitude level. It is estimated by sampling a sufficiently large input dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.3">SS Coverage.</head><p>In SS Coverage, given a decision neuron n k+1, j , the concrete evaluation aims to select one of its condition neurons n k,i at layer k such that the test input that is generated negates the signs of n k,i and n k +1, j while the remainder of n k +1, j 's condition neurons preserve their respective signs. This is achieved by the following δ (r ):</p><formula xml:id="formula_30">arg max x -c k • |u[x] k,i | : true<label>(18)</label></formula><p>Intuitively, given the decision neuron n k +1, j , Equation ( <ref type="formula" target="#formula_30">18</ref>) selects the condition that is closest to the change of activation sign (i.e., yields the smallest |u[x] k,i |).</p><p>7.1.4 Neuron Boundary Coverage. We transform the requirement r in Equation ( <ref type="formula" target="#formula_31">19</ref>) into the following δ (r ) when it is not satisfied by T ; it selects the neuron that is closest to either the higher or lower boundary.</p><formula xml:id="formula_31">arg max x c k • (u[x] k,i -h k,i ) : true arg max x c k • (l k,i -u[x] k,i ) : true<label>(19)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">SYMBOLIC GENERATION OF NEW CONCRETE INPUTS</head><p>This section presents our approach for Line 8 of Algorithm 1. That is, given a concrete input t and a requirement r , we need to find the next concrete input t ′ by symbolic analysis. This new t ′ will be added into the test suite (Line 10 of Algorithm 1). The symbolic analysis techniques to be considered include the linear programming in <ref type="bibr" target="#b22">[23]</ref>, global optimisation for the L 0 norm in <ref type="bibr" target="#b20">[21]</ref>, and a new optimisation algorithm that will be introduced below. We regard optimisation algorithms as symbolic analysis methods because, similarly to constraint solving methods, they work with a set of test cases in a single run.</p><p>To simplify the presentation, the following description may, for each algorithm, focus on some specific coverage requirements, but we remark that all algorithms can work with all the requirements given in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1">Symbolic Analysis using Linear Programming</head><p>As explained in Section 4, given an input x, the DNN instance N [x] maps to an activation pattern ap[x] that can be modeled using Linear Programming (LP). In particular, the following linear constraints <ref type="bibr" target="#b22">[23]</ref> yield a set of inputs that exhibit the same ReLU behaviour as x:</p><formula xml:id="formula_32">{u k,i = 1≤j ≤s k -1 {w k -1, j,i • v k-1,j } + b k,i | k ∈ [2, K], i ∈ [1..s k ]} (20) {u k,i ≥ 0 ∧ u k,i = v k,i | ap[x] k,i = true, k ∈ [2, K), i ∈ [1..s k ]} ∪{u k,i &lt; 0 ∧ v k,i = 0 | ap[x] k,i = false, k ∈ [2, K), i ∈ [1..s k ]}<label>(21)</label></formula><p>Continuous variables in the LP model are emphasized in bold.</p><p>• The activation value of each neuron is encoded by the linear constraint in <ref type="bibr" target="#b19">(20)</ref>, which is a symbolic version of Equation ( <ref type="formula" target="#formula_1">2</ref>) that calculates a neuron's activation value. • Given a particular input x, the activation pattern (Definition 4.1) ap[x] is known: ap[x] k,i is either true or false, which indicates whether the ReLU is activated or not for the neuron n k,i . Following (3) and the definition of ReLU in (1), for every neuron n k,i , the linear constraints in <ref type="bibr" target="#b20">(21)</ref> encode ReLU activation (when ap[x] k,i = true) or deactivation (when</p><formula xml:id="formula_33">ap[x] k,i = false).</formula><p>The linear model (denoted as C) given by ( <ref type="formula">20</ref>) and ( <ref type="formula" target="#formula_32">21</ref>) represents an input set that results in the same activation pattern as encoded. Consequently, the symbolic analysis for finding a new input t ′ from a pair (t, r ) of input and requirement is equivalent to finding a new activation pattern. Note that, to make sure that the obtained test case is meaningful, an objective is added to the LP model that minimizes the distance between t and t ′ . Thus, the use of LP requires that the distance metric is linear. For instance, this applies to the L ∞ -norm in <ref type="bibr" target="#b5">(6)</ref>, but not to the L 2 -norm. </p><formula xml:id="formula_34">{ap ′ k,i = ¬ap[t] k,i ∧ ∀k 1 &lt; k : 0≤i 1 ≤s k 1 ap ′ k 1 ,i 1 = ap[t] k 1 ,i 1 } (22)</formula><p>This activation pattern specifies the following conditions.</p><p>• n k,i 's activation sign is negated: this encodes the goal to activate n k,i . • In the new activation pattern ap ′ , the neurons before layer k preserve their activation signs as in ap[t]. Though there may exist multiple activation patterns that make n k,i activated, for the use of LP modeling one particular combination of activation signs must be pre-determined. • Other neurons are irrelevant, as the sign of n k,i is only affected by the activation values of those neurons in previous layers.</p><p>Finally, the new activation pattern ap ′ defined in ( <ref type="formula">22</ref>) is encoded by the LP model C using ( <ref type="formula">20</ref>) and ( <ref type="formula" target="#formula_32">21</ref>), and if there exists a feasible solution, then the new test input t ′ , which satisfies the requirement r , can be extracted from that solution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1.2">SS Coverage.</head><p>To satisfy an SS Coverage requirement r , we need to find a new test case such that, with respect to the input t, the activation signs of n k +1, j and n k,i are negated, while other signs of other neurons at layer k are equal to those for input t.</p><p>To achieve this, the following activation pattern ap ′ is constructed.</p><formula xml:id="formula_35">{ap ′ k,i = ¬ap[t] k,i ∧ ap ′ k +1, j = ¬ap[t] k +1, j ∧∀k 1 &lt; k : 1≤i 1 ≤s k 1 ap ′ k 1 ,i 1 = ap[t] k 1 ,i 1 } 8.1.</formula><p>3 Neuron Boundary Coverage. In case of the neuron boundary coverage, the symbolic analysis aims to find an input t ′ such that the activation value of neuron n k,i exceeds either its higher bound h k,i or its lower bound l k,i .</p><p>To achieve this, while preserving the DNN activation pattern ap[t], we add one of the following constraints to the LP program.</p><formula xml:id="formula_36">• If u[x] k,i -h k,i &gt; l k,i -u[x] k,i : u k,i &gt; h k,i ; • otherwise: u k,i &lt; l k,i .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2">Symbolic Analysis using Global Optimisation</head><p>The symbolic analysis for finding a new input can also be implemented by solving the global optimisation problem in <ref type="bibr" target="#b20">[21]</ref>. That is, by specifying the test requirement as an optimisation objective, we apply global optimisation to compute a test case that satisfies the test coverage requirement. Whenever we identify a test input t ′ that fails to pass this oracle, then it serves as evidence that the DNN lacks robustness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10">EXPERIMENTAL RESULTS</head><p>We have implemented the concolic testing approach presented in this paper in a tool we have named DeepConcolic 3 . We compare it with other tools for testing DNNs. The experiments are run on a machine with 24 core Intel(R) Xeon(R) CPU E5-2620 v3 and 2.4 GHz and 125 GB memory. We use a timeout of 12 h. All coverage results are averaged over 10 runs or more.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.1">Comparison with DeepXplore</head><p>We now compare DeepConcolic and DeepXplore <ref type="bibr" target="#b17">[18]</ref> on DNNs obtained from the MNIST and CIFAR-10 datasets. We remark that DeepXplore has been applied to further datasets.</p><p>For each tool, we start neuron cover testing from a randomly sampled image input. Note that, since DeepXplore requires more than one DNN, we designate our trained DNN as the target model and utilise the other two default models provided by DeepXplore. Table <ref type="table" target="#tab_5">2</ref> gives the neuron coverage obtained by the two tools. We observe that DeepConcolic yields much higher neuron coverage than DeepXplore in any of its three modes of operation ('light', 'occlusion', and 'blackout'). On the other hand, DeepXplore is much faster and terminates in seconds. -norm and L 0 -norm) and DeepXplore. Although DeepConcolic does not impose particular domain-specific constraints on the original image as DeepXplore does, concolic testing generates images that resemble "human perception". For example, based on the L ∞ -norm, it produces adversarial examples (Figure <ref type="figure" target="#fig_3">2</ref>, top row) that gradually reverse the black and white colours. For the L 0 -norm, DeepConcolic generates adversarial examples similar to those of DeepXplore under the 'blackout' constraint, which is essentially pixel manipulation. 3 The implementation and all data in this section are available online at <ref type="url" target="https://github.com/TrustAI/DeepConcolic">https://github.com/TrustAI/DeepConcolic</ref>  The full coverage report, including the average coverage and standard derivation, is given in Figure <ref type="figure" target="#fig_4">3</ref>. Table <ref type="table" target="#tab_6">3</ref> contains the adversarial example results. We have observed that the overhead for the symbolic analysis with global optimisation (Section 8.2) is too high. Thus, the SSC result with L 0 -norm is excluded.</p><p>Overall, DeepConcolic achieves high coverage and, using the robustness check (Definition 9.2), detects a significant number of adversarial examples. However, coverage of corner-case activation values (i.e., NBC) is limited.</p><p>Concolic testing is able to find adversarial examples with the minimum possible distance: that is, 1 255 ≈ 0.0039 for the L ∞ norm and 1 pixel for the L 0 norm. Figure <ref type="figure">4</ref> gives the average distance of adversarial examples (from one DeepConcolic run). Remarkably, for the same network, the number of adversarial examples found with NC can vary substantially when the distance metric is changed. This observation suggests that, when designing coverage criteria for DNNs, they need to be examined using a variety of distance metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.3">Results for Lipschitz Constant Testing</head><p>This section reports experimental results for the Lipschitz constant testing on DNNs. We test Lipschitz constants ranging over {0.01 : 0.01 : 20} on 50 MNIST images and 50 CIFAR-10 images respectively. Every image represents a subspace in D L 1 and thus a requirement in Equation <ref type="bibr" target="#b9">(10)</ref>. 10.3.1 Baseline Method. Since this paper is the first to test Lipschitz constants of DNNs, we compare our method with random test case generation. For this specific test requirement, given a predefined Lipschitz constant c, an input t 0 and the radius of norm ball (e.g., for L 1 and L 2 norms) or hypercube space (for L ∞ -norm) ∆, we randomly generate two test pairs t 1 and t 2 that satisfy the space constraint (i.e., ||t 1 -t 0 || D 2 ≤ ∆ and ||t 2 -t 0 || D 2 ≤ ∆), and then check whether Lip(t 1 , t 2 ) &gt; c holds. We repeat the random generation until we find a satisfying test pair or the number of repetitions is larger than a predefined threshold. We set such threshold as N r d = 1, 000, 000. Namely, if we randomly generate 1,000,000  Figure <ref type="figure">5</ref> (b) and (c) compare the Lipschitz constant coverage of test pairs from the random method and the concolic method on both MNIST and CIFAR-10 models. Our method significantly outperforms random test case generation. We note that covering a large Lipschitz constant range for DNNs is a challenging problem since most image pairs (within a certain high-dimensional space) can produce small Lipschitz constants (such as 1 to 2). This explains the reason why randomly generated test pairs concentrate in a range of less than 3. However, for safety-critical applications such as self-driving cars, a DNN with a large Lipschitz constant essentially indicates it is more vulnerable to adversarial perturbations <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21]</ref>. As a result, a test method that can cover larger Lipschitz constants provides a useful robustness indicator for a trained DNN. We argue that, for safety testing of DNNs, the concolic test method for Lipschitz constant coverage can complement existing methods to achieve significantly better coverage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11">CONCLUSIONS</head><p>In this paper, we propose the first concolic testing method for DNNs. We implement it in a software tool and apply the tool to evaluate the robustness of well-known DNNs. The generation of the test inputs can be guided by a variety of coverage metrics, including Lipschitz continuity. Our experimental results confirm that the combination of concrete execution and symbolic analysis delivers both coverage and automates the discovery of adversarial examples.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Definition 5 . 4 (</head><label>54</label><figDesc>NC Requirements). The set R N C of coverage requirements for Neuron Coverage is</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Overview of our concolic testing method</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Adversarial images, with L ∞ -norm for MNIST (top row) and L 0 -norm for CIFAR-10 (bottom row), generated by DeepConcolic and DeepXplore, the latter with image constraints 'light', 'occlusion', and 'blackout'.</figDesc><graphic coords="9,53.80,83.69,240.24,84.21" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Coverage results for different criteria 10.2 Results for NC, SCC, and NBC We give the results obtained with DeepConcolic using the coverage criteria NC, SSC, and NBC. DeepConcolic starts NC testing with one single seed input. For SSC and NBC, to improve the performance, an initial set of 1000 images are sampled. Furthermore, we only test a subset of the neurons for SSC and NBC. A distance upper bound of 0.3 (L ∞ -norm) and 100 pixels (L 0 -norm) is set up for collecting adversarial examples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :Figure 5 :</head><label>45</label><figDesc>Figure 4: (a) Distance of NC, SSC, and NBC on MINIST and CIFAR-10 datasets based on L ∞ norm; (b) Distance of NC and NBC on the two datasets based on L 0 norm.</figDesc><graphic coords="10,202.29,88.41,120.05,94.97" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Comparison with different coverage-driven DNN testing methods , Φ) such that L = {L k |k ∈ {1, . . . , K }} is a set of layers, T ⊆ L × L is a set of connections between layers, and Φ = {ϕ k |k ∈ {2, . . . , K }} is a set of activation functions. Each layer L k consists of s k neurons, and the l-th neuron of layer k is denoted by n k,l . We use v k,l to denote the value of n k,l . Values of neurons in hidden layers (with 1 &lt; k &lt; K) need to pass through a Rectified Linear Unit (ReLU)</figDesc><table><row><cell>• DeepXplore requires a set of DNNs to explore multiple gradi-</cell></row><row><cell>ent directions. The other methods, including DeepConcolic,</cell></row><row><cell>need a single DNN only.</cell></row><row><cell>• In contrast to the other methods, DeepConcolic can achieve</cell></row><row><cell>good coverage by starting from a single input; the other</cell></row><row><cell>methods need a non-trivial set of inputs.</cell></row><row><cell>• Until now, there is no conclusion on the best distance metric.</cell></row><row><cell>DeepConcolic can be parameterized with a desired norm</cell></row><row><cell>distance metric || • ||.</cell></row><row><cell>Moreover, DeepConcolic features a clean separation between the</cell></row><row><cell>generation of test inputs and the test oracle. This is a good fit for</cell></row><row><cell>traditional test case generation. The other methods use the oracle</cell></row><row><cell>as part of their objectives to guide the generation of test inputs.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>the activation values of each neuron n k,l of the network before and after ReLU are denoted as u[t] k,l and v[t] k,l , respectively, and the final classification label is label[t]. We write u[t] k and v[t] k for 1 ≤ k ≤ s k to denote the vectors of activations for neurons in layer k.• When the input is given, the activation or deactivation of each ReLU operator in the DNN is determined.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>• p, a 1 + a 2 , and a 1 -a 2 have the standard semantics.Note that T is finite. It is trivial to extend the definition of the satisfaction relation to an infinite subspace of inputs.</figDesc><table /><note><p>e 2 if T |= e 1 and T |= e 2 • T |= ¬e if not T |= e • T |= |{e 1 , ..., e m }| ▷◁ q if |{e i | T |= e i , i ∈ {1, ..., m}}| ▷◁ q For the evaluation of arithmetic expression a over an input t, • u[t] k,l and v[t] k,l derive their values from the activation patters of the DNN for test t, and c • u[t] k,l and c • v[t] k,l have the standard meaning where c is a coefficient,</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>8.1.1 Neuron Coverage. The symbolic analysis of neuron coverage takes the input test case t and requirement r on the activation of neuron n k,i , and returns a new test t ′ such that the test requirement is satisfied by the network instance N [t ′ ]. We have the activation pattern ap[t] of the given N [t], and can build up a new activation pattern ap ′ such that</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 :</head><label>2</label><figDesc>Neuron coverage of DeepConcolic and DeepXplore</figDesc><table><row><cell></cell><cell cols="2">DeepConcolic</cell><cell></cell><cell>DeepXplore</cell><cell></cell></row><row><cell></cell><cell cols="2">L ∞ -norm L 0 -norm</cell><cell>light</cell><cell cols="2">occlusion blackout</cell></row><row><cell>MNIST</cell><cell>97.60%</cell><cell>95.91%</cell><cell>80.77%</cell><cell>82.68%</cell><cell>81.61%</cell></row><row><cell>CIFAR-10</cell><cell>84.98%</cell><cell>98.63%</cell><cell>77.56%</cell><cell>81.48%</cell><cell>83.25%</cell></row><row><cell cols="6">Figure 2 presents several adversarial examples found by Deep-</cell></row><row><cell cols="2">Concolic (with L ∞</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>Adversarial examples by test criteria, distance metrics, and DNN models dist. adversary % minimum dist. adversary % minimum dist. adversary % minimum dist. test pairs and none of them can satisfy the Lipschitz constant requirement &gt; c, we treat this test as a failure and return the largest Lipschitz constant found and the corresponding test pair; otherwise, we treat it as successful and return the satisfying test pair. 10.3.2 Experimental Results. Figure 5 (a) depicts the Lipschitz Constant Coverage generated by 1,000,000 random test pairs and our concolic test generation method for image-1 on MNIST DNNs. As we can see, even though we produce 1,000,000 test pairs by random test generation, the maximum Lipschitz converage reaches only 3.23 and most of the test pairs are in the range [0.01, 2]. Our concolic method, on the other hand, can cover a Lipschitz range of [0.01, 10.38], where most cases lie in [3.5, 10], which is poorly covered by random test generation.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>L ∞ -norm</cell><cell></cell><cell></cell><cell></cell><cell>L 0 -norm</cell><cell></cell></row><row><cell></cell><cell>MNIST</cell><cell></cell><cell></cell><cell>CIFAR-10</cell><cell>MNIST</cell><cell></cell><cell>CIFAR-10</cell><cell></cell></row><row><cell cols="3">adversary % minimum NC 13.93% 0.0039</cell><cell>0.79%</cell><cell>0.0039</cell><cell>0.53%</cell><cell>1</cell><cell>5.59%</cell><cell>1</cell></row><row><cell>SSC</cell><cell>0.02%</cell><cell>0.1215</cell><cell>0.36%</cell><cell>0.0039</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>NBC</cell><cell>0.20%</cell><cell>0.0806</cell><cell>7.71%</cell><cell>0.0113</cell><cell>0.09%</cell><cell>1</cell><cell>4.13%</cell><cell>1</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>Complexity. Given a network N , a DR requirement formula r , and a test suite T , checking T |= r can be done in time that is polynomial in the size of T . Determining whether there exists a test suite T with T |= r is NP-complete.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>For some requirements, we might return two inputs t 1 and t 2 . Here, for simplicity, we describe the case for a single input. The generalisation to two inputs is straightforward.</p></note>
		</body>
		<back>

			
			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. This document is an overview of UK MOD (part) sponsored research and is released for informational purposes only. The contents of this document should not be interpreted as representing the views of the UK MOD, nor should it be assumed that they reflect any current or future UK MOD policy. The information contained in this document cannot supersede any statutory or contractual requirements or liabilities and is offered without prejudice or commitment.</p></div>
			</div>


			<div type="funding">
<div><p>Testing for <rs type="institution">Deep Neural Networks. In Proceedings of . ACM, New York, NY, USA</rs>, 11 pages. <ref type="url" target="https://doi.org/10.1145/nnnnnnn.nnnnnnn">https://doi.org/10. 1145/nnnnnnn.nnnnnnn</ref> * Kwiatkowska and Ruan are supported by <rs type="funder">EPSRC</rs> <rs type="grantName">Mobile Autonomy Programme Grant</rs> (<rs type="grantNumber">EP/M019918/1</rs>). Wu is supported by the <rs type="funder">CSC-PAG Oxford Scholarship</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_EYXnaFQ">
					<idno type="grant-number">EP/M019918/1</idno>
					<orgName type="grant-name">Mobile Autonomy Programme Grant</orgName>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>• For Neuron Coverage, the objective is to find a t ′ such that the specified neuron n k,i has ap[t ′ ] k,i =true. • In case of SS Coverage, given the neuron pair (n k,i , n k +1, j )</p><p>and the original input t, the optimisation objective becomes</p><p>• Regarding the Neuron Boundary Coverage, depending on whether the higher bound or lower bound for the activation of n k,i is considered, the objective of finding a new input t ′ is either</p><p>Readers are referred to <ref type="bibr" target="#b20">[21]</ref> for the details of the algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.3">Lipschitz Test Case Generation</head><p>Given a coverage requirement as in Equation ( <ref type="formula">10</ref>) for a subspace X , we let t 0 ∈ R n be the representative point of the subspace X to which t 1 and t 2 belong. The optimisation problem is to generate two inputs t 1 and t 2 such that</p><p>where || * || D 1 and || * || D 2 denote norm metrics such as the L 0 -norm, L 2 -norm or L ∞ -norm, and ∆ is the radius of a norm ball (for the L 1 and L 2 -norm) or the size of a hypercube (for the L ∞ -norm) centered on t 0 . The constant ∆ is a hyper-parameter of the algorithm.</p><p>The above problem can be efficiently solved by a novel alternating compass search scheme. Specifically, we alternate between solving the following two optimisation problems through relaxation <ref type="bibr" target="#b18">[19]</ref>, i.e., maximizing the lower bound of the original Lipschitz constant instead of directly maximizing the Lipschitz constant itself. To do so, we reformulate the original non-linear proportional optimisation as a linear problem when both norm metrics || * || D 1 and || * || D 2 are the L ∞ -norm.</p><p>The objective above enables the algorithm to search for an optimal t 1 in the space of a norm ball or hypercube centered on t 0 with radius ∆, maximising the norm distance of</p><p>is the lower bound of Lip(t 1 , t 0 ). Therefore, the search for a trace that minimises F (t 1 , t 0 ) increases the Lipschitz constant.</p><p>To solve the problem above we use the compass search method <ref type="bibr" target="#b1">[2]</ref>, which is efficient, derivative-free, and guaranteed to provide firstorder global convergence. Because we aim to find an input pair that refutes the given Lipschitz constant c instead of finding the largest possible Lipschitz constant, along each iteration, when we get t1 , we check whether Lip( t1 , t 0 ) &gt; c. If it holds, we find an input pair t1 and t 0 that satisfies the test requirement; otherwise, we continue the compass search until convergence or a satisfiable input pair is generated. If Equation ( <ref type="formula">24</ref>) is convergent and we can find an optimal t 1 as </p><p>Similarly, we use derivative-free compass search to solve the above problem and check whether Lip(t * 1 , t 2 ) &gt; c holds at each iterative optimisation trace t2 . If it holds, we return the image pair t * 1 and t2 that satisfies the test requirement; otherwise, we continue the optimisation until convergence or a satisfiable input pair is generated. If Equation ( <ref type="formula">25</ref>) is convergent at t * 2 , and we still cannot find such a input pair, we modify the objective function again by letting t * 1 = t * 2 in Equation ( <ref type="formula">25</ref>) and continue the search and satisfiability checking procedure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.3.3">Stage Three. If the function Lip(t *</head><p>1 , t * 2 ) fails to make progress in Stage Two, we treat the whole search procedure as convergent and have failed to find an input pair that can refute the given Lipschitz constant c. In this case, we return the best input pair we found so far, i.e., t * 1 and t * 2 , and the largest Lipschitz constant Lip(t * 1 , t 2 ) observed. Note that the returned constant is smaller than c.</p><p>In summary, the proposed method is an alternating optimisation scheme based on compass search. Basically, we start from the given t 0 to search for an image t 1 in a norm ball or hypercube, where the optimisation trajectory on the norm ball space is denoted as S(t 0 , ∆(t 0 ))) such that Lip(t 0 , t 1 ) &gt; c (this step is symbolic execution); if we cannot find it, we modify the optimisation objective function by replacing t 0 with t * 1 (the best concrete input found in this optimisation run) to initiate another optimisation trajectory on the space, i.e., S(t * 1 , ∆(t 0 )). This process is repeated until we have gradually covered the entire space S(∆(t 0 )) of the norm ball.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">TEST ORACLE</head><p>We provide details about the validity checking performed for the generated test inputs (Line 9 of Algorithm 1) and how the test suite is finally used to quantify the safety of the DNN. Definition 9.1 (Valid test input). We are given a set O of inputs for which we assume to have a correct classification (e.g., the training dataset). Given a real number b, a test input t ′ ∈ T is said to be</p><p>Intuitively, a test case t is valid if it is close to some of the inputs for which we have a classification. Given a test input t ′ ∈ T , we write O(t ′ ) for the input t ∈ O that has the smallest distance to t ′ among all inputs in O.</p><p>To quantify the quality of the DNN using a test suite T , we use the following robustness criterion. Definition 9.2 (Robustness Oracle). Given a set O of classified inputs, a test case t ′ passes the robustness oracle if arg max j v[t ′ ] K, j = arg max j v[O(t ′ )] K, j <ref type="bibr" target="#b26">(27)</ref> </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.07875</idno>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">Wasserstein GAN. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Derivative-Free and Blackbox Optimization</title>
		<author>
			<persName><forename type="first">Charles</forename><surname>Audet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Warren</forename><surname>Hare</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">Radu</forename><surname>Balan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maneesh</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongmian</forename><surname>Zou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.05217</idno>
		<title level="m">Lipschitz Properties for Deep Convolutional Networks</title>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Heuristics for Scalable Dynamic Test Generation</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Burnim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koushik</forename><surname>Sen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Automated Software Engineering, ASE. 23rd International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="443" to="446" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">KLEE: Unassisted and Automatic Generation of High-Coverage Tests for Complex Systems Programs</title>
		<author>
			<persName><forename type="first">Cristian</forename><surname>Cadar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Dunbar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawson</forename><forename type="middle">R</forename><surname>Engler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">OSDI</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="209" to="224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Automatically Generating Search Heuristics for Concolic Testing</title>
		<author>
			<persName><forename type="first">Sooyoung</forename><surname>Cha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seongjoon</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junhee</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hakjoo</forename><surname>Oh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th International Conference on Software Engineering, ICSE. ACM</title>
		<meeting>the 40th International Conference on Software Engineering, ICSE. ACM</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1244" to="1254" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">AI2: Safety and Robustness Certification of Neural Networks with Abstract Interpretation</title>
		<author>
			<persName><forename type="first">Timon</forename><surname>Gehr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Mirman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dana</forename><surname>Drachsler-Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Petar</forename><surname>Tsankov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Swarat</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Vechev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Security and Privacy (SP), 2018 IEEE Symposium</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">DART: Directed Automated Random Testing</title>
		<author>
			<persName><forename type="first">Patrice</forename><surname>Godefroid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nils</forename><surname>Klarlund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koushik</forename><surname>Sen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGPLAN Conference on Programming Language Design and Implementation</title>
		<meeting>the ACM SIGPLAN Conference on Programming Language Design and Implementation</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="213" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Automated Whitebox Fuzz Testing</title>
		<author>
			<persName><forename type="first">Patrice</forename><surname>Godefroid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">A</forename><surname>Michael Y Levin</surname></persName>
		</author>
		<author>
			<persName><surname>Molnar</surname></persName>
		</author>
		<author>
			<persName><surname>Others</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In NDSS</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="151" to="166" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">A Practical Tutorial on Modified Condition/Decision Coverage</title>
		<author>
			<persName><forename type="first">Kelly</forename><surname>Hayhurst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Veerhusen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Chilenski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leanna</forename><surname>Rierson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
		<respStmt>
			<orgName>NASA</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Safety Verification of Deep Neural Networks</title>
		<author>
			<persName><forename type="first">Xiaowei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marta</forename><surname>Kwiatkowska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Aided Verification, CAV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Exploratory Testing</title>
		<author>
			<persName><forename type="first">Cem</forename><surname>Kaner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Quality Assurance Institute Worldwide Annual Software Testing Conference</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Challenges and Opportunities with Concolic Testing</title>
		<author>
			<persName><forename type="first">Raghudeep</forename><surname>Kannavara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">J</forename><surname>Havlicek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Mark R Tuttle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandip</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Aerospace and Electronics Conference (NAECON)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015. 2015</date>
			<biblScope unit="page" from="374" to="378" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Reluplex: An Efficient SMT Solver for Verifying Deep Neural Networks</title>
		<author>
			<persName><forename type="first">Guy</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clark</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">L</forename><surname>Dill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Julian</surname></persName>
		</author>
		<author>
			<persName><surname>Mykel</surname></persName>
		</author>
		<author>
			<persName><surname>Kochenderfer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Aided Verification</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="97" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">Lei</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Juefei-Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiyuan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunyang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fuyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minhui</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><surname>Others</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.07519</idno>
		<title level="m">DeepGauge: Comprehensive and Multi-Granularity Testing Criteria for Gauging the Robustness of Deep Learning Systems</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Differentiable Abstract Interpretation for Provably Robust Neural Networks</title>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Mirman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timon</forename><surname>Gehr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Vechev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3575" to="3583" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Rectified Linear Units Improve Restricted Boltzmann Machines</title>
		<author>
			<persName><forename type="first">Vinod</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="807" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">DeepXplore: Automated Whitebox Testing of Deep Learning Systems</title>
		<author>
			<persName><forename type="first">Kexin</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinzhi</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junfeng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suman</forename><surname>Jana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th Symposium on Operating Systems Principles. ACM</title>
		<meeting>the 26th Symposium on Operating Systems Principles. ACM</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Relaxation in Optimization Theory and Variational Calculus</title>
		<author>
			<persName><forename type="first">T</forename><surname>Roubicek</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997">1997</date>
			<publisher>Walter de Gruyter</publisher>
			<pubPlace>Berlin</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Reachability Analysis of Deep Neural Networks with Provable Guarantees</title>
		<author>
			<persName><forename type="first">Wenjie</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaowei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marta</forename><surname>Kwiatkowska</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 27th International Joint Conference on Artificial Intelligence, IJCAI</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2651" to="2659" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">Wenjie</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Youcheng</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaowei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Kroening</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marta</forename><surname>Kwiatkowska</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.05805v1</idno>
		<title level="m">Global Robustness Evaluation of Deep Neural Networks with Provable Guarantees for L0 Norm</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">CUTE: A Concolic Unit Testing Engine for C</title>
		<author>
			<persName><forename type="first">Koushik</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Darko</forename><surname>Marinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gul</forename><surname>Agha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGSOFT Software Engineering Notes</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="263" to="272" />
			<date type="published" when="2005">2005. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">Youcheng</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaowei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Kroening</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.04792</idno>
		<title level="m">Testing Deep Neural Networks</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Intriguing Properties of Neural Networks</title>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">DeepTest: Automated Testing of Deep-Neural-Network-Driven Autonomous Cars</title>
		<author>
			<persName><forename type="first">Yuchi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kexin</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suman</forename><surname>Jana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baishakhi</forename><surname>Ray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th International Conference on Software Engineering</title>
		<meeting>the 40th International Conference on Software Engineering</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="303" to="314" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Test Input Generation with Java PathFinder</title>
		<author>
			<persName><forename type="first">Willem</forename><surname>Visser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Corina</forename><forename type="middle">S</forename><surname>Pasareanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarfraz</forename><surname>Khurshid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGSOFT Software Engineering Notes</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="97" to="107" />
			<date type="published" when="2004">2004. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Towards Optimal Concolic Testing</title>
		<author>
			<persName><forename type="first">Xinyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenbang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peixin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingyi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yun</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th International Conference on Software Engineering</title>
		<meeting>the 40th International Conference on Software Engineering</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="291" to="302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Multiscale Structural Similarity for Image Quality Assessment</title>
		<author>
			<persName><forename type="first">Zhou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eero</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Signals, Systems and Computers, Conference Record of the Thirty-Seventh Asilomar Conference on</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A Mathematical Theory of Deep Convolutional Neural Networks for Feature Extraction</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Wiatowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Helmut</forename><surname>Bölcskei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="page" from="1845" to="1866" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Feature-Guided Black-Box Safety Testing of Deep Neural Networks</title>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Wicker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaowei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marta</forename><surname>Kwiatkowska</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Tools and Algorithms for the Construction and Analysis of Systems</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="408" to="426" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">A Game-Based Approximate Verification of Deep Neural Networks with Provable Guarantees</title>
		<author>
			<persName><forename type="first">Min</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Wicker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenjie</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaowei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marta</forename><surname>Kwiatkowska</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03571</idno>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Symstra: A Framework for Generating Object-Oriented Unit Tests using Symbolic Execution</title>
		<author>
			<persName><forename type="first">Tao</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Darko</forename><surname>Marinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wolfram</forename><surname>Schulte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Notkin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Tools and Algorithms for the Construction and Analysis of Systems, TACAS (LNCS)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">3440</biblScope>
			<biblScope unit="page" from="365" to="381" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
