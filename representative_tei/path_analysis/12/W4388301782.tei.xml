<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Path Analysis for Effective Fault Localization in Deep Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Soroush</forename><surname>Hashemifar</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Engineering</orgName>
								<orgName type="institution">Iran University of Science and Technology</orgName>
								<address>
									<settlement>Tehran</settlement>
									<country key="IR">Iran</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Saeed</forename><surname>Parsa</surname></persName>
							<email>parsa@iust.ac.ir</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Engineering</orgName>
								<orgName type="institution">Iran University of Science and Technology</orgName>
								<address>
									<settlement>Tehran</settlement>
									<country key="IR">Iran</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Engineering</orgName>
								<orgName type="institution">Iran University of Science and Technology</orgName>
								<address>
									<settlement>Tehran</settlement>
									<country key="IR">Iran</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Akram</forename><surname>Kalaee</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Engineering</orgName>
								<orgName type="institution">Iran University of Science and Technology</orgName>
								<address>
									<settlement>Tehran</settlement>
									<country key="IR">Iran</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Path Analysis for Effective Fault Localization in Deep Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-21T21:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Spectrum-based Fault Localization</term>
					<term>Neural Pathway</term>
					<term>Deep Neural Networks</term>
					<term>Layer-wise Relevance Propagation</term>
					<term>Test Data Generation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep learning has revolutionized numerous fields, yet the reliability of Deep Neural Networks (DNNs) remains a concern due to their complexity and data dependency. Traditional software fault localization methods, such as Spectrum-based Fault Localization (SBFL), have been adapted for DNNs but often fall short in effectiveness. These methods typically overlook the propagation of faults through neural pathways, resulting in less precise fault detection. Research indicates that examining neural pathways, rather than individual neurons, is crucial because issues in one neuron can affect its entire pathway. By investigating these interconnected pathways, we can better identify and address problems arising from the collective activity of neurons. To address this limitation, we introduce the NP-SBFL method, which leverages Layer-wise Relevance Propagation (LRP) to identify essential faulty neural pathways. Our method explores multiple fault sources to accurately pinpoint faulty neurons by analyzing their interconnections. Additionally, our multi-stage gradient ascent (MGA) technique, an extension of gradient ascent (GA), enables sequential neuron activation to enhance fault detection. We evaluated NP-SBFL-MGA on the well-established MNIST and CIFAR-10 datasets, comparing it to other methods like DeepFault and NP-SBFL-GA, as well as three neuron measures: Tarantula, Ochiai, and Barinel. Our evaluation utilized all training and test samples-60,000 for MNIST and 50,000 for CIFAR-10-and revealed that NP-SBFL-MGA significantly outperformed the baselines in identifying suspicious pathways and generating adversarial inputs. Notably, Tarantula with NP-SBFL-MGA achieved a remarkable 96.75% fault detection rate compared to DeepFault's 89.90%. NP-SBFL-MGA also demonstrated comparable results to the baselines on additional benchmarks, highlighting a strong correlation between critical path coverage and the number of failed tests in DNN fault localization.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Deep Learning (DL) has profoundly transformed the way we address complex real-world problems, from image classification and speech recognition to natural language processing and software engineering tasks <ref type="bibr" target="#b0">[1]</ref>[2][3] <ref type="bibr" target="#b3">[4]</ref>. Despite the impressive accuracy of Deep Neural Networks (DNNs), their susceptibility to misclassification remains a significant challenge in realworld applications. This issue is evident in high-profile cases such as Tesla and Uber accidents, as well as incorrect healthcare diagnoses. These examples underscore the urgent need for robust assessment methods to ensure the reliability of DNNs, particularly in safety-and security-critical systems <ref type="bibr" target="#b4">[5]</ref>. Ensuring the trustworthiness of DNN-based systems is crucial for their widespread adoption <ref type="bibr" target="#b5">[6]</ref>.</p><p>However, the complex nature of DNN architectures makes it challenging to precisely understand the contribution of each parameter to task performance. Additionally, the behavior of a DNN is significantly influenced by the quality of the training data, complicating the task of gathering sufficient data to cover all potential DNN behaviors across various scenarios <ref type="bibr" target="#b6">[7]</ref>. Consequently, effective testing methodologies are crucial for assessing the reliability of DNN-based decision-making systems <ref type="bibr" target="#b7">[8]</ref>.</p><p>While traditional software testing methods are not directly applicable to DNNs, researchers have explored adopting advanced software testing principles to test DNNs <ref type="bibr" target="#b8">[9]</ref> <ref type="bibr" target="#b9">[10]</ref>. However, existing black-box DNN testing approaches have limitations in providing insights into the activation patterns of intermediate neurons and inputs that reveal unexpected network behavior. To address this, researchers have turned to white-box testing techniques from software engineering, such as differential algorithms, multi-granularity coverage criteria, metamorphic testing, combinatorial testing, mutation testing, MC/DC, symbolic execution, and Concolic testing <ref type="bibr" target="#b10">[11]</ref> <ref type="bibr" target="#b11">[12]</ref>[13] <ref type="bibr" target="#b13">[14]</ref>[15] <ref type="bibr" target="#b15">[16]</ref>[17] <ref type="bibr" target="#b17">[18]</ref>.</p><p>Software fault localization aims to identify the parts of a system responsible for incorrect behavior <ref type="bibr" target="#b18">[19]</ref>. Spectrum-based fault localization (SBFL) has proven to be a promising technique for locating faults in traditional software programs <ref type="bibr" target="#b19">[20]</ref>. In DNNs, only one method, DeepFault, currently employs traditional software fault localization to identify suspicious neurons <ref type="bibr" target="#b19">[20]</ref>. It utilizes measures such as Tarantula and Ochiai from the SBFL domain. Additionally, several recent approaches have been proposed to locate the most suspicious neurons within DNNs using statistical techniques <ref type="bibr" target="#b20">[21]</ref><ref type="bibr" target="#b21">[22]</ref><ref type="bibr" target="#b22">[23]</ref><ref type="bibr" target="#b23">[24]</ref>. However, despite advancements in fault localization methods for DNNs, significant gaps remain in the current research landscape.</p><p>Current research predominantly analyzes individual neuron behavior, overlooking the crucial interplay within neural pathways. This neuron-centric approach risks overlooking systemic issues impacting overall model performance. While methods like DeepFault <ref type="bibr" target="#b19">[20]</ref> effectively identify suspicious individual neurons, they inadequately address fault propagation through the complex DNN architecture. Scalability remains a challenge, particularly when analyzing the vast number of potential pathways in large networks. Moreover, techniques such as mutation-based fault localization, though insightful, suffer from computational overhead and a high false positive rate, limiting their practical utility. Therefore, we require holistic frameworks that explicitly consider the dynamics of neural connections to enable more effective and targeted fault localization, ultimately enhancing the robustness and reliability of DNNs across diverse architectures. This necessitates developing pathway-centric analysis methodologies capable of addressing the inherent complexities of DNNs.</p><p>This paper introduces NP-SBFL-MGA, a practical approach to fault localization in DNNs that overcomes the limitations of existing methods. Unlike methods focusing solely on individual neuron analysis, NP-SBFL-MGA employs a holistic perspective, examining entire neural pathways. By leveraging Layer-wise Relevance Propagation (LRP) <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26]</ref>, NP-SBFL-MGA identifies critical neurons layer-by-layer, significantly reducing the search space within complex networks. This layered analysis not only identifies faulty neurons but also allows for constructing faulty neural pathways, reflecting the intricate interdependencies between neurons. Moreover, the integrated Multi-stage Gradient Ascent (MGA) technique-an extension of Gradient Ascent (GA) <ref type="bibr" target="#b26">[27]</ref>-systematically activates neuron sequences, maintaining the activation of previously activated neurons on that path. By addressing both systemic interconnections among neurons, NP-SBFL-MGA provides a more comprehensive and efficient framework for fault localization, ultimately enhancing the robustness and reliability of DNN architectures. The effectiveness of the NP-SBFL-MGA approach is evaluated on two commonly used datasets, MNIST <ref type="bibr" target="#b27">[28]</ref> and CIFAR-10 <ref type="bibr" target="#b28">[29]</ref>, and compared with two baselines, DeepFault and NP-SBFL-GA.</p><p>Three measures-Tarantula, Ochiai, and Barinel-are utilized to evaluate suspicious components. These measures help to identify and rank parts of the neural network that are likely to be faulty. The experimental results demonstrate that the proposed approach outperforms the baselines in identifying faulty neural pathways and synthesizing adversarial inputs. It also identifies unique suspicious neurons, especially in complex models, and shows comparable results regarding the naturalness of synthesized inputs compared to DeepFault. Furthermore, a positive correlation is observed between the coverage of critical paths and the number of failed tests in DNN fault localization. The NP-SBFL-MGA approach achieves a fault detection rate of 96.75% using Tarantula, surpassing DeepFault with Ochiai (89.90%) and NP-SBFL-GA with Ochiai (60.61%).</p><p>In summary, the contributions to the field of fault localization for DNNs with a focus on neural paths are as follows:</p><p>ï‚· Proposing an efficient fault localization approach designed explicitly for DNNs, which targets identifying faulty neural pathways. This approach leverages three established SBFL techniques, namely Tarantula, Ochiai, and Barinel, to pinpoint faults within neural pathways. The innovation addresses the inherent complexities of fault localization in sophisticated DNN architectures, improving accuracy and efficiency. ï‚· Developing the MGA algorithm to maximize the activation of defective neural pathways. This algorithm sequentially activates neurons, ensuring the prior activation of preceding neurons in the pathway under test. The MGA algorithm assesses the efficacy of fault detection and localization. This advancement enhances the precision of fault localization, reducing false positives and improving the reliability of DNNs. ï‚· Conducting an extensive experimental evaluation to assess the performance of the proposed algorithms. The results of these experiments demonstrate the robustness of the approach in accurately identifying and activating faulty neural pathways. Notably, a remarkable fault detection rate of 96.75% has been achieved, underscoring the method's effectiveness in localizing faults within deep neural networks and outperforming existing techniques. Furthermore, the NP-SBFL-MGA approach has exhibited strong generalizability across various datasets, including Fashion MNIST, GTSRB, FER2013, and Caltech101, thereby validating the applicability and strength of the approach. This extensive evaluation provides empirical evidence of the approach's effectiveness and versatility, ensuring its practical utility in diverse scenarios. This paper is structured as follows: Section 2 provides a comprehensive overview that includes the historical context of traditional software fault localization, an examination of fault types in DNNs, and an introduction to relevance propagation. Section 3 summarizes the existing research on DNN fault localization. Section 4 introduces our novel approach, NP-SBFL. Section 5 describes the implementation of our tools and the underlying assumptions. Section 6 elaborates on the experimental setup evaluation methodologies and addresses the study's potential limitations. Section 7 reports the results of our experiments. The paper concludes with Section 8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>This section begins with an overview of fault localization in traditional software. It then introduces the fault taxonomy specific to DNN models. Finally, it provides a detailed explanation of the concept of neuron relevancy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Fault Localization in Traditional Software</head><p>Fault localization (FL) can be considered a type of white box testing that aims to identify source code elements likely to produce faults <ref type="bibr" target="#b18">[19]</ref>. An FL technique tests a program (P) against a test oracle (T), which reports tests as passed or failed status. Subsequently, an FL measurement technique determines the likelihood (suspiciousness) of each program element causing a fault during execution.</p><p>Spectrum-based fault localization (SBFL) <ref type="bibr" target="#b29">[30]</ref> belongs to a family of debugging techniques that identify potentially faulty code by analyzing both passing and failing executions of a program. It infers statistical properties from these executions and provides developers with a ranked list of potentially faulty statements to investigate. Given a faulty program and a set of test cases, an automated debugging tool based on SBFL generates this ranked list, allowing developers to inspect the statements individually until they find the bug.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Faults Taxonomy in Deep Neural Networks</head><p>There are fundamental differences in the definition and detection of faults between regular software programs and DNN models <ref type="bibr" target="#b30">[31]</ref>. Regular programs express their logic through control flow, whereas DNN programs rely on weights among neurons and various activation functions. In software programs, bugs are often identified by comparing ground-truth outputs with expected outputs. As long as there is a difference between the ground-truth output and the expected outputs, any discrepancy indicates a bug. Conversely, a DNN-based program learns from a training dataset, and misclassifications during inference are termed failure cases. However, since a DNN model cannot guarantee complete, accurate classifications, such failures do not necessarily imply the presence of a bug. To address this, DeepDiagnosis <ref type="bibr" target="#b30">[31]</ref> discusses eight failure symptoms and their underlying causes. Additionally, Neural Trojans represent another symptom leading to misbehavior in DNNs <ref type="bibr" target="#b31">[32]</ref>. These failure roots and symptoms are summarized in Table <ref type="table" target="#tab_0">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Layer-Wise Relevance Propagation (LRP)</head><p>Layer-wise relevance propagation (LRP) <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26]</ref> is an explainable artificial intelligence (XAI) technique applicable to neural network models that handle inputs such as images, videos, or text <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b33">34]</ref>. LRP operates by propagating the prediction ğ‘“(ğ‘¥) backward through the neural network using specifically designed local propagation rules.</p><p>The propagation process employed by LRP adheres to a conservation property, where the relevance received by a neuron must be equally redistributed to the neurons in the lower layer. This behavior is akin to Kirchoff's conservation laws observed in electrical circuits. Assuming that ğ‘¥ ğ‘— ğ‘™ denotes the input value to neuron ğ‘— in layer ğ‘™ and ğ‘¤ ğ‘—ğ‘˜ ğ‘™,ğ‘™+1 denotes its corresponding weight to a neuron ğ‘˜ in layer ğ‘™ + 1, the relevance scores (ğ‘… ğ‘™ ) are propagated from a given layer to the neurons of the upper layer using the rule depicted in Eq. <ref type="bibr" target="#b0">(1)</ref>.  The denominator in Eq. ( <ref type="formula">1</ref>) ensures the conservation property is maintained. The propagation process concludes once the input features are reached. By applying this rule to all neurons in the network, the layer-wise conservation property âˆ‘ ğ‘… ğ‘— = âˆ‘ ğ‘… ğ‘˜ ğ‘˜ ğ‘— can be easily verified and extended to a global conservation property âˆ‘ ğ‘… ğ‘– = ğ‘“(ğ‘¥) ğ‘– . The overall LRP procedure is illustrated in Figure <ref type="figure">1</ref>.</p><formula xml:id="formula_0">ğ‘… ğ‘™ ğ‘— = âˆ‘ ğ‘¥ ğ‘— ğ‘™ . ğ‘¤ ğ‘—ğ‘˜ ğ‘™,ğ‘™+1 âˆ‘ (ğ‘¥ ğ‘– ğ‘™ . ğ‘¤ ğ‘–ğ‘˜ ğ‘™,ğ‘™+1 ) ğ‘– ğ‘… ğ‘™+1 ğ‘˜ ğ‘˜ (1)</formula><p>In the context of a classifier function ğ‘“ operating on input data ğ‘¥ and assigning it a class label ğ‘¦, the logits associated with class ğ‘¦, denoted as ğ‘” ğ‘“ (ğ‘¥), are identified. LRP is a reliable method for understanding decision-making processes by determining essential features in the input data. It analyzes the importance of each dimension in the input data to interpret the classification decision made by the classifier function ğ‘“. This is achieved by determining the relevance of each input dimension through Eq. <ref type="bibr" target="#b1">(2)</ref>.</p><formula xml:id="formula_1">ğ‘” ğ‘“ (ğ‘¥) = âˆ‘ ğ‘… ğ‘™ ğ‘˜ ğ‘˜ , for each layer l (2)</formula><p>Figure <ref type="figure">1</ref>. Illustration of the LRP procedure. Each neuron redistributes to the lower layer as much as it has received from a higher layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Related work</head><p>In this section, we provide a comprehensive discussion on DNN fault localization methods, incorporating insights from recent research to present a clear picture of the state-of-the-art in this field. Faults in DNNs can arise from various sources, such as deep learning libraries, model construction, and training codes. Numerous tools and techniques have been proposed to address these issues, each with its advantages and limitations. CRADLE <ref type="bibr" target="#b34">[35]</ref> and LEMON <ref type="bibr" target="#b35">[36]</ref> are tools designed to detect and localize bugs in deep learning libraries through differential testing and guided mutation, respectively. They are effective for identifying discrepancies in library functions and generating robust models. However, they may not address faults in model construction or training codes. DeepLocalize <ref type="bibr" target="#b20">[21]</ref>, AUTOTRAINER <ref type="bibr" target="#b36">[37]</ref>, and CARE <ref type="bibr" target="#b37">[38]</ref> focus on analyzing internal values and adjusting trainable parameters. These methods offer deep insights into the model's operation, enabling precise fault localization and repair.</p><p>Nonetheless, these methods can be computationally intensive and may require extensive data for analysis. ReMoS <ref type="bibr" target="#b38">[39]</ref>, UMLAUT <ref type="bibr" target="#b39">[40]</ref>, DeepDiagnosis <ref type="bibr" target="#b30">[31]</ref>, Arachne <ref type="bibr" target="#b40">[41]</ref>, and NNrepair <ref type="bibr" target="#b21">[22]</ref> employ various strategies such as model slicing, heuristic checks, bug pattern monitoring, bidirectional localization, and concolic execution to localize and repair faults. While these tools are instrumental in identifying bugs within DNN models, they often fall short of addressing hyperparameterspecific bugs in CNNs <ref type="bibr" target="#b41">[42]</ref>. NeuraLint <ref type="bibr" target="#b42">[43]</ref> offers a static analysis approach to scrutinize CNN models for structural bugs, simplifying the model using graph transformations for direct bug pattern checks. Although efficient, its effectiveness may vary across different datasets <ref type="bibr" target="#b43">[44]</ref>. RNNRepair <ref type="bibr" target="#b44">[45]</ref> utilizes influence analysis to interpret and repair Recurrent Neural Networks (RNNs) by incorporating new influential training data samples. This method offers a dynamic repair mechanism but may not be suitable for all types of DNNs. Louloudakis et al. <ref type="bibr" target="#b45">[46]</ref> presented a technique for fault localization and repair in deep learning framework conversions, filling a crucial gap in the literature. However, this approach may be specific to certain pre-trained models and conversion tools.</p><p>Each approach offers a unique perspective on fault localization in DNNs and can be classified into four distinct groups: Spectrum-Based Fault Localization, Causality-Based Fault Localization, Mutation-Based Fault Localization, and Adaptive Learning-Based Fault Localization. The subsequent discussion in this section will explore these classes in detail, providing insights into their advantages, limitations, and applicability to various scenarios in DNN fault localization. Our approach, NP-SBFL, will be introduced as a novel method that complements these existing techniques by focusing on relevancy-wise spectrum analysis to localize vulnerable pathways in DNNs, a concept previously unexplored in the literature. This practical approach aims to enhance the accuracy of fault localization by considering the relevancy of neural pathways, thereby contributing to the advancement of fault localization methods in deep learning systems. The choice of method for DNN fault localization depends on the specific requirements of the DL system and the nature of the faults to be addressed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Spectrum-Based Fault Localization</head><p>Spectrum-Based Fault Localization (SBFL) has emerged as a prominent method for identifying faulty neurons or weights contributing to the suboptimal performance of DNNs. DeepFault, introduced by Eniser et al. <ref type="bibr" target="#b19">[20]</ref>, is a pioneering method that focuses on pinpointing neurons responsible for poor DNN performance by analyzing the hit spectrum of each neuron against a test set. This method effectively identifies suspicious neurons whose weights have not been correctly calibrated, leading to inadequate DNN performance. However, DeepFault operates at a coarser granularity by localizing neurons, necessitating an additional step to detect faulty neural weights for patch generation. Additionally, it requires setting an arbitrary threshold for neural activation values, unlike other methods, such as Arachne, which rely on gradient loss and the forward impact of each neuron weight for localization.</p><p>ArchRepair <ref type="bibr" target="#b46">[47]</ref> takes a different approach by focusing on the block level of DNNs, identifying and repairing vulnerable blocks within the network. By considering both the architecture and weights, ArchRepair provides a more comprehensive repair solution than methods focusing only on individual neurons or weights. However, this method has some limitations. It works well with block-level architecture but may not be suitable for all DNNs or situations. Additionally, repairing at the block level can be complex, as developers need to understand how different blocks interact and affect the entire network.</p><p>DeepCover <ref type="bibr" target="#b47">[48]</ref> employs a statistical fault localization approach, extracting heatmap explanations from DNN inputs to aid fault localization. This method aims to provide a more quantitative understanding of fault localization but may lack the interpretability of other methods focusing on individual neurons <ref type="bibr" target="#b19">[20]</ref> or blocks <ref type="bibr" target="#b46">[47]</ref>.</p><p>MODE <ref type="bibr" target="#b48">[49]</ref>, while not publicly available, is described as a white-box DNN testing technique that leverages suspiciousness values from SBFL to enhance the hit spectrum of neurons. It aims to identify neurons with improperly calibrated weights responsible for the DNN's inadequate performance. However, this method might be challenging to implement and understand due to its use of state differential analysis and input selection.</p><p>PAFL <ref type="bibr" target="#b49">[50]</ref> employs Probabilistic Finite Automata (PFAs) to manage RNN internal states by transforming RNN models into PFAs and then computing the suspiciousness score on n consecutive state transitions (i.e., n-grams) using SBFL on the PFA for more precise fault localization. This method offers a novel approach to fault localization in RNNs but may be limited to RNN models and require a deeper understanding of PFAs. Additionally, using PFAs and n-grams can complicate the fault localization process.</p><p>When comparing these SBFL techniques to NP-SBFL, several key differences emerge. NP-SBFL adapts traditional SBFL to locate faulty neural pathways within DNNs. It employs the LRP technique to identify critical neurons and determine which ones are faulty. Furthermore, NP-SBFL introduces a Multi-stage Gradient Ascent method to effectively activate a sequence of neurons while maintaining the activation of previous neurons on the path. This method offers a more nuanced approach to fault localization by considering the pathways through which faults propagate within the network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Causality-based fault localization</head><p>Causality-based fault localization techniques are essential for identifying and correcting faults in DNNs. These techniques leverage causality principles to pinpoint neurons responsible for incorrect outputs. For instance, the CARE framework <ref type="bibr" target="#b37">[38]</ref> employs a causality-based approach for fault localization, utilizing the Particle Swarm Optimization (PSO) algorithm to optimize parameters. However, the computation of causality operators can be resource-intensive, potentially slowing down the fault localization process.</p><p>Liang et al. <ref type="bibr" target="#b50">[51]</ref> introduced BIRDNN (Behavior-Imitation Repair of DNNs), a novel framework that examines the states of neurons in DNNs. BIRDNN analyzes the behavioral differences between expected outcomes on positive samples and unexpected outcomes on negative samples. By leveraging behavior imitation, BIRDNN offers two repair methodologies: retraining-based and fine-tuning-based. The retraining-based method involves altering the labels of negative sample inputs to match the outputs of nearby positive samples, followed by retraining the DNNs to correct erroneous behavior. The finetuning-based method identifies the neurons most responsible for incorrect predictions by analyzing behavioral differences between positive and negative samples. BIRDNN then uses PSO to adjust these neurons, aiming to maintain the DNN's performance while reducing errors. Additionally, BIRDNN addresses domain-wise repair problems (DRPs) by employing a sampling-based technique to understand DNN behaviors across different domains, thereby repairing faulty DNNs in a probably approximately correct manner.</p><p>Although promising, the aforementioned methods face several challenges. They can be computationally intensive, making them less practical for large-scale networks. The complexity of these methods often scales with the size of the network, potentially limiting their use in real-world applications with vast and intricate architectures. Additionally, relying on optimization algorithms can introduce difficulties such as slow convergence and the risk of not finding the global optimum. The interpretability of the results is another concern, as understanding the causal relationships within a DNN can be complex and often requires specialized expertise. Lastly, these methods may depend heavily on the availability of extensive and welllabeled data, which is not always accessible, and they might struggle to adapt to dynamic environments where the behavior of DNNs changes over time.</p><p>While NP-SBFL is adept at pinpointing where faults occur within the network, causality-based approaches complement this by providing insights into why these faults happen. Together, they offer a complete picture of fault localization in DNNs: NP-SBFL provides a map of the problematic areas, and causality-based approaches explain the terrain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Mutation-based fault localization</head><p>Mutation-based fault localization (MBFL) techniques, such as DeepMufl by Ghanbari et al. <ref type="bibr" target="#b43">[44]</ref>, introduce deliberate changes or mutations to a pre-trained DNN model to observe the effects on its performance. This method helps identify the parts of the network most sensitive to errors, indicating potential faults. DeepMufl has demonstrated effectiveness in bug detection, nearly doubling the performance of other tools and identifying 21 bugs undetected by different methods. Additionally, it highlights the impact of mutation selection on the time efficiency of fault localization, suggesting that the choice of mutations can significantly influence the speed of the process. DeepMufl outperforms other tools like DeepLocalize, DeepDiagnosis, and UMLAUT in terms of the number of bugs detected.</p><p>DFauLo, a dynamic data fault localization technique by Yin et al. <ref type="bibr" target="#b51">[52]</ref>, prioritizes data faults in DL datasets, enabling targeted corrections without the need to review the entire dataset. Inspired by mutation-based code fault localization, DFauLo uses differences between DNN mutants to identify potential data faults. DFauLo generates multiple DNN model mutants, extracts features from these mutants, and maps them into a suspiciousness score that indicates the probability of the given data being a fault. It is the first dynamic data fault localization technique that prioritizes suspected data based on user feedback and provides generalizability to unseen data faults during training.</p><p>However, these approaches face several challenges. They are known to be less time-efficient due to the extensive computational work involved in applying and analyzing numerous mutations. The high computational power required can be a hurdle for most researchers or institutions. Choosing mutations and interpreting their impact adds complexity and labor to the fault localization task. Additionally, there is a risk of false positives, where mutations may falsely indicate the presence of a fault, potentially triggering unnecessary debugging efforts.</p><p>Comparing these approaches with NP-SBFL, we see that NP-SBFL offers a structured approach to fault localization. While mutation-based methods provide insights into network sensitivity, NP-SBFL focuses on the propagation pathways of faults, potentially leading to more targeted repairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Adaptive Learning-Based Fault Localization</head><p>Software engineering research is rapidly adopting machine learning and deep learning technologies. Large Language Models (LLMs) are at the forefront of this integration, significantly enhancing various tasks such as code generation, completion, summarization, repair, bug fixing, and review. The expertise of LLMs in handling code-related tasks has spurred the development of innovative methods, including DeepFD <ref type="bibr" target="#b22">[23]</ref> and Deep4Deep <ref type="bibr" target="#b52">[53]</ref>. These approaches employ a learning-based strategy for diagnosing and localizing faults within DNNs. They train classifiers using features extracted from mutant models, enabling the identification of different types of faults. However, these learning-based frameworks primarily focus on DNN hyperparameters and may not be as effective with CNNs, which possess distinct architectural characteristics and specific applications. To address this gap, DeepCNN <ref type="bibr" target="#b41">[42]</ref> emerges as a solution for localizing and repairing faults in CNN programs. It leverages a transformer model for processing code sequences, learning from normal and faulty CNN models to distinguish between various bug types and determine their root causes. During the repair phase, DeepCNN refactors the code, fine-tuning hyperparameters to rectify the model, thereby offering a comprehensive approach to fault diagnosis and repair in CNNs.</p><p>Learning-based frameworks for fault localization in neural networks face several limitations. Firstly, they tend to specialize, often failing to be universally applicable across various neural network architectures, especially CNNs. Secondly, these frameworks depend heavily on the quality and quantity of data used for learning, which can significantly impact their effectiveness. Additionally, there is a notable challenge in generalizing the learned fault patterns to unseen scenarios or different types of neural networks, limiting the frameworks' versatility. Lastly, the computational demand for training classifiers for fault localization can be substantial, making the process time-consuming and resource-intensive. These limitations highlight the need for more adaptable, data-efficient, and computationally efficient learning-based frameworks for fault localization in neural networks.</p><p>While NP-SBFL shares the common goal of fault diagnosis and localization with learning-based frameworks, it operates on a distinct principle. NP-SBFL specifically focuses on identifying the neural pathways within DNNs that are likely to contribute to faults. This pathway-centric approach differs from the data-driven, classifier-training methods typically used in learning-based frameworks. Therefore, NP-SBFL can be considered orthogonal to learning-based approaches because it provides a unique perspective and methodology for fault detection and repair. While learning-based frameworks rely on patterns learned from data to identify faults, NP-SBFL uses structural analysis of the DNN itself. This structural scrutiny can complement data-driven insights, especially when data is scarce or not representative of all possible fault scenarios. Together, they can offer a more robust fault localization strategy by combining the strengths of both approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Comparative Analysis of NP-SBFL</head><p>To highlight the unique position of NP-SBFL in the current research landscape, it stands out among DNN fault localization techniques by explicitly addressing the vulnerabilities in neural pathways. This focus differentiates it from traditional methods like DeepFault and ArchRepair, which tend to target faulty neurons or architectural components, often requiring complicated steps to trace the faults back to the weights responsible for those issues. In contrast, NP-SBFL employs LRP to identify critical neurons directly and utilizes the MGA strategy to activate neural sequences in a nuanced manner <ref type="bibr" target="#b53">[54]</ref>. This approach not only enhances NP-SBFL's ability to pinpoint fault locations but also provides more precise insights into how these faults propagate throughout the DNN's architecture, making it a more precise and effective solution.</p><p>Compared to causality-based fault localization methods like CARE and BIRDNN, NP-SBFL offers distinct advantages. While causality-focused methods excel at identifying the root causes of errors and suggesting corrective actions through behavioral analysis, they often come with high computational costs and complexities due to their optimization algorithms. Their dependence on large datasets can also limit their effectiveness in dynamic settings where model behavior changes frequently. On the other hand, NP-SBFL's emphasis on structural analysis of neural pathways provides a more efficient alternative that avoids many of these pitfalls. By zeroing in on how faults spread within the network, NP-SBFL streamlines the fault diagnosis process without the heavy resource demands typical of many causality-driven methods.</p><p>Additionally, when compared to mutation-based and adaptive learning techniques, NP-SBFL holds its own by prioritizing structural insights over methods reliant on mutation sensitivity analysis or data-driven learning. Techniques like DeepMufl often require significant computational resources and may generate false positives, complicating the fault localization process. Similarly, adaptive learning techniques usually struggle to generalize across different neural network architectures, particularly CNNs. NP-SBFL's focus on neural pathways not only complements these methods but also strengthens the overall fault localization framework by integrating structural examination. This unique approach allows NP-SBFL to function both as a standalone solution and as a supportive tool that enhances other fault detection strategies, creating a more holistic method for addressing DNN faults.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Proposed Approach</head><p>This section introduces our NP-SBFL white-box approach, which presents a systematic testing method for DNNs. The strategy aims to identify and locate highly erroneous neural pathways within a DNN. NP-SBFL systematically analyzes and synthesizes input data for a pre-trained DNN to effectively locate problematic execution pathways. This process generates novel inputs specifically designed to activate the identified pathways. A comprehensive overview of NP-SBFL is provided in Section 4.1, while Sections 4.2 to 4.4 delve into the specific details of the NP-SBFL steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Overview</head><p>This section provides a comprehensive overview of NP-SBFL. As illustrated in Figure <ref type="figure" target="#fig_1">2</ref>, the method begins by identifying critical neurons within each network layer. These neurons are then analyzed using SBFL techniques to locate faulty neurons, which are subsequently linked to form faulty neural paths. A synthetic dataset is generated to specifically activate these suspected paths. This allows us to assess whether the identified paths lead to misclassifications, providing a performance evaluation of our fault localization. A multi-stage gradient ascent method is employed to activate these paths efficiently.</p><p>Moreover, Figure <ref type="figure">3</ref> provides a concrete example of suspicious neuron identification. Figure <ref type="figure">3</ref>(a) illustrates the application of LRP to visualize the internal decision-making process. LRP assigns relevance scores to each neuron, tracing back from the output layer, thereby quantifying their contribution to the network's prediction. Based on these scores, critical neurons in each layer are identified. For instance, Figure <ref type="figure">3</ref>(b) demonstrates a thresholding approach where neurons with relevance scores exceeding the sum of relevance scores in the input layer (e.g., 0.2 + 0.1 + 0.2 = 0.5 in this case) are classified as critical. Finally, Figure <ref type="figure">3</ref>(c) depicts the application of SBFL methods to identify suspicious neurons among these critical neurons. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Neural Pathways Analysis</head><p>In this section, we present the method for identifying the essential decision path for a given input. As mentioned earlier, the relevance of each neuron ğ‘… ğ‘– ğ‘™ signifies its contribution to the final decision. A higher relevance value indicates that neuron ğ‘– at layer ğ‘™ has a more significant impact on the decision-making process. Since a DNN's prediction relies on extracting highquality features at each layer, the critical neurons with high relevance reveal the logic behind the decision-making process of the DNN <ref type="bibr" target="#b54">[55]</ref>. Critical neurons positively contribute to the model's decision, indicated by a positive relevance value. To identify these neurons, we select the minimal subset whose cumulative relevance surpasses a dynamically predefined threshold. This threshold is a fraction of the total cumulative relevance, ğ‘” ğ‘“ (ğ‘¥), for the input x. Using ğ‘” ğ‘“ (ğ‘¥) as a basis ensures consistency across different architectures and inputs, addressing the challenge of setting a fixed threshold. Furthermore, since relevance values can be negative, this approach implicitly normalizes the relevance values, preventing excessively positive values from disproportionately influencing the selection of critical neurons. Following <ref type="bibr" target="#b54">[55]</ref>, we formally define the condition for critical neurons at each layer ğ‘™ ğ‘– , as shown in Eq. ( <ref type="formula" target="#formula_2">3</ref>).</p><formula xml:id="formula_2">âˆ‘ ğ‘… ğ‘™ ğ‘– ğ‘–âˆˆğ‘™ ğ‘– &gt; ğ›¼ â€¢ ğ‘” ğ‘“ (ğ‘¥)<label>(3)</label></formula><p>In Eq. ( <ref type="formula" target="#formula_2">3</ref>), Î± is a hyper-parameter that controls the volume of neurons considered critical at each layer; a smaller Î± results in fewer critical neurons.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Suspicious Neural Pathways Identification</head><p>For the next step, NP-SBFL analyzes neural activity to identify faulty neurons in each layer. Inspired by DeepFault <ref type="bibr" target="#b19">[20]</ref>, it characterizes neuron behavior during execution. However, unlike DeepFault, which considers all neurons collectively, NP-SBFL focuses on critical neurons in each layer, as their combined activity is crucial to the DNN's decision-making. The quantities ğ´ ğ‘ ğ‘ and ğ´ ğ‘“ ğ‘ represent the number of times a neuron was activated by the test input, resulting in a passed or failed decision, respectively. Similarly, ğ´ ğ‘ ğ‘› and ğ´ ğ‘“ ğ‘› represent the counts for inactive neurons leading to passed or failed decisions.</p><p>Algorithm 1 details the suspiciousness scoring process.</p><p>For each neuron, NP-SBFL generates a "hit spectrum" under a test set T, reflecting its dynamic behavior. This involves recording activation and relevance values for each neuron (lines 2-3) and identifying critical neurons within each layer (lines 4-7). The algorithm subsequently evaluates the suspiciousness of each critical neuron individually (lines 8-27), thereby creating a sequence of potentially defective neurons that extends from the input layer to the output layer. Finally, a layerspecific suspiciousness score is calculated for each neuron based on its hit spectrum (line 28) using one of three established metrics: Tarantula <ref type="bibr" target="#b55">[56]</ref>, Ochiai <ref type="bibr" target="#b56">[57]</ref>, and Barinel <ref type="bibr" target="#b29">[30]</ref> (Table <ref type="table" target="#tab_2">2</ref>). Higher scores indicate a higher likelihood of a neuron's mistraining and contribution to incorrect DNN outputs. These suspiciousness metrics prioritize neurons that are frequently activated during incorrect decisions and infrequently activated during correct decisions. The algorithm ranks neurons within each layer by suspiciousness, selecting the top ğ‘˜ most likely faulty neurons to form a pattern of interconnected suspicious neurons. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Suspiciousness-Guided Test Data Generation</head><p>To evaluate potentially faulty neural pathways identified in previous steps, we employ activation maximization to stimulate suspicious neurons flagged by our NP-SBFL method. Activation maximization, another XAI technique <ref type="bibr" target="#b53">[54]</ref>, generates inputs designed to maximize the activation of specific neurons or output classes, thereby revealing influential features in the network's predictions. We utilize Gradient Ascent (GA), a common activation maximization approach <ref type="bibr" target="#b57">[58]</ref>, which iteratively adjusts the input data along the gradient of the target neuron's activation function. This process, formalized as an optimization problem (Eq. 4), seeks a local maximum in the non-convex input space.</p><formula xml:id="formula_3">ğ‘¥ * = ğ‘ğ‘Ÿğ‘”ğ‘šğ‘ğ‘¥ ğ‘¥ ğ‘› ğ‘ğ‘ğ‘¡ğ‘–ğ‘£ğ‘ğ‘¡ğ‘–ğ‘œğ‘› ğ‘™ (ğœƒ; ğ‘¥)<label>(4)</label></formula><p>In Eq. ( <ref type="formula" target="#formula_3">4</ref>), Î¸ represents the fixed neural network parameters, and ğ‘› ğ‘ğ‘ğ‘¡ğ‘–ğ‘£ğ‘ğ‘¡ğ‘–ğ‘œğ‘› ğ‘™ (ğœƒ; ğ‘¥) corresponds to the activation of a neuron n of layer l in a network for an input data x. This optimization works by increasing the gradients of the input data, i.e., moving x in the direction of the gradient of ğ‘› ğ‘ğ‘ğ‘¡ğ‘–ğ‘£ğ‘ğ‘¡ğ‘–ğ‘œğ‘› ğ‘™ (ğœƒ; ğ‘¥). Like other gradient-based techniques, hyper-parameters such as the learning rate and a stopping criterion must be chosen, such as the maximum number of iterations for gradient ascent updates in our experiments.</p><p>However, a standard GA approach may not effectively activate the complex, multi-neuron pathways identified by NP-SBFL. Therefore, we introduce a novel multi-stage gradient ascent (MGA) method. MGA sequentially activates each neuron in a pathway while maintaining the activation of preceding neurons. This is achieved by maximizing a more complex, layerwise loss function (Eq. 5), thus ensuring the activation of the desired pathway without unintentionally deactivating neurons in preceding layers.</p><formula xml:id="formula_4">ğ‘ğ‘Ÿğ‘”ğ‘šğ‘ğ‘¥ ğ‘ ğ‘™ + âˆ‘ ğ‘ ğ‘™ â€² ğ‘™ â€² âˆˆ {1,2,â€¦,ğ‘™} -|ğ‘ ğ‘™ -ğ‘ ğ‘™ â€² | where ğ‘ ğ‘™ = âˆ‘ ğ‘› ğ‘ğ‘ğ‘¡ğ‘–ğ‘£ğ‘ğ‘¡ğ‘–ğ‘œğ‘› ğ‘™ (ğœƒ; ğ‘¥) ğ‘› âˆˆ ğ‘ ğ‘¢ğ‘ ğ‘ğ‘–ğ‘ğ‘–ğ‘œğ‘¢ğ‘  ğ‘›ğ‘’ğ‘¢ğ‘Ÿğ‘œğ‘›ğ‘ <label>(5)</label></formula><p>In Eq. ( <ref type="formula" target="#formula_4">5</ref>), ğ‘ ğ‘™ denotes the cumulative activation value of layer l. Algorithm 2 details how the gradient of this loss is added to the input image (line 11), effectively synthesizing an image that activates the suspicious paths. We ultimately ensure that the generated image stays within the acceptable range of [0, 1] by implementing domain constraints (line 12) and normalizing the pixel values. The efficacy of MGA is evaluated in subsequent sections. for ğ‘¥ âˆˆ ğ‘‡ do 3.</p><p>for iteration i = 1, â€¦, I do 4.</p><p>feed ğ‘¥ to the network 5.</p><p>if ğ‘¥ is correctly classified then 6.</p><p>for layer l = 1, â€¦, L do 7.</p><p>for neuron ğ‘› âˆˆ ğ‘†ğ‘[ğ‘™] do 8.</p><p>calculate loss objective using Eq. ( <ref type="formula" target="#formula_4">5</ref> return ğ‘ ğ‘¦ğ‘›ğ‘¡â„ğ‘’ğ‘ ğ‘–ğ‘§ğ‘’ğ‘‘ğ·ğ‘ğ‘¡ğ‘ğ‘ ğ‘’ğ‘¡</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Implementation and Assumptions</head><p>Implementation. We present a PyTorch v2.0-based tool designed to simplify the evaluation and application of the NP-SBFL approach. Its intuitive interface facilitates integration into existing workflows for both researchers and practitioners. The full implementation and experimental results are publicly available at <ref type="url" target="https://github.com/soroushhashemifar/NP-SBFL">https://github.com/soroushhashemifar/NP-SBFL</ref>.</p><p>Assumptions. The proposed approach identifies faulty pathways in neural networks that are responsible for incorrect predictions. This relies on LRP, which decomposes neuron relevance across input features, quantifying each neuron's contribution to the final prediction. This approach assumes a feed-forward architecture enabling relevance propagation through layers. By focusing on highly relevant neurons-those most influential in the network's decision-makingsuspiciousness measures pinpoint neurons causing inaccuracies.</p><p>Our method leverages activation maximization, which requires the DNN to be differentiable, have interpretable learned representations, and exhibit convergence during optimization with efficiently calculable input gradients. We also assume a well-trained DNN with discriminative features and a continuous input space, ensuring that small input changes yield small output changes for reliable interpretation.</p><p>In essence, our approach identifies critical neurons driving the network's decisions. By integrating LRP and measures of suspiciousness, we can pinpoint critical faulty pathways, enhancing the network's accuracy. The reliance on activation maximization, along with a well-trained DNN and continuous input space, improves the interpretability and reliability of our results. Section 7.3 provides a detailed discussion of the limitations imposed by these assumptions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Evaluation</head><p>This section presents our experimental evaluation of NP-SBFL, a novel technique for localizing faulty neuronal sequences in neural networks. Initially, we describe our experimental setup, including the dataset, network architecture, and evaluation metrics. Our research focuses on three key aspects: a comparative analysis of fault localization approaches, the impact of different suspiciousness metrics on NP-SBFL's effectiveness, and the relationship between critical path coverage and test failures. Next, we present and analyze the experimental results, comparing the performance of various techniques and assessing NP-SBFL's efficacy under different conditions. Finally, we address potential threats to validity and the mitigation strategies employed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Experimental Setup</head><p>We evaluated the NP-SBFL approach using two benchmark datasets: MNIST <ref type="bibr" target="#b27">[28]</ref>, which includes 60,000 training images and 10,000 testing images of handwritten digits (28 Ã— 28 pixels), and CIFAR-10 <ref type="bibr" target="#b28">[29]</ref>, which contains 50,000 training images and 10,000 testing images objects from ten categories (32 Ã— 32 pixels). We employed six DNNs, as detailed in Table <ref type="table" target="#tab_5">3</ref>. These DNNs varied in architecture and parameter count: three were fully-connected networks that achieved at least 95% accuracy on MNIST, while three were convolutional networks (incorporating max-pooling and ReLU activation functions) that achieved at least 70% accuracy on CIFAR-10. We experimented with varying numbers of suspicious neurons (ğ‘˜), selecting ğ‘˜ âˆˆ {1, 5, 10} for MNIST and ğ‘˜ âˆˆ {10, 30, 50} for CIFAR-10 models. Extensive experimentation optimized the hyperparameters of Algorithm 2, ensuring reproducibility. Input perturbation parameters, listed in Table <ref type="table" target="#tab_6">4</ref>, were empirically determined. We used two gradient ascent synthesizers: a single-stage (NP-SBFL-GA) and a multi-stage version (NP-SBFL-MGA). For comparison, we used DeepFault, employing the step sizes recommended in its original publication. The maximum allowed perturbation distance (d) was constrained (Table <ref type="table" target="#tab_6">4</ref>) to avoid exceeding the input data range. Further investigation into optimal step sizes and the parameter d is planned for future work. Experiments were conducted on an Ubuntu desktop with 16 GB RAM and an Intel Core i7-4790K CPU @ 4.00GHz (8 cores). A criticality coefficient (Î±) of 0.7 was used throughout. Given the use of ReLU activation functions, a neuron was considered activated if its output exceeded 0.0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Results and Analysis</head><p>Our experimental evaluation aims to address the following research questions:</p><p>ï‚· RQ1 (Validation): Does empirical evaluation validate the effectiveness of NP-SBFL in identifying suspicious paths and its ability to outperform DeepFault, a neuron-based suspiciousness selection strategy, in synthesizing adversarial inputs that lead to the misclassification of previously correctly classified inputs by DNNs? ï‚· RQ2 (Comparison): How do NP-SBFL-GA, NP-SBFL-MGA, and DeepFault instances with different measurements of suspiciousness compare against each other? The evaluation is conducted by analyzing the results produced by different instances using Tarantula <ref type="bibr" target="#b55">[56]</ref>, Ochiai <ref type="bibr" target="#b56">[57]</ref>, and Barinel <ref type="bibr" target="#b29">[30]</ref>. ï‚· RQ3 (Fault Detection Rate): Which of the selected approaches exhibits a greater fault detection rate? ï‚· RQ4 (Correlation): Is there a correlation between the coverage of critical paths and the number of failed tests in DNN fault localization? ï‚· RQ5 (Locating Unique Suspicious Neurons): Which approach is more effective in locating unique suspicious neurons among the different instances? ï‚· RQ6 (Quality of Synthesized Inputs): How well can NP-SBFL synthesize inputs of acceptable quality? ï‚· RQ7 (Generalizability): How robust is the NP-SBFL-MGA method when applied to diverse datasets? This section details experiments designed to investigate the impact of activating suspicious prediction paths on model performance and vulnerability. We selected accurately classified samples from the test set of a pre-trained model. These samples were then modified using an input synthesis technique to emphasize features potentially leading to misclassification, effectively "activating" these suspicious paths. The modified samples were re-evaluated, and the resulting misclassification rate was calculated. We ultimately analyzed the ratio of synthesized samples that triggered suspicious pathways, resulting in misclassification. This analysis provides insights into the effectiveness of targeting specific prediction paths to induce erroneous model behavior.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.1">RQ1 (Validation)</head><p>We employed the NP-SBFL workflow to analyze DNNs, as detailed in Table <ref type="table" target="#tab_5">3</ref>. This involved identifying the ğ‘˜ most suspicious neurons (using a chosen suspiciousness metric) and synthesizing new, correctly classified inputs that activate these neurons. The final column of Table <ref type="table" target="#tab_5">3</ref> indicates the number of correctly classified inputs used in each analysis. DNN performance was evaluated using standard metrics-cross-entropy loss and accuracy-on a per-class basis, considering the class-specific activation patterns.</p><p>Tables 5-7 present the average loss and accuracy for inputs synthesized using DeepFault, NP-SBFL-GA, and NP-SBFL-MGA. These results, obtained using Tarantula, Ochiai, and Barinel suspiciousness measures on MNIST and CIFAR-10 models, show average performance across all synthesized inputs. Table <ref type="table" target="#tab_7">5</ref> reveals that DeepFault, when combined with Tarantula and Barinel, consistently yielded significantly lower prediction performance than Ochiai. Interestingly, Tarantula and Barinel exhibited comparable performance. Tables <ref type="table" target="#tab_8">6</ref> and<ref type="table" target="#tab_9">7</ref> demonstrate a similar trend for NP-SBFL-GA and NP-SBFL-MGA, with Ochiai and Barinel performing similarly, and both underperforming relative to Tarantula. The consistent performance of Barinel compared to Tarantula and Ochiai suggests that different suspiciousness metrics may identify similar critical network regions, supporting the effectiveness of NP-SBFL's pathway-centric approach. This also indicates that the weights of the neurons identified as suspicious by NP-SBFL may be inadequately trained, providing valuable insight for improving DNN training. Therefore, NP-SBFL not only facilitates adversarial input generation but also identifies areas requiring additional training to enhance robustness against attacks.</p><p>Figures <ref type="figure">4</ref> and<ref type="figure">5</ref> illustrate the superior performance of NP-SBFL (both variants) over DeepFault in generating adversarial examples for MNIST and CIFAR models, as evidenced by lower loss and higher accuracy scores. This advantage stems from NP-SBFL's unique approach. Unlike DeepFault, which examines neurons individually, NP-SBFL considers the interconnected pathways within the neural network, mirroring the network's actual information processing. This holistic perspective allows NP-SBFL to generate more effective adversarial inputs, better reflecting the complexities of real-world data and thus providing a more rigorous evaluation of the network's robustness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 4. Loss and accuracy of MNIST on the synthesized data for different approaches and suspicious measures.</head><p>Furthermore, NP-SBFL leverages LRP to detect critical neurons, ensuring that the generated inputs are not random perturbations but strategically targeted attacks on the network's vulnerabilities. The sequential neuron activation in NP-SBFL-MGA, via the MGA technique, further enhances this capability. By mimicking the layered processing in deep neural networks, NP-SBFL-MGA exploits the cumulative effects of neuron activations, uncovering more subtle vulnerabilities (further discussed in RQ2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 5. Loss and accuracy of CIFAR on the synthesized data for different approaches and suspicious measures.</head><p>To rigorously assess the statistical significance of NP-SBFL-MGA's performance, we employed the Wilcoxon rank-sum test (95% confidence level) <ref type="bibr" target="#b58">[59]</ref> on the results in Table <ref type="table" target="#tab_10">8</ref>. This non-parametric test was chosen for its suitability with the often non-normal distributions found in machine learning performance metrics. We also calculated Vargha and Delaney's Ã‚12 effect size statistic <ref type="bibr" target="#b59">[60]</ref> to quantify the magnitude of performance differences. Ã‚12 provides a more informative measure than pvalues alone, indicating the probability that one method will outperform another. An Ã‚12 value greater than 0.5 suggests that the first method has a higher likelihood of outperforming the second method. For CIFAR models, NP-SBFL-MGA consistently achieved Ã‚12 equal to 1 against DeepFault and NP-SBFL-GA across all metrics (Tarantula, Ochiai, Barinel), demonstrating its clear superiority. While the advantage over DeepFault and NP-SBFL-GA is less pronounced for MNIST models, the Ã‚12 values remained above 0.5 for most metrics, indicating that NP-SBFL-MGA generally outperforms these methods. Statistical significance tests (p-values) confirm the superior performance of NP-SBFL-MGA. For CIFAR-10 models, all comparisons yielded p-values &lt; 0.001, strongly supporting the conclusion that NP-SBFL-MGA's improved performance is not due to chance. While MNIST model results showed some variation in p-values across different performance metrics, accuracy comparisons against DeepFault remained statistically significant (p &lt; 0.05).</p><p>The observed variation in the adversarial input generation of NP-SBFL-MGA on the MNIST dataset, marked by diminished accuracy without a corresponding increase in loss, can be attributed to the fundamental simplicity of the dataset. MNIST's grayscale handwritten digits lack the complexity and variability of datasets like CIFAR-10. This more straightforward structure likely leads to less diverse activation patterns, hindering NP-SBFL-MGA's pathway-centric approach, which excels in exploiting intricate neural network behaviors. Its effectiveness is better demonstrated on datasets with richer feature sets and more complex activation patterns, allowing it to effectively differentiate between nuanced pathway activations.</p><p>Tables <ref type="table" target="#tab_7">5</ref><ref type="table" target="#tab_8">6</ref><ref type="table" target="#tab_9">7</ref>and Figures <ref type="figure">6</ref><ref type="figure">7</ref>illustrate the impact of varying the number of suspicious neurons (ğ‘˜) on model performance. For NP-SBFL-MGA, increasing ğ‘˜ generally improved accuracy and reduced loss, except for a single instance (MNIST, ğ‘˜ = 10). This contrasts sharply with alternative methods, which consistently showed a negative correlation between ğ‘˜ and performance. This behavior suggests that NP-SBFL-MGA leverages larger sets of suspicious neurons effectively. By expanding its search across a broader range of neurons, it identifies a more diverse set of pathways contributing to the network's output, thus generating adversarial inputs that more comprehensively target network vulnerabilities and significantly impact accuracy.</p><p>The effectiveness of the multi-stage gradient ascent in NP-SBFL-MGA appears to benefit from increasing the parameter ğ‘˜, which controls the number of considered neurons. A larger ğ‘˜ allows for finer-grained manipulation of the input space, potentially uncovering more effective adversarial examples by exploring a more comprehensive range of the network's critical paths. This leads to adversarial inputs better aligned with the model's vulnerabilities, impacting accuracy and loss. The exception observed with MNIST and ğ‘˜ = 10 might stem from the dataset's simplicity; fewer neurons suffice to capture its essential features. Beyond a certain point, however, increasing ğ‘˜ may yield diminishing returns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 6. Accuracy of MNIST and CIFAR on the synthesized data using different approaches and suspicious measures for a variant number of suspicious neurons, ğ‘˜: MNIST(CIFAR). Figure 7. Loss of MNIST and CIFAR on the synthesized data using different approaches and suspicious measures for a variant number of suspicious neurons, ğ‘˜: MNIST(CIFAR).</head><p>In contrast, both DeepFault and NP-SBFL-GA exhibit performance degradation as ğ‘˜ increases. DeepFault's reliance on the highest-suspiciousness neurons leads to an overemphasis on specific neurons at higher ğ‘˜ values, neglecting crucial interneuron interactions and broader network context. This results in adversarial inputs that activate many neurons but lack diversity, hindering their effectiveness in exploiting true vulnerabilities, unlike the pathway-centric NP-SBFL method.</p><p>NP-SBFL-GA's single-stage adversarial input synthesis, applied to the top-ğ‘˜ neurons, becomes computationally more challenging as ğ‘˜ grows. Simultaneously maximizing the activation of many neurons constitutes a complex optimization problem that does not scale well, as it treats all selected neurons as equally important. Conversely, NP-SBFL-MGA's multistage approach, while selecting the top ğ‘˜ neurons, addresses this scalability issue. Activating suspicious neurons in a sequential manner, whether layer-by-layer or group-by-group, allows for a more controlled and focused optimization process at each stage. This approach enhances the effectiveness of exploring the adversarial input space, even when dealing with a larger value of ğ‘˜.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RQ1 (Validation):</head><p>There is empirical evidence of suspicious neural pathways that could be causing inadequate DNN performance. NP-SBFL-MGA instances with different suspiciousness measures significantly improve the localization of low performance compared to other approaches for most DNN models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.2">RQ2 (Comparison)</head><p>Our comparative analysis of three fault localization techniques-NP-SBFL-GA, NP-SBFL-MGA, and DeepFault-using various suspiciousness metrics (Tarantula, Ochiai, and Barinel) reveals significant performance differences. Statistical tests (Wilcoxon rank-sum and Vargha-Delaney's Ã‚12) confirmed these differences, as detailed in Tables <ref type="table" target="#tab_11">9</ref> and<ref type="table" target="#tab_12">10</ref> (MNIST and CIFAR-10 datasets, respectively).  </p><formula xml:id="formula_5">ïƒ¼ ïƒ¼ ïƒ¼ ïƒ¼ ïƒ¼ ïƒ¼ Ochiai ïƒ¼ ïƒ¼ ïƒ¼ ïƒ¼ ïƒ¼ ïƒ¼ Barinel ïƒ¼ ïƒ¼ ïƒ¼ ïƒ¼ ïƒ¼ ïƒ¼ Loss Tarantula ïƒ¼ ïƒ¼ ïƒ¼ ïƒ¼ ïƒ¼ ïƒ¼ Ochiai ïƒ¼ ïƒ¼ ïƒ¼ ïƒ¼ ïƒ¼ ïƒ¼ Barinel ïƒ¼ ïƒ¼ ïƒ¼ ïƒ¼ ïƒ¼ ïƒ¼</formula><formula xml:id="formula_6">ïƒ¼ ïƒ¼ ïƒ¼ ïƒ¼ ïƒ¼ ïƒ¼ Ochiai ïƒ¼ ïƒ¼ ïƒ¼ ïƒ¼ ïƒ¼ ïƒ¼ Barinel ïƒ¼ ïƒ¼ ïƒ¼ ïƒ¼ ïƒ¼ ïƒ¼ Loss Tarantula ïƒ¼ ïƒ¼ ïƒ¼ ïƒ¼ ïƒ¼ ïƒ¼ Ochiai ïƒ¼ ïƒ¼ ïƒ¼ ïƒ¼ ïƒ¼ ïƒ¼ Barinel ïƒ¼ ïƒ¼ ïƒ¼ ïƒ¼ ïƒ¼ ïƒ¼</formula><p>NP-SBFL-MGA consistently outperformed the others, regardless of the chosen metric. This suggests that its identified neural pathways are more strongly correlated with DNN performance degradation than those found by DeepFault. A pathwaycentric approach, therefore, appears to provide a more accurate representation of DNN information processing and more effectively pinpoints critical performance issues. Moreover, NP-SBFL-MGA's multi-stage gradient ascent better navigates the complex error landscapes of DNNs compared to a single-stage approach.</p><p>Our findings also highlight the lack of a universally superior suspiciousness metric, reflecting the inherent complexity of DNN fault localization and mirroring challenges in traditional software debugging. This underscores the need for a comprehensive, multifaceted approach to detect and locate faults within neural networks reliably.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RQ2 (Comparison):</head><p>NP-SBFL-MGA with any suspiciousness measure is statistically superior to other instances in uncovering the low performance of models. These findings have significant implications for the field of neural network development and suggest that focusing on specific pathways rather than single neurons can lead to more accurate fault localization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.3">RQ3 (Fault Detection Rate)</head><p>Our analysis of fault detection rates across different approaches reveals significant performance variations. Table <ref type="table" target="#tab_0">11</ref> shows that NP-SBFL-MGA, using the Tarantula suspiciousness measure, consistently achieves the highest fault detection rates, excelling in both critical (C) and faulty (F) pathway activation across all models. This highlights a strong correlation between pathway activation and fault detection in DNNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 11. The ratios of covered critical paths (C) and faulty paths (F) activating faulty pathways for Tarantula, Ochiai, and</head><p>Barinel ( from left to right) in different models and approaches where DF: DeepFault, NG: NP-SBFL-GA and NM: NP-SBFL-MGA. The best results are shown in bold. DeepFault, employing the Ochiai measure, ranks second. While effective at identifying faulty neurons, it is outperformed by NP-SBFL-MGA's pathway-centric strategy. This suggests that NP-SBFL-MGA's broader network behavior analysis leads to superior fault localization. Figure <ref type="figure">8</ref> visually confirms these findings, demonstrating a positive correlation between critical pathway coverage and failed tests-a relationship further explored statistically in RQ4. Specifically, Figure <ref type="figure">8</ref>(a) illustrates Tarantula's effectiveness in identifying faulty pathways, while Figure <ref type="figure">8</ref>(b) highlights Ochiai's strength in detecting faulty neurons. However, DeepFault's performance using Tarantula and Barinel is comparatively lower, especially for CIFAR-10, indicating potential limitations with these measures for complex image classification. Conversely, NP-SBFL-GA and NP-SBFL-MGA exhibit comparably high performance, further supporting the effectiveness of pathway-centric approaches for fault detection in DNNs.</p><formula xml:id="formula_7">Model ğ‘˜ Tarantula Ochiai Barinel DF NG NM DF NG NM DF NG NM C F C F C F C F C F C F C F C F C F MNIST_1 1</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 4. A comparison between the averaged ratios of failed synthesized tests and the synthesized samples activating faulty</head><p>pathways for various suspicious measurements in different models and approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RQ3 (Fault Detection Rate):</head><p>NP-SBFL-MGA using Tarantula is the most effective approach regarding the ratio of failed synthesized test samples activating faulty pathways for all models. Moreover, Ochiai is an acceptable option for detecting faulty neurons, while Tarantula is a good alternative for detecting faulty pathways.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.4">RQ4 (Correlation)</head><p>Figure <ref type="figure">9</ref> visualizes the correlation between critical pathway coverage and test failure rates. Our methodology for establishing this relationship involves analyzing the activation of specific neurons in synthetic test data. We determine the impact of neuron activation on test failures by generating binary fault masks for each layer. These masks are constructed by assigning a value of 1 to the indices of the top-ğ‘˜ most suspicious neurons, based on their calculated suspiciousness scores. The remaining indices are set to 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 5. The correlation between the rate of covered critical pathways and failed tests across all instances and models.</head><p>Synthesized samples were then assessed: if a critical neuron in each layer was activated, as denoted by a 1 in the respective binary vector, a counter for activated faulty neurons was increased. At the same time, another counter was used to tally the samples that were incorrectly classified, indicating failed tests. We calculated two key ratios. The first is the ratio of "failed tests activating faulty paths," which is obtained by dividing the number of failed tests by the overall number of tests conducted. The second is the ratio of "synthesized samples activating faulty paths," determined by dividing the count of activated faulty neurons by the total number of tests, encompassing both those that passed and those that failed. This methodology enabled us to directly link neuron activation to test outcomes, systematically analyzing the impact of specific neuron activations on DNN performance. Figure <ref type="figure">9</ref> generally shows a positive correlation: higher critical pathway coverage correlates with a higher failure rate. This suggests that effectively activating critical pathways increases the likelihood of uncovering faults. However, exceptions exist, notably with DeepFault (Ochiai; MNIST/CIFAR) and NP-SBFL-GA (Tarantula; MNIST). This commonality points to the limitation of activation maximization: activating one neuron might deactivate others due to the network's weighted connections. Consequently, activation maximization algorithms may fail to activate all targeted neurons, primarily when conflicting gradients exist for suspicious neurons. For instance, some gradients might be positive (requiring increased input feature values), while others are negative (requiring decreased values). To quantify this correlation, we employed the Spearman rank correlation coefficient. This non-parametric measure is robust to non-linear relationships and non-normal data distributions, making it suitable for analyzing the relationship between critical pathway coverage and failure rate.</p><p>Table <ref type="table" target="#tab_15">12</ref> presents the results. We observed a significant positive Spearman correlation (p â‰¤ 0.05, indicated in bold) across all DNN models. This robustly supports our hypothesis: increased critical pathway coverage strongly predicts a higher failure rate, demonstrating the effectiveness of our approach in fault detection within DNNs. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RQ4 (Correlation):</head><p>There is a significant positive correlation between the rate of covered critical pathways and the percentage of failed tests in all instances of approaches and models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.5">RQ5 (Locating Unique Suspicious Neurons)</head><p>Figure <ref type="figure">10</ref> compares the overlap in neurons flagged as suspicious by three fault localization techniques (Tarantula, Ochiai, and Barinel) applied to different DNN architectures and varying numbers of suspicious neurons (ğ‘˜). The figure reveals that Ochiai and Barinel frequently identify the same neurons, albeit with differing suspiciousness scores. This suggests these methods share a common underlying principle but vary in sensitivity. Conversely, Tarantula identifies a distinct set of suspicious neurons, particularly in convolutional networks. This divergence likely indicates Tarantula's distinctive methodology, which emphasizes elements of neuron behavior and network dynamics that Ochiai and Barinel overlook. Furthermore, neurons identified by Tarantula are often associated with higher reliability, indicating a stronger connection to network misclassifications. This implies that Tarantula effectively determines neurons that have a more direct influence on the network's output. In conclusion, Figure <ref type="figure">10</ref> demonstrates the importance of carefully choosing a fault localization method and suggests the potential value of combining multiple methods for a more complete picture of DNN vulnerabilities.</p><p>Figure <ref type="figure">11</ref> further investigates the commonality of suspicious neurons identified by two distinct methods: DeepFault (using Ochiai) and NP-SBFL-MGA (using Tarantula). Using ğ‘˜ = 10 for MNIST and ğ‘˜ = 50 for CIFAR, we observe substantial overlap in fully connected networks, likely due to their simpler architecture allowing for easier tracing of individual neuron influence. However, this overlap decreases significantly in convolutional networks. This discrepancy stems from the fundamental differences in their approaches: DeepFault assesses the correlation between neuron activation and output errors, whereas NP-SBFL-MGA quantifies the neurons' direct contribution to error magnitude. This methodological difference explains the contrasting sets of suspicious neurons identified, highlighting NP-SBFL-MGA's ability to determine neurons with a more substantial impact on error patterns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 6. A comparison between the ratios of common neurons in each layer of a DNN model for NP-SBFL-MGA using</head><p>Ochiai against Barinel (left) and Tarantula against Ochiai/Barinel (right).</p><p>. Figure <ref type="figure">7</ref>. A comparison between the ratio of common neurons in each layer of a DNN model for the best results of NP-SBFL-MGA and DeepFault.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RQ5 (Locating Unique Suspicious Neurons):</head><p>Ochiai and Barinel detect typical suspicious neurons but with varying suspicion levels among the different suspicious measures. However, Tarantula identifies distinct neurons, particularly in convolutional networks, and detects neurons with higher reliability than those detected by Ochiai and Barinel. Additionally, more common neurons between DeepFault and NP-SBFL-MGA are observed in fully connected networks. Significant differences are found in the suspicious neurons detected by NP-SBFL-MGA for complex models like convolutional networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.6">RQ6 (Quality of Synthesized Inputs)</head><p>This section evaluates the quality of synthetic inputs generated by NP-SBFL-MGA and the DeepFault baseline. We assessed this quality using several metrics: ğ¿ 1 (Manhattan), ğ¿ 2 (Euclidean), and ğ¿ âˆ (Chebyshev) distances between original and synthesized images, along with the inception score (IS) <ref type="bibr" target="#b60">[61]</ref> and FrÃ©chet Inception Distance (FID) <ref type="bibr" target="#b61">[62]</ref> to measure naturalness. These analyses were conducted across varying values of ğ‘˜ (number of suspicious neurons). Table <ref type="table" target="#tab_16">13</ref> summarizes the results. For MNIST, DeepFault maintained consistent perturbation levels regardless of ğ‘˜, while NP-SBFL-MGA showed increasing distances with higher ğ‘˜. Both methods yielded similar IS scores. However, for CIFAR models, DeepFault achieved superior FID scores, indicating more natural-looking synthesized images.  Table <ref type="table" target="#tab_17">14</ref> compares distances derived from various suspiciousness measures: Ochiai, Tarantula, and Barinel. Generally, the distances produced by these measures were comparable across different approaches, except for DeepFault using the Ochiai measure on the CIFAR model, which exhibited a notable divergence. Interestingly, DeepFault with Ochiai achieved the best FID score for CIFAR, while NP-SBFL with Tarantula achieved the best FID for MNIST. On MNIST, DeepFault using Tarantula and Barinel exhibited similarly low distances.</p><p>These findings provide valuable insights into the quality of generated inputs and the relative strengths and weaknesses of NP-SBFL-MGA and DeepFault across different datasets and metrics. The results underscore the importance of considering both perturbation levels and image naturalness when evaluating the efficacy of these methods. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.7">RQ7 (Generalizability)</head><p>We evaluated the effectiveness of the NP-SBFL approach using diverse neural network architectures on the MNIST and CIFAR-10 datasets. To assess its generalizability, we expanded our experiments to include Fashion-MNIST <ref type="bibr" target="#b62">[63]</ref>, FER2013 <ref type="bibr" target="#b63">[64]</ref>, GTSRB <ref type="bibr" target="#b64">[65]</ref>, and Caltech101 <ref type="bibr" target="#b65">[66]</ref> (details in Table <ref type="table" target="#tab_18">15</ref>). This selection ensures a broad evaluation across varying image domains, class numbers, and dataset sizes. We retained the CIFAR-2 architecture for its manageable complexity. Prior research questions identified NP-SBFL-MGA (employing the Tarantula metric with ğ‘˜ = 50) as the most effective configuration for detecting faulty pathways in convolutional neural networks. We consequently employed this configuration to assess generalizability across the mentioned datasets. Table <ref type="table" target="#tab_19">16</ref> displays the findings, illustrating the accuracy and loss of synthesized inputs, the count of failed tests (F) and covered critical paths (C) that activated erroneous pathways. Importantly, lower synthesized accuracy alongside higher F and C values indicates superior fault detection. This demonstrates that the synthesized inputs effectively activated faulty pathways, contributing to increased error rates. These metrics are consistent with Section 6.3.6. The consistently high F and C values across most datasets strongly support the generalizability of NP-SBFL-MGA. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RQ7 (Generalizability):</head><p>NP-SBFL-MGA with the optimal configuration demonstrates high fault detection rates across diverse benchmarks, specifically Fashion MNIST, FER2013, GTSRB, and Caltech101.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Discussion</head><p>The NP-SBFL approach offers significant advancements in understanding and diagnosing errors within DNNs. By focusing on the analysis of neural pathways, NP-SBFL effectively identifies the critical regions responsible for misclassifications, surpassing the limitations of neuron-centric approaches. This pathway-centric analysis directly reflects the interconnected nature of DNN information processing, providing a more holistic and accurate representation of the DNN's decision-making process.</p><p>Leveraging LRP, NP-SBFL identifies critical neurons based on the relevance of their activations. This ensures that generated adversarial examples are not random but strategically target the network's most vulnerable learned representations, thus increasing the likelihood of inducing misclassification.</p><p>Furthermore, the NP-SBFL-MGA variant employs a multi-gradient ascent (MGA) technique that sequentially activates neurons, mirroring the layered architecture of DNNs. This approach leverages the cumulative effects of neuron activations to uncover subtle vulnerabilities. Our empirical assessment illustrates the efficacy of NP-SBFL-MGA on complex datasets such as CIFAR-10, where the presence of extensive feature sets and complex activation patterns allows for the identification of network vulnerabilities. Conversely, this level of complexity may not be as beneficial when applied to simpler datasets like MNIST, which exhibit less varied activation patterns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Implications for DNN Reliability</head><p>NP-SBFL offers a significant advancement in identifying vulnerabilities within DNNs. Unlike traditional neuron-centric methods such as DeepFault, NP-SBFL focuses on the interactions between neurons, mirroring the DNN's actual information processing. This pathway-centric approach provides a more refined understanding of network behavior, leading to more precise fault localization. Although computationally intensive due to its use of layer-wise relevance propagation and multistage gradient ascent, the resulting thoroughness ultimately improves efficiency by minimizing time-consuming trial-anderror debugging.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Potential Applications</head><p>The benefits of NP-SBFL extend to enhancing the reliability, safety, and transparency of DNN-based systems across various industries. In healthcare, NP-SBFL's precise fault localization improves the accuracy of diagnoses and the correction of errors in medical imaging and diagnostics, ultimately leading to better patient outcomes. Similarly, in autonomous driving, where reliability is critical, NP-SBFL contributes to developing more robust systems resistant to adversarial attacks, ensuring safer operation in unpredictable environments. By identifying and strengthening vulnerable pathways, NP-SBFL also enhances the security of DNNs against malicious inputs. Additionally, its ability to determine critical pathways improves DNN interpretability, mitigating the black box problem and fostering greater user trust and regulatory compliance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Limitations</head><p>The NP-SBFL methodology, while offering significant benefits, encounters several constraints that are slated for future investigation and improvement:</p><p>ï‚· Lack of Support for RNN Architectures: The current version of NP-SBFL does not cater to RNNs, which are pivotal for analyzing time-series data. Plans are underway to integrate unfolding techniques to facilitate LRP within RNNs, thereby broadening the method's scope. ï‚· Assumptions of Differentiability and Representational Quality: According to the assumptions discussed in Section 5, activation maximization relies heavily on the underlying model being differentiable and capable of producing meaningful feature representations. This assumption may fail in networks that are inadequately trained, overfitted, or not functioning properly. For instance, an under-trained neural network may produce erratic and inconsistent outputs, leading to biased activation maximization results that do not accurately reflect the actual characteristics of the data. Similarly, an overfitted model may excessively align with the noise in the training dataset, leading to erroneous conclusions regarding the significance of various features. A practical example can be seen in image classification tasks where a model trained on a limited dataset may maximize the activation based on irrelevant and spurious features like background noise rather than actual object features. The requirement for a continuous input space also poses a challenge, as real-world inputs may exhibit discrete or categorical characteristics, limiting the applicability of the results in practical settings. ï‚· Constraints of Discrete and Categorical Input Spaces: The requirement for a continuous input space in activation maximization presents significant challenges, mainly when dealing with real-world data that often possess discrete or categorical attributes. In many practical applications, such as natural language processing or recommendation systems, inputs are not continuous but consist of distinct categories (e.g., words, user IDs, or product types). For instance, if an activation maximization approach is employed on a model trained for text classification, the model might optimize for generating inputs without meaningful representation in actual texts; because words and phrases cannot be smoothly interpolated. This limitation restricts the usefulness of the activation maximization findings, as the results may not translate well to scenarios where inputs are fundamentally categorical, thereby limiting their practical applications. ï‚· Limitation to Classification Tasks: Currently, NP-SBFL does not support regression models, which are essential in applications such as autonomous driving for forecasting continuous variables like steering angles. We aim to adapt NP-SBFL for these models by incorporating metamorphic testing, which assesses model performance across varied input transformations. ï‚· Potential for Artificial Input Generation: The absence of neuron perturbation verification during test data generation raises the potential for creating artificial inputs. To counter this, we recommend measuring the divergence between synthesized and original inputs using metrics such as the Euclidean or Manhattan distance. Introducing statistical anomaly detection can further ensure that synthesized inputs are consistent with natural data distributions. Additionally, adaptive learning algorithms could refine the synthesis process based on the model's feedback to previous inputs, fostering the generation of more realistic adversarial examples. ï‚· Scalability Issues in Pathway Extraction: Defining pathways within DNNs presents challenges due to the numerous pathways and the impact of training data on DNN behavior. The vast number of pathways in more extensive networks complicates path calculations, while the variability of DNN behavior based on training data makes it difficult to obtain sufficient data to cover all potential scenarios. Furthermore, implementing interpretative methods on extensive and intricate deep neural networks may present challenges concerning scalability and computational complexity. As networks become more intricate, the resources required for identifying critical neurons and erroneous pathways can exponentially increase, posing scalability challenges. To combat this, we propose employing pruning strategies to simplify the network by eliminating non-essential neurons and connections, thereby diminishing the computational load for pathway extraction. Approaches like magnitude-based pruning, which targets neurons with minimal weights, and advanced concepts such as the Lottery Ticket Hypothesis, which identifies a minimal subset of neurons whose removal impairs the network's learning capability, may prove beneficial.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4">Threats to Validity</head><p>Construct Validity. When conducting experiments, there is a possibility of challenges to the accuracy of the results. Poor accuracy can occur due to various factors, including the selection of datasets and DNN models. To ensure the reliability of our research, we have taken steps to address these potential concerns. Firstly, we have used well-known and widely studied public datasets, such as MNIST and CIFAR-10. Furthermore, we have applied our NP-SBFL method to multiple DNN models with distinct architectures, all of which have shown competitive prediction accuracies, as shown in Table <ref type="table" target="#tab_5">3</ref>. Additionally, we have incorporated established suspiciousness measures from the field of fault localization in software engineering, outlined in Algorithm 1, to mitigate any potential threats related to identifying suspicious neural pathways. These efforts contribute to the overall robustness and validity of our research findings. Internal Validity. To ensure the reliability of NP-SBFL-MGA in producing new inputs that trigger potentially suspicious neural pathways, we assessed potential threats to internal validity. We utilized various distance metrics to verify that the generated inputs closely resemble the original inputs and are comparable to those generated by DeepFault. We also took measures to prevent NP-SBFL-MGA's suspiciousness measures from accidentally outperforming the baselines. We performed a non-parametric statistical test, specifically the Wilcoxon rank-sum test for statistical significance at a 95% confidence level and Vargha and Delaney's Ã‚12 statistics for the effect size measure to compare the performance of NP-SBFL-MGA and the baselines, and assess any significant differences. External Validity. To address potential concerns regarding external validity, NP-SBFL must analyze the internal architecture of DNNs and collect data on the activation patterns of neurons to evaluate their degree of suspicion accurately. To achieve this, we utilized PyTorch to develop NP-SBFL, enabling comprehensive white-box analysis of DNNs. While we have examined various measures of suspicion, we acknowledge the potential existence of other measures. Furthermore, we have validated NP-SBFL against multiple instances of DNNs trained on widely used datasets to ensure its practicality. However, further experiments are needed to assess the efficacy of NP-SBFL in different domains and networks <ref type="bibr" target="#b61">[62]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>Our research introduces NP-SBFL, a pioneering fault localization method for Deep Neural Networks (DNNs), addressing the critical need for enhanced quality assurance in safety-and security-critical systems. Utilizing Layer-wise Relevance Propagation, NP-SBFL effectively detects and validates defective neurons, showcasing enhanced performance compared to current methodologies. This advancement not only highlights the potential for improved reliability in DNNs but also establishes a new standard for fault detection within these systems. In the future, we anticipate broadening the application of NP-SBFL to encompass a wider range of DNN models and datasets, particularly those critical to the field of autonomous vehicle technology. Despite its advantages, NP-SBFL has several limitations that need further investigation, including a lack of support for RNNs, assumptions of differentiability and representational quality, constraints of discrete and categorical input spaces, restriction to classification tasks, potential for artificial input generation, and scalability issues in pathway extraction. In future work, we plan to extend NP-SBFL to support RNNs for sequence prediction tasks. We will develop techniques to handle poorly trained or overfitted models, ensuring accurate activation maximization. Additionally, we intend to explore methods to manage categorical data, making findings more applicable to real-world scenarios. We will expand NP-SBFL's application to regression models through metamorphic testing. Ensuring the realism of synthesized inputs is another priority, and we propose implementing neuron perturbation verification to validate that the generated adversarial inputs are realistic and representative of potential real-world scenarios. To address scalability issues, we will investigate network pruning strategies to improve the efficiency of pathway extraction. Finally, we intend to test NP-SBFL on a broader range of datasets, including those with higher complexity and diversity, to validate the robustness and generalizability of our method across different domains.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Decreasing When the accuracy of a target model is decreasing or fluctuates during training, or the loss metric is fluctuates -improper training data very high/low learning rate incorrect activation functions 5 Unchanged Weight When weights have an unimportant influence on outputs -the very low learning rate -During the back-propagation stage, gradients may grow exponentially, which results in infinite or NaN (not a number) values -the very high learning rate -improper weight initialization improper input data very large batch size 7 Vanishing Gradient During the backpropagation stage, the value of the gradient may become so small or drop to zero -too many layers -the very low learning rate improper activation function for hidden layers incorrect weight initialization 8 Neural Trojan The Trojans are activated in rare conditions, and when a Trojan is activated, the behavior of the network deviates substantially from the Trojan-free network -poisoning attack -altering training algorithm modifying operations binary-level attacks</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Overview of NP-SBFL methodology for localizing faulty paths in DNN</figDesc><graphic coords="9,55.15,90.92,484.97,107.24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="5,71.49,90.92,451.80,120.08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Typical failure symptoms and their root causes in a DNN.</figDesc><table><row><cell># Symptom</cell><cell>Description</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Suspiciousness measures used in NP-SBFL.</figDesc><table><row><cell>Suspiciousness Measure</cell><cell cols="3">Algebraic Formula</cell><cell>Description</cell></row><row><cell>Tarantula</cell><cell cols="3">ğ‘ ğ´ ğ‘“ ğ‘ + ğ´ ğ‘“ ğ´ ğ‘“ ğ‘›</cell></row><row><cell>Ochiai</cell><cell cols="2">ğ´ ğ‘“ ğ‘ ğ‘ + ğ´ ğ‘“ ğ´ ğ‘“ ğ‘› + ğ´ ğ‘“ ğ‘</cell><cell>ğ´ ğ‘ ğ‘ ğ‘ + ğ´ ğ‘ ğ´ ğ‘ ğ‘›</cell><cell>The variables A p c and A f c represent the number of times an active n neuron led to a passed and failed test, respectively. Similarly, A p and A f n indicate the same cases for an inactive neuron. The input</cell></row><row><cell></cell><cell cols="3">âˆš(ğ´ ğ‘“ ğ‘ + ğ´ ğ‘“ ğ‘› )  *  (ğ´ ğ‘“ ğ‘ + ğ´ ğ‘ ğ‘ )</cell><cell>samples activate these neurons.</cell></row><row><cell>Barinel</cell><cell>1 -</cell><cell cols="2">ğ‘ ğ´ ğ‘ ğ‘ + ğ´ ğ‘ ğ´ ğ‘“ ğ‘</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 .</head><label>3</label><figDesc>Configuration of all DNN models used in NP-SBFL.</figDesc><table><row><cell>Dataset</cell><cell>Model</cell><cell>#Trainable Params</cell><cell>Architecture</cell><cell>Accuracy</cell><cell>#Correctly Classified Samples</cell></row><row><cell></cell><cell>MNIST_1</cell><cell>27,420</cell><cell>5 * &lt;30&gt;, &lt;10&gt;</cell><cell>96.6 %</cell><cell>9631</cell></row><row><cell>MNIST</cell><cell>MNIST_2</cell><cell>22,975</cell><cell>6 * &lt;25&gt;, &lt;10&gt;</cell><cell>95.8 %</cell><cell>9581</cell></row><row><cell></cell><cell>MNIST_3</cell><cell>18,680</cell><cell>8 * &lt;20&gt;, &lt;10&gt;</cell><cell>95 %</cell><cell>9512</cell></row><row><cell></cell><cell>CIFAR_1</cell><cell>411,434</cell><cell>2 * &lt;32@3x3&gt;, 2 * &lt;64@3x3&gt;, 4 * &lt;128&gt;, &lt;10&gt;</cell><cell>70.1 %</cell><cell>7066</cell></row><row><cell>CIFAR-10</cell><cell>CIFAR_2</cell><cell>724,010</cell><cell>2 * &lt;32@3x3&gt;, 2 * &lt;64@3x3&gt;, 2 * &lt;256&gt;, &lt;10&gt;</cell><cell>72.6 %</cell><cell>7413</cell></row><row><cell></cell><cell>CIFAR_3</cell><cell>1,250,858</cell><cell>2 * &lt;32@3x3&gt;, 2 * &lt;64@3x3&gt;, &lt;512&gt;, &lt;10&gt;</cell><cell>76.1 %</cell><cell>7634</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 .</head><label>4</label><figDesc>The hyper-parameters of different approaches and models.</figDesc><table><row><cell>Model</cell><cell cols="2">DeepFault step size distance</cell><cell cols="2">NP-SBFL-GA step size distance</cell><cell cols="2">NP-SBFL-MGA step size distance</cell></row><row><cell>MNIST-1</cell><cell>1</cell><cell>0.1</cell><cell>1</cell><cell>0.5</cell><cell>5</cell><cell>0.006</cell></row><row><cell>MNIST-2</cell><cell>1</cell><cell>0.1</cell><cell>1</cell><cell>0.5</cell><cell>5</cell><cell>0.006</cell></row><row><cell>MNIST-3</cell><cell>1</cell><cell>0.1</cell><cell>1</cell><cell>0.5</cell><cell>5</cell><cell>0.006</cell></row><row><cell>CIFAR-1</cell><cell>10</cell><cell>0.1</cell><cell>10</cell><cell>0.1</cell><cell>5</cell><cell>0.01</cell></row><row><cell>CIFAR-2</cell><cell>10</cell><cell>0.1</cell><cell>10</cell><cell>0.1</cell><cell>5</cell><cell>0.02</cell></row><row><cell>CIFAR-3</cell><cell>10</cell><cell>0.1</cell><cell>10</cell><cell>0.1</cell><cell>5</cell><cell>0.04</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 .</head><label>5</label><figDesc>Loss and accuracy of inputs synthesized by DeepFault on all the selected models. The best results per suspiciousness measure are shown in bold. ğ‘˜ represents the number of suspicious neurons.</figDesc><table><row><cell>Model</cell><cell>Measure</cell><cell></cell><cell>Tarantula</cell><cell></cell><cell></cell><cell>Ochiai</cell><cell></cell><cell></cell><cell>Barinel</cell><cell></cell></row><row><cell></cell><cell></cell><cell>ğ‘˜ = 1</cell><cell>ğ‘˜ = 5</cell><cell>ğ‘˜ = 10</cell><cell>ğ‘˜ = 1</cell><cell>ğ‘˜ = 5</cell><cell>ğ‘˜ = 10</cell><cell>ğ‘˜ = 1</cell><cell>ğ‘˜ = 5</cell><cell>ğ‘˜ = 10</cell></row><row><cell>MNIST-1</cell><cell>Loss</cell><cell>0.0526</cell><cell>0.0514</cell><cell>0.0419</cell><cell>0.0499</cell><cell>0.0451</cell><cell>0.0270</cell><cell>0.0526</cell><cell>0.0514</cell><cell>0.0419</cell></row><row><cell></cell><cell>Accuracy</cell><cell>38.20</cell><cell>49.27</cell><cell>56.54</cell><cell>53.70</cell><cell>56.58</cell><cell>68.43</cell><cell>38.20</cell><cell>49.27</cell><cell>56.54</cell></row><row><cell>MNIST-2</cell><cell>Loss</cell><cell>0.0396</cell><cell>0.0224</cell><cell>0.0141</cell><cell>0.0363</cell><cell>0.0155</cell><cell>0.0167</cell><cell>0.0396</cell><cell>0.0224</cell><cell>0.0141</cell></row><row><cell></cell><cell>Accuracy</cell><cell>43.82</cell><cell>58.01</cell><cell>71.93</cell><cell>48.87</cell><cell>69.01</cell><cell>68.56</cell><cell>43.82</cell><cell>58.01</cell><cell>71.93</cell></row><row><cell>MNIST-3</cell><cell>Loss Accuracy</cell><cell>0.0326 62.30</cell><cell>0.0229 72.98</cell><cell>0.0133 80.19</cell><cell>0.0467 56.21</cell><cell>0.0182 75.97</cell><cell>0.0154 78.40</cell><cell>0.0326 62.30</cell><cell>0.0229 72.98</cell><cell>0.0133 80.19</cell></row><row><cell></cell><cell></cell><cell>ğ‘˜ = 10</cell><cell>ğ‘˜ = 30</cell><cell>ğ‘˜ = 50</cell><cell>ğ‘˜ = 10</cell><cell>ğ‘˜ = 30</cell><cell>ğ‘˜ = 50</cell><cell>ğ‘˜ = 10</cell><cell>ğ‘˜ = 30</cell><cell>ğ‘˜ = 50</cell></row><row><cell>CIFAR-1</cell><cell>Loss Accuracy</cell><cell>0.0176 42.28</cell><cell>0.0146 47.56</cell><cell>0.0142 49.46</cell><cell>0.0170 47.80</cell><cell>0.0089 66.94</cell><cell>0.0070 73.22</cell><cell>0.0176 42.28</cell><cell>0.0146 47.56</cell><cell>0.0142 49.46</cell></row><row><cell>CIFAR-2</cell><cell>Loss Accuracy</cell><cell>0.0086 62.59</cell><cell>0.0115 55.06</cell><cell>0.0110 57.26</cell><cell>0.0107 61.75</cell><cell>0.0053 78.64</cell><cell>0.0037 85.87</cell><cell>0.0086 62.59</cell><cell>0.0115 55.06</cell><cell>0.0110 57.26</cell></row><row><cell>CIFAR-3</cell><cell>Loss Accuracy</cell><cell>0.0024 93.0</cell><cell>0.0020 96.30</cell><cell>0.0035 86.40</cell><cell>0.0018 99.24</cell><cell>0.0018 99.44</cell><cell>0.0018 99.56</cell><cell>0.0024 93.0</cell><cell>0.0020 96.30</cell><cell>0.0035 86.40</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 .</head><label>6</label><figDesc>Loss and accuracy of all models on the synthesized data for NP-SBFL-GA on all the selected models. The best results per suspiciousness measure are shown in bold. ğ‘˜ represents the number of suspicious neurons.</figDesc><table><row><cell>Model</cell><cell>Measure</cell><cell></cell><cell>Tarantula</cell><cell></cell><cell></cell><cell>Ochiai</cell><cell></cell><cell></cell><cell>Barinel</cell><cell></cell></row><row><cell></cell><cell></cell><cell>ğ‘˜ = 1</cell><cell>ğ‘˜ = 5</cell><cell>ğ‘˜ = 10</cell><cell>ğ‘˜ = 1</cell><cell>ğ‘˜ = 5</cell><cell>ğ‘˜ = 10</cell><cell>ğ‘˜ = 1</cell><cell>ğ‘˜ = 5</cell><cell>ğ‘˜ = 10</cell></row><row><cell>MNIST-1</cell><cell>Loss</cell><cell>0.1553</cell><cell>0.0280</cell><cell>0.0113</cell><cell>0.1340</cell><cell>0.0383</cell><cell>0.0201</cell><cell>0.1340</cell><cell>0.0383</cell><cell>0.0201</cell></row><row><cell></cell><cell>Accuracy</cell><cell>25.07</cell><cell>66.80</cell><cell>81.58</cell><cell>34.39</cell><cell>64.45</cell><cell>75.28</cell><cell>34.39</cell><cell>64.45</cell><cell>75.28</cell></row><row><cell>MNIST-2</cell><cell>Loss</cell><cell>0.0274</cell><cell>0.0271</cell><cell>0.0208</cell><cell>0.0548</cell><cell>0.0392</cell><cell>0.0273</cell><cell>0.0548</cell><cell>0.0392</cell><cell>0.0273</cell></row><row><cell></cell><cell>Accuracy</cell><cell>65.41</cell><cell>61.51</cell><cell>68.40</cell><cell>46.65</cell><cell>59.38</cell><cell>64.46</cell><cell>46.65</cell><cell>59.38</cell><cell>64.46</cell></row><row><cell>MNIST-3</cell><cell>Loss Accuracy</cell><cell>0.1297 40.65</cell><cell>0.0275 72.93</cell><cell>0.0271 74.20</cell><cell>0.0819 41.61</cell><cell>0.0574 59.85</cell><cell>0.0290 73.08</cell><cell>0.0819 41.61</cell><cell>0.0574 59.85</cell><cell>0.0275 74.40</cell></row><row><cell></cell><cell></cell><cell>ğ‘˜ = 10</cell><cell>ğ‘˜ = 30</cell><cell>ğ‘˜ = 50</cell><cell>ğ‘˜ = 10</cell><cell>ğ‘˜ = 30</cell><cell>ğ‘˜ = 50</cell><cell>ğ‘˜ = 10</cell><cell>ğ‘˜ = 30</cell><cell>ğ‘˜ = 50</cell></row><row><cell>CIFAR-1</cell><cell>Loss Accuracy</cell><cell>0.0106 59.97</cell><cell>0.0065 72.95</cell><cell>0.0052 77.73</cell><cell>0.0107 61.47</cell><cell>0.0093 65.49</cell><cell>0.0049 79.63</cell><cell>0.0107 61.47</cell><cell>0.0093 65.49</cell><cell>0.0051 79.06</cell></row><row><cell>CIFAR-2</cell><cell>Loss Accuracy</cell><cell>0.0146 47.38</cell><cell>0.0122 51.73</cell><cell>0.0119 52.39</cell><cell>0.0132 48.18</cell><cell>0.0158 45.01</cell><cell>0.0151 45.74</cell><cell>0.0132 48.18</cell><cell>0.0158 45.01</cell><cell>0.0151 45.74</cell></row><row><cell>CIFAR-3</cell><cell>Loss Accuracy</cell><cell>0.0102 56.06</cell><cell>0.0074 65.65</cell><cell>0.0076 65.20</cell><cell>0.0083 63.03</cell><cell>0.0062 71.69</cell><cell>0.0070 67.89</cell><cell>0.0083 63.03</cell><cell>0.0062 71.9</cell><cell>0.0070 67.89</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 .</head><label>7</label><figDesc></figDesc><table><row><cell>Model</cell><cell>Measure</cell><cell></cell><cell>Tarantula</cell><cell></cell><cell></cell><cell>Ochiai</cell><cell></cell><cell></cell><cell>Barinel</cell><cell></cell></row><row><cell></cell><cell></cell><cell>ğ‘˜ = 1</cell><cell>ğ‘˜ = 5</cell><cell>ğ‘˜ = 10</cell><cell>ğ‘˜ = 1</cell><cell>ğ‘˜ = 5</cell><cell>ğ‘˜ = 10</cell><cell>ğ‘˜ = 1</cell><cell>ğ‘˜ = 5</cell><cell>ğ‘˜ = 10</cell></row><row><cell>MNIST-1</cell><cell>Loss</cell><cell>0.0442</cell><cell>0.0317</cell><cell>0.0679</cell><cell>0.0293</cell><cell>0.0713</cell><cell>0.0392</cell><cell>0.0293</cell><cell>0.0713</cell><cell>0.0392</cell></row><row><cell></cell><cell>Accuracy</cell><cell>47.52</cell><cell>51.68</cell><cell>34.18</cell><cell>46.53</cell><cell>28.43</cell><cell>41.0</cell><cell>46.53</cell><cell>28.43</cell><cell>41.0</cell></row><row><cell>MNIST-2</cell><cell>Loss</cell><cell>0.0437</cell><cell>0.0450</cell><cell>0.0452</cell><cell>0.0393</cell><cell>0.0394</cell><cell>0.0429</cell><cell>0.0393</cell><cell>0.0394</cell><cell>0.0429</cell></row><row><cell></cell><cell>Accuracy</cell><cell>33.99</cell><cell>30.04</cell><cell>35.06</cell><cell>33.04</cell><cell>35.01</cell><cell>34.51</cell><cell>33.04</cell><cell>35.01</cell><cell>34.51</cell></row><row><cell>MNIST-3</cell><cell>Loss Accuracy</cell><cell>0.0423 26.04</cell><cell>0.0896 25.47</cell><cell>0.0665 30.52</cell><cell>0.0201 38.19</cell><cell>0.0911 19.50</cell><cell>0.0652 24.17</cell><cell>0.0201 38.19</cell><cell>0.0911 19.50</cell><cell>0.0652 24.17</cell></row><row><cell></cell><cell></cell><cell>ğ‘˜ = 10</cell><cell>ğ‘˜ = 30</cell><cell>ğ‘˜ = 50</cell><cell>ğ‘˜ = 10</cell><cell>ğ‘˜ = 30</cell><cell>ğ‘˜ = 50</cell><cell>ğ‘˜ = 10</cell><cell>ğ‘˜ = 30</cell><cell>ğ‘˜ = 50</cell></row><row><cell>CIFAR-1</cell><cell>Loss Accuracy</cell><cell>0.0160 30.76</cell><cell>0.0234 23.06</cell><cell>0.0209 21.27</cell><cell>0.0158 31.61</cell><cell>0.0182 26.80</cell><cell>0.0205 21.22</cell><cell>0.0158 31.61</cell><cell>0.0182 26.80</cell><cell>0.0205 19.17</cell></row><row><cell>CIFAR-2</cell><cell>Loss Accuracy</cell><cell>0.0196 38.01</cell><cell>0.0233 32.22</cell><cell>0.0236 33.91</cell><cell>0.0320 20.41</cell><cell>0.0469 17.65</cell><cell>0.0547 15.28</cell><cell>0.0320 20.41</cell><cell>0.0469 17.65</cell><cell>0.0547 15.28</cell></row><row><cell>CIFAR-3</cell><cell>Loss Accuracy</cell><cell>0.0183 41.19</cell><cell>0.0222 36.57</cell><cell>0.0255 30.59</cell><cell>0.0231 25.50</cell><cell>0.0242 19.57</cell><cell>0.0363 15.03</cell><cell>0.0231 25.50</cell><cell>0.0234 19.83</cell><cell>0.0363 15.03</cell></row></table><note><p>Loss and accuracy of all models on the synthesized data for NP-SBFL-MGA on all the selected models. The best results per suspiciousness measure are shown in bold. ğ‘˜ represents the number of suspicious neurons.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 8 .</head><label>8</label><figDesc>The statistical test compares the performance of NP-SBFL-MGA instances with other approaches.</figDesc><table><row><cell>Approach</cell><cell>Model</cell><cell>Measure</cell><cell>Ã‚12</cell><cell>Tarantula p-value</cell><cell>Ã‚12</cell><cell>Ochiai p-value</cell><cell>Ã‚12</cell><cell>Barinel p-value</cell></row><row><cell>DeepFault</cell><cell>MNIST CIFAR</cell><cell>Accuracy Loss Accuracy Loss</cell><cell>0.87 0.62 1 0.97</cell><cell>&lt; 0.001 0.01 &lt; 0.001 &lt; 0.001</cell><cell>1 0.48 1 0.97</cell><cell>&lt; 0.001 0.04 &lt; 0.001 &lt; 0.001</cell><cell>0.92 0.35 1 0.97</cell><cell>&lt; 0.001 0.10 &lt; 0.001 &lt; 0.001</cell></row><row><cell>NP-SBFL-GA</cell><cell>MNIST CIFAR</cell><cell>Accuracy Loss Accuracy Loss</cell><cell>0.72 0.55 1 1</cell><cell>0.005 0.02 &lt; 0.001 &lt; 0.001</cell><cell>0.85 0.11 1 0.98</cell><cell>0.001 0.36 &lt; 0.001 &lt; 0.001</cell><cell>0.85 0.11 1 0.98</cell><cell>0.001 0.36 &lt; 0.001 &lt; 0.001</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 9 .</head><label>9</label><figDesc>The statistical test compares the performance of the different approaches on MNIST. The highlighted cells mean no significant difference between the two instances in the same row and column.</figDesc><table><row><cell>Approach</cell><cell cols="2">Performance Metric Measure</cell><cell>DeepFault Tarantula Ochiai Barinel Tarantula Ochiai Barinel Tarantula Ochiai Barinel NP-SBFL-GA NP-SBFL-MGA</cell></row><row><cell></cell><cell></cell><cell>Tarantula</cell><cell></cell></row><row><cell></cell><cell>Accuracy</cell><cell>Ochiai</cell><cell></cell></row><row><cell>DeepFault</cell><cell></cell><cell>Barinel Tarantula</cell><cell></cell></row><row><cell></cell><cell>Loss</cell><cell>Ochiai</cell><cell></cell></row><row><cell></cell><cell></cell><cell>Barinel</cell><cell></cell></row><row><cell></cell><cell></cell><cell>Tarantula</cell><cell></cell></row><row><cell></cell><cell>Accuracy</cell><cell>Ochiai</cell><cell></cell></row><row><cell>NP-SBFL-GA</cell><cell>Loss</cell><cell>Barinel Tarantula Ochiai Barinel</cell><cell>ïƒ¼ ïƒ¼</cell></row><row><cell></cell><cell></cell><cell>Tarantula</cell><cell></cell></row><row><cell></cell><cell>Accuracy</cell><cell></cell><cell></cell></row><row><cell>NP-SBFL-MGA</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 10 .</head><label>10</label><figDesc>The statistical test compares the performance of the different approaches on CIFAR. The highlighted cells mean no significant difference between the two instances in the same row and column.</figDesc><table><row><cell>Approach</cell><cell cols="2">Performance Metric Measure</cell><cell>DeepFault Tarantula Ochiai Barinel Tarantula Ochiai Barinel Tarantula Ochiai Barinel NP-SBFL-GA NP-SBFL-MGA</cell></row><row><cell></cell><cell></cell><cell>Tarantula</cell><cell></cell></row><row><cell></cell><cell>Accuracy</cell><cell>Ochiai</cell><cell></cell></row><row><cell>DeepFault</cell><cell></cell><cell>Barinel Tarantula</cell><cell></cell></row><row><cell></cell><cell>Loss</cell><cell>Ochiai</cell><cell></cell></row><row><cell></cell><cell></cell><cell>Barinel</cell><cell></cell></row><row><cell>NP-SBFL-GA</cell><cell>Accuracy</cell><cell>Tarantula Ochiai Barinel Tarantula</cell><cell>ïƒ¼ ïƒ¼</cell></row><row><cell></cell><cell>Loss</cell><cell>Ochiai</cell><cell></cell></row><row><cell></cell><cell></cell><cell>Barinel</cell><cell></cell></row><row><cell></cell><cell></cell><cell>Tarantula</cell><cell></cell></row><row><cell></cell><cell>Accuracy</cell><cell></cell><cell></cell></row><row><cell>NP-SBFL-MGA</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 12 .</head><label>12</label><figDesc>Correlation results between the number of covered critical pathways and the detected faults by different approaches. The bolds refer to statistically significant correlations (p-value &lt;= 0:05).</figDesc><table><row><cell>Approach</cell><cell>Measure</cell><cell>Spearman</cell><cell>MNIST</cell><cell>p-value</cell><cell>Spearman</cell><cell>CIFAR</cell><cell>p-value</cell></row><row><cell></cell><cell>Tarantula</cell><cell>0.99</cell><cell></cell><cell>&lt; 0.001</cell><cell>1</cell><cell></cell><cell>0</cell></row><row><cell>DeepFault</cell><cell>Ochiai</cell><cell>0.93</cell><cell></cell><cell>&lt; 0.001</cell><cell>0.91</cell><cell></cell><cell>&lt; 0.001</cell></row><row><cell></cell><cell>Barinel</cell><cell>0.99</cell><cell></cell><cell>&lt; 0.001</cell><cell>1</cell><cell></cell><cell>0</cell></row><row><cell></cell><cell>Tarantula</cell><cell>0.92</cell><cell></cell><cell>&lt; 0.001</cell><cell>1</cell><cell></cell><cell>0</cell></row><row><cell>NP-SBFL-GA</cell><cell>Ochiai</cell><cell>1</cell><cell></cell><cell>0</cell><cell>0.93</cell><cell></cell><cell>&lt; 0.001</cell></row><row><cell></cell><cell>Barinel</cell><cell>0.90</cell><cell></cell><cell>&lt; 0.001</cell><cell>0.97</cell><cell></cell><cell>&lt; 0.001</cell></row><row><cell></cell><cell>Tarantula</cell><cell>1</cell><cell></cell><cell>0</cell><cell>0.97</cell><cell></cell><cell>&lt; 0.001</cell></row><row><cell>NP-SBFL-MGA</cell><cell>Ochiai</cell><cell>0.97</cell><cell></cell><cell>&lt; 0.001</cell><cell>0.98</cell><cell></cell><cell>&lt; 0.001</cell></row><row><cell></cell><cell>Barinel</cell><cell>0.99</cell><cell></cell><cell>&lt; 0.001</cell><cell>0.995</cell><cell></cell><cell>&lt; 0.001</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 13 .</head><label>13</label><figDesc>A comparison between the quality of synthesized input over different ğ‘˜ values. ğ‘˜: MNIST(CIFAR).</figDesc><table><row><cell>ğ‘˜</cell><cell>Approach</cell><cell>ğ¿ 1</cell><cell>MNIST ğ¿ 2</cell><cell>ğ¿ âˆ</cell><cell>IS (mean)</cell><cell>CIFAR IS (std)</cell><cell>FID</cell></row><row><cell>1 (10)</cell><cell>DeepFault NP-SBFL-MGA</cell><cell>7280.18 10345.26</cell><cell>388.39 529.63</cell><cell>25.42 44.11</cell><cell>1.00 1.00</cell><cell>0.00 0.00</cell><cell>42.22 48.72</cell></row><row><cell>5 (30)</cell><cell>DeepFault NP-SBFL-MGA</cell><cell>6115.85 12870.71</cell><cell>336.88 642.78</cell><cell>25.40 46.61</cell><cell>1.00 1.00</cell><cell>0.00 0.00</cell><cell>41.32 49.51</cell></row><row><cell>10 (50)</cell><cell>DeepFault NP-SBFL-MGA</cell><cell>6149.98 13636.72</cell><cell>338.95 682.66</cell><cell>25.41 47.13</cell><cell>1.00 1.00</cell><cell>0.00 0.00</cell><cell>42.73 49.91</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 14 .</head><label>14</label><figDesc>A comparison between the quality of synthesized input over different suspicious measures.</figDesc><table><row><cell>Measure</cell><cell>Approach</cell><cell>ğ¿ 1</cell><cell>MNIST ğ¿ 2</cell><cell>ğ¿ âˆ</cell><cell>IS (mean)</cell><cell>CIFAR IS (std)</cell><cell>FID</cell></row><row><cell>Tarantula</cell><cell>DeepFault NP-SBFL-MGA</cell><cell>6022.74 12314.54</cell><cell>334.31 619.14</cell><cell>25.37 46.27</cell><cell>1.00 1.00</cell><cell>0.00 0.00</cell><cell>46.90 48.22</cell></row><row><cell>Ochiai</cell><cell>DeepFault NP-SBFL-MGA</cell><cell>7500.54 12276.75</cell><cell>395.60 618.33</cell><cell>25.49 45.80</cell><cell>1.01 1.00</cell><cell>0.00 0.00</cell><cell>32.47 49.95</cell></row><row><cell>Barinel</cell><cell>DeepFault NP-SBFL-MGA</cell><cell>6022.74 12261.40</cell><cell>334.31 617.59</cell><cell>25.37 45.78</cell><cell>1.00 1.00</cell><cell>0.00 0.00</cell><cell>46.90 49.96</cell></row></table><note><p><p>RQ6 (Quality of Synthesized Inputs):</p>DeepFault outperforms NP-SBFL-MGA in generating high-quality synthesized inputs.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table 15 .</head><label>15</label><figDesc>Additional datasets used to evaluate NP-SBFL-MGA, number of classes in each dataset, and the initial accuracy on them.</figDesc><table><row><cell>Benchmark</cell><cell>Domain</cell><cell># Classes</cell><cell># Samples</cell><cell># Channels</cell></row><row><cell>Fashion MNIST</cell><cell>Clothing</cell><cell>10</cell><cell>70,000</cell><cell>Grayscale</cell></row><row><cell>FER2013</cell><cell>Facial Emotions</cell><cell>43</cell><cell>35,887</cell><cell>Grayscale</cell></row><row><cell>GTSRB</cell><cell>Autonomous Driving</cell><cell>7</cell><cell>39,209 training and 12,630 test images</cell><cell>RGB</cell></row><row><cell>Caltech101</cell><cell>Nature</cell><cell>101</cell><cell>9,146</cell><cell>RGB</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head>Table 16 .</head><label>16</label><figDesc>A comparison between the loss/accuracy of synthesized inputs, the ratios of the activated critical paths (C) and the faulty paths (F) activating faulty pathways, and the quality of synthesized input over different benchmarks.</figDesc><table><row><cell></cell><cell cols="2">Fault Detection Metrics</cell><cell></cell><cell></cell><cell></cell><cell cols="2">Quality Metrics</cell><cell></cell><cell></cell></row><row><cell>Benchmark</cell><cell>Synthesized Loss / Synthesized Accuracy</cell><cell>C</cell><cell>F</cell><cell>ğ¿ 1</cell><cell>ğ¿ 2</cell><cell>ğ¿ âˆ</cell><cell>IS (mean)</cell><cell>IS (std)</cell><cell>FID</cell></row><row><cell>Fashion MNIST</cell><cell>0.0185 / 35.56</cell><cell cols="2">80.19 73.50</cell><cell cols="3">15184.73 676.53 108.37</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>FER2013</cell><cell>0.0293 / 34.59</cell><cell cols="2">99.69 99.66</cell><cell cols="3">30977.73 1054.85 157.99</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>GTSRB</cell><cell>0.0774 / 20.17</cell><cell cols="2">99.74 99.94</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>1.002</cell><cell>0.0</cell><cell>55.87</cell></row><row><cell>Caltech101</cell><cell>0.0284 / 42.97</cell><cell>100</cell><cell>100</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>1.004</cell><cell cols="2">0.001 50.55</cell></row></table></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Declaration of generative AI and AI-assisted technologies in the writing process</head><p>During the preparation of this work, the authors used ChatGPT-3.5 and Gemini to improve the language and readability of the work. After using this tools/services, the authors reviewed and edited the content as needed and take full responsibility for the content of the publication.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep Learning for Real-Time Atari Game Play Using Offline Monte-Carlo Tree Search Planning</title>
		<author>
			<persName><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 28th Int. Conf. Neural Information Processing Systems (NIPS 2014)</title>
		<meeting>28th Int. Conf. Neural Information essing Systems (NIPS 2014)<address><addrLine>Cambridge, MA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="3338" to="3344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Speech Recognition Using Deep Learning Algorithms</title>
		<author>
			<persName><forename type="first">O</forename><surname>Abdel-Hamid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Penn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 2012 IEEE Int. Conf. Acoustics, Speech and Signal Processing (ICASSP)</title>
		<meeting>2012 IEEE Int. Conf. Acoustics, Speech and Signal essing (ICASSP)<address><addrLine>Kyoto, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="4277" to="4280" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A Primer on Neural Network Models for Natural Language Processing</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Artif. Intell. Res</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page" from="345" to="420" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Machine/Deep Learning for Software Engineering: A Systematic Literature Review</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1109/TSE.2022.3173346</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Softw. Eng</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1188" to="1231" />
			<date type="published" when="2023-03">Mar. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep Learning for Fault Localization in Software Systems</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.jss.2018.06.073</idno>
	</analytic>
	<monogr>
		<title level="j">J. Syst. Softw</title>
		<imprint>
			<biblScope unit="volume">144</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">DeepXplore: Automated Whitebox Testing of Deep Learning Systems</title>
		<author>
			<persName><forename type="first">K</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jana</surname></persName>
		</author>
		<idno type="DOI">10.1145/3132747.3132785</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. 26th Symp. Operating Systems Principles</title>
		<meeting>26th Symp. Operating Systems Principles<address><addrLine>Shanghai, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Testing Deep Neural Networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.1109/TSE.2018.2859963</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Softw. Eng</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1116" to="1131" />
			<date type="published" when="2018-12">Dec. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">DeepRoad: GAN-based Metamorphic Autonomous Driving System Testing</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Du</surname></persName>
		</author>
		<idno type="DOI">10.1145/3238147.3238187</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. 33rd IEEE/ACM Int. Conf. Automated Software Engineering (ASE)</title>
		<meeting>33rd IEEE/ACM Int. Conf. Automated Software Engineering (ASE)<address><addrLine>Montpellier, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="132" to="142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Machine Learning Testing: Survey, Landscapes and Horizons</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Harman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.1109/TSE.2019.2962027</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Softw. Eng</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">On Testing Machine Learning Programs</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">B</forename><surname>Braiek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Khomh</surname></persName>
		</author>
		<idno type="DOI">10.1016/J.JSS.2020.110542</idno>
	</analytic>
	<monogr>
		<title level="j">J. Syst. Softw</title>
		<imprint>
			<biblScope unit="volume">164</biblScope>
			<biblScope unit="page">110542</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">DeepXplore: Automated Whitebox Testing of Deep Learning Systems</title>
		<author>
			<persName><forename type="first">K</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jana</surname></persName>
		</author>
		<idno type="DOI">10.1145/3361566</idno>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="137" to="145" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">DeepGauge: Multi-Granularity Testing Criteria for Deep Learning Systems</title>
		<author>
			<persName><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<idno type="DOI">10.1145/3238147.3238202</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. 33rd IEEE/ACM Int. Conf. Automated Software Engineering (ASE)</title>
		<meeting>33rd IEEE/ACM Int. Conf. Automated Software Engineering (ASE)<address><addrLine>Montpellier, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="120" to="131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">DeepTest: Automated Testing of Deep-Neural-Network-Driven Autonomous Cars</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ray</surname></persName>
		</author>
		<idno type="DOI">10.1145/3180155.3180220</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. 40th Int. Conf. Software Engineering (ICSE)</title>
		<meeting>40th Int. Conf. Software Engineering (ICSE)<address><addrLine>Gothenburg, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="303" to="314" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A Combinatorial Approach to Testing Deep Neural Networkbased Autonomous Driving Systems</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chandrasekaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">N</forename><surname>Kacker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Kuhn</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICSTW52544.2021.00022</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Software Testing, Verification and Validation Workshops (ICSTW)</title>
		<meeting>IEEE Int. Conf. Software Testing, Verification and Validation Workshops (ICSTW)</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="57" to="66" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">DeepMutation++: A Mutation Testing Framework for Deep Learning Systems</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Hu</surname></persName>
		</author>
		<idno type="DOI">10.1109/ASE.2019.00126</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. 34th IEEE/ACM Int. Conf. Automated Software Engineering (ASE)</title>
		<meeting>34th IEEE/ACM Int. Conf. Automated Software Engineering (ASE)<address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1158" to="1161" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Structural Test Coverage Criteria for Deep Neural Networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.1145/3358233</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Embed. Comput. Syst</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">DeepConcolic: Testing and Debugging Deep Neural Networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICSE-COMPANION.2019.00051</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. 41st Int. Conf. Software Engineering: Companion Proceedings (ICSE-Companion)</title>
		<meeting>41st Int. Conf. Software Engineering: Companion eedings (ICSE-Companion)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="111" to="114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Symbolic Execution for Importance Analysis and Adversarial Generation in Neural Networks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Gopinath</surname></persName>
		</author>
		<idno type="DOI">10.1109/ISSRE.2019.00039</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. 30th Int. Symp. Software Reliability Engineering (ISSRE)</title>
		<meeting>30th Int. Symp. Software Reliability Engineering (ISSRE)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="313" to="322" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A Survey on Software Fault Localization</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">E</forename><surname>Wong</surname></persName>
		</author>
		<idno type="DOI">10.1109/TSE.2016.2521368</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Softw. Eng</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="707" to="740" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">DeepFault: Fault Localization for Deep Neural Networks</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">F</forename><surname>Eniser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gerasimou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sen</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-16722-6_10</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. 22nd Int. Conf. Fundamental Approaches to Software Engineering</title>
		<meeting>22nd Int. Conf. Fundamental Approaches to Software Engineering</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="171" to="191" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">DeepLocalize: Fault Localization for Deep Neural Networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Wardat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Rajan</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICSE43902.2021.00034</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. 43rd Int. Conf. Software Engineering (ICSE)</title>
		<meeting>43rd Int. Conf. Software Engineering (ICSE)</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="251" to="262" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">NNrepair: Constraint-Based Repair of Neural Network Classifiers</title>
		<author>
			<persName><forename type="first">M</forename><surname>Usman</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-81685-8_1</idno>
	</analytic>
	<monogr>
		<title level="s">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Kroening</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><forename type="middle">S</forename><surname>PÄƒsÄƒreanu</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">12759</biblScope>
			<biblScope unit="page" from="3" to="25" />
			<date type="published" when="2021">2021</date>
			<publisher>Springer</publisher>
			<pubPlace>Cham, Switzerland</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">DeepFD: Automated Fault Diagnosis and Localization for Deep Learning Programs</title>
		<author>
			<persName><forename type="first">J</forename><surname>Cao</surname></persName>
		</author>
		<idno type="DOI">10.1145/3510003.3510099</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. 44th Int. Conf. Software Engineering (ICSE &apos;22)</title>
		<meeting>44th Int. Conf. Software Engineering (ICSE &apos;22)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="573" to="585" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Fault Localization Based on Wide &amp; Deep Learning Model by Mining Software Behavior</title>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1016/J.FUTURE.2021.09.026</idno>
	</analytic>
	<monogr>
		<title level="j">Future Gener. Comput. Syst</title>
		<imprint>
			<biblScope unit="volume">127</biblScope>
			<biblScope unit="page" from="309" to="319" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">On Pixel-Wise Explanations for Non-Linear Classifier Decisions by Layer-Wise Relevance Propagation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bach</surname></persName>
		</author>
		<idno type="DOI">10.1371/journal.pone.0130140</idno>
	</analytic>
	<monogr>
		<title level="j">PLoS ONE</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note>p. e0130140</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">What Is Relevant in a Text Document?: An Interpretable Machine Learning Approach</title>
		<author>
			<persName><forename type="first">L</forename><surname>Arras</surname></persName>
		</author>
		<idno type="DOI">10.1371/journal.pone.0181142</idno>
	</analytic>
	<monogr>
		<title level="j">PLoS ONE</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2017">0181142. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6034</idno>
		<title level="m">Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">THE MNIST DATABASE of Handwritten Digits</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J C</forename><surname>Burges</surname></persName>
		</author>
		<ptr target="http://yann.lecun.com/exdb/mnist/" />
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
		<respStmt>
			<orgName>Courant Institute of Mathematical Sciences</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Learning Multiple Layers of Features from Tiny Images</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
		<respStmt>
			<orgName>Univ. of Toronto</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">M.S. thesis</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Spectrum-Based Fault Localization for Multiple Faults</title>
		<author>
			<persName><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-981-33-6179-9_8</idno>
	</analytic>
	<monogr>
		<title level="m">Essential Spectrum-based Fault Localization</title>
		<meeting><address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="83" to="91" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">DeepDiagnosis: Automatically Diagnosing Faults and Recommending Actionable Fixes in Deep Learning Programs</title>
		<author>
			<persName><forename type="first">M</forename><surname>Wardat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">D</forename><surname>Cruz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Rajan</surname></persName>
		</author>
		<idno type="DOI">10.1145/3510003.3510071</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. 44th Int. Conf. Software Engineering (ICSE &apos;22)</title>
		<meeting>44th Int. Conf. Software Engineering (ICSE &apos;22)</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="561" to="572" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Neural Trojans</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Srivastava</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCD.2017.16</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. 2017 IEEE Int. Conf. Computer Design (ICCD)</title>
		<meeting>2017 IEEE Int. Conf. Computer Design (ICCD)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="45" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Understanding Patch-Based Learning of Video Data by Explaining Predictions</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Anders</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Montavon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Samek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>MÃ¼ller</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-28954-6_16</idno>
	</analytic>
	<monogr>
		<title level="m">Explainable AI: Interpreting, Explaining and Visualizing Deep Learning</title>
		<editor>
			<persName><forename type="first">W</forename><surname>Samek</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Montavon</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><forename type="middle">K</forename><surname>Hansen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>MÃ¼ller</surname></persName>
		</editor>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="297" to="309" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Importance Estimation for Neural Network Pruning</title>
		<author>
			<persName><forename type="first">P</forename><surname>Molchanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mallya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tyree</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Frosio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2019.01152</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="11264" to="11272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">CRADLE: Cross-Backend Validation to Detect and Localize Bugs in Deep Learning Libraries</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">V</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lutellier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Tan</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICSE.2019.00126</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. 41st Int. Conf. Software Engineering</title>
		<meeting>41st Int. Conf. Software Engineering</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1027" to="1038" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deep Learning Library Testing via Effective Model Generation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 28th ACM Joint Meeting European Software Engineering Conf. and Symp. Foundations of Software Engineering</title>
		<meeting>28th ACM Joint Meeting European Software Engineering Conf. and Symp. Foundations of Software Engineering</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="788" to="799" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">AUTOTRAINER: An Automatic DNN Training Problem Detection and Repair System</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICSE.2021.00034</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. 43rd Int. Conf. Software Engineering</title>
		<meeting>43rd Int. Conf. Software Engineering</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="359" to="371" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Causality-Based Neural Network Repair</title>
		<author>
			<persName><forename type="first">B</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">H</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 44th Int. Conf. Software Engineering</title>
		<meeting>44th Int. Conf. Software Engineering</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="338" to="349" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">ReMoS: Reducing Defect Inheritance in Transfer Learning via Relevant Model Slicing</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICSE.2022.00156</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. 44th Int. Conf. Software Engineering (ICSE)</title>
		<meeting>44th Int. Conf. Software Engineering (ICSE)</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1856" to="1868" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Umlaut: Debugging Deep Learning Programs Using Program Structure and Model Behavior</title>
		<author>
			<persName><forename type="first">E</forename><surname>Schoop</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hartmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CHI Conf., 2021</title>
		<meeting>CHI Conf., 2021</meeting>
		<imprint>
			<biblScope unit="page" from="1" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Arachne: Search-Based Repair of Deep Neural Networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Softw. Eng. Methodol</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="26" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">DeepCNN: A Dual Approach to Fault Localization and Repair in Convolutional Neural Networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Wardat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Al-Alaj</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Automatic Fault Detection for Deep Learning Programs Using Graph Transformations</title>
		<author>
			<persName><forename type="first">A</forename><surname>Nikanjam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">B</forename><surname>Braiek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Morovati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Khomh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Softw. Eng. Methodol</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="27" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Mutation-Based Fault Localization of Deep Neural Networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ghanbari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Arshad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Rajan</surname></persName>
		</author>
		<idno type="DOI">10.1109/ASE.2023.00156</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. 38th IEEE/ACM Int. Conf. Automated Software Engineering (ASE)</title>
		<meeting>38th IEEE/ACM Int. Conf. Automated Software Engineering (ASE)</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="1301" to="1313" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">RNNrepair: Automatic RNN Repair via Model-Based Analysis</title>
		<author>
			<persName><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Machine Learning</title>
		<meeting>Int. Conf. Machine Learning</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="11383" to="11392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<author>
			<persName><forename type="first">N</forename><surname>Louloudakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Gibson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rajan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2312.15101</idno>
		<title level="m">Fix-Con: Automatic Fault Localization and Repair of Deep Learning Model Conversions</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">ArchRepair: Block-Level Architecture-Oriented Repairing for Deep Neural Networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Softw. Eng. Methodol</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1" to="31" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Explaining Image Classifiers Using Statistical Fault Localization</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chockler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kroening</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conf. Computer Vision</title>
		<meeting>European Conf. Computer Vision</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="391" to="406" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">MODE: Automated Neural Network Model Debugging via State Differential Analysis and Input Selection</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Grama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 26th ACM Joint Meeting European Software Engineering Conf. and Symp. Foundations of Software Engineering</title>
		<meeting>26th ACM Joint Meeting European Software Engineering Conf. and Symp. Foundations of Software Engineering</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="175" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Pafl: Probabilistic Automaton-Based Fault Localization for Recurrent Neural Networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ishimoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kondo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ubayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kamei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Softw. Technol</title>
		<imprint>
			<biblScope unit="volume">155</biblScope>
			<biblScope unit="page">107117</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Repairing Deep Neural Networks Based on Behavior Imitation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.03365</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Dynamic Data Fault Localization for Deep Neural Networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 31st ACM Joint European Software Engineering Conf. and Symp. Foundations of Software Engineering</title>
		<meeting>31st ACM Joint European Software Engineering Conf. and Symp. Foundations of Software Engineering</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="1345" to="1357" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">An Effective Data-Driven Approach for Localizing Deep Learning Faults</title>
		<author>
			<persName><forename type="first">M</forename><surname>Wardat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">D</forename><surname>Cruz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Rajan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2307.08947</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Visualizing Deep Convolutional Neural Networks Using Natural Pre-Images</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mahendran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">120</biblScope>
			<biblScope unit="page" from="233" to="255" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">NPC: Neuron Path Coverage via Characterizing Decision Logic of Deep Neural Networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Softw. Eng. Methodol</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="27" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Empirical Evaluation of the Tarantula Automatic Fault-Localization Technique</title>
		<author>
			<persName><forename type="first">A</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Harrold</surname></persName>
		</author>
		<idno type="DOI">10.1145/1101908.1101949</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. 2005 ACM SIGSOFT Int. Symp. Foundations of Software Engineering</title>
		<meeting>2005 ACM SIGSOFT Int. Symp. Foundations of Software Engineering</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="273" to="282" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">A practical evaluation of spectrum-based fault localization</title>
		<author>
			<persName><forename type="first">R</forename><surname>Abreu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zoeteweij</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Golsteijn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J C</forename><surname>Van Gemund</surname></persName>
		</author>
		<idno type="DOI">10.1016/J.JSS.2009.06.035</idno>
	</analytic>
	<monogr>
		<title level="j">J. Syst. Softw</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1780" to="1792" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Understanding neural networks through deep visualization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Clune</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Fuchs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lipson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.06579</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Elementary statistics for the social sciences: Study guide</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Capon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1988">1988</date>
			<publisher>Wadsworth Publishing Company</publisher>
			<pubPlace>Belmont, CA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">A critique and improvement of the CL common language effect size statistics of McGraw and Wong</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vargha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">D</forename><surname>Delaney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Educ. Behav. Stat</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="101" to="132" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Improved techniques for training GANs</title>
		<author>
			<persName><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="2234" to="2242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">GANs trained by a two-time-scale update rule converge to a local Nash equilibrium</title>
		<author>
			<persName><forename type="first">M</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 31st Int. Conf. Neural Inf. Process. Syst. (NIPS&apos;17</title>
		<meeting>31st Int. Conf. Neural Inf. ess. Syst. (NIPS&apos;17<address><addrLine>Red Hook, NY</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates Inc</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="6629" to="6640" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms</title>
		<author>
			<persName><forename type="first">H</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Rasul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Vollgraf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.07747</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Challenges in Representation Learning: Facial Expression Recognition Challenge</title>
		<author>
			<persName><forename type="first">I</forename><surname>Dumitru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Cukierski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<ptr target="https://kaggle.com/competitions/challenges-in-representation-learning-facial-expression-recognition-challenge" />
	</analytic>
	<monogr>
		<title level="j">Kaggle</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">The German traffic sign recognition benchmark: a multi-class classification competition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Stallkamp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schlipsing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Salmen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Igel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 2011 Int. Joint Conf. Neural Netw</title>
		<meeting>2011 Int. Joint Conf. Neural Netw</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1453" to="1460" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">) [Data set]</title>
		<author>
			<persName><forename type="first">F.-F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Andreeto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<idno type="DOI">10.22002/D1.20086</idno>
	</analytic>
	<monogr>
		<title level="j">CaltechDATA</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="issue">1.0</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note>Caltech</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
