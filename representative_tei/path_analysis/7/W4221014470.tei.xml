<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">This paper has been accepted for publication and is forthcoming in International Journal of Human-Computer Interaction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Hyesun</forename><surname>Choung</surname></persName>
							<email>choungh@msu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">College of Communication Arts and Science</orgName>
								<orgName type="institution">Michigan State University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Prabu</forename><surname>David</surname></persName>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Department of Media and Information</orgName>
								<orgName type="department" key="dep2">Department of Communication</orgName>
								<orgName type="institution">Michigan State University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Arun</forename><surname>Ross</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Michigan State University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">This paper has been accepted for publication and is forthcoming in International Journal of Human-Computer Interaction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1080/10447318.2022.2050543</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-14T19:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Choung</term>
					<term>H.</term>
					<term>David</term>
					<term>P.</term>
					<term>&amp; Ross</term>
					<term>A Artificial intelligence</term>
					<term>trust in AI</term>
					<term>technology acceptance model (TAM)</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>As AI-enhanced technologies become common in a variety of domains, there is an increasing need to define and examine the trust that users have in such technologies. Given the progress in the development of AI, a correspondingly sophisticated understanding of trust in the technology is required. This paper addresses this need by explaining the role of trust on the intention to use AI technologies. Study 1 examined the role of trust in the use of AI voice assistants based on survey responses from college students. A path analysis confirmed that trust had a significant effect on the intention to use AI, which operated through perceived usefulness and participants' attitude toward voice assistants. In Study 2, using data from a representative sample of the U.S. population, different dimensions of trust were examined using exploratory factor analysis, which yielded two dimensions: human-like trust and functionality trust. The results of the path analyses from Study 1 were replicated in Study 2, confirming the indirect effect of trust and the effects of perceived usefulness, ease of use, and attitude on intention to use. Further, both dimensions of trust shared a similar pattern of effects within the model, with functionality-related trust exhibiting a greater total impact on usage intention than human-like trust. Overall, the role of trust in the acceptance of AI technologies was significant across both studies. This research contributes to the advancement and application of the TAM in AI-related applications and offers a multidimensional measure of trust that can be utilized in the future study of trustworthy AI.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Trust in AI and its Role in the Acceptance of AI Technologies</head><p>Inspired by human intelligence, artificial intelligence (AI) systems are becoming increasingly adept in their ability to learn, reason, self-correct and emulate human decisions in some domains <ref type="bibr" target="#b36">(Russell et al., 2016;</ref><ref type="bibr" target="#b53">Watson, 2019)</ref>. AI is ubiquitous in modern life, enabling smart technologies and applications such as smart home devices, fitness trackers, autonomous driving systems, and social media platforms <ref type="bibr" target="#b13">(Gorwa et al., 2020;</ref><ref type="bibr" target="#b25">Lockey et al., 2021)</ref>. Further, AI technologies are equipped with varying degrees of autonomy that minimize the need for human control or oversight, and many AI-enabled devices are imbued with anthropomorphic features and natural language processing capabilities, turning them into social actors <ref type="bibr" target="#b53">(Watson, 2019)</ref>. The pervasiveness and impact of AI has raised concerns about its ethics and principles of governance.</p><p>Experts contend that without guiding principles the application of AI in critical areas such as health, finances, and criminal justice will perpetuate the same biases in human thinking, thus robbing the technology of its potential to create new systems that enlighten and augment human intelligence to correct social wrongs <ref type="bibr" target="#b0">(AI HLEG, 2019)</ref>. Also, there is a growing fear that our reliance on AI comes at the cost of human agency, and that fear is further stoked by the black-box nature of machine learning algorithms, which generate a response that mimics human response without adequate explanation of the underlying process <ref type="bibr" target="#b1">(Barredo Arrieta et al., 2020)</ref>. Such concerns, compounded by the erosion of trust in institutions and governments <ref type="bibr" target="#b7">(Edelman, 2021)</ref>, underscore the importance of building trustworthy AI system to instill confidence among users <ref type="bibr" target="#b11">(Gillath et al., 2021;</ref><ref type="bibr" target="#b50">Thiebes et al., 2020)</ref> and to further our understanding of the social and psychological mechanisms of trust in human-AI interaction <ref type="bibr" target="#b11">(Gillath et al., 2021)</ref>. Extant literature on trust in AI and algorithm-driven technologies has focused on exploring the key principles of trustworthy AI systems such as beneficence, non-maleficence, autonomy, justice, and explicability <ref type="bibr" target="#b50">(Thiebes et al., 2020)</ref>. Some studies have empirically examined the roles of key principles (e.g., fairness, accountability, transparency, explainablity) in shaping perceptions of and experience with the technologies (e.g., <ref type="bibr" target="#b41">Shin, 2021a)</ref>. Understanding the role of trust in relation to these other well-studied determinants of technology acceptance provides a useful theoretical framework for research and practice in trustworthy AI. Moreover, understanding the nature of the different dimensions underlying the concept of trust provides a fuller integration of the literature on trust with the literature on technology acceptance.</p><p>Accordingly, the primary goal of this study is to examine the role of trust as a holistic construct within the technology acceptance model (TAM) framework. In addition, we explore the core dimensions of trust perceptions and their relationship to the determinants of the acceptance of AI technologies.</p><p>The TAM is a theoretical framework for explaining the technology usage behavior that has been validated in different technologies across various populations <ref type="bibr" target="#b52">(Venkatesh &amp; Davis, 2000)</ref>. The original TAM proposed perceived usefulness and perceived ease of use to be the two determinants of future usage intention <ref type="bibr" target="#b5">(Davis, 1989)</ref>. Subsequent studies have incorporated other external factors into the TAM, and trust has been examined as a key factor in the acceptance of emerging technologies (K. <ref type="bibr" target="#b56">Wu et al., 2011)</ref>. Trust combines characteristics, intentions, and behaviors (J. D. <ref type="bibr" target="#b22">Lee &amp; See, 2004;</ref><ref type="bibr" target="#b28">Mayer et al., 1995)</ref> and is required to build mutuality and interdependence between parties. Building on the key literature on organizational trust <ref type="bibr" target="#b28">(Mayer et al., 1995)</ref> and trust in technology <ref type="bibr" target="#b29">(Mcknight et al., 2011)</ref>, which offer three core dimensions (benevolence/helpfulness, integrity/reliability, and competence/functionality), this paper offers a twofold conceptualization of trust: 1) trust in the human-like aspects of AI, which pertains largely to the character of the technology, and 2) trust in the functionality of AI, such as its ability, reliability and safety.</p><p>In Study 1, we examine the role of trust in the acceptance and use of AI conversational agents such as Siri, Alexa, and Google using data from a convenience sample of college students.</p><p>After confirming the role of trust in the acceptance of conversational agents, in Study 2, we replicated the findings with a nationally representative sample and added the following extensions: (1) instead of measuring trust just in AI conversational agents, we examined trust in smart technologies in general, and (2) we extracted the technical and human-like dimensions of AI using exploratory factor analysis <ref type="bibr" target="#b28">(Mayer et al., 1995;</ref><ref type="bibr" target="#b29">Mcknight et al., 2011)</ref> and evaluated them separately to predict intentions to use smart technologies.</p><p>By confirming the significant role of trust and demonstrating its added utility to TAM, we suggest that trust and the TAM framework can be applied to AI-driven applications, especially technologies with greater risks and/or human-like characteristics. Moreover, the multidimensional approach to trust in AI and the related measurements will be useful for researchers interested in predicting and understanding the role of human trust in AI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Trustworthy AI</head><p>The uniqueness of AI is characterized by the integration of its functionality with humanlike capabilities <ref type="bibr" target="#b20">(Krafft et al., 2020)</ref>. In this study, we adopt the following definition of AI offered by the <ref type="bibr">OECD (2019, pp. 23-24)</ref>: "An AI system is a machine-based system that can, for a given set of human-defined objectives, make predictions, recommendations, or decisions influencing real or virtual environments. AI systems are designed to operate with varying levels of autonomy." Unlike traditional technologies in which users have complete control over their functioning, AI's capacity for autonomous functioning creates risks and uncertainties for users.</p><p>Moreover, AI relies heavily on machine learning algorithms, which have been characterized as black boxes because of the inner workings of these algorithms are not easily explainable, and the way in which artificial agents learn and attain a particular answer may not be comprehensible to human actors. The black box nature of AI creates unpredictability and uncertainties, highlighting the importance of trust as users cope with the complexities and potential risks associated with AI's decision-making.</p><p>As corporations, governments, and citizens recognize the potential power and impact of AI as a sociotechnical system, great emphasis has been placed on building trustworthy AI. In recent years, government organizations (e.g., OECD, EU), tech companies (e.g., Google, Microsoft), and professional association (e.g., IEEE) have issued frameworks for the development and deployment of trustworthy AI. Their guidelines commonly recommend that trustworthy AI should be built into the system by design <ref type="bibr" target="#b14">(Hagendorff, 2020;</ref><ref type="bibr" target="#b17">Jobin et al., 2019)</ref>.</p><p>Many recent studies on trust in AI have focused on exploring factors that contribute to trustworthy AI <ref type="bibr" target="#b11">(Gillath et al., 2021;</ref><ref type="bibr" target="#b25">Lockey et al., 2021;</ref><ref type="bibr" target="#b33">Rheu et al., 2020)</ref>. Through a series of studies, Shin identified and empirically examined key factors that contribute to trustworthy AI systems and the role of trust in shaping people's perceptions of AI attributes (e.g., usefulness, performance, accuracy, credibility) in algorithm-driven technologies <ref type="bibr" target="#b39">(Shin, 2020b</ref><ref type="bibr" target="#b42">(Shin, , 2021b))</ref>. <ref type="bibr" target="#b46">Shin and Park (2019)</ref> examined the role of fairness, accountability, and transparency (FAT) in predicting users' satisfaction with algorithms and found that users with a lower level of trust considered FAT issues more skeptically. Later, <ref type="bibr" target="#b39">Shin (2020b)</ref> extended the FAT framework to the fairness, accountability, transparency, and explainable (FATE) framework by introducing "explainability" as an additional determinant of trustworthiness and perceptions of the usefulness of algorithmic recommendations. Shin's most recent studies have further explored the relationship between FATE, trust, user perceptions <ref type="bibr" target="#b43">(Shin, 2021c</ref>, and acceptance <ref type="bibr" target="#b42">(Shin, 2021b)</ref>. For example, Shin (2021e) examined the explainability of algorithms as a key predictor of FAT and tested whether explainability and FAT further predict users' trust in and perceptions of the functional attributes (i.e., accuracy, personalization, credibility) of algorithmic journalism.</p><p>Overall, Shin's findings demonstrate FATE are core requirements of trustworthy AI and that trust plays a pivotal role in shaping positive perceptions of and user experience with AI technologies.</p><p>Anthropomorphism has also been commonly associated with trust in AI. Studies have found that the embodiment of an agent can increase trust because users are better able to perceive its social presence (K. <ref type="bibr" target="#b19">Kim et al., 2018;</ref><ref type="bibr" target="#b44">Shin, 2021d;</ref><ref type="bibr" target="#b54">Waytz et al., 2014)</ref>. Anthropomorphism has been associated with greater trust resilience, which prevents loss of trust <ref type="bibr" target="#b6">(de Visser et al., 2016)</ref>. However, other studies have reported weak or no effects of anthropomorphism on trust <ref type="bibr" target="#b8">(Erebak &amp; Turgut, 2019;</ref><ref type="bibr" target="#b15">Hancock et al., 2011)</ref>. <ref type="bibr" target="#b12">Glikson and Woolley (2020)</ref> identified factors that predict cognitive and emotional trust in AI. They described how tangibility and immediacy behaviors affect both cognitive and emotional trust in AI, while transparency, reliability, and task characteristics predict cognitive trust, and anthropomorphism predicts emotional trust in AI.</p><p>Overall, the literature suggests that trust plays a critical role in the perceptions and acceptance of AI technologies. Past studies have examined trust in various applications of AI, including algorithmic journalism <ref type="bibr">(Shin, 2020c)</ref>, healthcare AI (M. K. <ref type="bibr" target="#b24">Lee &amp; Rich, 2021)</ref>, and AI used for hiring and work evaluations (M. K. <ref type="bibr" target="#b23">Lee, 2018)</ref>. Among numerous applications and services equipped with AI technology, we focus on AI technologies in the form of consumer products. This is because people interact most frequently with AI assistants, such as Siri and Alexa, or smart home systems that have been in the consumer market for long enough to build initial trust, thus allowing us to apply the concept. In the following section, we apply trust in AI to the TAM framework to validate whether trust can be another key determinant of AI technology acceptance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Trust and Use of AI Technologies: An Extended TAM</head><p>The TAM offers a useful theoretical framework that can validate the aforementioned significance of trust in AI to predict the acceptance of AI. Originally developed by <ref type="bibr" target="#b5">Davis (1989)</ref>, the TAM has been widely employed and tested to understand the adoption of new technologies <ref type="bibr" target="#b30">(McLean &amp; Osei-Frimpong, 2019)</ref>. The original TAM introduced perceived usefulness and perceived ease of use as the core concepts that explain individuals' adoption of technology <ref type="bibr" target="#b5">(Davis, 1989)</ref>. Later, more elaborate versions of the TAM (i.e., TAM 2, TAM 3) were developed, incorporating additional external factors, such as social norms and perceived enjoyment <ref type="bibr">(Venkatesh &amp; Bala, 2008;</ref><ref type="bibr" target="#b52">Venkatesh &amp; Davis, 2000)</ref>, and treating attitude as a predictor of behavior.</p><p>Following the traditional TAM framework, we first propose the following hypotheses: H1: A positive attitude toward using an AI technology predicts a greater willingness to use it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H2:</head><p>The perceived ease of using an AI technology positively influences a) perception of its usefulness, b) trust perception, c) attitude, and d) usage intention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H3:</head><p>The perceived usefulness of an AI technology positively influences a) attitude toward using it and b) willingness to use it.</p><p>As trust is considered to be another significant predictor for the adoption of new technologies <ref type="bibr" target="#b47">(S√∂llner et al., 2016)</ref>, previous studies have investigated the connections between trust and the TAM. The role of trust was tested in the use of a new information system <ref type="bibr">(Tung et al., 2008)</ref> and different online services, such as online games (J. <ref type="bibr">Wu &amp; Liu, 2007)</ref>, banking <ref type="bibr">(Suh &amp; Han, 2002)</ref>, social network sites <ref type="bibr">(Sledgianowski &amp; Kulviwat, 2009)</ref>, and shopping <ref type="bibr" target="#b10">(Gefen et al., 2003)</ref>, among others. A meta-analysis (K. <ref type="bibr" target="#b56">Wu et al., 2011)</ref> revealed that trust exhibited a significantly positive impact on the key TAM constructs. Recent studies continue to support the relevance of trust in TAM. For example, Shin (2020a) demonstrated that trust can positively influence continuous use of news recommendation system. In a later study, <ref type="bibr" target="#b41">Shin (2021a)</ref> showed that trust in AI positively predict usefulness and ease of use of AI. <ref type="bibr" target="#b2">Beldad and Hegner (2018)</ref> found that trust did not directly affect the usage intention regarding a health tracking app; rather, it influenced users' perceptions of the app's usefulness. Likewise, studies have found that trust affects usage intention indirectly rather than directly, through increasing usefulness and positive attitudes.</p><p>This study follows the hypotheses tested by other TAM-related studies that have proposed trust as an antecedent of perceived usefulness and subsequent to perceived ease of use while having a direct effect on attitude <ref type="bibr" target="#b10">(Gefen et al., 2003;</ref><ref type="bibr" target="#b18">J. B. Kim, 2012;</ref><ref type="bibr" target="#b56">K. Wu et al., 2011)</ref>.</p><p>Accordingly, we propose the following hypotheses: H4: Users' trust in an AI technology influences a) perceived usefulness and b) attitudes toward the technology.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H5:</head><p>The influence of trust on behavioral intention will be mediated by a) perceived usefulness, b) attitude, and c) both perceived usefulness and attitude.</p><p>By integrating trust within TAM, we propose the following model to test the hypotheses stated above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>[Figure 1 Near Here]</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multidimensional Approach to Trust in AI</head><p>Trust is a fundamental human mechanism that is required to cope with vulnerability, uncertainty, complexity, and ambiguity, which collectively constitute a risk. Trust is defined as "a psychological state comprising the intention to accept vulnerability based upon positive expectations of the intentions or behavior of another" <ref type="bibr">(Rousseau et al., 1998, p. 395)</ref>.</p><p>Traditionally, trust is tied to relationships between people and is required to build mutuality and interdependence between parties in human communication.</p><p>Despite the recognition of the importance of the various aspects of trust, few studies have addressed how to assess social and psychological trust in AI. Glikson and Woolley pointed out the "great variance in measures used to assess human trust in AI," which may "discourage researchers from collaborating and limit research implications to a specific discipline (p.651)."</p><p>To address this shortcoming, we conceptualize and examine different dimensions of trust, including trustees' characteristics, intentions, and behaviors (J. D. <ref type="bibr" target="#b22">Lee &amp; See, 2004;</ref><ref type="bibr" target="#b28">Mayer et al., 1995)</ref>. Table <ref type="table">1</ref> summarizes the trust concepts and the basis of trust in different agents (people, technology, and automation) discussed in the literature.</p><p>[Table <ref type="table">1</ref> About Here] <ref type="bibr" target="#b28">Mayer et al. (1995)</ref> define trust in humans as an amalgam of one's belief in another's ability, benevolence, and integrity. Ability refers to skills and competencies to successfully complete a given task. Benevolence pertains to whether the trustee has positive intentions that are not based purely on self-interest. And integrity describes the trustee's sense of morality and justice, such that the trusted party's behaviors are consistent, predictable, and honest. Trust is vital to understanding interpersonal interaction, as it contributes to perceived reliability and integrity.</p><p>Past studies have applied interpersonal trust to human technology relationships <ref type="bibr" target="#b3">(Calhoun et al., 2019)</ref>, especially when the technology has human-like characteristics <ref type="bibr" target="#b11">(Gillath et al., 2021)</ref>.</p><p>Other studies have noted that human users have a distinctive view on interacting with technologies, and those works have suggested that the principles of trust in interpersonal relations cannot be directly applicable to human-to-machine trust (e.g., <ref type="bibr" target="#b26">Madhavan &amp; Wiegmann, 2007)</ref>. <ref type="bibr" target="#b29">McKnight et al. (2011)</ref> explained that trust in technology is qualitatively different from trust in people. The key distinction is that a human is a moral agent, whereas technology lacks volition and moral agency. The authors elaborated on three dimensions of trust in technology, functionality, reliability, and helpfulness, which they related to human trust. Functionality refers to the capability of the technology, which the authors likened to human ability. Reliability is the consistency of operation, which is like integrity, and helpfulness indicates whether the specific technology is useful to users and is like the benevolence dimension of human trust.</p><p>AI could be understood as one type of technology, but at the same time, it is involved in replacing tasks and decisions made by humans. In that sense, it is more than just a technology and McKnight's definition and the dimensions of trust in technology are not perfectly applicable to AI technologies. Unlike traditional technologies that rely on user input and the execution of rules programmed by a human, AI implies certain levels of autonomy. In one of McKnight's later studies, <ref type="bibr" target="#b21">Lankton et al. (2015)</ref> demonstrated that for a technology with greater humanness, a trust-in-humans scale works better at predicting relevant outcomes than a trust-in-technology scale. This finding suggests that trust in technology is a dynamic concept that is still evolving and variable based on the context and the characteristics of the trusted agent. We consider that in the context of AI, both trust in people and trust in technology are relevant, since AI technologies are often depicted as being capable of human qualities, including reasoning and motivations, which can induce high expectations and initial trust <ref type="bibr" target="#b12">(Glikson &amp; Woolley, 2020)</ref>.</p><p>Trust in automation is another relevant conceptual framework proposed by <ref type="bibr" target="#b22">Lee and See (2004)</ref>. Similar to trust in people and trust in technology, trust in automation is summarized in three categories: 1) performance, 2) process, and 3) purpose. Performance refers to the operational characteristics of automation, including its reliability and ability. Process concerns how suitable the automation is to achieve users' goals. In interpersonal relationships, this corresponds to the consistency of behaviors associated with adherence to a set of norms <ref type="bibr" target="#b28">(Mayer et al., 1995)</ref>. Purpose describes why the automation was developed and the designers' intent.</p><p>In the current study, building on previous work regarding trust, we focus on the two main dimensions of trust in AI: 1) human-like trust in AI, and 2) functionality trust in AI. The former dimension pertains to the social and cultural values of the algorithms and the values and ethics that undergird the design of the AI technology. The latter dimension pertains to the competency and expertise of the technological features. For human-like trust in AI, trust is in the agent, not in the specific actions and operations of the agent (J. D. <ref type="bibr" target="#b22">Lee &amp; See, 2004</ref>). People will exhibit greater trust in an AI technology whose algorithm is transparent and explainable to them <ref type="bibr" target="#b42">(Shin, 2021b)</ref>. Moreover, the perception of fairness and justice will be associated with human trust in AI. The two types of trust in AI are expected to be different in their antecedents and effects. We expected that both dimensions of trust would influence technology adoption and use <ref type="bibr" target="#b37">(Schmidt et al., 2020)</ref>, especially for technologies that are associated with risk and uncertainty. An important extension of the second study is the comparison between the two dimensions of trust in AI (the human-like dimension and the functionality dimension) and their influence on people's acceptance of AI technologies, which is proposed as a research question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RQ:</head><p>Is there a difference in the influence between the human-like dimension and the functionality dimension of trust in AI within the TAM framework?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Study 1</head><p>In Study 1, we aimed to gain an initial understanding of the role of trust in the adoption and use of AI technologies. The primary goal of this study was to test H1-H5. Among various applications with AI technologies, we chose voice assistants or conversational agents, such as Apple's Siri, Amazon's Alexa, and Google Assistant, which have dramatically increased in use over the past few years <ref type="bibr" target="#b49">(Terzopoulos &amp; Satratzemi, 2020)</ref>, warranting a closer examination of interaction with the technology and motivations underlying the adoption and use.</p><p>Voice assistants are designed to be more human-like (McLean &amp; Osei-Frimpong, 2019), with their voice feature encouraging users to adopt the same social responses that they habitually use in their interpersonal relationships <ref type="bibr" target="#b32">(Reeves &amp; Nass, 1996)</ref>. Today, with increasingly realistic and socially capable chatbots, research explores the similarities between how people treat other people and how they treat technologies <ref type="bibr" target="#b9">(Gaudiello et al., 2016;</ref><ref type="bibr" target="#b48">Sundar et al., 2017)</ref>. The anthropomorphic nature of voice assistants could make trust more critical. Therefore, in Study 1, we focused on trust in AI voice assistants and examined its significance in predicting the adoption and use.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Participants and Procedure</head><p>An online survey was conducted from October to December 2020. Undergraduate students from a large Midwest university were invited to participate, and responses from 312 students were collected. Of the participants, 56% were female, and their ages ranged from 18 to 29 (median age was 21). Ethnicities represented included Caucasians (66%), Asians (21%), African Americans (6%), and Hispanics (5%). Ninety-six percent of participants reported that they had used at least one type of AI voice assistant. Respondents received extra credit for completing the survey, which took about 10 minutes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Measures</head><p>The appendix summarizes the survey questionnaires and the reliability of the variables.</p><p>The constructs "perceived ease of use" and "perceived usefulness" were measured with five items, each adapted from the original TAM scale <ref type="bibr" target="#b5">(Davis, 1989)</ref>. Trust in an AI voice assistant was measured with four items that we created. Behavioral intention to use or continue to use an AI voice assistant was measured with three items from the TAM 2 scale <ref type="bibr" target="#b52">(Venkatesh &amp; Davis, 2000)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Analytic Approach</head><p>A path analysis was used to test the hypothesized relationships among the variables and the model constructed in Figure <ref type="figure">1</ref>. The analysis was conducted in R using the lavaan package <ref type="bibr" target="#b34">(Rosseel, 2012)</ref>. Furthermore, to test the fit of the data, maximum likelihood estimation was employed, using the following fit indices: the chi-square (ùúí2) statistic, the comparative fit index (CFI), the Tucker-Lewis index (TLI), the root mean square error of approximation (RMSEA), and the standardized root mean residual (SRMR). If ùúí2 is not significant, the CFI and TLI values are .90 or higher, and the SRMR and RMSEA values are .10 or less, then the model is considered to have a good fit. The statistical significance of the indirect effects was tested by examining bootstrapped (k = 1,000) 95% confidence intervals (CIs). Intercorrelations among the seven constructs were also determined using correlation analysis before the structural model was tested. The values in Table <ref type="table" target="#tab_0">2</ref> indicate that the constructs are moderately correlated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>[Table 2 Near Here]</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>It was hypothesized that AI voice assistant usage intention is influenced by four factors, namely, perceived ease of use, trust perception, perceived usefulness, and attitude (as illustrated in Figure <ref type="figure">1</ref>). The model further indicates that perceived ease of use, trust, and perceived usefulness relate to one another, and the path model demonstrated a good fit with the data: ùúí2 (1) = .32, p = .58; RMSEA = .00 [.00, .12]; SRMR = .01; CFI = 1.00; TLI = 1.01.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>[Table 3 About Here] [Figure 2 About Here]</head><p>Regression estimates indicated that all four proposed predictors of AI voice assistant usage intention have statistically significant direct and indirect effects on the dependent variables. Perceived ease of use, perceived usefulness, and attitude were positive predictors of usage intention. Moreover, trust positively predicted perceived usefulness. Figure <ref type="figure">2</ref> illustrates the tested model with the standard path coefficients for the relationships among the variables.</p><p>The first set of hypotheses (H1-H3) examined the intention to use voice assistants using the traditional determinants of the TAM with the addition of trust. As hypothesized, a positive attitude toward using an AI voice assistant was associated with a greater usage intention (ùõΩ = .62, p &lt; .001; H1 is supported). In addition, perceived ease of use was positively associated with the perceived usefulness of a voice assistant (ùõΩ = .36, p &lt; .001), trust perception (ùõΩ = .39, p &lt; .001), attitude (ùõΩ = .26, p &lt; .001), and usage intention (ùõΩ = .19, p &lt; .05). Therefore, H2 was supported.</p><p>The perceived usefulness of an AI voice assistant also positively influenced users' attitude toward it (ùõΩ = .42, p &lt; .001) and willingness to use it (ùõΩ = .20, p &lt; .01), thus supporting H3.</p><p>These findings yield support for the traditional TAM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>[Table 4 Near Here]</head><p>The second set of hypotheses (H4 and H5) examined the direct and indirect effects of trust in the TAM. The results of indirect effects revealed that all mediated paths depicted were statistically significant, including the hypothesized (H5) paths (see Table <ref type="table" target="#tab_1">3</ref>). Trust was associated with increased perceived usefulness, which in turn increased usage intention (H4a).</p><p>Trust also predicted positive attitudes, which in turn influenced usage intention (H4b). Moreover, trust predicted perceived usefulness, which in turn increased positive attitudes and usage intention (H4c). Therefore, H4 was supported.</p><p>Finally, the total effects of each predictor were calculated, and perceived ease of use had the largest total effect on usage intention (ùõΩ = .63; p &lt; .001), followed by perceived usefulness (ùõΩ = .46; p &lt; .001) and trust perception (ùõΩ = .32; p &lt; .001).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion</head><p>Study 1 presents an empirical test of the TAM for the acceptance of AI voice assistants.</p><p>The validated model presents a high explanatory power, with 52% of variance explained (R 2 = .52). The results of this study indicate that trust perception with the two TAM constructsperceived ease of use and perceived usefulness-significantly predicts attitudes toward and inclination to use voice assistants. The finding is in line with studies that have tested the impact of these constructs on the adoption of various forms of technology (e.g., <ref type="bibr" target="#b2">Beldad &amp; Hegner, 2018;</ref><ref type="bibr" target="#b4">Choi &amp; Ji, 2015)</ref>.</p><p>Among the key predictors, perceived usefulness had a greater direct effect on usage intention than perceived ease of use. However, the total effect of perceived ease of use was greater than that of perceived usefulness. This finding suggests that to motivate individuals to adopt or continue to use an AI voice assistant, they should find the technology easy to use and useful. Thus, voice assistants should not be complicated to use, and they should provide users with useful functionalities and the benefits that people expect from using them.</p><p>In addition, we found that trust can influence and is influenced by the factors included in the TAM. Perceived ease of use contributes to trust, and trust in AI voice assistants, in turn, predicts positive attitudes and perceived usefulness, which were associated with greater usage intention. Although trust does not directly affect usage intention, the construct appears to be pivotal in improving user perception of the utility of voice assistants and in building positive attitudes toward them. An implication of this result is that people are inclined to regard a technology as beneficial if they trust it. In contrast, a lack of trust could raise concerns about the potential threats and risks of the technology instead of its benefits. Therefore, designing a trustworthy AI technology is vital, along with improving its functionality and providing an easyto-use interface. Also, current voice assistants are designed to be more human-like than previous attempts, and many individuals are communicating with voice assistants as part of their everyday life in the same way they interact with other humans <ref type="bibr" target="#b48">(Sundar et al., 2017)</ref>, making trust more relevant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Study 2</head><p>In the second study, we offer an in-depth analysis of the dimensions of trust in AI within the TAM framework (the RQ). In addition, the findings from the first study (H1-H5) are replicated with a representative national sample. Study 2 extends the previous study's findings by 1) treating and measuring trust as a multidimensional construct, 2) recruiting participants that are representative of the general U.S. population, and 3) extending the acceptance of AI-based voice agents to all consumer-based AI, grouped as smart technologies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Participants and Procedure</head><p>An online survey was conducted in April 2021 using the national Qualtrics panel. To ensure the representativeness of participants, we adopted a quota sampling method that set quotas for gender, age, and race based on U.S. Census data. A total of 640 respondents (50.1% female, age M = 46.43, SD = 17.83, 65.9% White, 12% Black or African American, 12% Hispanic, 5.9% Asian, 4.4% other) participated in the study. The average time for survey completion was 10 minutes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Measures</head><p>The same TAM-related items (i.e., perceived ease of use, perceived usefulness, attitude, behavior intention) from Study 1 were used in Study 2 (see the appendix). The items to measure trust in AI were different from Study 1 as we constructed the items with the consideration of three pillars of the construct used in trust in humans <ref type="bibr" target="#b28">(Mayer et al., 1995)</ref> and trust in technology <ref type="bibr" target="#b29">(Mcknight et al., 2011)</ref>: benevolence/helpfulness, integrity/reliability, and competence/functionality. Items from the three constructs were factor analyzed, which yielded a two-factor structure, based on a principal components exploratory factor analysis (EFA) with an Oblimin rotation. The items measuring benevolence and integrity were grouped together, and competence was treated as another dimension. We named the first dimension as human-like trust in AI (six items), and the second dimension a functionality trust in AI (five items).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>[Table 5 near here] Analytic Approach</head><p>Two separate path models examined the human-like trust and functionality-trust in AI using the same analytic approach from Study 1. Intercorrelations among the constructs are presented in Table <ref type="table">6</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>[Table 6 Near Here]</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>H1-H5 and the RQ were addressed with two path models depicted in Figures <ref type="figure">3</ref> and<ref type="figure">4</ref>.</p><p>The models fit the data well: human trust in AI, ùúí 2 (1) = 4.92, p = .03; RMSEA = .08 [.02, .15]; SRMR = .01; CFI = 1.00; TLI = .98; technology-related trust in AI, ùúí 2 (1) = 4.85, p = .03 RMSEA = .08 [.02, .15]; SRMR = .01; CFI = 1.00; TLI = .98. Although the chi-square test was significant, it is highly sensitive to a large sample size. All other measures of fit (CFI above .95, SRMR below .05, and RMSEA below .08) showed that the models were adequate in reproducing covariances among the variables.</p><p>We hypothesized that people's intention to continue using smart technologies is influenced by four factors: perceived ease of use, trust (the human-like dimension and the functionality dimension), perceived usefulness, and attitude. Consistent with the findings from Study 1, regression estimates indicated that all proposed predictors of intention to use smart technologies have a statistically significant effect.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>[Figures 3 and 4 Near Here]</head><p>[Tables <ref type="table" target="#tab_4">7</ref> and<ref type="table" target="#tab_5">8</ref> About Here] Figure <ref type="figure">3</ref> and Figure <ref type="figure">4</ref> show the standardized path coefficients for the relationships among the variables. In both models, attitude was a statistically significant predictor of intention to use smart technologies (ùõΩ = .65, p &lt; .001), thus supporting H1. Furthermore, as hypothesized, perceived ease of use was positively associated with perceived usefulness (ùõΩ = .46, p &lt;.001; ùõΩ = .44, p &lt; .001), both trust dimensions (ùõΩ = .56, p &lt; .001; ùõΩ = .51, p &lt; .001), attitude (ùõΩ = .16, p &lt; .001; ùõΩ = .17, p &lt; .001), and usage intention (ùõΩ = .14, p &lt; .001), thus offering robust evidence for H2. Finally, the perceived usefulness of AI smart technologies predicted attitude (ùõΩ = .54, p &lt; .001; ùõΩ = .53, p &lt; .001) and usage intention (ùõΩ = .22, p &lt; .001) in both models (H3 supported).</p><p>These findings are in line with the initial findings from Study 1 and support the TAM's applicability to users' acceptance of AI technologies in smart objects used by consumers.</p><p>[Tables <ref type="table" target="#tab_6">9</ref> and<ref type="table" target="#tab_7">10 About Here]</ref> Regarding the role of the two trust dimensions in the TAM, both the human-like and the functionality dimensions of trust had statistically significant indirect effects on the intention to use AI smart technologies, thus supporting H5. The two types of trust were associated with perceived usefulness, which predicted greater usage intention (H4a). Trust also predicted positive attitudes, which in turn affected usage intention (H4b). Finally, trust dimensions predicted perceived usefulness, which in turn increased positive attitudes and usage intention (H4c). Therefore, additional support was gained for H4.</p><p>In both path models, participants' intention to use a smart technology was predicated on its perceived ease of use (total effect ùõΩ = .71; p &lt; .001; ùõΩ = .71; p &lt; .001) and perceived usefulness (total effect ùõΩ = .57; p &lt; .001; ùõΩ = .56; p &lt; .001). In answering the RQ, there was no clearly discernable difference between the human-like trust and functionality-related trust in their pattern and magnitude of the relationships with other TAM constructs. However, the total effects of the trust dimensions revealed that the functionality-related trust dimension exhibited a greater total impact on usage intention than the human-like trust dimension (total effect of human-like trust: ùõΩ = .36; p &lt; .001; total effect of functionality-related trust: ùõΩ = .42; p &lt; .001).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion</head><p>The results of the second study with the general population provide additional support for the key determinants (perceived ease of use, usefulness, and trust) of a user's attitude and behavioral intention associated with the acceptance of AI technologies suggested by the extended TAM. These findings are consistent with Study 1 and prior literature <ref type="bibr" target="#b10">(Gefen et al., 2003;</ref><ref type="bibr" target="#b55">I.-L. Wu &amp; Chen, 2005)</ref>.</p><p>Additionally, the results indicate that the two dimensions of trust in AI-the human-like trust dimension and the functionality dimension-significantly predict perceived usefulness and positive attitude toward smart technology, which in turn, predict greater usage intention. Both human-like trust in AI and functionality-related trust in AI had a positive impact on perceived usefulness, attitude, and usage intention, and the functionality dimension of trust had a greater total impact than the human-like trust dimension. This suggests that while both trust constructs can be useful in understanding the multidimensional aspects and measurement of trust in AI, trust in the functionality of AI is particularly useful when applying the TAM to understand AI acceptance. At the same time, the significance of the human-like properties of AI, such as social graces, protection of privacy, protocols for ensuring fairness and avoiding bias were also significant. It appears the duality of AI as both a functional and social technology must be considered when designing trustworthy AI. Further, human-like trust may explain emotional trust and emotional attachment to technology.</p><p>The distinctive perceptions of human-like trust and functionality trust align with studies that propose multiple dimensions and factors of trust in AI and automation <ref type="bibr" target="#b12">(Glikson &amp; Woolley, 2020;</ref><ref type="bibr" target="#b16">Hoff &amp; Bashir, 2015;</ref><ref type="bibr" target="#b22">J. D. Lee &amp; See, 2004)</ref>. For instance, <ref type="bibr" target="#b12">Glikson and Woolley (2020)</ref> organized the literature on trust in AI based on the factors that shaped users' cognitive and emotional trust. The cognitive construct involves the rational evaluation of AI while emotional trust is typically influenced by irrational factors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>General Discussion</head><p>Our findings provide further support for the utility of the TAM in explaining the acceptance of AI technologies and the added contribution of trust within the model. The addition of trust to the TAM model was examined in two empirical studies conducted with different AI technology applications and respondent populations. In Study 1 and Study 2, we found that perceived ease of use and perceived usefulness were essential determinants for the use of AI technologies, although perceived ease of use had a consistently greater impact on the acceptance of these technologies. This impact may be because consumers tend to believe that AI technologies are complicated and difficult to use. The model explains that when people consider an AI technology to be easy to use, they are more likely to trust the technology and consider it useful. Therefore, for AI-based consumer technologies, making the technology easy and straightforward enough to operate without much difficulty is critical for its uptake.</p><p>Our studies confirmed the significant role of trust in shaping people's attitudes and acceptance of AI technologies with the TAM framework. As past research has already confirmed, trust in AI can be built through embedding specific principles in the design and governance of AI, as suggested in recent publications using the fairness, accountability, transparency, explainability framework <ref type="bibr" target="#b39">(Shin, 2020b;</ref><ref type="bibr" target="#b46">Shin &amp; Park, 2019)</ref>. In addition to the key attributes of trustworthy AI, our findings highlight the importance of the experiential aspect of AI technology to creating and maintaining trust. Indeed, trust is relevant throughout the lifecycle of a technology. Users who participate in an experience with technology build ideas about whether it is useful and easy to use based on their experience of use. Thus, the concept of trustworthy AI should not be limited to the design but should also integrate the direct experience of users, and the TAM offers a useful framework in that regard.</p><p>Overall, our findings suggest that future studies investigating the acceptance of AI must include the trust construct as an integral component of their predictive models. Given the central role of trust, there is a strong practical need to understand what facilitates trust in AI and what contributes to the design of trustworthy AI.</p><p>Additionally, this study offers concepts and measurements for two dimensions of trust in AI. We operationalized trust in AI with a human-like dimension to capture the automatic priming of social cues, intentionality, and moral agency, although AI not human. This dimension was a significant predictor of the acceptance of technology, as was the functionality dimension. The distinction between human-like trust and functionality-related trust in AI, and their measures tested in this study, will be useful for future AI research, as trust will be even more critical in AI applications in high-stake domains such as autonomous vehicles and medical decision-making.</p><p>We also urge researchers to reflect upon the various dimensions of trust because our study findings show a clear perceptual difference exists between the two dimensions. Hence, studies that improve our understanding of the antecedents and effects of human-like and technology-related trust in AI will be useful. Another useful approach, not covered in this study, is that of multi-level or multi-layer trust in AI. <ref type="bibr" target="#b16">Hoff and Bashir (2015)</ref> suggested three layers of trust in automation (dispositional trust, situational trust, and learned trust) based on their review of trust in AI. Such trust models offer additional insights into research on trustworthy AI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>Given the progress that has been made in AI development, a more sophisticated understanding of trust in AI is required than was previously the case. This study examined different dimensions of trust within the established TAM framework, which has been used widely to examine the acceptance of various new technologies <ref type="bibr" target="#b27">(Maranguniƒá &amp; Graniƒá, 2015)</ref>. In the days ahead, the influence of AI on everyday life is expected to continuously grow, and given the black box nature of the underlying mechanisms of AI, the importance of trust in AI is also likely to grow. The key contributions of our study are summarized below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Theoretical Contributions</head><p>Our core theoretical contribution lies in extending the classical TAM framework by including trust-specific aspects of AI. Overall, our findings suggest that future studies investigating the acceptance of AI must include the trust construct as an integral part of their predictive models. Given the central role of trust, there is a strong need to understand what facilitates trust in AI and what contributes to the design of trustworthy AI.</p><p>In addition, the TAM was used to assess the effects of different dimensions of trust. The multidimensional approach provides a new lens for conceptualizing the variability of people's trust perceptions in AI. Its structure can be applied to help guide future research. For example, specific ethical principles of AI may align with specific dimensions of trust. The trust-building factors already outlined in previous research may be closely related to a specific dimension of trust. For example, anthropomorphism may increase human-like trust perceptions, yet it may have no impact on perceptions of functionality. Similarly, people may not simultaneously possess either high or low trust in both dimensions of trust. In such cases, understanding the effects of a specific type of trust can be useful. Therefore, we urge future research to examine trust in AI by including its different dimensions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Practical Contributions</head><p>This research also contributes to the growing body of practice that seeks to understand the role of trust in accepting AI technologies. Our findings suggest that AI technologies should be easy to use, useful, and trusted. When such characteristics are coupled with other factors that contribute to trustworthy perceptions of AI (e.g., FATE, anthropomorphism), the technologies are likely to be accepted and used by many people. As much as the designers of AI systems care about making the systems useful and easy to use, designing AI embedded with values that makes them trustworthy should be a critical design concern.</p><p>By conceptualizing and examining possible dimensions of trust in AI, this study contributes to understanding how to ensure such abstract trust issues in AI are addressed; how to design AI systems that are value-oriented, ethical, and human-centered; and how to govern AI in support of trustworthy AI systems. Moreover, our multidimensional framework can be used to develop interventions and design procedures that encourage appropriate levels of trust.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Limitations and Future Research Directions</head><p>This study has some limitations that offer promising research directions for the future.</p><p>First, we focused on AI technology applications in the form of consumer products (i.e., voice assistants and smart technologies), which tend be associated with lower risks. Future research could further explore high-stakes AI (e.g., self-driving cars, AI in healthcare) and different AIpowered applications with other human-like characteristics, such as robots and embedded algorithms, to examine the role of trust and additional critical factors in the adoption and use of such technologies.</p><p>Second, trust is a dynamic concept that varies depending on the stage of technology. For example, the trust life cycle can be divided into initial trust and ongoing trust, depending on one's initial experience, and these forms of trust may play different roles (J. B. <ref type="bibr" target="#b18">Kim, 2012)</ref>.</p><p>Future studies could also develop measurement scales for differentiated targets of trust (e.g., technology itself, the company, or governing bodies; <ref type="bibr" target="#b47">S√∂llner et al., 2016)</ref>. Third, our path models are based on correlations and cannot guarantee causal effects of trust on the acceptance of AI technologies. We proposed trust as an antecedent of perceived usefulness based on the findings from the past TAM literature (K. <ref type="bibr" target="#b56">Wu et al., 2011)</ref>. Still, an opposite direction of the relationship was also found to be valid in another study <ref type="bibr" target="#b38">(Shin, 2020a)</ref>.</p><p>Therefore, future studies can test a causal model to determine the exact relationships among TAM constructs to extend current findings.</p><p>Finally, as with any online panel, our study respondents may not be truly representative of the general population. The fact that all the study participants had access to computers and the internet skewed the sample toward a higher acceptance of technology. Participants in an online survey are likely to have greater access to and experience with various forms of information technology. Therefore, future studies should examine more representative samples with varying levels of experience with technologies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Tables and Figures</head><p>Table <ref type="table">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dimensions of Trust</head><p>Basis of trust in people <ref type="bibr" target="#b28">Mayer et al., (1995)</ref> Basis of trust in technology <ref type="bibr" target="#b21">Lankton et al. (2015)</ref> Basis of trust in automation <ref type="bibr">Lee &amp; Moray (1992)</ref> Competence     Note. For perceived ease of use, perceived usefulness, attitude, and behavior intention items, the same question wordings were used in Study 1 and Study 2 except the description of AI technologies. Study 1 participants were asked about their perceptions of "AI virtual assistant" and Study 2 participants were asked about "AI smart technologies."</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Figure 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .Figure 4 .</head><label>34</label><figDesc>Figure 3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>sincerely concerned about addressing the problems of human users. (Benevolence) Smart technologies try to be helpful and do not operate out of selfish interest. (Benevolence) Smart technologies are truthful in their dealings. (Integrity) Smart technologies keep their commitments and deliver on their promises. (Integrity) Smart technologies are honest and do not abuse the information and advantage they have over their users. Smart technologies are competent in their area of expertise. (Competence) Smart technologies are reliable. (Competence) Smart technologies are dependable. (Competence)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2 .</head><label>2</label><figDesc> Means, and Standard Deviations (Study 1)    </figDesc><table><row><cell>/Ability</cell><cell></cell><cell>Functionality</cell><cell></cell><cell cols="2">Performance</cell></row><row><cell>Integrity</cell><cell></cell><cell>Reliability</cell><cell></cell><cell>Process</cell><cell></cell></row><row><cell>Benevolence</cell><cell></cell><cell>Helpfulness</cell><cell></cell><cell>Purpose</cell><cell></cell></row><row><cell>Variable</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell></row><row><cell>1. Ease of use</cell><cell>1</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>2. Trust</cell><cell>38**</cell><cell>1</cell><cell></cell><cell></cell><cell></cell></row><row><cell>3. Usefulness</cell><cell>.46**</cell><cell>.48**</cell><cell>1</cell><cell></cell><cell></cell></row><row><cell>4. Attitude</cell><cell>.56**</cell><cell>.56**</cell><cell>.70**</cell><cell>1</cell><cell></cell></row><row><cell>5. Behavior Intention</cell><cell>.50**</cell><cell>.40**</cell><cell>.59**</cell><cell>.70**</cell><cell>1</cell></row><row><cell>M</cell><cell>3.81</cell><cell>3.03</cell><cell>3.42</cell><cell>3.57</cell><cell>3.77</cell></row><row><cell>SD</cell><cell>.81</cell><cell>.83</cell><cell>.91</cell><cell>.82</cell><cell>1.02</cell></row><row><cell>Note. ** p &lt;.01</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 .</head><label>3</label><figDesc>Standardized Regression Path Analysis (Study 1)</figDesc><table><row><cell>Paths</cell><cell>ùõΩ</cell><cell>p-value</cell><cell>Hypotheses</cell></row><row><cell>PE ‚Üí PU</cell><cell>.36</cell><cell>&lt;.001</cell><cell>H2a</cell></row><row><cell>PE ‚Üí TR</cell><cell>.39</cell><cell>&lt;.001</cell><cell>H2b</cell></row><row><cell>PE ‚Üí ATT</cell><cell>.26</cell><cell>&lt;.001</cell><cell>H2c</cell></row><row><cell>PE ‚Üí BI</cell><cell>.19</cell><cell>.012</cell><cell>H2d</cell></row><row><cell>PU ‚Üí ATT</cell><cell>.42</cell><cell>&lt;.001</cell><cell>H3a</cell></row><row><cell>PU ‚Üí BI</cell><cell>.20</cell><cell>.007</cell><cell>H3b</cell></row><row><cell>TR ‚Üí PU</cell><cell>.39</cell><cell>&lt;.001</cell><cell>H4a</cell></row><row><cell>TR ‚Üí ATT</cell><cell>.23</cell><cell>&lt;.001</cell><cell>H4b</cell></row><row><cell>ATT ‚Üí BI</cell><cell>.62</cell><cell>&lt;.001</cell><cell>H1</cell></row><row><cell cols="4">Note. PE = Perceived ease of use; PU = Perceived usefulness; TR = Trust; ATT = Attitude; BI =</cell></row><row><cell>Behavior intention</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 .</head><label>4</label><figDesc>Analysis of Indirect Effects (Study 1)</figDesc><table><row><cell>Paths</cell><cell>Effect (95% CI)</cell><cell>LLCI</cell><cell>ULCI</cell><cell>Hypotheses</cell></row><row><cell>PE ‚Üí ATT ‚Üí BI</cell><cell>.16</cell><cell>.10</cell><cell>.23</cell><cell></cell></row><row><cell>PE ‚Üí PU ‚Üí BI</cell><cell>.07</cell><cell>.02</cell><cell>.15</cell><cell></cell></row><row><cell>PE ‚Üí PU ‚Üí ATT ‚Üí BI</cell><cell>.09</cell><cell>.06</cell><cell>.16</cell><cell></cell></row><row><cell>PE ‚Üí TRU ‚Üí ATT ‚Üí BI</cell><cell>.05</cell><cell>.03</cell><cell>.09</cell><cell></cell></row><row><cell>PE ‚Üí TRU ‚Üí PU ‚Üí BI</cell><cell>.03</cell><cell>.01</cell><cell>.06</cell><cell></cell></row><row><cell>PE ‚Üí TRU ‚Üí PU ‚Üí ATT ‚Üí BI</cell><cell>.04</cell><cell>.02</cell><cell>.07</cell><cell></cell></row><row><cell>PU ‚Üí ATT ‚Üí BI</cell><cell>.26</cell><cell>.18</cell><cell>.36</cell><cell></cell></row><row><cell>TRU ‚Üí PU ‚Üí BI</cell><cell>.08</cell><cell>.02</cell><cell>.15</cell><cell>H4a</cell></row><row><cell>TRU‚Üí ATT ‚Üí BI</cell><cell>.14</cell><cell>.08</cell><cell>.22</cell><cell>H4b</cell></row><row><cell>TRU ‚Üí PU‚Üí ATT ‚Üí BI</cell><cell>.03</cell><cell>.06</cell><cell>.16</cell><cell>H4c</cell></row><row><cell cols="5">Note. PE = Perceived ease of use; PU = Perceived usefulness; TR = Trust; ATT = Attitude; BI =</cell></row><row><cell>Behavior intention</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 .</head><label>5</label><figDesc>Results From an Exploratory Factor Analysis of the Trust in AI Questionnaire (Study 2) Note. N = 640. The extraction method was principal axis factoring with an oblique (promax with Kaiser normalization) rotation. Factor loadings above .60 are in bold.</figDesc><table><row><cell>Trust in AI items</cell><cell cols="3">Factor loading Communality 1 2</cell></row><row><cell>Factor 1: Human-like trust in AI</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Smart technologies care about our well-being. (Benevolence)</cell><cell>.92</cell><cell>-.08</cell><cell>.75</cell></row><row><cell>Smart technologies are sincerely concerned about addressing the problems of human users. (Benevolence)</cell><cell>.89</cell><cell>-.04</cell><cell>.74</cell></row><row><cell>Smart technologies try to be helpful and do not operate out of selfish interest. (Benevolence)</cell><cell>.81</cell><cell>-.02</cell><cell>.64</cell></row><row><cell>Smart technologies are truthful in their dealings. (Integrity)</cell><cell>.79</cell><cell>.09</cell><cell>.73</cell></row><row><cell>Smart technologies keep their commitments and deliver on their promises. (Integrity)</cell><cell>.60</cell><cell>.29</cell><cell>.70</cell></row><row><cell>Smart technologies are honest and do not abuse the</cell><cell></cell><cell></cell><cell></cell></row><row><cell>information and advantage they have over their users.</cell><cell>.82</cell><cell>.05</cell><cell>.73</cell></row><row><cell>(Integrity)</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Factor 2: Functionality trust in AI</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Smart technologies work well. (Competence)</cell><cell>.04</cell><cell>.81</cell><cell>.71</cell></row><row><cell>Smart technologies have the features necessary to complete key tasks. (Competence)</cell><cell>.08</cell><cell>.76</cell><cell>.67</cell></row><row><cell>Smart technologies have the features necessary to complete key tasks. (Competence)</cell><cell>-.03</cell><cell>.87</cell><cell>.72</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 7 .</head><label>7</label><figDesc>Standardized Regression Path Analysis with Human-like Trust in AI (Study 2)</figDesc><table><row><cell>Paths</cell><cell>ùõΩ</cell><cell>p-value</cell><cell>Hypotheses</cell></row><row><cell>PE ‚Üí PU</cell><cell>.46</cell><cell>&lt;.001</cell><cell>H2a</cell></row><row><cell>PE ‚Üí HTR</cell><cell>.56</cell><cell>&lt;.001</cell><cell>H2b</cell></row><row><cell>PE ‚Üí ATT</cell><cell>.16</cell><cell>&lt;.001</cell><cell>H2c</cell></row><row><cell>PE ‚Üí BI</cell><cell>.14</cell><cell>&lt;.001</cell><cell>H2d</cell></row><row><cell>PU ‚Üí ATT</cell><cell>.54</cell><cell>&lt;.001</cell><cell>H3a</cell></row><row><cell>PU ‚Üí BI</cell><cell>.22</cell><cell>&lt;.001</cell><cell>H3b</cell></row><row><cell>HTR ‚Üí PU</cell><cell>.30</cell><cell>&lt;.001</cell><cell>H4a</cell></row><row><cell>HTR ‚Üí ATT</cell><cell>.30</cell><cell>&lt;.001</cell><cell>H4b</cell></row><row><cell>ATT ‚Üí BI</cell><cell>.65</cell><cell>&lt;.001</cell><cell>H1</cell></row><row><cell cols="4">Note. PE = Perceived ease of use; PU = Perceived usefulness; HTR = Human trust in AI; ATT =</cell></row><row><cell cols="2">Attitude; BI = Behavior intention</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 8 .</head><label>8</label><figDesc>Standardized Regression Path Analysis with Functionality Trust in AI (Study 2)</figDesc><table><row><cell>Smart technologies are reliable. (Competence)</cell><cell>-.06</cell><cell>.92</cell><cell>.78</cell></row><row><cell>Smart technologies are dependable. (Competence)</cell><cell>.03</cell><cell>.84</cell><cell>.74</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 9 .</head><label>9</label><figDesc>Analysis of Indirect Effects with Human Trust in AI (Study 2) PE = Perceived ease of use; PU = Perceived usefulness; HTR = Human trust in AI; ATT = Attitude; BI = Behavior intention</figDesc><table><row><cell>Paths</cell><cell>Effect (95% CI)</cell><cell>LLCI</cell><cell>ULCI</cell><cell>Hypotheses</cell></row><row><cell>PE ‚Üí ATT ‚Üí BI</cell><cell>.10</cell><cell>.05</cell><cell>.16</cell><cell></cell></row><row><cell>PE ‚Üí PU ‚Üí BI</cell><cell>.10</cell><cell>.05</cell><cell>.16</cell><cell></cell></row><row><cell>PE ‚Üí PU ‚Üí ATT ‚Üí BI</cell><cell>.16</cell><cell>.12</cell><cell>.21</cell><cell></cell></row><row><cell>PE ‚Üí HTR ‚Üí ATT ‚Üí BI</cell><cell>.11</cell><cell>.08</cell><cell>.15</cell><cell></cell></row><row><cell>PE ‚Üí HTR ‚Üí PU ‚Üí BI</cell><cell>.04</cell><cell>.02</cell><cell>.06</cell><cell></cell></row><row><cell>PE ‚Üí HTR ‚Üí PU ‚Üí ATT ‚Üí BI</cell><cell>.06</cell><cell>.04</cell><cell>.08</cell><cell></cell></row><row><cell>PU ‚Üí ATT ‚Üí BI</cell><cell>.35</cell><cell>.27</cell><cell>.43</cell><cell></cell></row><row><cell>HTR ‚Üí PU ‚Üí BI</cell><cell>.07</cell><cell>.03</cell><cell>.10</cell><cell>H4a</cell></row><row><cell>HTR ‚Üí ATT ‚Üí BI</cell><cell>.19</cell><cell>.14</cell><cell>.26</cell><cell>H4b</cell></row><row><cell>HTR ‚Üí PU‚Üí ATT ‚Üí BI</cell><cell>.10</cell><cell>.07</cell><cell>.14</cell><cell>H4c</cell></row><row><cell>Note.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 10 .</head><label>10</label><figDesc>Analysis of Indirect Effects with Technology Trust in AI (Study 2) PE = Perceived ease of use; PU = Perceived usefulness; TTR = Technology trust in AI; ATT = Attitude; BI = Behavior intention</figDesc><table><row><cell>Paths</cell><cell>Effect (95% CI)</cell><cell>LLCI</cell><cell>ULCI</cell><cell>Hypotheses</cell></row><row><cell>PE ‚Üí ATT ‚Üí BI</cell><cell>.11</cell><cell>.06</cell><cell>.17</cell><cell></cell></row><row><cell>PE ‚Üí PU ‚Üí BI</cell><cell>.10</cell><cell>.05</cell><cell>.15</cell><cell></cell></row><row><cell>PE ‚Üí PU ‚Üí ATT ‚Üí BI</cell><cell>.15</cell><cell>.11</cell><cell>.21</cell><cell></cell></row><row><cell>PE ‚Üí TTR ‚Üí ATT ‚Üí BI</cell><cell>.10</cell><cell>.06</cell><cell>.15</cell><cell></cell></row><row><cell>PE ‚Üí TTR ‚Üí PU ‚Üí BI</cell><cell>.04</cell><cell>.02</cell><cell>.07</cell><cell></cell></row><row><cell>PE ‚Üí TTR ‚Üí PU ‚Üí ATT ‚Üí BI</cell><cell>.07</cell><cell>.05</cell><cell>.10</cell><cell></cell></row><row><cell>PU ‚Üí ATT ‚Üí BI</cell><cell>.34</cell><cell>.28</cell><cell>.43</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>Smart technologies care about our well-being.</figDesc><table><row><cell>Attitude</cell><cell cols="2">I feel positive toward [AI virtual assistants/AI</cell><cell>1 (strongly</cell><cell>ùõº = .89 ùõº = .90</cell></row><row><cell>toward the AI</cell><cell>smart technologies]</cell><cell></cell><cell>disagree) -5</cell><cell></cell></row><row><cell>technologies</cell><cell cols="2">I feel that using [AI virtual assistants/AI</cell><cell>(strongly</cell><cell></cell></row><row><cell>(Study 1 &amp;</cell><cell>smart technologies] is pleasant</cell><cell></cell><cell>agree)</cell><cell></cell></row><row><cell>Study 2)</cell><cell cols="2">Perceived Using [AI virtual assistants/AI smart</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Usefulness technologies] is a good idea</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">R 2 = .54 Using [AI virtual assistants/AI smart</cell><cell></cell><cell></cell></row><row><cell>.46*** Behavioral intention of Perceived Ease of Use non-users (Study 1 &amp;</cell><cell cols="3">technologies] is a smart way to get things done I intend to use [AI virtual assistants/AI smart .22*** 1 (strongly .54*** technologies] in a future disagree) -5 (strongly I predict that I would use [AI virtual .14*** agree) assistants/AI smart technologies]</cell><cell>ùõº = .98 ùõº = .91</cell></row><row><cell>Study 2) .56*** Behavioral</cell><cell cols="2">Attitude R 2 = .70 Using [AI virtual assistants/AI smart .16*** technologies] is something I would do in a .30*** future. I intend to continue using [AI virtual</cell><cell>.65***</cell><cell>Usage Intention R 2 = .75 ùõº = .95</cell></row><row><cell>intention of</cell><cell>assistants/AI smart technologies]</cell><cell></cell><cell></cell><cell></cell></row><row><cell>users (Study 1 &amp; Study 2)</cell><cell cols="2">I predict that I would continue using [AI .30*** virtual assistanst/AI smart technologies] Human-like Trust in AI Using [AI virtual assistants/AI smart R 2 = .28 technologies] is something I would continue</cell><cell></cell><cell></cell></row><row><cell></cell><cell>to do.</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Human-like</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>trust in smart</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>technologies</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>(Study 2)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Perceived</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Usefulness</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>R 2 = .56</cell><cell></cell><cell></cell><cell></cell></row><row><cell>.44***</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">.22***</cell><cell></cell></row><row><cell></cell><cell cols="2">.53***</cell><cell></cell><cell></cell></row><row><cell>Perceived</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Ease of Use</cell><cell cols="2">.14***</cell><cell></cell></row><row><cell></cell><cell>.17***</cell><cell>Attitude</cell><cell></cell><cell>Usage Intention</cell></row><row><cell>.51***</cell><cell>.38***</cell><cell>R 2 = .68</cell><cell>.65***</cell><cell>R 2 = .75</cell></row><row><cell></cell><cell cols="2">.31***</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Functionality Trust in AI</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>R 2 = .30</cell><cell></cell><cell></cell><cell></cell></row></table><note><p>* p &lt; .05, ** p &lt; .01, *** p &lt; .001 * p &lt; .05, ** p &lt; .01, *** p &lt; .001</p></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Ethics guidelines for trustworthy AI</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">I</forename><surname>Hleg</surname></persName>
		</author>
		<idno type="DOI">10.2759/346720</idno>
		<ptr target="https://data.europa.eu/doi/10.2759/346720" />
	</analytic>
	<monogr>
		<title level="j">European Commission</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Explainable Artificial Intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI</title>
		<author>
			<persName><forename type="first">A</forename><surname>Barredo Arrieta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>D√≠az-Rodr√≠guez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Del Ser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bennetot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tabik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Barbado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gil-Lopez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Molina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Benjamins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chatila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Herrera</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.inffus.2019.12.012</idno>
		<ptr target="https://doi.org/10.1016/j.inffus.2019.12.012" />
	</analytic>
	<monogr>
		<title level="j">Information Fusion</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page" from="82" to="115" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Expanding the Technology Acceptance Model with the Inclusion of Trust, Social Influence, and Health Valuation to Determine the Predictors of German Users&apos; Willingness to Continue using a Fitness App: A Structural Equation Modeling Approach</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Beldad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Hegner</surname></persName>
		</author>
		<idno type="DOI">10.1080/10447318.2017.1403220</idno>
		<ptr target="https://doi.org/10.1080/10447318.2017.1403220" />
	</analytic>
	<monogr>
		<title level="j">International Journal of Human-Computer Interaction</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="882" to="893" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Linking precursors of interpersonal trust to human-automation trust: An expanded typology and exploratory experiment</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">S</forename><surname>Calhoun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bobko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Gallimore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Lyons</surname></persName>
		</author>
		<idno type="DOI">10.1080/21515581.2019.1579730</idno>
		<ptr target="https://doi.org/10.1080/21515581.2019.1579730" />
	</analytic>
	<monogr>
		<title level="j">Journal of Trust Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="28" to="46" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Investigating the Importance of Trust on Adopting an Autonomous Vehicle</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">G</forename><surname>Ji</surname></persName>
		</author>
		<idno type="DOI">10.1080/10447318.2015.1070549</idno>
		<ptr target="https://doi.org/10.1080/10447318.2015.1070549" />
	</analytic>
	<monogr>
		<title level="j">International Journal of Human-Computer Interaction</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="692" to="702" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Perceived Usefulness, Perceived Ease of Use, and User Acceptance of Information Technology</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">D</forename><surname>Davis</surname></persName>
		</author>
		<idno type="DOI">10.2307/249008</idno>
		<ptr target="https://doi.org/10.2307/249008" />
	</analytic>
	<monogr>
		<title level="j">MIS Quarterly</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">319</biblScope>
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Almost human: Anthropomorphism increases trust resilience in cognitive agents</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>De Visser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Monfort</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mckendrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A B</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">E</forename><surname>Mcknight</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Parasuraman</surname></persName>
		</author>
		<idno type="DOI">10.1037/xap0000092</idno>
		<ptr target="https://doi.org/10.1037/xap0000092" />
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: Applied</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="331" to="349" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Edelman trust barometer 2021</title>
		<author>
			<persName><surname>Edelman</surname></persName>
		</author>
		<ptr target="https://www.edelman.com/sites/g/files/aatuss191/files/2021-03/2021%20Edelman%20Trust%20Barometer.pdf" />
		<imprint>
			<date type="published" when="2021">2021</date>
			<publisher>Annual Edelman Trust Barometer</publisher>
			<biblScope unit="page">58</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Caregivers&apos; attitudes toward potential robot coworkers in elder care</title>
		<author>
			<persName><forename type="first">S</forename><surname>Erebak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Turgut</surname></persName>
		</author>
		<idno>10111- 018-0512-0</idno>
		<ptr target="https://doi.org/10.1007/s" />
	</analytic>
	<monogr>
		<title level="j">Technology &amp; Work</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="327" to="336" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>Cognition</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Trust as indicator of robot functional and social acceptance. An experimental study on user conformation to iCub answers</title>
		<author>
			<persName><forename type="first">I</forename><surname>Gaudiello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Zibetti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lefort</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chetouani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ivaldi</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.chb.2016.03.057</idno>
		<ptr target="https://doi.org/10.1016/j.chb.2016.03.057" />
	</analytic>
	<monogr>
		<title level="j">Computers in Human Behavior</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="633" to="655" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Trust and TAM in Online Shopping: An Integrated Model</title>
		<author>
			<persName><forename type="first">D</forename><surname>Gefen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Karahanna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">W</forename><surname>Straub</surname></persName>
		</author>
		<idno type="DOI">10.2307/30036519</idno>
		<ptr target="https://doi.org/10.2307/30036519" />
	</analytic>
	<monogr>
		<title level="j">MIS Quarterly</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="51" to="90" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Attachment and trust in artificial intelligence</title>
		<author>
			<persName><forename type="first">O</forename><surname>Gillath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Branicky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Keshmiri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Spaulding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers in Human Behavior</title>
		<imprint>
			<biblScope unit="page">10</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Human trust in artificial intelligence: Review of empirical research</title>
		<author>
			<persName><forename type="first">E</forename><surname>Glikson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Woolley</surname></persName>
		</author>
		<idno type="DOI">10.5465/annals.2018.0057</idno>
		<ptr target="https://doi.org/10.5465/annals.2018.0057" />
		<imprint>
			<date type="published" when="2020">2020</date>
			<publisher>Academy of Management Annals</publisher>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="627" to="660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Algorithmic content moderation: Technical and political challenges in the automation of platform governance</title>
		<author>
			<persName><forename type="first">R</forename><surname>Gorwa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Binns</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Katzenbach</surname></persName>
		</author>
		<idno type="DOI">10.1177/2053951719897945</idno>
		<ptr target="https://doi.org/10.1177/2053951719897945" />
	</analytic>
	<monogr>
		<title level="j">Big Data &amp; Society</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">2053951719897945</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The ethics of AI ethics: An evaluation of guidelines</title>
		<author>
			<persName><forename type="first">T</forename><surname>Hagendorff</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11023-020-09517-8</idno>
		<ptr target="https://doi.org/10.1007/s11023-020-09517-8" />
	</analytic>
	<monogr>
		<title level="j">Minds and Machines</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="99" to="120" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Hancock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Billings</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">E</forename><surname>Schaefer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Visser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Parasuraman</surname></persName>
		</author>
		<idno type="DOI">10.1177/0018720811417254</idno>
		<ptr target="https://doi.org/10.1177/0018720811417254" />
	</analytic>
	<monogr>
		<title level="m">A Meta-Analysis of Factors Affecting Trust in Human-Robot Interaction</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="page" from="517" to="527" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Trust in automation: Integrating empirical evidence on factors that influence trust</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Hoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bashir</surname></persName>
		</author>
		<idno type="DOI">10.1177/0018720814547570</idno>
		<ptr target="https://doi.org/10.1177/0018720814547570" />
	</analytic>
	<monogr>
		<title level="j">Human Factors</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="407" to="434" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The global landscape of AI ethics guidelines</title>
		<author>
			<persName><forename type="first">A</forename><surname>Jobin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ienca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Vayena</surname></persName>
		</author>
		<idno type="DOI">10.1038/s42256-019-0088-2</idno>
		<ptr target="https://doi.org/10.1038/s42256-019-0088-2" />
	</analytic>
	<monogr>
		<title level="j">Nature Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="389" to="399" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">An empirical study on consumer first purchase intention in online shopping: Integrating initial trust and TAM</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Kim</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10660-012-9089-5</idno>
		<ptr target="https://doi.org/10.1007/s10660-012-9089-5" />
	</analytic>
	<monogr>
		<title level="j">Electronic Commerce Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="125" to="150" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Does a digital assistant need a body? The influence of visual embodiment and social behavior on the perception of intelligent virtual agents in AR</title>
		<author>
			<persName><forename type="first">K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Boelling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Haesler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bailenson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Bruder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">F</forename><surname>Welch</surname></persName>
		</author>
		<idno type="DOI">10.1109/ISMAR.2018.00039</idno>
		<ptr target="https://doi.org/10.1109/ISMAR.2018.00039" />
	</analytic>
	<monogr>
		<title level="m">IEEE International Symposium on Mixed and Augmented Reality (ISMAR)</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page" from="105" to="114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Defining AI in Policy versus Practice</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M</forename><surname>Krafft</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Katell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Bugingo</surname></persName>
		</author>
		<idno type="DOI">10.1145/3375627.3375835</idno>
		<ptr target="https://doi.org/10.1145/3375627.3375835" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society</title>
		<meeting>the AAAI/ACM Conference on AI, Ethics, and Society</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="72" to="78" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Technology, humanness, and trust: Rethinking trust in technology</title>
		<author>
			<persName><forename type="first">N</forename><surname>Lankton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Mcknight</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tripp</surname></persName>
		</author>
		<idno type="DOI">10.17705/1jais.00411</idno>
		<ptr target="https://doi.org/10.17705/1jais.00411" />
	</analytic>
	<monogr>
		<title level="j">Journal of the Association for Information Systems</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="880" to="918" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Trust in automation: Designing for appropriate reliance</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>See</surname></persName>
		</author>
		<idno type="DOI">10.1518/hfes.46.1.50_30392</idno>
		<ptr target="https://doi.org/10.1518/hfes.46.1.50_30392" />
	</analytic>
	<monogr>
		<title level="j">Human Factors</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="50" to="80" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Understanding perception of algorithmic decisions: Fairness, trust, and emotion in response to algorithmic management</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Lee</surname></persName>
		</author>
		<idno type="DOI">10.1177/2053951718756684</idno>
		<ptr target="https://doi.org/10.1177/2053951718756684" />
	</analytic>
	<monogr>
		<title level="j">Big Data &amp; Society</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">2053951718756684</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Who is included in human perceptions of AI?: Trust and perceived fairness around healthcare AI and cultural mistrust</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Rich</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A review of trust in artificial intelligence: Challenges, vulnerabilities and future directions</title>
		<author>
			<persName><forename type="first">S</forename><surname>Lockey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Gillespie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Holm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">A</forename><surname>Someh</surname></persName>
		</author>
		<idno type="DOI">10.24251/HICSS.2021.664</idno>
		<ptr target="https://doi.org/10.24251/HICSS.2021.664" />
	</analytic>
	<monogr>
		<title level="m">Hawaii International Conference on System Sciences</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Effects of information source, pedigree, and reliability on operator interaction with decision support systems</title>
		<author>
			<persName><forename type="first">P</forename><surname>Madhavan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Wiegmann</surname></persName>
		</author>
		<idno type="DOI">10.1518/001872007X230154</idno>
		<ptr target="https://doi.org/10.1518/001872007X230154" />
	</analytic>
	<monogr>
		<title level="j">Human Factors: The Journal of the Human Factors and Ergonomics Society</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="773" to="785" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Technology acceptance model: A literature review from 1986 to 2013</title>
		<author>
			<persName><forename type="first">N</forename><surname>Maranguniƒá</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Graniƒá</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10209-014-0348-1</idno>
		<ptr target="https://doi.org/10.1007/s10209-014-0348-1" />
	</analytic>
	<monogr>
		<title level="j">Universal Access in the Information Society</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="81" to="95" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">An integrative model of organizational trust: Past, present, and future</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">D</forename><surname>Schoorman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Academy of Management Review</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="709" to="734" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Trust in a specific technology: An investigation of its components and measures</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Mcknight</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Carter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Thatcher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">F</forename><surname>Clay</surname></persName>
		</author>
		<idno type="DOI">10.1145/1985347.1985353</idno>
		<ptr target="https://doi.org/10.1145/1985347.1985353" />
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Management Information Systems</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="25" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Hey Alexa ‚Ä¶ examine the variables influencing the use of artificial intelligent in-home voice assistants</title>
		<author>
			<persName><forename type="first">G</forename><surname>Mclean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Osei-Frimpong</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.chb.2019.05.009</idno>
		<ptr target="https://doi.org/10.1016/j.chb.2019.05.009" />
	</analytic>
	<monogr>
		<title level="j">Computers in Human Behavior</title>
		<imprint>
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="page" from="28" to="37" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Artificial intelligence in society</title>
		<idno type="DOI">10.1787/eedfee77-en</idno>
		<ptr target="https://doi.org/10.1787/eedfee77-en" />
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>OECD Publishing</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">The media equation: How people treat computers, television, and new media like real people and places (1. paperback ed)</title>
		<author>
			<persName><forename type="first">B</forename><surname>Reeves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">I</forename><surname>Nass</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996">1996</date>
			<publisher>CSLI Publ</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Systematic Review: Trust-Building Factors and Implications for Conversational Agent Design</title>
		<author>
			<persName><forename type="first">M</forename><surname>Rheu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huh-Yoo</surname></persName>
		</author>
		<idno type="DOI">10.1080/10447318.2020.1807710</idno>
		<ptr target="https://doi.org/10.1080/10447318.2020.1807710" />
	</analytic>
	<monogr>
		<title level="j">International Journal of Human-Computer Interaction</title>
		<imprint>
			<biblScope unit="page" from="1" to="16" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">lavaan: An R Package for Structural Equation Modeling</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Rosseel</surname></persName>
		</author>
		<idno type="DOI">10.18637/jss.v048.i02</idno>
		<ptr target="https://doi.org/10.18637/jss.v048.i02" />
	</analytic>
	<monogr>
		<title level="j">Journal of Statistical Software</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Not so different after all: A cross-discipline view of trust</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Rousseau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">B</forename><surname>Sitkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Burt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Camerer</surname></persName>
		</author>
		<idno type="DOI">10.5465/amr.1998.926617</idno>
		<ptr target="https://doi.org/10.5465/amr.1998.926617" />
	</analytic>
	<monogr>
		<title level="j">Academy of Management Review</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="393" to="404" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Norvig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Edwards</surname></persName>
		</author>
		<title level="m">Artificial intelligence: A modern approach</title>
		<imprint>
			<publisher>Pearson</publisher>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note>Third edition, Global edition</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Transparency and trust in artificial intelligence systems</title>
		<author>
			<persName><forename type="first">P</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Biessmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Teubner</surname></persName>
		</author>
		<idno type="DOI">10.1080/12460125.2020.1819094</idno>
		<ptr target="https://doi.org/10.1080/12460125.2020.1819094" />
	</analytic>
	<monogr>
		<title level="j">Journal of Decision Systems</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="260" to="278" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">How do users interact with algorithm recommender systems? The interaction of users, algorithms, and performance</title>
		<author>
			<persName><forename type="first">D</forename><surname>Shin</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.chb.2020.106344</idno>
		<ptr target="https://doi.org/10.1016/j.chb.2020.106344" />
	</analytic>
	<monogr>
		<title level="j">Computers in Human Behavior</title>
		<imprint>
			<biblScope unit="volume">109</biblScope>
			<biblScope unit="page">106344</biblScope>
			<date type="published" when="2020">2020a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">User perceptions of algorithmic decisions in the personalized AI system:Perceptual evaluation of fairness, accountability, transparency, and explainability</title>
		<author>
			<persName><forename type="first">D</forename><surname>Shin</surname></persName>
		</author>
		<idno type="DOI">10.1080/08838151.2020.1843357</idno>
		<ptr target="https://doi.org/10.1080/08838151.2020.1843357" />
	</analytic>
	<monogr>
		<title level="j">Journal of Broadcasting &amp; Electronic Media</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="541" to="565" />
			<date type="published" when="2020">2020b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Expanding the Role of Trust in the Experience of Algorithmic Journalism: User Sensemaking of Algorithmic Heuristics in Korean Users</title>
		<author>
			<persName><forename type="first">D</forename><surname>Shin</surname></persName>
		</author>
		<idno type="DOI">10.1080/17512786.2020.1841018</idno>
		<ptr target="https://doi.org/10.1080/17512786.2020.1841018" />
	</analytic>
	<monogr>
		<title level="j">Journalism Practice</title>
		<imprint>
			<biblScope unit="page" from="1" to="24" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Embodying algorithms, enactive artificial intelligence and the extended cognition: You can see as much as you know about algorithm</title>
		<author>
			<persName><forename type="first">D</forename><surname>Shin</surname></persName>
		</author>
		<idno type="DOI">10.1177/0165551520985495</idno>
		<ptr target="https://doi.org/10.1177/0165551520985495" />
	</analytic>
	<monogr>
		<title level="j">Journal of Information Science</title>
		<imprint>
			<biblScope unit="page">0165551520985495</biblScope>
			<date type="published" when="2021">2021a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">The effects of explainability and causability on perception, trust, and acceptance: Implications for explainable AI</title>
		<author>
			<persName><forename type="first">D</forename><surname>Shin</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.ijhcs.2020.102551</idno>
		<ptr target="https://doi.org/10.1016/j.ijhcs.2020.102551" />
	</analytic>
	<monogr>
		<title level="j">International Journal of Human-Computer Studies</title>
		<imprint>
			<biblScope unit="volume">146</biblScope>
			<biblScope unit="page">102551</biblScope>
			<date type="published" when="2021">2021b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">How do people judge the credibility of algorithmic sources?</title>
		<author>
			<persName><forename type="first">D</forename><surname>Shin</surname></persName>
		</author>
		<idno type="DOI">10.1007/s00146-021-01158-4</idno>
		<ptr target="https://doi.org/10.1007/s00146-021-01158-4" />
	</analytic>
	<monogr>
		<title level="j">AI &amp; SOCIETY</title>
		<imprint>
			<date type="published" when="2021">2021c</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">The perception of humanness in conversational journalism: An algorithmic information-processing perspective</title>
		<author>
			<persName><forename type="first">D</forename><surname>Shin</surname></persName>
		</author>
		<idno type="DOI">10.1177/1461444821993801</idno>
		<ptr target="https://doi.org/10.1177/1461444821993801" />
		<imprint>
			<date type="published" when="2021">2021d</date>
			<publisher>New Media &amp; Society</publisher>
			<biblScope unit="page">1461444821993801</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Why does explainability matter in news analytic systems? Proposing explainable analytic journalism</title>
		<author>
			<persName><forename type="first">D</forename><surname>Shin</surname></persName>
		</author>
		<idno type="DOI">10.1080/1461670X.2021.1916984</idno>
		<ptr target="https://doi.org/10.1080/1461670X.2021.1916984" />
	</analytic>
	<monogr>
		<title level="j">Journalism Studies</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1047" to="1065" />
			<date type="published" when="2021">2021e</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Role of fairness, accountability, and transparency in algorithmic affordance</title>
		<author>
			<persName><forename type="first">D</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">J</forename><surname>Park</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.chb.2019.04.019</idno>
		<ptr target="https://doi.org/10.1016/j.chb.2019.04.019" />
	</analytic>
	<monogr>
		<title level="j">Computers in Human Behavior</title>
		<imprint>
			<biblScope unit="volume">98</biblScope>
			<biblScope unit="page" from="277" to="284" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Why different trust relationships matter for information systems users</title>
		<author>
			<persName><forename type="first">M</forename><surname>S√∂llner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Leimeister</surname></persName>
		</author>
		<idno type="DOI">10.1057/ejis.2015.17</idno>
		<ptr target="https://doi.org/10.1057/ejis.2015.17" />
	</analytic>
	<monogr>
		<title level="j">European Journal of Information Systems</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="274" to="287" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Cheery companions or serious assistants? Role and demeanor congruity as predictors of robot attraction and use intentions among senior citizens</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Sundar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">H</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Waddell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J</forename><surname>Kim</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.ijhcs.2016.08.006</idno>
		<ptr target="https://doi.org/10.1016/j.ijhcs.2016.08.006" />
	</analytic>
	<monogr>
		<title level="j">International Journal of Human-Computer Studies</title>
		<imprint>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="88" to="97" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Voice Assistants and Smart Speakers in Everyday Life and in Education</title>
		<author>
			<persName><forename type="first">G</forename><surname>Terzopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Satratzemi</surname></persName>
		</author>
		<idno type="DOI">10.15388/infedu.2020.21</idno>
		<ptr target="https://doi.org/10.15388/infedu.2020.21" />
	</analytic>
	<monogr>
		<title level="j">Informatics in Education</title>
		<imprint>
			<biblScope unit="page" from="473" to="490" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">S</forename><surname>Thiebes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sunyaev</surname></persName>
		</author>
		<idno type="DOI">10.1007/s12525-020-00441-4</idno>
		<ptr target="https://doi.org/10.1007/s12525-020-00441-4" />
	</analytic>
	<monogr>
		<title level="j">Trustworthy artificial intelligence. Electronic Markets</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Computing machinery and intelligence</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Turing</surname></persName>
		</author>
		<idno type="DOI">10.1093/mind/LIX.236.433</idno>
		<ptr target="https://doi.org/10.1093/mind/LIX.236.433" />
	</analytic>
	<monogr>
		<title level="j">Mind</title>
		<imprint>
			<biblScope unit="issue">236</biblScope>
			<biblScope unit="page" from="433" to="460" />
			<date type="published" when="1950">1950</date>
		</imprint>
	</monogr>
	<note>LIX</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">A theoretical extension of the Technology Acceptance Model: Four longitudinal field studies</title>
		<author>
			<persName><forename type="first">V</forename><surname>Venkatesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">D</forename><surname>Davis</surname></persName>
		</author>
		<idno type="DOI">10.1287/mnsc.46.2.186.11926</idno>
		<ptr target="https://doi.org/10.1287/mnsc.46.2.186.11926" />
	</analytic>
	<monogr>
		<title level="j">Management Science</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="186" to="204" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">The rhetoric and reality of anthropomorphism in artificial intelligence</title>
		<author>
			<persName><forename type="first">D</forename><surname>Watson</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11023-019-09506-6</idno>
		<ptr target="https://doi.org/10.1007/s11023-019-09506-6" />
	</analytic>
	<monogr>
		<title level="j">Minds and Machines</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="417" to="440" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">The mind in the machine: Anthropomorphism increases trust in an autonomous vehicle</title>
		<author>
			<persName><forename type="first">A</forename><surname>Waytz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Heafner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Epley</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.jesp.2014.01.005</idno>
		<ptr target="https://doi.org/10.1016/j.jesp.2014.01.005" />
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Social Psychology</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page" from="113" to="117" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">An extension of Trust and TAM model with TPB in the initial adoption of on-line tax: An empirical study</title>
		<author>
			<persName><forename type="first">I.-L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-L</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.ijhcs.2005.03.003</idno>
		<ptr target="https://doi.org/10.1016/j.ijhcs.2005.03.003" />
	</analytic>
	<monogr>
		<title level="j">International Journal of Human-Computer Studies</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="784" to="808" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">A meta-analysis of the impact of trust on technology acceptance model: Investigation of moderating influence of subject and context type</title>
		<author>
			<persName><forename type="first">K</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.ijinfomgt.2011.03.004</idno>
		<ptr target="https://doi.org/10.1016/j.ijinfomgt.2011.03.004" />
	</analytic>
	<monogr>
		<title level="j">International Journal of Information Management</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="572" to="581" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
