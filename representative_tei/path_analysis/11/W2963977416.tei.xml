<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Deep Neural Networks for Vehicle Re-ID with Visual-spatio-temporal Path Proposals</title>
				<funder ref="#_gFsAw2T #_yNnzwqC">
					<orgName type="full">Hong Kong Innovation and Technology Support Programme</orgName>
				</funder>
				<funder ref="#_cp3z5QG">
					<orgName type="full">China Postdoctoral Science Foundation</orgName>
				</funder>
				<funder>
					<orgName type="full">SenseTime Group Limited</orgName>
				</funder>
				<funder ref="#_wtG7yyk #_DTR7gRA #_TGdwV68">
					<orgName type="full">General Research Fund</orgName>
				</funder>
				<funder ref="#_Nv9usuX #_XKPy6rM #_tP8SSWm">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2017-08-13">13 Aug 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yantao</forename><surname>Shen</surname></persName>
							<email>ytshen@ee.cuhk.edu.hk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electronic Engineering</orgName>
								<orgName type="institution">The Chinese University of Hong</orgName>
								<address>
									<country>Kong</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tong</forename><surname>Xiao</surname></persName>
							<email>xiaotong@ee.cuhk.edu.hk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electronic Engineering</orgName>
								<orgName type="institution">The Chinese University of Hong</orgName>
								<address>
									<country>Kong</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
							<email>hsli@ee.cuhk.edu.hk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electronic Engineering</orgName>
								<orgName type="institution">The Chinese University of Hong</orgName>
								<address>
									<country>Kong</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shuai</forename><surname>Yi</surname></persName>
							<email>yishuai@sensetime.com</email>
							<affiliation key="aff1">
								<orgName type="laboratory">SenseTime Group Limited</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
							<email>xgwang@ee.cuhk.edu.hk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electronic Engineering</orgName>
								<orgName type="institution">The Chinese University of Hong</orgName>
								<address>
									<country>Kong</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Deep Neural Networks for Vehicle Re-ID with Visual-spatio-temporal Path Proposals</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2017-08-13">13 Aug 2017</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:1708.03918v1[cs.CV]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-21T21:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Vehicle re-identification is an important problem and has many applications in video surveillance and intelligent transportation. It gains increasing attention because of the recent advances of person re-identification techniques. However, unlike person re-identification, the visual differences between pairs of vehicle images are usually subtle and even challenging for humans to distinguish. Incorporating additional spatio-temporal information is vital for solving the challenging re-identification task. Existing vehicle re-identification methods ignored or used oversimplified models for the spatio-temporal relations between vehicle images. In this paper, we propose a two-stage framework that incorporates complex spatio-temporal information for effectively regularizing the re-identification results. Given a pair of vehicle images with their spatiotemporal information, a candidate visual-spatio-temporal path is first generated by a chain MRF model with a deeply learned potential function, where each visual-spatiotemporal state corresponds to an actual vehicle image with its spatio-temporal information. A Siamese-CNN+Path-LSTM model takes the candidate path as well as the pairwise queries to generate their similarity score. Extensive experiments and analysis show the effectiveness of our proposed method and individual components.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Vehicle recognition is an active research field in computer vision, which includes applications such as vehicle classification <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b25">26]</ref>, vehicle detection <ref type="bibr" target="#b30">[31]</ref>, and vehicle segmentation <ref type="bibr" target="#b29">[30]</ref>. Vehicle re-identification (re-ID), which aims at determining whether two images are taken from the same vehicle, has recently drawn increasing attention from the research community <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b26">27]</ref>. It has important applications in video surveillance, public security, and intelligent transportation.</p><p>Derived from person re-identification algorithms[1, 3, 12], most existing vehicle re-identification approaches <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b26">27]</ref> rely only on appearance information. Such a problem setting is particularly challenging, since different cars could have very similar colors and shapes, especially for those belonging to the same manufacturer. Subtle cues for identification, such as license plates and special decorations, might be unavailable due to non-frontal camera viewpoints, low resolution, or poor illumination of the vehicle images. Therefore, it is not practical to use only appearance information for accurate vehicle re-identification.</p><p>To cope with such limitations, there are preliminary attempts on incorporating spatio-temporal information of the input images for more accurate vehicle re-identification. In <ref type="bibr" target="#b27">[28]</ref>, Liu et al. utilized the time and geo-location information for each vehicle image. A spatio-temporal affinity is calculated between every pair of images. However, it favors pairs of images that are close to each other in both spatial and temporal domains. Such a spatio-temporal regularization is obviously over-simplified. More importantly, vital spatio-temporal path information of the vehicles provided by the dataset is ignored. The necessity of such spatiotemporal path prior is illustrated in Figure <ref type="figure">1</ref>. If a vehicle is observed at both camera A and C, the same vehicle has to appear at camera B as well. Therefore, given a pair of vehicle images at location A and C, if an image with similar appearance is never observed at camera B at a proper time, their matching confidence should be very low.</p><p>In this paper, we propose to utilize such spatio-temporal path information to solve the problem. The main contribution of our method is two-fold. <ref type="bibr" target="#b0">(1)</ref> We propose a two-stage framework for vehicle re-identification. It first proposes a series of candidate visual-spatio-temporal paths with the query images as the starting and ending states. In the second stage, a Siamese-CNN+Path-LSTM network is utilized to determine whether each query pair has the same vehicle identity with the spatio-temporal regularization from the candidate path. In this way, all the visual-spatio-temporal states along the candidate path are effectively incorporated to estimate the validness confidence of the path. Such information is for the first time explored for vehicle reidentification. (2) To effectively generate visual-spatiotemporal path proposals, we model the paths by chain MRF, which could be optimized efficiently by the max-sum algorithm. A deep neural network is proposed to learn the pairwise visual-spatio-temporal potential function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>Vehicle re-identification. Because of the quick advances of person re-identification approaches, vehicle reidentification started to gain attentions in recent years. Feris et al. <ref type="bibr" target="#b9">[10]</ref> proposed an approach on attribute-based search of vehicles in surveillance scenes. The vehicles are classified by different attributes such as car types and colors. The retrieval is then conducted by searching vehicles with similar attributes in the database. Dominik et al. <ref type="bibr" target="#b44">[45]</ref> utilized 3D bounding boxes for rectifying car images and then concatenated color histogram features of pairs of vehicle images. A binary linear SVM is trained to verify whether the pair of images have the same identity or not. Liu et al. <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28]</ref> proposed a vehicle re-identification dataset VeRi-776 with a large number of cars captured by 20 cameras in a road network. Vehicle appearances, spatio-temporal information and license plates are independently used to learn the similarity scores between pairs of images. For the appearance cues, a deep neural network is used to estimate the visual similarities between vehicle images. Other vehicle recognition algorithms mainly focused on fine-grained car model classification <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b41">42]</ref> instead of identifying the vehicles with the same or different identities.</p><p>Deep neural networks. In recent years, convolutional deep neuron networks have shown their effectiveness in large-scale image classification <ref type="bibr" target="#b21">[22]</ref>, object detection <ref type="bibr" target="#b22">[23]</ref> and visual relationship detection <ref type="bibr" target="#b24">[25]</ref>. For sequential data, the family of Recurrent Neural Networks including Long-Short Term Memory Network(LSTM) <ref type="bibr" target="#b13">[14]</ref> and Gated Recurrent Neural Network <ref type="bibr" target="#b3">[4]</ref> have achieved great success for tasks including image captioning <ref type="bibr" target="#b17">[18]</ref>, speech recognition <ref type="bibr" target="#b10">[11]</ref>, visual question answering <ref type="bibr" target="#b1">[2]</ref>, person search <ref type="bibr" target="#b23">[24]</ref>, immediacy prediction <ref type="bibr" target="#b4">[5]</ref>, video classification <ref type="bibr" target="#b43">[44]</ref> and video detection <ref type="bibr" target="#b16">[17]</ref>. These works show that RNN is able to capture the temporal information in the sequential data and learn effective temporal feature representations, which inspires us to use LSTM network for learning feature representations for classifying visual-spatio-temporal paths.</p><p>Person re-identification. Person re-identification is a challenging problem that draws increasing attention in recent years <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b36">37]</ref>. State-of-the-art person reidentification methods adopted deep learning techniques. Ahmed et al. <ref type="bibr" target="#b0">[1]</ref> designed a pairwise verification CNN model for person re-identification with a pair of cropped pedestrian images as input and employed a binary verification loss function for training. Xiao et al. <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b40">41]</ref> trained CNN with classification loss to learn the deep feature of person. Ding et al. <ref type="bibr" target="#b7">[8]</ref> and Cheng et al. <ref type="bibr" target="#b2">[3]</ref> trained CNN with triplet samples and minimized feature distances between the same person and maximize the distances between different people. Besides the feature learning, a large number of metric learning methods for person re-identification were also proposed <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b42">43]</ref>. For person re-identification in multi-camera system, Hamdoun et al. <ref type="bibr" target="#b11">[12]</ref> proposed an approach that matches signatures based on interest-points descriptors collected on short video sequences for person re-identification scheme in multi-camera surveillance systems.</p><p>Spatio-temporal relations. Spatio-temporal relations are widely exploited for objects association in multi-camera systems <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b32">33]</ref>, Ellis et al. <ref type="bibr" target="#b8">[9]</ref> presented a method to learn both the topological and temporal transitions from trajectory data which are obtained independently from single view target tracking in a multi-camera network. Neumann et al. <ref type="bibr" target="#b32">[33]</ref> presented an approach that combines the structure and motion estimation in a unified framework to recover an accurate 3D spatio-temporal description of an object. Loy et al. <ref type="bibr" target="#b28">[29]</ref> proposed an approach for multi-camera activity correlation analysis which estimates the spatial and temporal topology of the camera network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Approach</head><p>For vehicle re-identification with computer vision, cameras in a road network are needed to capture images of passing-by vehicles. Due to factors including inappropriate camera viewpoints, low resolution of the images and motion blurs of vehicles, car plate information might not always be available for solving the task. Given a pair of vehicle images with their spatio-temporal information, the similarity score between the two vehicle images is needed to determine whether the two images have the same identity. Each image is associated with three types of information, i.e., visual appearance, the timestamp, and the geo- locatoin of the camera. We call such information the visualspatio-temporal state of the vehicle images. Our proposed approach takes two visual-spatio-temporal states as input and outputs their similarity score with a two-stage framework, which is illustrated in Figure <ref type="figure" target="#fig_1">2</ref>. In stage 1, instead of just considering the simple pairwise relations between two queries, our proposed approach first generates a candidate visual-spatio-temporal path with the two queries as starting and ending states. In stage 2, the candidate visual-spatiotemporal path acts as regularization priors and a Siamese-CNN+path-LSTM network is utilized to determine whether the queries have the same identity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Visual-spatio-temporal path proposals</head><p>Given a pair of queries, existing vehicle or person reidentification algorithms mainly considers the pairwise relations between the queries, e.g., the compatibility of the visual appearances and spatio-temporal states of the two queries. As illustrated in Figure <ref type="figure">1</ref>, such pairwise relations are usually over-simplified and cannot effectively distinguish difficult cases in practice. The visual-spatio-temporal path information could provide vital information for achieving more robust re-identification. The problem of identifying candidate visual-spatio-temporal paths is modeled as chain Markov Random Fields (MRF). Given a pair of queries, candidate spatial paths are first identified with the their geo-locations as starting and ending locations. The visual-spatio-temporal states along the spatial paths are then optimized with the deeply learned pairwise potential function to generate candidate visual-spatio-temporal paths.</p><p>To obtain candidate spatial paths for a pair of starting and ending locations, all possible spatial paths that the same vehicle has passed by are collected from the training set. For a large road network, the multiple candidate spatial paths between every pair of locations could be pre-collected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Chain MRF model for visual-spatio-temporal path proposal</head><p>From the set of candidate spatial paths, our approach proposes one candidate visual-spatio-temporal path for regularizing the vehicle re-identification result. The problem of identifying visual and temporal states along the candidate spatial paths is modeled as optimizing chain MRF models.</p><p>Let N denote the number of cameras on a candidate spatial path, where each of the N cameras is associated with one of the random variables X = {X 1 , X 2 , • • • , X N } on a chain MRF model. For the i-th random variable (camera) X i , its domain is the set of all k visual-spatio-temporal states (all k images with their spatio-temporal information) at this camera, S i = {s i,1 , • • • , s i,k }, where s i,j = {I i,j , t i,j , l i } is a triplet of the jth visual image at the ith camera, its timestamp t i,j , and the camera location l i .</p><p>Let p and q represent the visual-spatio-temporal states of the two queries. Obtaining the optimal visual-spatiotemporal path based on a candidate spatial path can be achieved by maximizing the following distribution,</p><formula xml:id="formula_0">p(x|x 1 = p, x N = q) = 1 Z ψ(p, x 2 )ψ(x N -1 , q) N -2 i=2 ψ(x i , x i+1 ),<label>(1)</label></formula><p>where ψ(x i , x i+1 ) is the pairwise potential function of x i and x i+1 being of the same car. Ideally, if x i and x i+1 denote the states with the same vehicle identity, a proper potential function would have a large value, while small otherwise. The ψ function is learned as a deep neural network and is introduced in details in the next subsection. The above probability needs to be maximized with proper time constraints,</p><formula xml:id="formula_1">x * = arg max x p(x|x 1 = p, x N = q),<label>(2)</label></formula><formula xml:id="formula_2">subject to t i,k * i ≤ t i+1,k * i+1 ∀i ∈ {1, • • • , N -1}, (3)</formula><p>where k * i and k * i+1 represent the indices of the optimal visual-spatio-temporal states for x i and x i+1 , respectively. The above constraints state that the obtained visual-spatiotemporal path must be feasible in time, i.e., the timestamps of vehicle images must be keep increasing along the path.</p><p>The distribution can be efficiently optimized by the maxsum algorithm, which is equivalent to dynamic programming for chain models <ref type="bibr" target="#b5">[6]</ref>. The maximum of the probability can be written as,</p><formula xml:id="formula_3">max x p(x|x 1 = p, x N = q) (4) = 1 Z ψ(p, x 2 )ψ(x N -1 , q) max x2 • • • max x N -1 N -1 i=2 ψ(x i , x i+1 ) (5) = 1 Z max x2 ψ(p, x 2 )ψ(x 2 , x 3 ) • • • max x N -1 ψ(x N -1 , x q ) • • •<label>(6)</label></formula><p>After obtaining the optimal state for each random variable (camera) on the candidate spatial path, the candidate visual-spatio-temporal path is generated.</p><p>For a pair of queries, multiple candidate visual-spatialtemporal paths are obtained. we define an empirical averaged potential for the optimed solution of each candidate path,</p><formula xml:id="formula_4">S(x * ) = 1 N -1 ψ(p, 2) + N -2 i=2 ψ(x * i , x * i+1 ) + ψ(x * N -1 , q) ,<label>(7)</label></formula><p>where 1/(N -1) normalizes the overall confidence for candidate visual-spatio-temporal paths with different lengths.</p><p>Then we choose the visual-spatio-temporal proposal among the candidate paths.</p><p>For every pair of queries, even if they do not have the same identity, the proposed algorithm always tries to generate the most feasible path in terms of both visual and spatiotemporal compatibility between neighboring cameras as the path proposals. Some examples of the candidate visualspatio-temporal paths are shown in Figure <ref type="figure">3</ref>.</p><p>For efficiency, when generating visual-spatio-temporal paths for all state pairs in the dataset, our method utilizes a systematic way to avoid redundant computation. For instance, suppose cameras A and C have two possible paths A -B 1 -C and A -B 2 -C. When calculating path proposals between queries on cameras A and C, the sub-paths A -B 1 and A -B 2 are also computed during the computation process, and can be reused by other queries on A -B 1 and A -B 2 . The details of time complexity analysis will be introduced in Section 4.4.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Deep neural networks as pairwise potential functions</head><p>The pairwise potential function ψ(x i , x i+1 ) in (1) evaluates the compatibility between two visual-spatio-temporal states for neighboring random variables x i and x i+1 . We learn the potential function ψ as a two-branch deep neural network, whose structure is illustrated in Figure <ref type="figure">4</ref>. The visual branch and spatio-temporal branch estimate pairwise compatibility between pairwise visual and spatio-temporal states.</p><p>The visual branch (Siamese-Visual) is designed as a Siamese network with a shared ResNet-50 <ref type="bibr" target="#b12">[13]</ref>. It takes two images I i,k and I i+1,j at cameras x i and x i+1 as inputs and utilizes features from the "global pooling" layers to describe their visual appearances. The visual similarity between the two images is computed as the inner-product of the two "global pooling" features followed by a sigmoid function.</p><p>The other branch computes the spatio-temporal compatibility. Given the timestamps {t i,k , t i+1,j } and the two geolocations {l i , l i+1 } of at cameras i and i + 1, the input features of the branch are calculated as their time difference and spatial difference, where t i,k denotes the timestamp of the k-th state at camera i.</p><formula xml:id="formula_5">∆t i,i+1 (k, j) = t i+1,j -t i,k ,<label>(8)</label></formula><formula xml:id="formula_6">∆d i,i+1 = |l i+1 -l i |,<label>(9)</label></formula><p>The scalar spatio-temporal compatibility is obtained by feeding the concatenated features, [∆t i,i+1 (k, j), ∆d i,i+1 ] T , into a Multi-Layer Perception (MLP) with two fully-connected layers and a ReLU nonlinearity function after the first layer and a sigmoid function after the second layer.</p><p>The outputs of the two branches are concatenated and input into a 2 × 1 fully-connected layer with a sigmoid function to obtain the final compatibility between the two states, which takes all visual, spatial and temporal information into consideration.</p><p>For training the pairwise potential network, Siamese-CNN, we first pretrain the ResNet-50 network to classify vehicle identity with the classification cross-entropy loss function. All pairs of visual-spatio-temporal states at neighboring random variables are then collected for finetuning the whole network. If a pair has the same vehicle identities, they are treated as positive samples, while the pairs with different identities are treated as negative ones. The positive-to-negative sampling ratio is set to 1:3. The two-branch network is trained with a 0-1 cross-entropy loss function and stochastic gradient descent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Siamese-CNN+Path-LSTM for query pair classification</head><p>Our proposed candidate visual-spatio-temporal path proposal algorithm generates the most feasible path for each query pair, even if they do not have the same identity. One naive solution of ranking the similarities of query pairs would be directly treating their maximum probability (Eq. ( <ref type="formula" target="#formula_0">1</ref>)) or the empirical averaged potential (Eq. ( <ref type="formula" target="#formula_4">7</ref>)) as the final similarity scores for ranking. However, there are limitations when either of the strategies is adopted. For calculating the maximum probability in Eq. ( <ref type="formula" target="#formula_0">1</ref>), the partition function Z needs to be calculated, which is generally time-consuming. For the empirical averaged potential in Eq. ( <ref type="formula" target="#formula_4">7</ref>), it is biased and would favor longer paths. An exmaple is illustrated in Figure <ref type="figure">5</ref>. Given two pairs of negative queries with different path lengths, since the path proposal algorithm tries to generate most feasible paths for both pairs, there might be only one identity switch along each path. The empirical averaged confidence for the longer path would be higher because the low pairwise confidence would be diminished by a larger N .</p><p>Given a pair of queries, we utilize their candidate visualspatio-temporal path as priors to determine whether the query pair has the same identity or not with a Siamese-CNN+path-LSTM network. The network structure is illustrated in Figure <ref type="figure" target="#fig_4">6</ref>. where the Siamese-CNN has the same structure as the overall network in Section 3.1.2. It directly takes the query pair as input and estimates the similarity between the queries.</p><p>For the candidate visual-spatio-temporal path, a path-LSTM is adopted to judge whether the path is valid or not. The framework of path-LSTM network is shown in Figure <ref type="figure" target="#fig_4">6</ref>. Each step of the path-LSTM processes a step along the candidate path and the length of the LSTM is therefore N -1. At each step, the input features of the path-LSTM, y i , is the concatenation of visual difference ∆I * i,i+1 , spatial difference ∆d * i,i+1 , and temporal difference ∆t * i,i+1 between the visual-spatio-temporal states at cameras i and i + 1, followed by a fully-connected layer with 32 output neurons and a ReLU non-linearity function, i.e., y i = f ([∆I * i,i+1 , ∆d * i,i+1 , ∆t * i,i+1 ] T ), where f denotes the feature transformation by the fully-connected layer.</p><p>Given the visual-spatio-temporal states on the candidate path, the image I i,k * i and its associated time stamp t i,k * i is fixed at camera i, where k * i denotes the optimized index of the visual-spatio-temporal state. The visual difference ∆I * i,i+1 is calculated as absolute difference between the visual features of I i,k * i and I i+1,k * i+1 , which are obtained as the ResNet-50 global pooling features followed by a 32neuron fully-connected layer and a ReLU function,</p><formula xml:id="formula_7">∆I * i,i+1 = R I i,k * i -R I i+1,k * i+1 , (<label>10</label></formula><formula xml:id="formula_8">)</formula><p>where R denotes the feature transformation for the input images. The spatial difference ∆d i,i+1 is calculated as ∆d i,i+1 = l i+1 -l i , and the temporal difference is</p><formula xml:id="formula_9">∆t i,i+1 = t i+1,k * i+1 -t i,k * i .<label>(11)</label></formula><p>The LSTM consists of a memory cell c t and three control gates: input gate ig, output gate og and forget gate f g at each time step t. With the input feature y t , The LSTM updates the memory cell c t and hidden state h t with the following equations,</p><formula xml:id="formula_10">f g t = σ(W f • [h t-1 , y t ]) + b f ) (12) ig t = σ(W i • [h t-1 , y t ]) + b i ) (13) og t = σ(W o • [h t-1 , y t ]) + b o ) (14) ct = tanh(W c • [h t-1 , y t ] + b c ) (15) c t = f g t * c t-1 + ig t * ct (16) h t = og t * tanh(c t ) (<label>17</label></formula><formula xml:id="formula_11">)</formula><p>where * represents the element-wise multiplication, W and b are the parameters.</p><p>The number of hidden neurons of our Path-LSTM is set to 32. The hidden feature of the Path-LSTM at the last step is fed into a fully-connected layer to obtain the valid-path confidence score, which is added with the pairwise similarity score generated by the Siamese-CNN to represent the final similarity score. In this way, the Path-LSTM provides important regularization for estimating final matching similarity.</p><p>Both the Siamese-CNN and Path-LSTM are pretrained separately and then finetuned jointly. Their training samples are prepared similarly to those for learning the pairwise potential function in Section 3.1.2. However, the training samples here are no longer restricted to only visual-spatiotemporal states from neighboring cameras but from any camera pair in the entire camera network. The Path-LSTM is first pretrained with the Adam algorithm <ref type="bibr" target="#b19">[20]</ref>. The whole Siamese-CNN+Path-LSTM network is then finetuned in an end-to-end manner with stochastic gradient descent and 0-1 cross-entropy loss function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Dataset and evaluation metric</head><p>For evaluating the effectiveness of our vehicle reidentification framework, we conduct experiments on the VeRi-776 dataset <ref type="bibr" target="#b27">[28]</ref>, which is the only existing vehicle reidentification dataset providing spatial and temporal annotations. The VeRi-776 dataset contains over 50,000 images of 776 vehicles with identity annotations, image timestamps, camera geo-locations, license plates, car types and colors information. Each vehicle is captured by 2 to 18 cameras in an urban area of 1km 2 during a 24-hour time period. The dataset is split into a train set consisting of 37,781 images of 576 vehicles, and a test set of 11,579 images belonging to 200 vehicles. A subset of 1,678 query images in the test set are used as to retrieve corresponding images from all other test images.</p><p>The mean average precision (mAP), top-1 accuracy and top-10 accuracy are chosen as the evaluation metric. Given each query image in the test image subset for retrieving other test images, the average precision for each query q is calculated by</p><formula xml:id="formula_12">AP (q) = n k=1 P (k) × rel(k) N gt<label>(18)</label></formula><p>where P (k) denotes the precision at cut-off k, rel(k) is an indication function equaling 1 if the item at rank k is a matched vehicle image, zero otherwise, n is the number for retrieval, and N gt denotes the number of ground truth retrievals for the query. The mean average precision for all query images is then calculated by</p><formula xml:id="formula_13">mAP = Q q=1 AP (q) Q , (<label>19</label></formula><formula xml:id="formula_14">)</formula><p>where Q is the number of all queries (1,678 for the dataset). Following the experimental setup in Liu et al. <ref type="bibr" target="#b27">[28]</ref>, for each query image, only images of the same vehicles from other cameras would be taken into account for calculating the mAP, top-1 and top-5 accuracies. Since our proposed method generates one visual-spatiotemporal candidate path for each pair of query images, we can extend the evaluation metrics to the whole sequence of images. For every pair of query images that have the same vehicle identity, we obtain the vehicle's actual visualspatio-temporal path and compare it with our candidate path using Jaccard Similarity <ref type="bibr" target="#b14">[15]</ref>,</p><formula xml:id="formula_15">JS(P p , P g ) = P p ∩ P g P p ∪ P g ,<label>(20)</label></formula><p>where P p is the set of retrieved images on the proposal path between the query pair, and P g is the set of the groundtruth images between them. We further define the average Jaccard Similarity for all query images,</p><formula xml:id="formula_16">AJS(P p , P g ) = 1 Q Q q=1 P pq ∩ P gq P pq ∪ P gq .<label>(21)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Compared With Vehicle Re-ID methods</head><p>We compare our proposed approach with the state-ofthe-art methods <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28]</ref> and design several baselines on the VeRi-776 dataset to evaluate the effectiveness of our proposed method.</p><p>• Siamese-CNN+Path-LSTM denotes our final approach which takes pairs of query visual-spatio-temporal states for Siamese-CNN and proposed visual-spatiotemporal path for the Path-LSTM to generate the final similarity score. Note that our approach did not utilize the plate information in the images as that in <ref type="bibr" target="#b27">[28]</ref>.</p><p>• FACT and FACT+Plate-SNN+STR. Liu et al. <ref type="bibr" target="#b26">[27]</ref> proposed the FACT model that combines deeply learned visual feature from GoogleNet <ref type="bibr" target="#b35">[36]</ref>, BOW-CN and BOW-SIFT feature to measure only the visual similarity between pairs of query images. In <ref type="bibr" target="#b27">[28]</ref>, they further integrated visual appearance, plate and spatiotemporal information for vehicle re-identification.</p><p>For appearance similarity, the same FACT model is adopted. They utilized a Siamese Neural Network (Plate-SNN) to compare visual similarity between plate regions. The spatio-temporal similarity is computed as</p><formula xml:id="formula_17">ST R(i, j) = |T i -T j | T max × δ(C i -C j ) D max ,<label>(22)</label></formula><p>where T i and T j are the timestamps of two queries, δ(C i , C j ) is the space distance between two cameras, and T max and D max are the maximum time distance and space distance in the whole dataset.</p><p>• Siamese-Visual. This baseline generates the similarity between a query pair with only pairwise visual information by using only the visual branch of the Siamese-CNN in Section 3.2. No spatio-temporal information is used for obtaining the similarity score.</p><p>• Siamese-Visual+STR. Instead of learning spatiotemporal relations by deep neural networks, this baseline sums up the scores by the above Siamese-Visual and the spatial-temporal relation score (STR, Eq. ( <ref type="formula" target="#formula_17">22</ref>) proposoed in <ref type="bibr" target="#b27">[28]</ref>. The weight between the two terms are manually searched for the best performance.</p><p>• Siamese-CNN. The baseline is the same as the Siamese-CNN in Section 3.2. Compared with the above baseline, it uses both visual and spatio-temporal information of the two queries for determine their similarity score. Candidate visual-spatio-temporal paths are not used in this baseline.</p><p>• Chain MRF model. After obtaining the candidate visual-spatio-temporal path for a query pair by the chain MRF model in Section 3.1.1, we directly utilize the empirical average by Eq. ( <ref type="formula" target="#formula_4">7</ref>) as the pairwise similarity of the query pair.</p><p>• Path-LSTM only. The proposed Path-LSTM estimate the validness score of the proposed visual-spatiotemporal path. We test only use Path-LSTM result without combining it with the Siamese-CNN.</p><p>• Siamese-CNN-VGG16. This is the same as Siamese-CNN but only replaces ResNet50 with VGG16.</p><p>• Path-LSTM-VGG16. This is the same as Path-LSTM but only replaces Siamese-CNN with Siamese-CNN-VGG16.</p><p>• Siamese-CNN-VGG16+Path-LSTM-VGG16. This is as same as Siamese-CNN+Path-LSTM but only replaces ResNet50 with VGG16.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Experiment Results</head><p>The mAP, top-1 and top-5 accuracies of methods are listed in Tables <ref type="table">1</ref> and<ref type="table" target="#tab_1">2</ref>. Figure <ref type="figure" target="#fig_5">7</ref> shows the CMC curves of the compared methods. Example vehicle re-identification results by our approach are shown in Figure <ref type="figure" target="#fig_6">8</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>mAP (%) FACT <ref type="bibr" target="#b26">[27]</ref> 18.49 FACT+Plate-SNN+STR <ref type="bibr" target="#b27">[28]</ref> 27  Our proposed two-stage approach, Siamese-CNN+Path-LSTM, outperforms state-of-the-art methods <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28]</ref> and all compared baselines, which demonstrates the effectiveness of our overall framework and individual components. Compared with Siamese-CNN, which only takes pairwise visual and spatio-temporal information into account, our final approach has a gain of 4% in terms of mAP and top-1 accuracy. Such a performance increase shows that the Path-LSTM with visual-spatio-temporal path proposal does provide vital priors for robustly estimating the vehicle similarities. Compared with Path-LSTM only, which only calculates path-validness scores with the proposed visual-spatiotemporal path, our final approach also has a 4% increase in terms of mAP and top-1 accuracy. This is because to generate the candidate path, our proposed chain MRF model always tries to discover the most feasible visual-spatiotemporal path. The visual-spatio-temporal state changes along the paths might sometimes be subtle and difficult to be captured by only pairwise differences of states at neighboring cameras. The obvious state difference between the query pair can sometimes be more easily captured by the Siamese-CNN. Therefore, the Path-LSTM acts as a strong prior for regularizing the Siamese-CNN results and their combination shows the best retrieval performance.</p><p>Compared with the Chain MRF model, the Path-LSTM has a 10% mAP gain and an increase of 25% top-1 accuracy. Such results demonstrate that the empirical average is not a robust path-validness indicator of the candidate paths. Our trained Path-LSTM is able to capture more subtle state changes on the candidate path for estimating correct path-validness scores. Compared with Siamese-Visual, Siamese-CNN has significant gains on mAP (∼ 25%) and top-1 accuracy (∼ 40%). It demonstrates that, unlike person re-identification, the spatio-temporal information is vital for vehicle re-identification, where the visual differences between different vehicles might be subtle for vehicles with the same color. Compared with Siamese-Visual+STR, which adopts the spatio-temporal relation score in <ref type="bibr" target="#b27">[28]</ref>, our Siamese-CNN achieves more accurate retrieval performance. Our deep neural network is able to capture more complex spatio-temporal relations between query pairs. The Path-LSTM shows strong capability on regularizing the retrieval results with candidate visual-spatio-temporal paths. The effectiveness of Path-LSTM relies on the cor-rectness of the candidate paths. If the candidate path does correspond to the actual path, the Path-LSTM might have negative impact on the final similarity score. For all pairs of queries that have the same vehicle identities, we obtain their ground-truth visual-spatio-temporal paths and compare them with our proposed ones. The averaged Jaccard Similarity is calculated. Our proposed chain MRF model with deeply learned potential function achieves an AJS of 96.39%.</p><p>We also test replacing ResNet50 with VGG16 in our pipeline. Our proposed overall framework (Siamese-VGG16+ PathLSTM-VGG16) and individual components (Path-LSTM-VGG16) outperform our VGG16 baseline (Siamese-CNN-VGG16).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Time Complexity Analysis</head><p>In the worst case, all pairwise potentials need to be calculated, resulting a complexity of O(M K 2 ), where M is the number of edges (connecting neighboring cameras) in the camera network, and K is the average number of states at each camera. After that, the time complexity of dynamic programming for all path proposals is also O(M K 2 ), which utilizes the technique described in Section 3.1.1 to avoid redundant computation. Amortized over the total number of Q query pairs, each query pair has an averaged time complexity of O(M K 2 /Q). In practice, for testing, pairwise scores are calculated between 19.4 million query pairs (11579 galleries and 1678 queries). Because of our systematic way of avoiding redundant computation, each query pair only requires 0.016s on average.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>In this paper, we proposed a two-stage framework for vehicle re-identification with both visual and spatiotemporal information. Existing methods ignored or used limited spatio-temporal information for regularizing the reidentification results. Our proposed approach incorporates important visual-spatial-temporal path information for regularization. A chain MRF model with deeply learned pairwise potential function is adopted to generate visual-spatiotemporal path proposals. Such candidate path proposals are evaluated by a Siamese-CNN+Path-LSTM to obtain similarity scores between pairs of queries. The proposed approach outperforms state-of-the-arts methods on the VeRi-776 dataset. Extensive component analysis of our framework demonstrates the effectiveness of our overall framework and individual components.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>…</head><label></label><figDesc>(b) No similar vehicle observed at B Figure 1: Illustration of spatio-temporal path information as important prior information for vehicle re-identification. (a) For vehicles with the same ID at A and C, it has to be observed at B. (b) If a vehicle with similar appearance and proper time is not observed at B, vehicles at A and C are unlikely to be the same vehicle.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Illustration of the overall framework. Given a pair of vehicle images, the visual-spatio-temporal path proposal is generated by optimizing a chain MRF model with a deeply learned potential function. The path proposal is further validated by the Path-LSTM and regularizes the similarity score by Siamese-CNN to achieve robust re-identification performance.</figDesc><graphic coords="3,124.36,72.32,94.09,163.75" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :Figure 4 :</head><label>34</label><figDesc>Figure 3: An example visual-spatio-temporal path proposal on the VeRi dataset [28] by our chain MRF model.</figDesc><graphic coords="4,320.68,251.26,57.24,66.79" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Valid path. Empirical averaged potential: 0.916 Figure5: Examples of empirical averaged potential favoring longer paths. The invalid longer path in (a) has a higher averaged potential than the valid path in (b).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: The network structure of the Path-LSTM. It takes visual and spatio-temporal differences of neighboring states along the path proposal as inputs, and estimates the path validness score.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: The CMC curves of different methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Example vehicle re-identification results (top5) by our proposed approach. The true positive is in green box otherwise red. The three rows are results of Siamese-Visual, Siamese-CNN and Siamese-CNN+Path-LSTM.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Top-1 and top-5 accuracies by compared methods on the VeRi-776 dataset<ref type="bibr" target="#b27">[28]</ref>.</figDesc><table><row><cell>.77</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgments. This work is supported in part by <rs type="funder">SenseTime Group Limited</rs>, in part by the <rs type="funder">General Research Fund</rs> through the <rs type="grantName">Research Grants Council of Hong Kong</rs> under Grants <rs type="grantNumber">CUHK14213616</rs>, <rs type="grantNumber">CUHK14206114</rs>, <rs type="grantNumber">CUHK14205615</rs>, <rs type="grantNumber">CUHK419412</rs>, <rs type="grantNumber">CUHK14203015</rs>, <rs type="grantNumber">CUHK14239816</rs>, <rs type="grantNumber">CUHK14207814</rs>, in part by the <rs type="funder">Hong Kong Innovation and Technology Support Programme</rs> Grant <rs type="grantNumber">ITS/121/15FX</rs>, and in part by the <rs type="funder">China Postdoctoral Science Foundation</rs> under Grant <rs type="grantNumber">2014M552339</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_wtG7yyk">
					<idno type="grant-number">CUHK14213616</idno>
					<orgName type="grant-name">Research Grants Council of Hong Kong</orgName>
				</org>
				<org type="funding" xml:id="_DTR7gRA">
					<idno type="grant-number">CUHK14206114</idno>
				</org>
				<org type="funding" xml:id="_TGdwV68">
					<idno type="grant-number">CUHK14205615</idno>
				</org>
				<org type="funding" xml:id="_Nv9usuX">
					<idno type="grant-number">CUHK419412</idno>
				</org>
				<org type="funding" xml:id="_XKPy6rM">
					<idno type="grant-number">CUHK14203015</idno>
				</org>
				<org type="funding" xml:id="_tP8SSWm">
					<idno type="grant-number">CUHK14239816</idno>
				</org>
				<org type="funding" xml:id="_gFsAw2T">
					<idno type="grant-number">CUHK14207814</idno>
				</org>
				<org type="funding" xml:id="_yNnzwqC">
					<idno type="grant-number">ITS/121/15FX</idno>
				</org>
				<org type="funding" xml:id="_cp3z5QG">
					<idno type="grant-number">2014M552339</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">An improved deep learning architecture for person re-identification</title>
		<author>
			<persName><forename type="first">E</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">K</forename><surname>Marks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Person re-identification by multi-channel parts-based cnn with improved triplet loss function</title>
		<author>
			<persName><forename type="first">D</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1259</idno>
		<title level="m">On the properties of neural machine translation: Encoderdecoder approaches</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Multi-task recurrent neural network for immediacy prediction</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3352" to="3360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">H</forename><surname>Cormen</surname></persName>
		</author>
		<title level="m">Introduction to algorithms</title>
		<imprint>
			<publisher>MIT press</publisher>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A multi-camera vision system for fall detection and alarm generation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Cucchiara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Prati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Vezzani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Systems</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="334" to="345" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep feature learning with relative distance comparison for person re-identification</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2993" to="3003" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning a multi-camera topology</title>
		<author>
			<persName><forename type="first">T</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Makris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint IEEE Workshop on Visual Surveillance and Performance Evaluation of Tracking and Surveillance (VS-PETS)</title>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="165" to="171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Large-scale vehicle detection, indexing, and search in urban surveillance videos</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Siddiquie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Petterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Datta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pankanti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">2</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Towards end-to-end speech recognition with recurrent neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1764" to="1772" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Person re-identification in multi-camera system by signature based on interest point descriptors collected on short video sequences</title>
		<author>
			<persName><forename type="first">O</forename><surname>Hamdoun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Moutarde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Stanciulescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Steux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Second ACM/IEEE International Conference on</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
	<note>In Distributed Smart Cameras, 2008. ICDSC 2008</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Etude comparative de la distribution florale dans une portion des Alpes et du Jura</title>
		<author>
			<persName><forename type="first">P</forename><surname>Jaccard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Impr. Corbaz</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="1901">1901</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Modeling inter-camera space-time and appearance relationships for tracking across non-overlapping views</title>
		<author>
			<persName><forename type="first">O</forename><surname>Javed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Shafique</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Rasheed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">109</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="146" to="162" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Object detection in videos with tubelet proposal networks</title>
		<author>
			<persName><forename type="first">K</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep visual-semantic alignments for generating image descriptions</title>
		<author>
			<persName><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3128" to="3137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Bayesian multi-camera surveillance</title>
		<author>
			<persName><forename type="first">V</forename><surname>Kettnaker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zabih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="1999">1999. 1999</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="253" to="259" />
		</imprint>
	</monogr>
	<note>IEEE Computer Society Conference on</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Large scale metric learning from equivalence constraints</title>
		<author>
			<persName><forename type="first">M</forename><surname>Koestinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hirzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wohlhart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="2288" to="2295" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Zoom out-and-in network with recursive training for object proposal</title>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<idno>CoRR, abs/1702.05711</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Person search with natural language description</title>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Vip-cnn: Visual phrase guided convolutional neural network</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep relative distance learning: Tell the difference between similar vehicles</title>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Large-scale vehicle reidentification in urban surveillance videos</title>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Multimedia and Expo (ICME), 2016 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007">2016. 1, 2, 6, 7</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A deep learning-based approach to progressive vehicle re-identification for urban surveillance</title>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008">2016. 1, 2, 4, 6, 7, 8</date>
			<biblScope unit="page" from="869" to="884" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Multi-camera activity correlation analysis</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009">2009. 2009</date>
			<biblScope unit="page" from="1988" to="1995" />
		</imprint>
	</monogr>
	<note>CVPR 2009. IEEE Conference on</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Mahendran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.06790</idno>
		<title level="m">Car segmentation and pose estimation using 3d object models</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Vehicle tracking across nonoverlapping cameras using joint kinematic and appearance features</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">C</forename><surname>Matei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">S</forename><surname>Sawhney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Samarasekera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="3465" to="3472" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Metric learning to rank</title>
		<author>
			<persName><forename type="first">B</forename><surname>Mcfee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">R</forename><surname>Lanckriet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Machine Learning (ICML-10)</title>
		<meeting>the 27th International Conference on Machine Learning (ICML-10)</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="775" to="782" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Spatio-temporal stereo using multi-resolution subdivision surfaces</title>
		<author>
			<persName><forename type="first">J</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Aloimonos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="181" to="193" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning to rank in person re-identification with metric ensembles</title>
		<author>
			<persName><forename type="first">S</forename><surname>Paisitkriangkrai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1846" to="1855" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Boxcars: 3d boxes as cnn input for improved fine-grained vehicle recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sochor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Herout</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Havel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Shape and appearance context modeling</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Doretto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sebastian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rittscher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision, 2007. ICCV 2007. IEEE 11th International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Distance metric learning for large margin nearest neighbor classification</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">K</forename><surname>Saul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="207" to="244" />
			<date type="published" when="2009-02">Feb. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Model-based face reconstruction using sift flow registration and spherical harmonics</title>
		<author>
			<persName><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">N</forename><surname>Ngan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pattern Recognition (ICPR), 2016 23rd International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1774" to="1779" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Learning deep feature representations with domain guided dropout for person re-identification</title>
		<author>
			<persName><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1249" to="1258" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Joint detection and identification feature learning for person search</title>
		<author>
			<persName><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A large-scale car dataset for fine-grained categorization and verification</title>
		<author>
			<persName><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Deep metric learning for person re-identification</title>
		<author>
			<persName><forename type="first">D</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pattern Recognition (ICPR), 2014 22nd International Conference on</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="34" to="39" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Beyond short snippets: Deep networks for video classification</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yue-Hei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hausknecht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName><surname>Toderici</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="4694" to="4702" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Vehicle re-identification for automatic video traffic surveillance</title>
		<author>
			<persName><forename type="first">D</forename></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Herout</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
