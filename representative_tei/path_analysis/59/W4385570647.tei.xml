<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">AMR-based Network for Aspect-based Sentiment Analysis</title>
				<funder>
					<orgName type="full">Beijing Key Laboratory of Industrial Bigdata System and Application</orgName>
				</funder>
				<funder ref="#_GZbtskn">
					<orgName type="full">National Nature Science Foundation of China</orgName>
				</funder>
				<funder ref="#_xAcaB3g">
					<orgName type="full">National Key Research and Development Program of China</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Fukun</forename><surname>Ma</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xuming</forename><surname>Hu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Aiwei</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yawen</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shuang</forename><surname>Li</surname></persName>
						</author>
						<author role="corresp">
							<persName><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
							<email>psyu@uic.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Illinois</orgName>
								<address>
									<addrLine>Chicago 1 {mfk22 hxm19,liuaw20,yyw19</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lijie</forename><surname>Wen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">AMR-based Network for Aspect-based Sentiment Analysis</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-29T00:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Aspect-based sentiment analysis (ABSA) is a fine-grained sentiment classification task. Many recent works have used dependency trees to extract the relation between aspects and contexts and have achieved significant improvements. However, further improvement is limited due to the potential mismatch between the dependency tree as a syntactic structure and the sentiment classification as a semantic task. To alleviate this gap, we replace the syntactic dependency tree with the semantic structure named Abstract Meaning Representation (AMR) and propose a model called AMR-based Path Aggregation Relational Network (APARN) to take full advantage of semantic structures. In particular, we design the path aggregator and the relation-enhanced selfattention mechanism that complement each other. The path aggregator extracts semantic features from AMRs under the guidance of sentence information, while the relationenhanced self-attention mechanism in turn improves sentence features with refined semantic information. Experimental results on four public datasets demonstrate 1.13% average F1 improvement of APARN in ABSA when compared with state-of-the-art baselines. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recent years have witnessed growing popularity of the sentiment analysis tasks in natural language processing <ref type="bibr" target="#b25">(Li and Hovy, 2017;</ref><ref type="bibr" target="#b3">Birjali et al., 2021)</ref>. Aspect-based sentiment analysis (ABSA) is a finegrained sentiment analysis task to recognize the sentiment polarities of specific aspect terms in a given sentence <ref type="bibr" target="#b20">(Jiang et al., 2011;</ref><ref type="bibr" target="#b27">Li et al., 2018;</ref><ref type="bibr" target="#b36">Seoh et al., 2021;</ref><ref type="bibr">Zhang et al., 2022a)</ref>. For example, here is a restaurant review "All the money went into the interior decoration, none of it went to the chefs" and the sentiment polarity of two aspects Sentence:</p><p>We were amazed at how small the dish was.</p><p>Figure <ref type="figure">1</ref>: Comparison of the dependency tree and the AMR. The aspect is red and the opinion term is blue.</p><p>"interior decoration" and "chefs" are positive and negative, respectively. Thus, ABSA can precisely recognize the corresponding sentiment polarity for any aspect, different from allocating a general sentiment polarity to a sentence in sentence-level sentiment analysis.</p><p>The key challenge for ABSA is to capture the relation between an aspect and its context, especially opinion terms. In addition, sentences with multiple aspects and several opinion terms make the problem more complex. To this end, some previous studies <ref type="bibr" target="#b43">(Wang et al., 2016;</ref><ref type="bibr" target="#b8">Chen et al., 2017;</ref><ref type="bibr" target="#b13">Gu et al., 2018;</ref><ref type="bibr" target="#b12">Du et al., 2019;</ref><ref type="bibr" target="#b29">Liang et al., 2019;</ref><ref type="bibr" target="#b46">Xing et al., 2019)</ref> have devoted the main efforts to attention mechanisms. Despite their achievements in aspect-targeted representations and appealing results, these methods always suffers noise from the mismatching opinion terms or irrelevant contexts.</p><p>On the other hand, more recent studies <ref type="bibr">(Zhang et al., 2019a;</ref><ref type="bibr" target="#b39">Tang et al., 2020;</ref><ref type="bibr" target="#b26">Li et al., 2021;</ref><ref type="bibr" target="#b45">Xiao et al., 2021)</ref> propose models explicitly exploit dependency trees, the syntactic structure of a sentence, to help attention mechanisms more accurately identify the interaction between the aspect and the opinion expressions. These models usually employ graph neural networks over the syntactic dependencies and display significant effectiveness. However, existing ABSA models still indicate two potential limitations. First, there appears to be a gap between the syntactic dependency structure and the semantic sentiment analysis task. Considering the sentence in Figure <ref type="figure">1</ref>, "small" semantically modifies "dish" and expresses negative sentiment, but both "small" and "dish" are syntactically dependent on "was". The determinant of sentiment should be the meaning of the sentence rather than the way it is expressed. Second, the output of natural language parsers including dependency parsers always contains inaccuracies <ref type="bibr" target="#b42">(Wang et al., 2020)</ref>. Without further adjustment, raw results of parsers can cause errors and be unsuitable for ABSA task.</p><p>To solve aforementioned challenges, we propose a novel architecture called AMR-based Path Aggregation Relational Network (APARN). For the first challenge, we introduce Abstract Meaning Representations (AMRs), a powerful semantic structure. For the AMR example in Figure <ref type="figure">1</ref>, "small" and "dish" are directly connected, while function words such as "were" and "at" disappear, which makes it easier to establish the aspect-opinion connection and shows the advantage of AMRs in ABSA. For the second challenge, we construct the path aggregator and the relation-enhanced self-attention mechanism. The path aggregator integrates the information from AMRs and sentences to obtain optimized relational features. This procedure not only encourages consistency between semantic structures and basic sentences, but also achieves the global feature by broadcasting local information along the path in the graph. Relation-enhanced self-attention mechanism then adds these relational feature back into attention weights of word features. Thanks to these modules, APARN acquires to utilize sentences and AMRs jointly and achieves higher accuracy on sentiment classification.</p><p>To summarize, our main contributions are highlighted as follows:</p><p>• We introduce Abstract Meaning Representations into the ABSA task. As a semantic structure, the AMR is more suitable for sentiment analysis task. analytical experiments further verify the significance of our model and the AMR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Parsed Structures</head><p>We perform some experiments and discussions for the characteristics of AMR compared to parsing structures already used for the ABSA task and how these characteristics affect our APARN.</p><p>Human-defined Structures Dependency trees and AMRs are parsed based on human-defined syntactic and semantic rules, respectively. Each word in a sentence becomes a node of the dependency tree, but in the AMR, relational words like function words and auxiliary words are represented as edges, while concept words like nouns and verbs are refined into nodes in the graph. With AMR aligning, we can map concept words in sentences to nodes in the graph and establish relations between them, while relation words are isolated.</p><p>To estimate the impact of dependency trees and AMRs in the ABSA task, we calculate the average distance between aspect words and opinion words in different parsed structures on the Restaurant dataset, called aspect-opinion distance (AOD). We also calculate the average distance between aspect words and all context words called aspectcontext distance (ACD), and divide AOD by ACD as relative aspect-opinion distance (rAOD). The distance between aspect words and isolated words is treated as sentence length. According to the result shown in Table <ref type="table" target="#tab_0">1</ref>, both dependency trees and AMRs have similar AOD smaller than original sentences, which indicates their benefits to capture relations about aspects. Due to the elimination of isolated words, the rAOD of AMRs is much less than dependency trees, which means smaller scope and easier focus. About 2.13% of opinion words are wrongly isolated, making the AOD of AMR (all words) a little bigger. But this is acceptable considering the improvement of rAOD and partially repairable by information from original sentences.</p><p>The above analysis is for graph skeletons, and we also explore the impact of edge labels of two structures in the ABSA task. Figure <ref type="figure">2</ref> compares the distribution of edge labels in aspect-opinion paths with the distribution of all edge labels. These distributions are clearly different, both in dependency trees and AMRs, which implies that edge labels can also help the ABSA task, especially in AMRs.</p><p>Based on these characteristics, we design the outer product sum module for APARN to mix sentence information into the graph, and design the path aggregator to collect graph skeleton and edge label information in AMRs.</p><p>Data-driven Structures Some existing studies use structures produced by data-driven models in the ABSA task <ref type="bibr" target="#b7">(Chen et al., 2020;</ref><ref type="bibr" target="#b9">Dai et al., 2021;</ref><ref type="bibr" target="#b6">Chen et al., 2022)</ref> and exhibit different effects from human-defined structures. Therefore, we design a relation-enhanced self-attention mechanism for APARN to integrate the graph information obtained by the path aggregator with the information from the pre-trained model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proposed Model</head><p>The overall architecture of our proposed model APARN is illustrated in Figure <ref type="figure" target="#fig_1">3</ref>. It consists of 3 parts: AMR preprocessing, path aggregator and relation-enhanced self-attention mechanism. In the ABSA task, a sentence s = {w 1 , w 2 , ..., w n } and a specific aspect term a = {a 1 , a 2 , ..., a m } are given to determine the corresponding sentiment polarity class c a , where a is a sub-sequence of s and c a ∈ {P ositive, N eutral, N egative}.</p><p>Many existing works use syntactic dependency trees to establish explicit or implicit connections between aspects and contexts. However, we believe that the sentiment analysis task is essentially about the meanings of sentences, so semantic structures like AMRs are more favorable for this task.</p><p>In addition, AMRs are more concise than dependency trees, making it easier to extract valuable information in training but more difficult to preprocess before training. We have to conduct a series of steps including: AMR parsing, AMR aligning and AMR embedding. Preprocessed AMRs still have errors and unsuitable parts for the task, so we design the path aggregator and the relation-enhanced self-attention mechanism to perform joint representation refinement and flexible feature fusion on the AMR graph and the original sentence.</p><p>Next, we elaborate on the details of our proposed APARN, including AMR preprocessing and embedding, the path aggregator and the relationenhanced self-attention mechanism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">AMR Preprocessing and Embedding</head><p>Parsing As we determine to employ the semantic structure AMR as an alternative of the syntactic structure dependency tree to better perform the semantic task ABSA, the first step is parsing the AMR from the input sentence. We choose the offthe-shelf parser SPRING <ref type="bibr" target="#b2">(Bevilacqua et al., 2021)</ref> for high quality AMR outputs.</p><p>Aligning Next, we align the AMR by the aligner LEAMR <ref type="bibr" target="#b4">(Blodgett and Schneider, 2021)</ref>. Based on the alignments, we manage to rebuild AMR relations between words in the sentence and get the transformed AMR with words as nodes.</p><p>Embedding After aligning, we now have transformed AMRs, which can also be called sentences with AMR relations. Then we need to obtain their embeddings for later representation learning by the model. For words in the sentence, also as the nodes in the AMR, we utilize BERT as an encoder to get contextual embeddings H = {h 1 , h 2 , ..., h n } like lots of previous works. For the edges in the AMR, we represent the relations between nodes as an adjacency matrix R = {r ij | 1 ≤ i, j ≤ n}, where r ij is the embedding of the edge label between word w i and word w j . If there is no edge between w i and w j in the AMR, we assign a "none" embedding to r ij . Edge label embeddings are also obtained from the pre-trained model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Path Aggregator</head><p>Path aggregator receives the mix of AMR embeddings R ∈ R dr×n×n and sentence embeddings H ∈ R dw×n , where d r and d w denote the dimensions of relation and word embeddings, respectively. Path aggregator outputs the relational fea- </p><formula xml:id="formula_0">ture matrix R AGG = {r AGG ij ∈ R dr | 1 ≤ i, j ≤ n}.</formula><p>This process integrates and condenses information from two different sources, AMRs and sentences, making semantic knowledge more apparent but parsing errors less influential.</p><p>Outer Product Sum We first add the outer product of two independent linear transformation of sentence embeddings H to the original AMR embeddings R to obtain sequence-enhanced relation embeddings R S ∈ R dr×n×n . On the one hand, as the outer product of H is the representation of word relations from the sentence perspective, its combination with the AMR embeddings R could enlarge the information base of the model to improve the generalization, also cross validate important features to improve the reliability. On the other hand, AMR embeddings R is usually quite sparse. The outer product sum operation ensures the basic density of the feature matrix and facilitates the subsequent representation learning by avoiding the fuzziness and dilution of numerous background "none" relations to the precious effective relations.</p><p>Path Aggregation Next, we perform the path aggregation on</p><formula xml:id="formula_1">R S = {r S ij | 1 ≤ i, j ≤ n} to calculate R AGG = {r AGG ij | 1 ≤ i, j ≤ n} as: r ′ S ij = LayerNorm(r S ij ),<label>(1)</label></formula><formula xml:id="formula_2">g in ij , g out ij = sigmoid(Linear(r ′ S ij )),<label>(2)</label></formula><formula xml:id="formula_3">a ij , b ij = g in ij ⊙ Linear(r ′ S ij ),<label>(3)</label></formula><formula xml:id="formula_4">r out ij = Linear(LayerNorm( k a ik ⊙ b kj )), (4) r AGG ij = g out ij ⊙ r out ij .</formula><p>(5)</p><p>The path aggregation has distinctive effect on both local and global dissemination of features. From the local view, the path aggregation covers all the 2-hop paths, so that it is very sensitive to neighborhood features, including the features around the aspect term which are really important for the ABSA task. From the global view, information in any long path can be summarized into the representation between the start and the end by several two-in-one operations in enough times of path aggregations. In other words, path aggregations make the features in matrix more inclusive and finally attain global features. In practice, because the ABSA task focuses more on the neighboring information and the BERT encoder with attention mechanisms has made the feature comprehensive enough, a single path aggregation can achieve quite good results.</p><p>Additionally, we also introduce a gating mechanism in the path aggregation to alleviate the disturbance of noise from insignificant relations. Finally, the output of path aggregation R AGG is transformed into the relational attention weight matrix</p><formula xml:id="formula_5">A AGG = {a AGG ij | 1 ≤ i, j ≤ n}</formula><p>by a linear transformation for subsequent calculation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Relation-Enhanced Self-Attention</head><p>The classic self-attention <ref type="bibr" target="#b41">(Vaswani et al., 2017)</ref> computes the attention weight by this formula:</p><formula xml:id="formula_6">A = sof tmax QW Q × (KW K ) T √ d ,<label>(6)</label></formula><p>where Q and K are input vectors with d dimensions, while W Q and W K are learnable weights with the same size of R d×d .</p><p>In our relation-enhanced self-attention, we added A AGG , the relational attention weight matrix from AMR into the original attention weight, which can be formulated as:</p><formula xml:id="formula_7">A R =sof tmax HW Q ×(HW K ) T √ d w +A AGG ,<label>(7)</label></formula><p>where input vectors W and Q are both replaced by the BERT embeddings H with d w dimensions. With A AGG , attention outputs are further guided by the semantic information from AMRs, which improves the efficient attention to semantic keywords. In addition, similar to path aggregator, we also introduced the gating mechanism into the relationenhanced self-attention as follows:</p><formula xml:id="formula_8">G = sigmoid(HW G ),<label>(8)</label></formula><formula xml:id="formula_9">H R = (HW V )A R ⊙ G,<label>(9)</label></formula><p>where W G and W V are trainable parameters and G is the gating matrix. Considering the small proportion of effective words in the whole sentence, the gating mechanism is conducive to eliminating background noise, making it easier for the model to focus on the more critical words.</p><p>Finally, with all these above calculations including relation-enhanced self-attention and gating mechanism, we obtain the relation-enhanced aspect representation H R a = {h R a 1 , h R a 2 , ..., h R am } for subsequent classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Model Training</head><p>The final classification features are concatenated by the original BERT aspect representation H a = mean{h a 1 , h a 2 , ..., h am } and the relationenhanced aspect representation H R a .</p><formula xml:id="formula_10">H f inal a = [H a , H R a ].<label>(10)</label></formula><p>It is passed through a fully connected softmax layer and mapped to probabilities over three sentiment polarities.</p><formula xml:id="formula_11">p(a) = sof tmax(W p H f inal a + b p ). (<label>11</label></formula><formula xml:id="formula_12">)</formula><p>We use cross-entropy loss as our objective function:</p><formula xml:id="formula_13">L CE = - (s,a)∈D c∈C y c a log p c (a), (<label>12</label></formula><formula xml:id="formula_14">)</formula><p>where y is the ground truth sentiment polarity, D contains all sentence-aspect pairs and C contains all sentiment polarities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we first introduce the relevant settings of the experiments, including the datasets used, implementation details and baseline methods for comparison. Then, we report the experimental results under basic and advanced settings. Finally, we select several representative examples for model analysis and discussion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets and Setup</head><p>Our experiments are conducted on four commonly used public standard datasets. The Twitter dataset is a collection of tweets built by <ref type="bibr" target="#b11">Dong et al. (2014)</ref>, while the Restaurant and Laptop dataset come from the SemEval 2014 Task <ref type="bibr" target="#b34">(Pontiki et al., 2014)</ref>. MAMS is a large-scale multi-aspect dataset provided by <ref type="bibr" target="#b21">Jiang et al. (2019)</ref>. Data statistics are shown in Appendix A.1.</p><p>In data preprocessing, we use SPRING <ref type="bibr" target="#b2">(Bevilacqua et al., 2021)</ref> as the parser and LEAMR (Blodgett and Schneider, 2021) as the aligner. APARN uses the BERT of bert-base-uncased version with max length as 100 and the relation-enhanced selfattention mechanism uses 8 attention heads. We reported accuracy and Macro-F1 as results which are the average of three runs with different random seeds. See Appendix A.2 for more details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Baseline Methods</head><p>We compare APARN with a series of baselines and state-of-the-art alternatives, including: 1) BERT <ref type="bibr" target="#b10">(Devlin et al., 2019)</ref> is composed of a general pre-trained BERT model and a classification layer adapted to the ABSA task. 2) DGEDT <ref type="bibr" target="#b39">(Tang et al., 2020)</ref> proposes a dual transformer structure based on dependency graph augmentation, which can simultaneously fuse representations of sequences and graphs.</p><p>3) R-GAT <ref type="bibr" target="#b42">(Wang et al., 2020)</ref> proposes a dependency structure adjusted for aspects and uses a relational GAT to encode this structure. 4) T-GCN <ref type="bibr" target="#b40">(Tian et al., 2021)</ref> proposes an approach to explicitly utilize dependency types for ABSA with type-aware GCNs. 5) DualGCN <ref type="bibr" target="#b26">(Li et al., 2021)</ref> proposes a dual GCN structure and regularization methods to merge features from sentences and dependency trees. 6) dotGCN <ref type="bibr" target="#b6">(Chen et al., 2022)</ref> proposes an aspectspecific and language-agnostic discrete latent tree as an alternative structure to dependency trees. 7) SSEGCN <ref type="bibr">(Zhang et al., 2022b)</ref> proposes an aspect-aware attention mechanism to enhance the node representations with GCN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Main Results</head><p>Table <ref type="table" target="#tab_1">2</ref> shows the experimental results of our model and the baseline models on four datasets under the same conventional settings as <ref type="bibr" target="#b26">Li et al. (2021)</ref>, where the best results are in bold and the second best results are underlined. Our APARN exhibits excellent results and achieves the best results on all 8 indicators of 4 datasets with an average margin more than one percent, which fully proves the effectiveness of this model.</p><p>Comparing the results of different datasets, we can find that the improvement of APARN on the Twitter dataset is particularly obvious. Compared to the best baselines, the accuracy rate has increased by 1.65% and the Macro-F1 has increased by 1.79%. The main reason is the similarity of the Twitter dataset to the AMR 3.0 dataset, the training dataset for the AMR parser we used. More than half of the corpus of the AMR 3.0 dataset comes from internet forums and blogs, which are similar to the Twitter dataset as they are both social media. As a result, the AMR parser has better output on the Twitter dataset, which in turn enables the model to extract more valuable features from it and leads to a considerable improvement. This difference among datasets also reflects the effectiveness of semantic information from AMR for the ABSA task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Comparative Experiments</head><p>We conduct comparative experiments to analyse the impact of models (APARN and T-GCN), parsed structures (AMR and dependency tree), and edge labels (with and without). T-GCN is selected instead of more recent models because they lack the ability to exploit edge labels and cannot receive AMRs as input. AMRs are the same as the basic experiments and dependency trees are parsed by Stanford CoreNLP Toolkits <ref type="bibr" target="#b32">(Manning et al., 2014)</ref>. "Without edge labels" means all labels are the same placeholder. The results are shown in Figure <ref type="figure" target="#fig_2">4</ref>.</p><p>From the perspective of models, APARN consistently outperforms T-GCN in any parsed structure and edge label settings, demonstrating the effectiveness of our APARN. From the perspective of parsed structures, AMRs outperform dependency trees in most model and edge label settings, except for the case of T-GCN without edge labels. The reason may be that the AMR without edge labels is sparse and semantically ambiguous, which does not match the design of the model.</p><p>From the perspective of edge labels, a graph with edge labels is always better than a graph without edge labels, whether it is an AMR or a dependency tree, whichever the model is. We can also notice that APARN has a greater improvement with the addition of edge labels, indicating that it can utilize edge labels more effectively. Besides, with the addition of edge labels, experiments using AMR have improved more than experiments using depen- dency trees, indicating that edge labels of the AMR contain richer semantic information and are more valuable for sentiment analysis, which is consistent with previous experiments in Figure <ref type="figure">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Further Analysis</head><p>Ablation Study To analyze the role of each module, we separately remove four key components of APARN. Results on four datasets are represented in Table <ref type="table" target="#tab_2">3</ref>.</p><p>According to the results, each of the four components contributes significantly to the performance of APARN. Removing Outer Product Sum results in a significant drop in performance, illustrating the importance of promoting consistency of information from sentences and AMRs. Removing Path Aggregator is worse than removing Relation in Self-Attention, indicating that unprocessed AMR information can only interfere with the model instead of being exploited by the model.</p><p>Comparing the results in different datasets, we can find that the model depends on information from sentences and AMRs differently on different datasets. On the Restaurant dataset, removing the Relation in Self-Attention component has less impact, while on the Twitter dataset, removing this component has a greater impact. This means the model utilizes sentence information more on the Restaurant dataset and AMR information more on the Twitter dataset. This is also consistent with the analysis of the main results: the AMR of Twitter dataset has higher quality due to the domain relatedness with the training dataset of the AMR parser, which in turn makes the model pay more attention to the information from the AMR on this dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AMR Parser Analysis</head><p>We conduct experiments using AMRs from different parsers on Twitter dataset, as displayed in Figure <ref type="figure">5</ref>. In addition to the SPRING parser mentioned before, we try two other parsers from <ref type="bibr">Zhang et al. (2019b)</ref> and <ref type="bibr" target="#b5">Cai and Lam (2020)</ref> and 84.3 Smatch score for AMR parsing task on AMR 2.0 dataset, which can be regarded as the quality of their output. From the figure, it is clear that the accuracy of ABSA task shows positive correlation with the Smatch score, which proves the positive effect of AMRs in the ABSA task and the importance of the high quality AMR.</p><p>Sentence Length Study Table <ref type="table" target="#tab_3">4</ref> compares the accuracy of APARN with and without path aggregator for sentences of different lengths in the Restaurant dataset. According to the table, we can see that the model achieves higher accuracy on short sentences, while the long sentences are more challenging. In addition, the model with the path aggregator has a larger relative improvement on long sentences than short sentences, indicating that the path aggregator can effectively help the model capture long-distance relations with AMR.</p><p>the atmosphere was crowded but it was a great bistro-type vibe BERT +AMR so if you want a nice ， enjoyable meal at montparnasse ， go early for the pre-theater prix-fixe BERT +AMR i ordered the smoked salmon and roe appetizer and it was off flavor BERT +AMR </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Case Study</head><p>As shown in Figure <ref type="figure" target="#fig_3">6</ref>, we selected three typical cases to visualize the aspect terms' attention to the context before and after adding information from the AMR, respectively.</p><p>From the first two examples, we can notice that the model focuses on the copula verb next to the opinion term without the AMR. While with the information from the AMR, the model can capture opinion terms through the attention mechanism more accurately. In the third example, without the AMR, the model pays more attention to words that are closer to the aspect term. With the semantic information from AMR, the model can discover opinion terms farther away from aspect terms.</p><p>These cases illustrate that the semantic structure information of AMR plays an important role in making the model focus on the correct opinion words. It also shows that the structure of our APARN can effectively utilize the semantic structure information in AMR to improve the performance in the ABSA task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Aspect-based Sentiment Analysis Traditional sentiment analysis tasks are usually sentence-level or document-level, while the ABSA task is an entity-level and fine-grained sentiment analysis task. Early methods <ref type="bibr" target="#b20">(Jiang et al., 2011;</ref><ref type="bibr" target="#b24">Kiritchenko et al., 2014)</ref> are mostly based on artificially constructed features, which are difficult to effectively model the relations between aspect terms and its context. With the development of deep neural networks, many recent works <ref type="bibr" target="#b43">(Wang et al., 2016;</ref><ref type="bibr" target="#b38">Tang et al., 2016;</ref><ref type="bibr" target="#b8">Chen et al., 2017;</ref><ref type="bibr">Fan et al., 2018;</ref><ref type="bibr" target="#b13">Gu et al., 2018;</ref><ref type="bibr" target="#b12">Du et al., 2019;</ref><ref type="bibr" target="#b29">Liang et al., 2019;</ref><ref type="bibr" target="#b46">Xing et al., 2019)</ref> have explored applying attention mechanisms to implicitly model the semantic relations of aspect terms and identify the key opinion terms in the context.</p><p>Another trend in ABSA studies is the explicit use of dependency trees. Some works <ref type="bibr" target="#b14">(He et al., 2018;</ref><ref type="bibr">Zhang et al., 2019a;</ref><ref type="bibr" target="#b37">Sun et al., 2019;</ref><ref type="bibr" target="#b19">Huang and Carley, 2019;</ref><ref type="bibr" target="#b49">Zhang and Qian, 2020;</ref><ref type="bibr" target="#b7">Chen et al., 2020;</ref><ref type="bibr" target="#b28">Liang et al., 2020;</ref><ref type="bibr" target="#b42">Wang et al., 2020;</ref><ref type="bibr" target="#b39">Tang et al., 2020;</ref><ref type="bibr" target="#b33">Phan and Ogunbona, 2020;</ref><ref type="bibr" target="#b26">Li et al., 2021;</ref><ref type="bibr" target="#b45">Xiao et al., 2021)</ref> extend GCN, GAT, and Transformer backbones to process syntactic dependency trees and develop several outstanding models. These models shorten the distance between aspect terms and opinion terms by dependency trees and alleviate the long-term dependency problem.</p><p>Recent studies have also noticed the limitations of dependency trees in the ABSA task. <ref type="bibr" target="#b42">Wang et al. (2020)</ref> proposes the reshaped dependency tree for the ABSA task. <ref type="bibr" target="#b7">Chen et al. (2020)</ref> propose to combine dependency trees with induced aspect-specific latent maps. <ref type="bibr" target="#b6">Chen et al. (2022)</ref> further proposed an aspect-specific and language-independent discrete latent tree model as an alternative structure for dependency trees. Our work is similar in that we also aim at the mismatch between dependency trees and the ABSA task, but different in that we introduce a semantic structure AMR instead of induced trees.</p><p>Abstract Meaning Representation AMR is a structured semantic representation that represents the semantics of sentences as a rooted, directed, acyclic graph with labels on nodes and edges. AMR is proposed by <ref type="bibr" target="#b1">Banarescu et al. (2013)</ref> to provide a specification for sentence-level comprehensive semantic annotation and analysis tasks. Research on AMR can be divided into two categories, AMR parsing <ref type="bibr" target="#b5">(Cai and Lam, 2020;</ref><ref type="bibr" target="#b44">Zhou et al., 2021;</ref><ref type="bibr" target="#b15">Hoang et al., 2021)</ref> and <ref type="bibr">AMR-to-Text (Zhao et al., 2020;</ref><ref type="bibr" target="#b0">Bai et al., 2020;</ref><ref type="bibr" target="#b35">Ribeiro et al., 2021)</ref>.</p><p>AMR has also been applied in many NLP tasks. <ref type="bibr" target="#b22">Kapanipathi et al. (2020)</ref> use AMR in question answering system. <ref type="bibr">Lim et al. (2020)</ref> employ AMR to improve common sense reasoning. <ref type="bibr" target="#b44">Wang et al. (2021)</ref> utilize AMR to add pseudo labels to unlabeled data in low-resource event extraction task. Our model also improves the performance of the ABSA task with AMR. Moreover, AMR also has the potential to be applied to a broader range of NLP tasks, including relation extraction <ref type="bibr" target="#b16">(Hu et al., 2020</ref><ref type="bibr">(Hu et al., , 2021a,b),b)</ref>, named entity recognition <ref type="bibr" target="#b47">(Yang et al., 2023)</ref>, natural language inference <ref type="bibr">(Li et al., 2022)</ref>, text-to-SQL <ref type="bibr" target="#b31">(Liu et al., 2022)</ref>, and more.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we propose APARN, AMR-based Path Aggregation Relational Network for ABSA. Different from the traditional ABSA model utilizing the syntactic structure like dependency tree, our model employs the semantic structure called Abstract Meaning Representation which is more harmony with the sentiment analysis task. We propose the path aggregator and the relation-enhanced selfattention mechanism to efficiently exploit AMRs and integrate information from AMRs and input sentences. These designs enable our model to achieve better results than existing models. Experiments on four public datasets show that APARN outperforms competing baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Limitations</head><p>The high computational complexity is one of the biggest disadvantages of the path aggregation. The time consumption and GPU memory used for multiple operations are expensive. So it is very desirable to use only one time of path aggregation due to attributes of the ABSA task in our APARN.</p><p>Another limitation of this work is that the performance of the model is still somewhat affected by the quality of the AMR parsing results. The good news is that the research on AMR parsing is continuing to make progress. In the future, APARN with higher quality AMRs is expected to further improve the level of the ABSA task.</p><p>Besides, this model is flawed in dealing with implicit and ambiguous sentiments in sentences. Implicit sentiment lacks corresponding opinion words, and ambiguous sentiment is subtle and not apparent. An example of this is the sentence "There was only one <ref type="bibr">[waiter]</ref> for the whole restaurant upstairs," which has an ambiguous sentiment associated with the aspect word "waiter". The golden label is "Neutral", but our model predicts it as "Negative".</p><p>Finally, generalization to other ABSA tasks such as end-to-end ABSA or ASTE is another restriction. Considering the complexity of the task, we only apply our motivation to sentiment classification in this paper. We will further generalize it to more complex sentiment analysis tasks in the future work.</p><p>We usually just get some of the dinner specials and they are very reasonably priced and very tasty   ably priced and very tasty". In its dependency tree, the distance between the aspect "dinner specails" and the opinion terms "reasonably priced" or "very tasty" is more than 3, while they are directly connected in the AMR.</p><p>The second sentence is "We parked on the block of Nina 's the place looked nice , with people obviously enjoying their pizzas". In its dependency tree, the distance between the aspect "place" and the opinion terms "nice" is 4, while they are directly connected in the AMR.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The overall architecture of APARN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Accuracy of APARN and T-GCN on Twitter dataset with different parsed structures and edge labels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Visualization of aspect terms' attention to the context in three cases. Aspect terms are highlighted in blue.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Dependency tree examples with aspects in red and opinion terms in blue.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: AMR examples with aspects in red and opinion terms in blue.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Aspect-opinion, aspect-context and relative aspect-opinion distances of different structures.</figDesc><table><row><cell>• We propose a new model APARN that in-tegrates information from original sentences</cell></row><row><cell>and AMRs via the path aggregator and the</cell></row><row><cell>relation-enhanced self-attention mechanism</cell></row><row><cell>to fully exploit semantic structure information</cell></row><row><cell>and relieve parser unreliability.</cell></row></table><note><p>• We experiment on four public datasets and our APARN outperforms state-of-the-art baselines, demonstrating its effectiveness. More Figure 2: Label distribution of edges in aspect-opinion paths and all edges, in dependency trees and AMRs. Labels are ordered by its density in aspect-opinion paths.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Results on four public datasets. Best performed baselines are underlined. All models are based on BERT.</figDesc><table><row><cell>Model</cell><cell cols="8">Restaurant Accuracy Macro-F1 Accuracy Macro-F1 Accuracy Macro-F1 Accuracy Macro-F1 Laptop Twitter MAMS</cell></row><row><cell>BERT (Devlin et al., 2019)</cell><cell>85.62</cell><cell>78.28</cell><cell>77.58</cell><cell>72.38</cell><cell>75.28</cell><cell>74.11</cell><cell>80.11</cell><cell>80.34</cell></row><row><cell>DGEDT (Tang et al., 2020)</cell><cell>86.30</cell><cell>80.00</cell><cell>79.80</cell><cell>75.60</cell><cell>77.90</cell><cell>75.40</cell><cell>-</cell><cell>-</cell></row><row><cell>R-GAT (Wang et al., 2020)</cell><cell>86.60</cell><cell>81.35</cell><cell>78.21</cell><cell>74.07</cell><cell>76.15</cell><cell>74.88</cell><cell>-</cell><cell>-</cell></row><row><cell>T-GCN (Tian et al., 2021)</cell><cell>86.16</cell><cell>79.95</cell><cell>80.88</cell><cell>77.03</cell><cell>76.45</cell><cell>75.25</cell><cell>83.38</cell><cell>82.77</cell></row><row><cell>DualGCN (Li et al., 2021)</cell><cell>87.13</cell><cell>81.16</cell><cell>81.80</cell><cell>78.10</cell><cell>77.40</cell><cell>76.02</cell><cell>-</cell><cell>-</cell></row><row><cell>dotGCN (Chen et al., 2022)</cell><cell>86.16</cell><cell>80.49</cell><cell>81.03</cell><cell>78.10</cell><cell>78.11</cell><cell>77.00</cell><cell>84.95</cell><cell>84.44</cell></row><row><cell cols="2">SSEGCN (Zhang et al., 2022b) 87.31</cell><cell>81.09</cell><cell>81.01</cell><cell>77.96</cell><cell>77.40</cell><cell>76.02</cell><cell>-</cell><cell>-</cell></row><row><cell>APARN (Ours)</cell><cell>87.76</cell><cell>82.44</cell><cell>81.96</cell><cell>79.10</cell><cell>79.76</cell><cell>78.79</cell><cell>85.59</cell><cell>85.06</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Ablation experimental results of our APARN.</figDesc><table><row><cell>Model</cell><cell cols="8">Restaurant Accuracy Macro-F1 Accuracy Macro-F1 Accuracy Macro-F1 Accuracy Macro-F1 Laptop Twitter MAMS</cell></row><row><cell cols="2">APARN -Outer Product Sum -Path Aggregator -Relation in Self-Attention 87.49 87.76 86.15 87.04 -Gate in Self-Attention 85.61</cell><cell>82.44 80.13 81.61 81.82 78.49</cell><cell>81.96 79.45 79.20 80.36 79.81</cell><cell>79.10 76.34 75.67 77.87 77.42</cell><cell>79.76 76.22 76.66 76.81 77.55</cell><cell>78.79 74.75 74.90 75.49 76.06</cell><cell>85.59 82.93 83.16 83.73 83.96</cell><cell>85.06 82.30 82.61 83.08 83.15</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>. These parsers achieve 76.3, 80.2 Accuracy of APARN with and without path aggregator for sentences of different lengths in the Restaurant dataset.</figDesc><table><row><cell>ABSA Accuracy</cell><cell>77 78 79 80</cell><cell cols="2">Zhang et al. (2019b)</cell><cell cols="2">Cai and Lam (2020)</cell><cell>SPRING</cell></row><row><cell></cell><cell cols="2">75 76</cell><cell>77</cell><cell cols="2">79 AMR Parsing Smatch 81 83</cell><cell>85</cell></row><row><cell cols="6">Figure 5: Accuracy of APARN on Twitter dataset with AMR from different parsers.</cell></row><row><cell cols="4">Sentence Length</cell><cell>&lt;15</cell><cell>15-24 25-34</cell><cell>&gt;35</cell></row><row><cell cols="6">w/o Path Aggregator 88.25 85.43 83.92 83.96</cell></row><row><cell cols="4">w. Path Aggregator</cell><cell cols="2">89.40 87.15 86.64 86.71</cell></row><row><cell cols="6">Relative Improvement +1.30% +2.01% +3.24% +3.28%</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>The code will be available at https://github.com/THU-BPM/APARN. * Corresponding Author.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1"><p>The Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing assistance.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>The work was supported by the <rs type="funder">National Key Research and Development Program of China</rs> (No. <rs type="grantNumber">2019YFB1704003</rs>), the <rs type="funder">National Nature Science Foundation of China</rs> (No. <rs type="grantNumber">62021002</rs>), <rs type="person">Tsinghua BNRist</rs> and <rs type="funder">Beijing Key Laboratory of Industrial Bigdata System and Application</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_xAcaB3g">
					<idno type="grant-number">2019YFB1704003</idno>
				</org>
				<org type="funding" xml:id="_GZbtskn">
					<idno type="grant-number">62021002</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>sentiment analysis: Tasks, methods, and challenges. arXiv preprint arXiv:2203.01054.</p><p>Zheng <ref type="bibr">Zhang, Zili Zhou, and Yanna Wang. 2022b</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Datasets</head><p>The statistics for the Restaurant dataset, Laptop dataset, Twitter dataset and MAMS dataset are shown in Table <ref type="table">5</ref>. Each sentence in these datasets is annotated with aspect terms and corresponding polarities. Following <ref type="bibr" target="#b26">Li et al. (2021)</ref>, we remove instances with the "conflict" label. So all datasets have three sentiment polarities: positive, negative and neutral. Throughout the research, we follow the Creative Commons Attribution 4.0 International Licence of the datasets. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Implementation Details</head><p>Preprocessing We use SPRING <ref type="bibr" target="#b2">(Bevilacqua et al., 2021)</ref> as the parser to obtain the AMRs of input sentences and use LEAMR <ref type="bibr" target="#b4">(Blodgett and Schneider, 2021)</ref> as the AMR aligner to establish the correspondence between the AMRs and sentences. The maximum length of the input sentence is set to 100, the shortage is made up with the special word "PAD" and the excess is truncated. Some edge labels are treated specially when mapping the edges of AMR to the relations between words. Edge labels suffixed with "-of" are used to avoid loops in AMR, so we swap their start and end points and remove the "-of" suffix, eg: the ":ARG0-of" relation from token i to token j is changed to the ":ARG0" relation from token j to token i . Edge labels prefixed with ":prep-" are used because there is no suitable preposition label in the AMR specification. We changed them to original prepositions, for example, ":prep-against" is changed to "against".</p><p>Model Structure and Training APARN uses the BERT of bert-base-uncased version as a pretrained encoder. The dimension of its output is 768, which is also used as the dimension of token representation in the path aggregator. The dimension of the AMR edge label embedding derived from the SPRING model is 1024. Due to computational efficiency and memory usage, this dimension is reduced to 376 through a linear layer as the dimension of the relational matrix features in the path aggregator. For the relation-enhanced selfattention mechanism, its gated multi-head attention mechanism uses 8 attention heads with the latent dimension size of 64. The total parameter size of APARN is about 130M and it takes about 8 minutes to train each epoch on a single RTX 3090 GPU with the batch size of 16.</p><p>During training, we use the Adam (Kingma and Ba, 2015) optimizer and use the grid search to find best hyper-parameters. The range of learning rate is [1 × 10 -5 , 5 × 10 -5 ]. Adam hyperparameter α is 0.9 and β is in (0.98, 0.99, 0.999). The BERT encoder and other parts of the model use dropout strategies with probability in [0.1, 0.5], respectively.</p><p>Each training lasts up to 15 epochs and the model is evaluated on validation data. For datasets without official validation data, we follow the settings of previous work <ref type="bibr" target="#b26">(Li et al., 2021)</ref>. The model with the highest accuracy among all evaluation results is selected as the final model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 More Comparison Examples</head><p>Here are two other comparison examples of dependency trees (Figure <ref type="figure">7</ref>) and AMRs (Figure <ref type="figure">8</ref>).</p><p>The first sentence is "We usually just get some of the dinner specials and they are very reason- A.1 B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided that it was specified? For the artifacts you create, do you specify intended use and whether that is compatible with the original access conditions (in particular, derivatives of data accessed for research purposes should not be used outside of research contexts)? Not applicable. Left blank.</p><p>B4. Did you discuss the steps taken to check whether the data that was collected / used contains any information that names or uniquely identifies individual people or offensive content, and the steps taken to protect / anonymize it? Not applicable. Left blank.</p><p>B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and linguistic phenomena, demographic groups represented, etc.? A.1 B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits, etc. for the data that you used / created? Even for commonly-used benchmark datasets, include the number of examples in train / validation / test splits, as these provide necessary context for a reader to understand experimental results. For example, small differences in accuracy on large test sets may be significant, while on small test sets they may not be. A.1 C Did you run computational experiments? 4 C1. Did you report the number of parameters in the models used, the total computational budget (e.g., GPU hours), and computing infrastructure used? A.2</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Online back-parsing for AMR-to-text generation</title>
		<author>
			<persName><forename type="first">Xuefeng</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linfeng</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.92</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1206" to="1219" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Abstract Meaning Representation for sembanking</title>
		<author>
			<persName><forename type="first">Laura</forename><surname>Banarescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claire</forename><surname>Bonial</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shu</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Madalina</forename><surname>Georgescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kira</forename><surname>Griffitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ulf</forename><surname>Hermjakob</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martha</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Schneider</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th Linguistic Annotation Workshop and Interoperability with Discourse</title>
		<meeting>the 7th Linguistic Annotation Workshop and Interoperability with Discourse<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="178" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">One SPRING to rule them both: Symmetric AMR semantic parsing and generation without a complex pipeline</title>
		<author>
			<persName><forename type="first">Michele</forename><surname>Bevilacqua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rexhina</forename><surname>Blloshmi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Fifth AAAI Conference on Artificial Intelligence and Thirty-Third Conference on Innovative Applications of Artificial Intelligence and The Eleventh Symposium on Educational Advances in Artificial Intelligence</title>
		<meeting><address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="12564" to="12573" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">A comprehensive survey on sentiment analysis: Approaches, challenges and trends</title>
		<author>
			<persName><forename type="first">Marouane</forename><surname>Birjali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammed</forename><surname>Kasri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abderrahim</forename><surname>Beni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hssane</forename></persName>
		</author>
		<idno type="DOI">10.1016/j.knosys.2021.107134</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
			<publisher>Knowledge-Based Systems</publisher>
			<biblScope unit="volume">226</biblScope>
			<biblScope unit="page">107134</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Probabilistic, structure-aware algorithms for improved variety, accuracy, and coverage of AMR alignments</title>
		<author>
			<persName><forename type="first">Austin</forename><surname>Blodgett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Schneider</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.257</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3310" to="3321" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">AMR parsing via graphsequence iterative inference</title>
		<author>
			<persName><forename type="first">Deng</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wai</forename><surname>Lam</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.119</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1290" to="1301" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Discrete opinion tree induction for aspect-based sentiment analysis</title>
		<author>
			<persName><forename type="first">Chenhua</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyang</forename><surname>Teng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhongqing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.acl-long.145</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 60th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2051" to="2064" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Inducing target-specific latent structures for aspect sentiment classification</title>
		<author>
			<persName><forename type="first">Chenhua</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyang</forename><surname>Teng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.451</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="5596" to="5607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Recurrent attention network on memory for aspect sentiment analysis</title>
		<author>
			<persName><forename type="first">Peng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhongqian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lidong</forename><surname>Bing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Yang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D17-1047</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="452" to="461" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Does syntax matter? a strong baseline for aspect-based sentiment analysis with RoBERTa</title>
		<author>
			<persName><forename type="first">Junqi</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianxiang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.naacl-main.146</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1816" to="1829" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Adaptive recursive neural network for target-dependent Twitter sentiment classification</title>
		<author>
			<persName><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuanqi</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ke</forename><surname>Xu</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/P14-2009</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics<address><addrLine>Baltimore, Maryland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="49" to="54" />
		</imprint>
	</monogr>
	<note>Short Papers). Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Capsule network with interactive attention for aspectlevel sentiment classification</title>
		<author>
			<persName><forename type="first">Chunning</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haifeng</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianxin</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1380</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China; Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2019. 2018</date>
			<biblScope unit="page" from="3433" to="3442" />
		</imprint>
	</monogr>
	<note>Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A position-aware bidirectional attention network for aspect-level sentiment analysis</title>
		<author>
			<persName><forename type="first">Shuqin</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lipeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuexian</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yin</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Computational Linguistics</title>
		<meeting>the 27th International Conference on Computational Linguistics<address><addrLine>Santa Fe, New Mexico, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="774" to="784" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Effective attention modeling for aspect-level sentiment classification</title>
		<author>
			<persName><forename type="first">Ruidan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sun</forename><surname>Wee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hwee Tou</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><surname>Dahlmeier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Computational Linguistics</title>
		<meeting>the 27th International Conference on Computational Linguistics<address><addrLine>Santa Fe, New Mexico, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1121" to="1131" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Ensembling graph predictions for amr parsing</title>
		<author>
			<persName><forename type="first">Thanh</forename><forename type="middle">Lam</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriele</forename><surname>Picco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yufang</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Young-Suk</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lam</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dzung</forename><surname>Phan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vanessa</forename><surname>Lopez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ramon</forename><surname>Fernandez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Astudillo</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Online. Curran Associates, Inc</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="8495" to="8505" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">SelfORE: Self-supervised relational feature learning for open relation extraction</title>
		<author>
			<persName><forename type="first">Xuming</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lijie</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yusong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><surname>Yu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.299</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3673" to="3682" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">2021a. Semi-supervised relation extraction via incremental meta self-training</title>
		<author>
			<persName><forename type="first">Xuming</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fukun</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenyao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lijie</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.findings-emnlp.44</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2021</title>
		<meeting><address><addrLine>Punta Cana</addrLine></address></meeting>
		<imprint>
			<publisher>Dominican Republic. Association for Computational Linguistics</publisher>
			<biblScope unit="page" from="487" to="496" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">2021b. Gradient imitation reinforcement learning for low resource relation extraction</title>
		<author>
			<persName><forename type="first">Xuming</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yawen</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lijie</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.emnlp-main.216</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Dominican Republic. Association for Computational Linguistics</publisher>
			<biblScope unit="page" from="2737" to="2746" />
		</imprint>
		<respStmt>
			<orgName>Online and Punta Cana</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Syntaxaware aspect level sentiment classification with graph attention networks</title>
		<author>
			<persName><forename type="first">Binxuan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kathleen</forename><surname>Carley</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1549</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5469" to="5477" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Target-dependent Twitter sentiment classification</title>
		<author>
			<persName><forename type="first">Long</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohua</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tiejun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Portland, Oregon, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="151" to="160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A challenge dataset and effective models for aspect-based sentiment analysis</title>
		<author>
			<persName><forename type="first">Qingnan</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruifeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Ao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Yang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1654</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6280" to="6285" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Question answering over knowledge bases by leveraging semantic parsing and neuro-symbolic reasoning</title>
		<author>
			<persName><forename type="first">Pavan</forename><surname>Kapanipathi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ibrahim</forename><surname>Abdelaziz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Srinivas</forename><surname>Ravishankar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ramón</forename><surname>Fernandez Astudillo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maria</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cristina</forename><surname>Cornelio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saswati</forename><surname>Dana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Achille</forename><surname>Fokoue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dinesh</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alfio</forename><surname>Gliozzo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sairam</forename><surname>Gurajada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hima</forename><surname>Karanam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naweed</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dinesh</forename><surname>Khandelwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Young-Suk</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunyao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Francois</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ndivhuwo</forename><surname>Luus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nandana</forename><surname>Makondo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tahira</forename><surname>Mihindukulasooriya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sumit</forename><surname>Naseem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucian</forename><surname>Neelam</surname></persName>
		</author>
		<author>
			<persName><surname>Popa</surname></persName>
		</author>
		<idno>CoRR, abs/2012.01707</idno>
		<editor>Gangi Reddy, Ryan Riegel, Gaetano Rossiello, Udit Sharma, G. P. Shrivatsa Bhargav, and Mo Yu</editor>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations, ICLR 2015</title>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05-07">2015. May 7-9, 2015</date>
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">NRC-Canada-2014: Detecting aspects and sentiment in customer reviews</title>
		<author>
			<persName><forename type="first">Svetlana</forename><surname>Kiritchenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><surname>Cherry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saif</forename><surname>Mohammad</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/S14-2076</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International Workshop on Semantic Evaluation</title>
		<meeting>the 8th International Workshop on Semantic Evaluation<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">2014. SemEval 2014</date>
			<biblScope unit="page" from="437" to="442" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Reflections on Sentiment/Opinion Analysis</title>
		<author>
			<persName><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-55394-8_3</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>Springer International Publishing</publisher>
			<biblScope unit="page" from="41" to="59" />
			<pubPlace>Cham</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Dual graph convolutional networks for aspect-based sentiment analysis</title>
		<author>
			<persName><forename type="first">Ruifan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fangxiang</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhanyu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaojie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.494</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="6319" to="6329" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Pair-level supervised contrastive learning for natural language inference</title>
		<author>
			<persName><forename type="first">'ang</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lijie</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><surname>Wen</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P18-1087</idno>
		<idno type="arXiv">arXiv:2201.10927</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<editor>
			<persName><forename type="first">Xin</forename><surname>Li</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Lidong</forename><surname>Bing</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Wai</forename><surname>Lam</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Bei</forename><surname>Shi</surname></persName>
		</editor>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2022. 2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="946" to="956" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Jointly learning aspect-focused and inter-aspect relations with graph convolutional networks for aspect sentiment analysis</title>
		<author>
			<persName><forename type="first">Bin</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rongdi</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lin</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiachen</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruifeng</forename><surname>Xu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.coling-main.13</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Computational Linguistics</title>
		<meeting>the 28th International Conference on Computational Linguistics<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="150" to="161" />
		</imprint>
	</monogr>
	<note>International Committee on Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A novel aspect-guided deep transition model for aspect based sentiment analysis</title>
		<author>
			<persName><forename type="first">Yunlong</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fandong</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinchao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yufeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1559</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5569" to="5580" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">2020. I know what you asked: Graph path learning using AMR for commonsense reasoning</title>
		<author>
			<persName><forename type="first">Jungwoo</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongsuk</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoonna</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kisu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heuiseok</forename><surname>Lim</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.coling-main.222</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Computational Linguistics</title>
		<meeting>the 28th International Conference on Computational Linguistics<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<biblScope unit="page" from="2459" to="2471" />
		</imprint>
	</monogr>
	<note>International Committee on Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Semantic enhanced text-to-sql parsing via iteratively learning schema linking graph</title>
		<author>
			<persName><forename type="first">Aiwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuming</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lijie</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of KDD</title>
		<meeting>of KDD</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1021" to="1030" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">The Stanford CoreNLP natural language processing toolkit</title>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jenny</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Bethard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Mcclosky</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/P14-5010</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations</title>
		<meeting>52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations<address><addrLine>Baltimore, Maryland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="55" to="60" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Modelling context and syntactical features for aspectbased sentiment analysis</title>
		<author>
			<persName><forename type="first">Minh</forename><surname>Hieu Phan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">O</forename><surname>Ogunbona</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.293</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3211" to="3220" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Harris Papageorgiou, Ion Androutsopoulos, and Suresh Manandhar</title>
		<author>
			<persName><forename type="first">Maria</forename><surname>Pontiki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimitris</forename><surname>Galanis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Pavlopoulos</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/S14-2004</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International Workshop on Semantic Evaluation (Se-mEval 2014)</title>
		<meeting>the 8th International Workshop on Semantic Evaluation (Se-mEval 2014)<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="27" to="35" />
		</imprint>
	</monogr>
	<note>SemEval-2014 task 4: Aspect based sentiment analysis</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Smelting gold and silver for improved multilingual AMR-to-Text generation</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">R</forename><surname>Leonardo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Ribeiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Pfeiffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iryna</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><surname>Gurevych</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.emnlp-main.57</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Dominican Republic. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="742" to="750" />
		</imprint>
		<respStmt>
			<orgName>Online and Punta Cana</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Open aspect target sentiment classification with natural language prompts</title>
		<author>
			<persName><forename type="first">Ronald</forename><surname>Seoh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Birle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mrinal</forename><surname>Tak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haw-Shiuan</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Pinette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alfred</forename><surname>Hough</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.emnlp-main.509</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Dominican Republic</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="6311" to="6322" />
		</imprint>
		<respStmt>
			<orgName>Online and Punta Cana</orgName>
		</respStmt>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Aspect-level sentiment analysis via convolution over dependency tree</title>
		<author>
			<persName><forename type="first">Kai</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Mensah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongyi</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xudong</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1569</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5679" to="5688" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Aspect level sentiment classification with deep memory network</title>
		<author>
			<persName><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D16-1021</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="214" to="224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Dependency graph enhanced dual-transformer structure for aspect-based sentiment classification</title>
		<author>
			<persName><forename type="first">Hao</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donghong</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiji</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.588</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="6578" to="6588" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Aspect-based sentiment analysis with type-aware graph convolutional networks and layer ensemble</title>
		<author>
			<persName><forename type="first">Yuanhe</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guimin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Song</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.naacl-main.231</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2910" to="2922" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ł Ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Relational graph attention network for aspect-based sentiment analysis</title>
		<author>
			<persName><forename type="first">Kai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhou</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaojun</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.295</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3229" to="3238" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Attention-based LSTM for aspectlevel sentiment classification</title>
		<author>
			<persName><forename type="first">Yequan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minlie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D16-1058</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="606" to="615" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">CLEVE: Contrastive Pre-training for Event Extraction</title>
		<author>
			<persName><forename type="first">Ziqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaozhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juanzi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.491</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="6283" to="6297" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">BERT4GCN: Using BERT intermediate layers to augment GCN for aspect-based sentiment classification</title>
		<author>
			<persName><forename type="first">Zeguan</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiarun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingliang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Congjian</forename><surname>Deng</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.emnlp-main.724</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Online and Punta Cana, Dominican Republic. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="9193" to="9200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Earlier attention? aspect-aware LSTM for aspect-based sentiment analysis</title>
		<author>
			<persName><forename type="first">Lejian</forename><surname>Bowen Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dandan</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fuzheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhongyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heyan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><surname>Huang</surname></persName>
		</author>
		<idno type="DOI">10.24963/ijcai.2019/738</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence</title>
		<meeting>the Twenty-Eighth International Joint Conference on Artificial Intelligence<address><addrLine>Macao, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-08-10">2019. 2019. August 10-16, 2019</date>
			<biblScope unit="page" from="5313" to="5319" />
		</imprint>
	</monogr>
	<note>ijcai.org</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Gaussian prior reinforcement learning for nested named entity recognition</title>
		<author>
			<persName><forename type="first">Yawen</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuming</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fukun</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shu'ang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aiwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lijie</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Aspect-based sentiment classification with aspectspecific graph convolutional networks</title>
		<author>
			<persName><forename type="first">Chen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiuchi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawei</forename><surname>Song</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1464</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4568" to="4578" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Convolution over hierarchical syntactic and lexical graphs for aspect level sentiment analysis</title>
		<author>
			<persName><forename type="first">Mi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tieyun</forename><surname>Qian</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.286</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3540" to="3549" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">AMR parsing as sequence-tograph transduction</title>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xutai</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Duh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1009</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="80" to="94" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Wenxuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lidong</forename><surname>Bing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wai</forename><surname>Lam</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Did you discuss the experimental setup, including hyperparameter search and best-found hyperparameter values?</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">error bars around results, summary statistics from sets of experiments), and is it transparent whether you are reporting the max, mean</title>
		<author>
			<persName><surname>C3</surname></persName>
		</author>
		<idno>and A.2</idno>
	</analytic>
	<monogr>
		<title level="m">Did you report descriptive statistics about your results</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">for preprocessing, for normalization, or for evaluation), did you report the implementation, model, and parameter settings used</title>
		<author>
			<persName><surname>C4</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NLTK, Spacy, ROUGE, etc.)? A.2 D Did you use human annotators</title>
		<imprint/>
	</monogr>
	<note>If you used existing packages. or research with human participants? Left blank</note>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Did you report the full text of instructions given to participants, including e.g., screenshots, disclaimers of any risks to participants or annotators</title>
		<author>
			<persName><surname>D1</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>No response</note>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Did you report information about how you recruited (e.g., crowdsourcing platform, students) and paid participants, and discuss if such payment is adequate given the participants&apos; demographic</title>
		<author>
			<persName><surname>D2</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>No response</note>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Did you discuss whether and how consent was obtained from people whose data you&apos;re using/curating? For example, if you collected data via crowdsourcing, did your instructions to crowdworkers explain how the data would be used? No response. D4. Was the data collection protocol approved (or determined exempt) by an ethics review board? No response. D5. Did you report the basic demographic and geographic characteristics of the annotator</title>
		<imprint/>
	</monogr>
	<note>population that is the source of the data? No response</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
