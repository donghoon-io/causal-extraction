<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Dynamic Structural Equation Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><roleName>Muthén</roleName><forename type="first">Tihomir</forename><surname>Asparouhov</surname></persName>
							<email>tiho-mir@statmodel.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Muthén &amp; Muthén</orgName>
								<address>
									<settlement>Los Angeles</settlement>
									<country>CA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ellen</forename><forename type="middle">L</forename><surname>Hamaker</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Utrecht University</orgName>
								<address>
									<addrLine>3463 Stoner Avenue</addrLine>
									<postCode>90066</postCode>
									<settlement>Muthén Los Angeles</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Bengt</forename><surname>Muthén</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Muthén &amp; Muthén</orgName>
								<address>
									<settlement>Los Angeles</settlement>
									<country>CA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Dynamic Structural Equation Models</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1070-5511 print / 1532-8007</idno>
					</monogr>
					<idno type="DOI">10.1080/10705511.2017.1406803</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-14T17:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Baysian methods</term>
					<term>dynamic factor analysis</term>
					<term>intensive longitudinal data</term>
					<term>time series analysis</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This article presents dynamic structural equation modeling (DSEM), which can be used to study the evolution of observed and latent variables as well as the structural equation models over time. DSEM is suitable for analyzing intensive longitudinal data where observations from multiple individuals are collected at many points in time. The modeling framework encompasses previously published DSEM models and is a comprehensive attempt to combine timeseries modeling with structural equation modeling. DSEM is estimated with Bayesian methods using the Markov chain Monte Carlo Gibbs sampler and the Metropolis-Hastings sampler. We provide a detailed description of the estimation algorithm as implemented in the Mplus software package. DSEM can be used for longitudinal analysis of any duration and with any number of observations across time. Simulation studies are used to illustrate the framework and study the performance of the estimation method. Methods for evaluating model fit are also discussed.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In the last several years intensive longitudinal data (ILD) with many repeated measurements from a large number of individuals have become quite common. These data are often collected using smartphones or other electronic devices and are referred to as ambulatory assessments (AA), daily diary data, ecological momentary assessment (EMA) data, or experience sampling methods (ESM) data (cf. <ref type="bibr" target="#b26">Trull &amp; Ebner-Priemer, 2014)</ref>. The accumulation of these types of data naturally leads to an increasing demand for statistical methods that allow us to model the dynamics over time as well as individual differences therein using ILD.</p><p>One of the most common methods for longitudinal analysis in the social sciences is growth modeling where an observed or latent variable is modeled as a function of time, for example, a linear function of time. The coefficients of the function, for example, intercept and slope, which determine the trajectory for the variable, are subject-specific random effects. Frequently such growth models are expressed as multivariate models (i.e., in wide format), especially when the number of observations for each person is small, for example less than 10. This allows us to introduce additional autocorrelations, or to add time-specific parameters such as time-specific residual variances. However, a multivariate model is not suitable for modeling longer longitudinal analysis such as 30 or more observations across time for several different reasons. The first reason is that the model can become computationally intensive for longer longitudinal data. A univariate model with 30 observations will require modeling the joint distribution for all 30 observations, which involves a 30 × 30 variance covariance matrix. A bivariate model would require a 60 × 60 variance covariance matrix, and so on. The dimensions of the joint distribution increase rapidly and can easily become computationally prohibitive.</p><p>An alternative specification of a growth model is as a two-level model (i.e., in long format), where each cluster consists of all the observations for one individual. Using cross-classified modeling, this approach can be extended to allow time-specific random effects in addition to subjectspecific random effects, as described in <ref type="bibr" target="#b2">Asparouhov and Muthén (2016)</ref>. Such an approach can accommodate longitudinal studies of any duration and number of observations. However, it does not accommodate autoregressive modeling where consecutive observations are directly related, rather than through subject-specific effects.</p><p>The novel modeling framework that we present here is a direct extension of the cross-classified modeling framework for ILD, as described in <ref type="bibr" target="#b2">Asparouhov and Muthén (2016)</ref>:</p><p>We simply add to that framework the ability to regress any variable, observed or latent, not only on other variables at that same time point, but also on itself and other variables at previous time points. This extended framework is referred to as dynamic structural equation modeling (DSEM), and it combines four different modeling techniques: multilevel modeling, time-series modeling, structural equation modeling (SEM), and time-varying effects modeling (TVEM). Each of these four techniques addresses different aspects of the data and is used to model different correlations that are found in such data. The multilevel modeling is based on correlations that are due to individual-specific effects. The time-series modeling is based on correlations due to proximity of observations. The SEM is based on correlations between different variables. The TVEM is based on correlations due to the same stage of evolution. The goal of the DSEM framework is to parse out and model these four types of correlations and thereby give us a fuller picture of the dynamics found in ILD.</p><p>The DSEM model described here can also be viewed as the multilevel extension of the dynamic factor models described in <ref type="bibr" target="#b19">Molenaar (1985)</ref>, <ref type="bibr" target="#b30">Zhang and Nesselroade (2007)</ref>, and <ref type="bibr" target="#b29">Zhang, Hamaker, and Nesselroade (2008)</ref>. Time-series models for observed and latent variables date back to <ref type="bibr" target="#b16">Kalman (1960)</ref> and are applied extensively in engineering and econometrics. In most such applications, however, multivariate time series data of a single case (i.e., N = 1) are analyzed. In contrast, the ILD discussed in this article come from a sample of individuals, with repeated measures nested in individuals and in time points, and the DSEM framework discussed here accommodates this more complex modeling need. Analyzing a random sample of individuals as usual allows us to make inferences about individuals who are not in the sample, which is something that cannot be done when a single individual is analyzed. Thus, the DSEM framework will allow us to make inferences for individuals outside of the sample as well as for future observations for individuals in the sample.</p><p>The outline of this article is as follows. First we present the general DSEM model and the model estimation using Bayesian methods. Next we discuss methods for model fit evaluation. We then illustrate the framework with multiple simulation studies and conclude with a summary discussion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>THE GENERAL DSEM MODEL</head><p>The general DSEM model consists of three separate models. The most general model is the cross-classified DSEM model, which incorporates both individual and time-specific random effects. The second most general model is the two-level DSEM model, which incorporates individual-specific random effects only. This model could actually be the most popular and useful model, as it is easier to estimate, identify, and interpret. The third model is the single-level DSEM model for time-series data from a single individual (cf. <ref type="bibr" target="#b30">Zhang &amp; Nesselroade, 2007)</ref>. In the latter model, there are no random effects. Here we describe the most general cross-classified DSEM model. The two-level and the single-level DSEM models are special cases of the cross-classified DSEM model.</p><p>Fundamental to the DSEM framework is the decomposition of the observed scores into three components. Let Y it be a vector of measurements for individual i at time t, where the ith individual is observed at times t ¼ 1; 2; :::; T i . The cross-classified DSEM model begins with the following decomposition:</p><formula xml:id="formula_0">Y it ¼ Y 1;it þ Y 2;i þ Y 3;t ;</formula><p>(1) where Y 2;i and Y 3;t are individual-specific and time-specific contributions and Y 1;it is the deviation of individual i at time t.</p><p>The two-level DSEM model only makes use of the first two components (i.e., Y 3;t is omitted), and the single-level model is based on simply using Y it ¼ Y 1;it . All three components are latent normally distributed random vectors and are used to form three sets of structural equation models, one on each level. Next, we begin by describing the two between-level models for the individual-specific and the time-specific components. Subsequently, we describe the within-level model for Y 1;it . Then we discuss how to handle categorical observations, and how to account for unequal intervals between the observations. We end this section with some final remarks about the framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The Between-Level Models</head><p>From the decomposition just presented, we obtain two between-level components; that is, the individual-specific component Y 2;i and the time-specific component Y 3;t . Their structural equation models take the usual form of a measurement equation and a structural equation; that is,</p><formula xml:id="formula_1">Y 2;i ¼ ν 2 þ Λ 2 η 2;i þ K 2 X 2;i þ ε 2;i</formula><p>(2)</p><formula xml:id="formula_2">η 2;i ¼ α 2 þ B 2 η 2;i þ Γ 2 X 2;i þ 2;i (3) Y 3;t ¼ ν 3 þ Λ 3 η 3;t þ K 3 X 3;i þ ε 3;t (4) η 3;t ¼ α 3 þ B 3 η 3;t þ Γ 3 X t þ 3;t :<label>(5)</label></formula><p>The vector X 2;i is a vector of individual-specific, time-invariant covariates, and X 3;t is a vector of time-specific but individual-invariant covariates. Similarly, η 2;i is a vector of individual-specific, time-invariant latent variables, and η 3;t is a vector of time-specific, individual-invariant latent variables.</p><p>The variables ε 2;i ; 2;i ; ε 3;t ; 3;t are zero mean residuals as usual and the remaining vectors and matrices in this equation are nonrandom model parameters.</p><p>Although the preceding equations do not include regressions among Y components, such regressions are typically achieved by creating a latent variable equal to the Y variable, that is, the Y variable would be a perfect error-free indicator for a latent variable. Once such latent variables are included in the model, the regression between the Y variables is specified as a regression between the corresponding latent variables using the structural equations. This is a simple way to reduce the number of matrices in these equations and is somewhat of a tradition in the SEM literature, but it has no implication for model specification or estimation.</p><p>In the preceding specification we did not include observed dependent variables, but such variables are easy to accommodate as well. That is, in addition to the latent decomposition parts of the variables Y it , the vectors Y 2;i and Y 3;t can also include observed variables that are subjectspecific or time-specific.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The Within-Level Model</head><p>The within-level part of the DSEM model is described by the following equations, which now include time-series components:</p><formula xml:id="formula_3">Y 1;it ¼ ν 1 þ X L l¼0 Λ 1;l η 1;i;tÀl þ X L l¼0 R l Y 1;i;tÀl þ X L l¼0 K 1;l X 1;i;tÀl þ ε 1;it (6) η 1;it ¼ α 1 þ X L l¼0 B 1;l η 1;i;tÀl þ X L l¼0 Q l Y 1;i;tÀl þ X L l¼0 Γ 1;l X 1;i;tÀl þ 1;it :<label>(7)</label></formula><p>Here x 1;it is a vector of observed covariates for individual i at time t and η 1;it is a vector of latent variables for individual i at time t.</p><p>In the preceding equations the latent variables η, the dependent variables Y, and the covariates X at times t, t -1, …, t -L can be used to predict the latent variables η and the dependent variables Y at time t. Including the lagged predictors X in the preceding equations is somewhat inconsequential, and we do this mostly for completeness. The covariate X 1;i;tÀl is not any different from the covariate X 1;i;t because the model does not include distributional assumptions about the covariates X and is essentially a model for the conditional distribution of ½Y jX . Including the lagged covariates X 1;i;tÀl does not involve any special statistical consideration with one small exception of the initial unobserved values, which we address later.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Latent centering</head><p>The dependent variables Y, on the left and the right side of the preceding equations are not the actual observed quantities Y it but rather the within-level component Y 1;it . These are sometimes referred to as the centered variables because Y 1;it ¼ Y it À Y 2;i À Y 3;t . The variables Y 2;i and Y 3;t can be interpreted as the mean for individual i and the mean for time t, which are thus subtracted to form the pure realization for individual i at time t excluding any global effects specific for individual i and time t.</p><p>Centering is inconsequential for the variables on the left side of the equations but is important for the variables on the right side of the equations and is well established in the multilevel modeling literature (e.g., <ref type="bibr" target="#b23">Raudenbush &amp; Bryk, 2002)</ref>. In principle, one can use the corresponding observed sample means instead of the latent true means Y 2;i and Y 3;t to center the variables. However, that will produce biased estimates, because the sample mean is different from the true mean and has a sampling error that will be unaccounted for. In multilevel models this has been documented in <ref type="bibr" target="#b18">Lüdtke et al. (2008)</ref>, where the bias is shown to occur for the between-level regression coefficients. In time-series models the bias has been documented in <ref type="bibr" target="#b21">Nickell (1981)</ref> and <ref type="bibr" target="#b13">Hamaker and Grasman (2015)</ref>, where the bias occurs for the mean of the individual-specific autoregressive coefficients. In both cases the bias disappears as the cluster size increases and the difference between true mean and sample mean vanishes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Random slopes and loadings</head><p>In addition to the preceding equations, we allow for random regression, random loading, and random intercept parameters at the within level that vary across individuals, over time, or both. A random within-level parameter s can be decomposed as</p><formula xml:id="formula_4">s ¼ s 2;i þ s 3;t ;<label>(8)</label></formula><p>where s 2;i is an individual-specific random effect; that is, an individual-specific latent variable that is an element of the vector η 2;i modeled in the Level 2 structural model. Similarly, s 3;t is a time-specific random effect; that is, a time-specific latent variable that is a part of the vector η 3;t modeled in the Level 3 structural model. If we introduce the indexes i and t for the structural parameters in Equations 6 and 7, we get</p><formula xml:id="formula_5">Y 1;it ¼ ν 1 þ X L l¼0 Λ 1;lit η 1;i;tÀl þ X L l¼0 R lit Y 1;i;tÀl þ X L l¼0 K 1;lit X 1;i;tÀl þ ε 1;it (9) DYNAMIC STRUCTURAL EQUATION MODELS η 1;it ¼ α 1;it þ X L l¼0 B 1;lit η 1;i;tÀl þ X L l¼0 Q lit Y 1;i;tÀl þ X L l¼0 Γ 1;lit X 1;i;tÀl þ 1;it (10)</formula><p>with the additional specification that every parameter varying with i and t is decomposed as in Equation <ref type="formula" target="#formula_4">8</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Random residual variances</head><p>In addition to the preceding random effects, we can also allow residual variances V on the within level to be random parameters. That is, the model parameters Varðε 1;it Þ and Varð 1;it Þ can be random as follows:</p><formula xml:id="formula_6">V ¼ Expðs 2;i þ s 3;t Þ;<label>(11)</label></formula><p>where s 2;i is an individual-specific normally distributed random effect, and s 3;t is a time-specific normally distributed random effect. Again these random effects are elements of the higher level latent variable vectors η 2;i and η 3;t . Note that whereas random structural parameters such as loadings and slopes have a normal distribution (i.e., these are normally distributed random effects), random residual variance parameters have a log-normal distribution. This is necessary to ensure that the variance parameters remain positive during the Markov chain Monte Carlo (MCMC) estimation. Furthermore, this random variance approach applies only to univariate variance parameters and it does not include random multivariate variance-covariance matrices. It is somewhat more difficult to construct random positive definite variance-covariance matrices, based on random effects that could also be used in linear models, such as Equation 3 and Equation 5, and remain positive definite for any individual and any set of covariates while being easy to interpret. However, it is possible to construct random variance-covariance matrices by introducing factors with random variances (cf. <ref type="bibr" target="#b12">Hamaker, Asparouhov, Brose, Schmiedek, &amp; Muthén, 2017)</ref>, or via random loadings Cholesky decomposition. Both of these approaches are somewhat more complex, not just in implementation, but in interpretation as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Including moving-average terms</head><p>The DSEM model incorporates only the autoregressive modeling as a time-series feature, but can easily accommodate the moving average modeling because it includes latent variable modeling. Consider, for example the ARMA(1,1) model</p><formula xml:id="formula_7">Y t ¼ μ þ aY tÀ1 þ η t þ bη tÀ1 :<label>(12)</label></formula><p>The moving average part of this model is nothing more than a latent variable and its lagged 1 latent variable predicting the dependent variable Y. Thus the ARMA models are a special case of the DSEM model. Similarly accommodating ARIMA models amounts to fixing the regression coefficients of Y t on Y tÀl to ðÀ1Þ lþ1 m l , where m is the degree of integration.</p><p>For example, fixing parameter a to 1 in Equation 12 yields the ARIMA(0,1,1) model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Starting up the process</head><p>One final issue that should be specified in the preceding model is the fact that the variables Y 1;i;tÀl , X 1;i;tÀl , and η 1;i;tÀl can have a time index that is zero or negative in the preceding model. For example when t l the time index t À l 0 appears in Equations 6 and 7. Such variables never appear in the model as dependent variables and thus we have to provide a specification of some kind. In this treatment we have chosen a method that is similar to the one used in <ref type="bibr" target="#b30">Zhang and Nesselroade (2007)</ref>. We treat all of these variables as auxiliary parameters that have their own prior. Such a prior could be difficult to specify in practical settings, however, and thus we propose the following method, which estimates the prior during a burn-in phase of the MCMC estimation. In the first iteration all the variables with nonpositive time index are set to zero. After each MCMC iteration during the burn-in phase of the estimation a new prior is computed as follows. The prior for Y 1;it for t 0 is set to be the normal prior with mean and variance the sample mean and variance of Y 1;it over all t &gt; 0 values. Similarly the prior is set for X 1;it and η 1;it for t 0. Note that none of the burn-in iterations are used to construct the final posterior distribution of the parameters. This is essential to preserve the integrity of the MCMC sequence. This method is easy to use and appears to be quite well tuned. It is the default option in Mplus and is based on 100 burn-in iterations. Note that when the time series model is sufficiently large with 30 or more observations, it is very unlikely that the prior specification affects the estimation. The effect of this prior tends to fade away beyond the first few time periods. However, when the number of time periods in the time series is small, such as less than 20, one can expect that the prior will have some small effect on the estimates. The burn-in phase prior estimation method we propose here appears to be working quite well even for short time series.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Categorical Variables</head><p>Categorical variables can easily be accommodated in the preceding model through the probit link function. For each categorical variable Y ijt in the model, j ¼ 1; :::; p, taking the values from 1 to m j , we assume that there is a normally distributed latent variable Y Ã ijt and threshold parameters τ 1j ; :::; τ m j À1j such that</p><formula xml:id="formula_8">Y ijt ¼ m , τ mÀ1j Y Ã ijt &lt;τ mj ;<label>(13)</label></formula><p>where τ 0j ¼ À1 and τ m j j ¼ 1. The preceding definition essentially converts a categorical variable Y ijt into an unobserved continuous variable Y Ã ijt . The model is then defined using Y Ã ijt instead of Y ijt in Equation <ref type="formula">1</ref>. Note that the τ parameters are nonrandom parameters and the random intercept parameters Y 2;ij and Y 3;jt provide a random and uniform shift for these threshold parameters; that is, a certain degree of uniformity is assumed when the variable is ordered polytomous. Such an assumption does not exist for binary variables and when the variable is binary, depending on the structural model at hand, the single threshold parameter can be replaced by a mean parameter for Y Ã ijt . This kind of parametrization yields a more efficient estimation by avoiding the slow mixing of <ref type="bibr" target="#b7">Cowles (1996)</ref> algorithm for sampling thresholds (see also <ref type="bibr" target="#b1">Asparouhov and Muthén 2010)</ref>. This is also the reason why sometimes models with binary variables tend to be easier to estimate as compared to models with ordered polytomous variables with more than one category, despite the fact that ordered polytomous variables carry more information in the analysis and more information generally means better model identification and more precise estimation.</p><p>Note that whereas the categorical variable modeling given in Equation 13 relies on the underlying continuous variable Y Ã ijt , the actual model application does not require such an interpretation. The variables Y Ã ijt are just a convenience for formulating the model. The model can equivalently be formulated without the underlying continuous variables Y Ã ijt and directly on the model-implied discrete probabilities PðY ijt ¼ mÞ using the model-implied probit regression. Thus even when the underlying continuous variable is deemed an unacceptable concept from a substantive point of view, this model is applicable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Continuous Time Dynamic Modeling</head><p>The DSEM model as described thus far applies to situations where the time variable can be scaled so that each person is observed at times 1; 2; :::; T i . This assumption is reasonable, for example, in daily diary applications where each subject is observed once a day. However, it is unrealistic in other applications where multiple observations are taken per day or in situations where observations are so dispersed that a daily scale is unrealistic. In many cases individual observations are taken at uneven time intervals or at random. The times of observations could be considered real values rather than integer values that would call for continuous time modeling.</p><p>We can resolve this problem by resetting the time variable using scaling, shifting, and rounding so that the continuous times of observations are well approximated by integer values. In its essence this process amounts to the following. Using a small value δ we divide the time line using an equally spaced grid where δ represents the length of the grid intervals. The times of observations are rounded to the nearest grid time point, which thereby converts the continuous times of observations to integer times of observations. We then fill in the data with missing values for those integers that were not the nearest for an observed continuous time point. The complete details of the algorithm implemented in Mplus are given in Appendix A. Understanding the process of discretization is also important for a proper interpretation of the DSEM results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Final Remarks on the General DSEM Model</head><p>For identification purposes, restrictions need to be imposed on the preceding general model. For example, mean structure parameters can exist only on one of the levels for most common situations; that is, ν j will be fixed to 0 on two out of the three levels. Other identifying restrictions need to be imposed along the lines of standard structural equation models.</p><p>The preceding model is the time-series generalization of the time-intensive model described in Section 8.3 of <ref type="bibr" target="#b2">Asparouhov and Muthén (2016)</ref>. The remarkable and daring features of this model are that longitudinal data of any length are allowed, an unlimited number of random effects can be estimated without a substantial computational burden, and no two observations in the data are truly independent of each other, as the time-series and subject-specific random effects correlate data within each subjects and the time-specific effects correlate data across subjects.</p><p>The DSEM model is a two-level model, but because it is a multivariate model, it can be used to formulate three-level DSEM models where the first level is written in a multivariate wide format. This is particularly the case when the first level contains only a small number of observations. One such example is described in <ref type="bibr" target="#b14">Jahng, Wood, and Trull (2008)</ref>, where the three-level structure is as follows: subjects, days, and observations within days. The number of observations within a day is typically a number smaller than 10 and thus can be represented with a 10-dimensional vector. Using this approach, it is possible to model within-day autocorrelation structures and between-days autocorrelation structures; that is, construct three-level DSEM models.</p><p>The DSEM model estimation is implemented in Mplus Version 8, with three notable exceptions that might be resolved in future Mplus implementations. The three exceptions are as follows: (a) the parameters R l and Q l cannot be random for when l = 0; (b) the parameters Λ 1;l , B 1;l and the parameter in Equation 11 can be random, but cannot include a time-specific random effect; and (c) for categorical variables the lagged variables Y Ã ij;tÀl are not a part of the model; that is, for categorical variables time-series models can be built only through latent variables η it , or other continuous dependent or independent variables.</p><p>In conclusion, the cross-classified DSEM model presented earlier allows us to study the evolution across time not just of the observed and latent variables, but also of the structural DYNAMIC STRUCTURAL EQUATION MODELS model as well. The two-level DSEM model is a special case of the cross-classified DSEM model and eliminates Y 3;t , X 3;t , and η 3;t variables from the model as well as Equations 4 and 5. In Equation 1 the component Y 3;t is eliminated and thus the main decomposition is the usual within-between decomposition that is fundamental to two-level structural equation models. The structural model is assumed to be time-invariant. However, this does not imply that the variable distribution is time invariant: Time-varying covariates, including the time variable t itself, can still be included in the model, and thus trends and growth models can be estimated in addition to the subject-specific time-series models.</p><p>Furthermore, the single-level DSEM model is a special case of the two-level DSEM model and it essentially contains just one cluster and no random effects. Equation 1 reduces to Y 1;it ¼ Y it and the variables Y 2;i , X 2;i , and η 2;i are removed from the model as well as Equations 2 and 3. In fact, the model is completely specified only by Equations 6 and 7. Because we have just one cluster or individual in the model, the index i can be removed from the model.</p><p>Finally, note that the cross-classified DSEM model requires the time scale to be aligned across all individuals so that a time-specific effect s 3;t has the same meaning for all individuals at time t. Not every ILD set is suitable for the cross-classified DSEM model. Consider, for example, an observational study in which time t is simply the time since the first observation was recorded; in this case, no particular effect might be expected at time t that applies to every subject in the study. On the other hand, if the study was on subjects that enrolled in a treatment and time t represents the time since enrollment in the treatment, it is natural to expect that time-specific effects at time t can exist and apply to all subjects in the data; in that case, the crossclassified DSEM model can be used. Note that the two-level DSEM model has no particular requirements on the time scale and is thus suitable for any ILD analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The Residual DSEM Model</head><p>The residual DSEM (RDSEM) model is a slight variation of the DSEM model. In this model we separate the structural and the autoregressive portion of the within-level model. Such separation leads to a simplified interpretation of the structural part of the model, and the autoregressive part of the model is absorbed in the residuals. Such models are discussed, for example, in <ref type="bibr" target="#b3">Bolger and Laurenceau (2013)</ref>. These models are also of interest when we want to estimate a cross-sectional model with time-series data, meaning that all structural effects are for variables observed from the same time period and no structural effects involve variables from different periods. By allowing autoregressive relationship for the residuals we account for the nonindependence of observations that is due to proximity of times of observations. The RDSEM model is a natural time-series extension of the two-level long model for time-series data. The extension is that we add autoregressive relationships for the residuals. The structural part of the RDSEM model is therefore the same as the two-level long model and the interpretation of the structural coefficients is the same as in standard two-level models.</p><p>The precise definition of the RDSEM model is as follows. On the between level, the RDSEM model is identical to the DSEM model, whereas the within level consists of two parts: a structural part and an autoregressive residual part. The structural part involves only relationships between variables at lag 0 (i.e., variables observed at the same time period) and is given by the following two equations:</p><formula xml:id="formula_9">Y 1;it ¼ ν 1 þ Λ 1;0 η 1;it þ R 0 Y 1;it þ K 1;0 X 1;it þ Ŷ1;it (14) η 1;it ¼ α 1 þ B 1;0 η 1;it þ Q 0 Y 1;it þ Γ 1;0 X 1;it þ η1;it :<label>(15)</label></formula><p>Here the variables Ŷ1;it and η1;it represent the residuals of the preceding structural models. Note also that the structural model does not involve any variables from previous periods and is essentially a standard SEM model for the observed and latent variables at time t. The second autoregressive part of the model is for the residual variable Ŷ1;it and η1;it and is given by the following equations:</p><formula xml:id="formula_10">Ŷ1;it ¼ X L l¼1 Λ 1;l η1;i;tÀl þ X L l¼1 R l Ŷ1;i;tÀl þ ε 1;it (16) η1;it ¼ X L l¼1 B 1;l η1;i;tÀl þ X L l¼1 Q l Ŷ1;i;tÀl þ 1;it :<label>(17)</label></formula><p>Note that in the preceding equations the summation index begins at l = 1 because all relationships between variables from the current period (i.e., l = 0) are modeled in Equations 14 and 15.</p><p>As an illustration, consider the following single-level factor analysis RDSEM model suggested to us by Phil Wood (personal communication, May 1, 2017):</p><formula xml:id="formula_11">Y pt ¼ ν p þ λ p η t þ εpt (18) η t ¼ r 0 η tÀ1 þ t : (19) εpt ¼ r p εp;tÀ1 þ ε pt (20)</formula><p>In this model the factor η t is modeled as an AR(1) process as well as the residual for each factor indicator.</p><p>Because the structural part of the RDSEM model does not involve variables across different time periods, the time interval δ (i.e., the distance between two consecutive observations) should not affect that model. The autoregressive part of the RDSEM model will be affected by the choice of δ but not the structural part. From a practical point of view, this property of the RDSEM model can be very appealing, especially when there is no natural choice for δ.</p><p>The RDSEM model can be viewed as a special case of the DSEM model where the residual variables Ŷ1;it and η1;it are modeled as within-level latent variables. This approach, however, introduces a new set of residual variables in the model for Y 1;it and η 1;it that have zero variances. With Bayesian MCMC estimation, however, zero-variance variables typically cause convergence problems and thus the new set of residual variables must have a small positive variance. Therefore the RDSEM model can be approximated by the DSEM model, but this approximation is not exact. In addition, because of the small variance residuals, the estimation of the RDSEM model via the DSEM approximation can be quite slow. In the upcoming release of Mplus Version 8.1, the RDSEM estimation is tackled directly, avoiding small variance residuals and slow convergence. This new estimation algorithm is discussed in Appendix C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MODEL ESTIMATION</head><p>The model estimation without the time-series features is described in <ref type="bibr" target="#b2">Asparouhov and Muthén (2016)</ref> and <ref type="bibr" target="#b1">Asparouhov and Muthén (2010)</ref>. A substantial portion of that estimation algorithm also applies directly to the estimation of the DSEM model. We summarize the general framework briefly and then we provide details on the estimation that are specific to DSEM. The details that are not provided here can be found in <ref type="bibr" target="#b1">Asparouhov and Muthén (2010)</ref> and are related to Bayesian estimation of SEM. Alternatively, these details can be found in <ref type="bibr" target="#b0">Arminger and Muthen (1998)</ref> or <ref type="bibr" target="#b17">Lee (2007)</ref>.</p><p>The estimation is based on the MCMC algorithm via the Gibbs sampler. All model parameters, latent variables, random effects, between-Level 2 components, between-Level 3 components, and missing data are arranged into blocks. Each of these blocks is updated (new value is being generated) from the conditional distribution of that block, conditional on all other remaining blocks and the data. This process is repeated until a stable posterior distribution for all blocks is obtained. The goal of the block arrangement is to assure that each block has an explicit or manageable conditional distribution. In addition, the blocks are arranged in such a way that elements that are highly correlated are generated simultaneously as to improve the quality of the MCMC mixing. To achieve that, we arrange the blocks to be as large as possible, keeping the conditional distributions explicit. Then within each block we arrange the elements into the smallest possible subblocks that are conditionally independent and can be generated separately.</p><p>The MCMC estimation, unlike maximum likelihood (ML) estimation, has the ability to absorb new modeling features easily, meaning that the estimation would not change dramatically when a new model feature is added. This is because the MCMC estimation is based on many conditional distributions rather than one joint distribution. Thus when a new feature is added to the model, not all conditional distributions are affected. As an example, consider the conditional distribution of the underlying Y Ã it for a categorical dependent variable. Computing the conditional distribution of Y Ã it can be done by the same method we would apply without the time-series features of the model. Similarly the methodology for updating the threshold parameters is not changed by the time-series features of the model.</p><p>Let θ represent all nonrandom model parameters. We split θ into three blocks: intercepts, slope, and loading parameters θ 1 ; variance, covariance, and correlation parameters θ 2 ; and threshold parameters θ 3 . Priors for each of these parameters have to be specified. Proper, improper, and informative conjugate prior specification for the various parameters are discussed in <ref type="bibr" target="#b1">Asparouhov and Muthén (2010)</ref>. Here we generally assume noninformative priors for all the parameters but informative priors can be facilitated as well in the MCMC estimation.</p><p>All unknown quantities in the DSEM model are placed in the following 13 blocks, which are updated one at a time during the MCMC estimation.</p><p>• B1: Y 2;i • B2: All random slopes s 2;i • B3: Y 3;t • B4: All random slopes s 3;t • B5: Other latent variables η 2;i and η 3;t • B6: Latent variables η 1;it , including initial conditions where t 0 • B7: Missing data variables Y it • B8: Initial conditions Y 1;it and X 1;it for t 0 • B9: Threshold parameters for all categorical variables θ 3 • B10: Underlying variables Y Ã it for all categorical variables • B11: Nonrandom intercepts, slope, and loadings parameters θ 1 • B12: Nonrandom variance, covariance, and correlation parameters θ 2 • B13: Random variance parameters In certain cases, blocks can be combined to improve mixing quality and the speed of the computation. For example, if R l and Q l are nonrandom parameters, blocks B1 and B2 can be combined and blocks B3 and B4 can be combined. That is because the joint conditional distribution of B1 and B2 is normal. It is not normal if R l and Q l are random because Equation 9 will contain the product of elements of B1 and elements of B2. Similar logic applies to B3 and B4.</p><p>To complete the description of the MCMC estimation, the conditional distribution of each of the preceding blocks, conditional on all other blocks and the data, should be specified. The technical details of deriving these conditional distributions are given in Appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DYNAMIC STRUCTURAL EQUATION MODELS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MODEL FIT AND MODEL COMPARISON</head><p>The easiest method for model comparison in the DSEM framework is to evaluate significance of individual parameters through the credibility intervals produced by the Bayesian estimation. This is particularly effective when models are nested and model comparison is essentially a test of significance of effects. However, in more complicated model comparisons such significance testing is not available. This section discusses the deviance information criterion (DIC) and comparisons of sample and estimated quantities as methods for evaluating model fit and model comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DIC</head><p>A commonly used criterion for model comparison in Bayesian analysis is the DIC that was first introduced by <ref type="bibr" target="#b25">Spiegelhalter, Best, Carlin, and van der Linde, (2002)</ref>. The DIC can be computed when all the dependent variables are continuous using the usual formulas. The deviance is computed as minus two times the log-likelihood</p><formula xml:id="formula_12">DðθÞ ¼ À2 logðpðY jθÞÞ;<label>(21)</label></formula><p>where θ represents all model parameters and Y represents all observed dependent variables. The effective number of parameters p D is computed as follows:</p><formula xml:id="formula_13">p D ¼ D À Dð θÞ;<label>(22)</label></formula><p>where D represents the average deviance across the MCMC iterations and θ represents the average model parameters across the MCMC iterations. The DIC criterion is then computed as</p><formula xml:id="formula_14">DIC ¼ p D þ D:<label>(23)</label></formula><p>DIC can be used to compare any number of competing models, and these could be nested or not. The best model is the model with the lowest DIC value. The effective number of parameters p D should generally be close to the size of the vector θ and is the penalty for model complexity of this information criterion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparability of the DIC</head><p>Despite this seemingly clear definition, there is substantial variation in how the DIC is actually computed and defined (cf. <ref type="bibr" target="#b6">Celeux, Forbes, Robert, &amp; Titterington, 2006)</ref>. The source of the variation is the definition of θ, and in particular in depends on whether latent variables are treated as parameters or not. If a latent variable is treated as a parameter, it is a part of the vector θ and the likelihood used in the definition of the deviance is conditional on that latent variable. If a latent variable is not treated as a parameter, it is not a part of the vector θ and the likelihood used in the definition of the deviance is the marginal likelihood; that is, the latent variable has to be integrated out (for a similar discussion in the context of the Akaike's information criterion, see <ref type="bibr" target="#b27">Vaida &amp; Blanchard, 2005)</ref>.</p><p>Consider, for example, a one-factor analysis model. If the factor is treated as a parameter, pðY jθÞ is the likelihood conditional on the factor where all indicators are independent of each other conditional on that factor. If the factor is not treated as a parameter, pðY jθÞ is computed without conditioning on the factor and instead using the modelimplied variance-covariance matrix where the indicators are not independent. These two different ways of computing the DIC will naturally produce different p D and naturally will be on a completely different scale and incomparable, despite the fact that the model is the same. In more complicated models, even more variation can occur as some latent variables can be included as parameters and some might not. This phenomenon makes the DIC somewhat trickier to use in latent variable rich DSEM models, as one has to always check that the definitions of DIC are comparable.</p><p>Consider a different example that consists of three models. Model 1 is a two-indicator one-factor model example where we treat the factor as a parameter. Model 2 is the same as Model 1, but the variance of the factor is fixed to zero, which is equivalent to the model of two independent indicator variables. Model 3 is the model of two correlated indicators without any factors. In this example, the two different formulations of Model 2 yield the same DIC. Thus Model 2 DIC is comparable to Model 1 DIC. Model 2 DIC is also comparable to Model 3 DIC. However, Model 1 DIC is not comparable to Model 3 DIC (despite the fact that they are the same model); that is, model comparability is not transitive.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Stability of the DIC estimate</head><p>An additional complication that arises in the computation of DIC for the DSEM model is that when latent variables are treated as parameters, the number of parameters p D becomes so large and so many parameters have to be integrated through the MCMC iterations that the DIC precision is difficult to achieve. It is not unusual that convergence for the model parameters is easily achieved, but a stable DIC estimate requires many more iterations, and there might be cases where it is practically infeasible to obtain a stable DIC estimate. In such cases, the imprecision that remains could be bigger than the DIC difference in the models we are trying to compare.</p><p>Therefore, we recommend verifying that the DIC estimate has converged by running the MCMC estimation with different random seeds for the same model, and comparing the DIC estimates across the different runs to evaluate the precision of the DIC. Despite all these difficulties, the DIC is the most practical way to compare models when simple parameter significance tests are not enough.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Formal definition of the DIC for the DSEM model</head><p>The definition of the DIC consists of the list of latent variables that are treated as parameters. As in <ref type="bibr" target="#b2">Asparouhov and Muthén (2016)</ref>, for DIC with two-level and crossclassified models all random effect variables such as random loadings, random slopes, and random variances, as well as the random intercept variables Y 2;i and Y 3;t , are treated as parameters. In addition, any latent variable on the within level that is lagged in a time-series model is treated as a parameter; that is, any latent variable η 1;i;t that is also used on the right side of Equations 6 and 7 in its lagged version η 1;i;tÀl is treated as a parameter. Clearly, this increases the number of parameters p D of the DIC substantially, usually much more so than the between-level random effects. A between-level random effect increases p D by N, whereas a within-level lagged latent variable increases p D by N Á T. Similarly, the missing values for Y it for every dependent variable that is lagged, meaning it is used on the right side of Equations 6 and 7 in lagged form, is also treated as a parameter.</p><p>Given that these variables (i.e., latent variables and missing values) are conditioned, the variables Y it are independent across time and persons, and the likelihood is computed as follows:</p><formula xml:id="formula_15">logðPðY jθÞÞ ¼ X i;t logðPðY it jθÞÞ;<label>(24)</label></formula><p>where PðY it jθÞ is the likelihood for a single-level SEM model for individual i at time t. Thus, treating the lagged latent variables on the within level and the lagged missing data as model parameters makes the computation of the DIC quite simple. In principle, it is possible to compute the DIC unconditional on lagged latent variables and missing data; however, such a computation would be substantially more complex than Equation <ref type="formula" target="#formula_15">24</ref>, and would require an iterative algorithm for computing PðY it jY i1 ; ::Y i;tÀ1 Þ, which in turn might result in a substantially slower computational algorithm.</p><p>To summarize, the DIC can be used to compare two or more DSEM models if the list of latent variables that are treated as parameters is the same, and it is provided as standard output when doing DSEM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparing Sample Statistics and Their Corresponding Model-Estimated Quantities</head><p>Another array of possibilities for evaluating model fit is to compare sample statistics and their corresponding model-estimated quantities. This is particularly effective for the two-level DSEM model. Let μ i be the modelestimated mean for a single dependent variable Y for subject i. Let Y iÃ be the sample mean for subject i; that is, Y iÃ ¼ P T i t¼1 Y it =T i . From these two quantities we can compute the following statistics of model fit:</p><formula xml:id="formula_16">R ¼ Corðμ i ; Y iÃ Þ (25) MSE ¼ X N i¼1 ðμ i À Y iÃ Þ 2 =N : (26)</formula><p>Here R is the correlation between estimated and observed means across the clusters or individuals and MSE is the mean squared error of the estimated versus observed mean. If we compare two competing models, we want to select the model with smaller MSE and higher R, as it will better represent the data. Note that such model fit evaluation is useful not just for two-level DSEM models, but also for general two-level models.</p><p>It is important to realize that this comparison is most reliable under the condition that there is no missing data. When data are missing and missing at random (MAR) rather than missing completely at random (MCAR), the sample quantities Y iÃ will not necessarily be the mean of Y in cluster i and the model-estimated μ i could be the more accurate estimate for that mean. Cautious inference in the presence of missing data can still be made using R and MSE. However, these statistics are undeniably not as reliable as in the case of no missing data and discrepancy between model-estimated values and sample values could simply be the result of MAR and not MCAR missing data.</p><p>Note also that R and MSE can be computed for any observed model variable and statistic. For example, instead of the mean of Y we can compute the sample and model-estimated variance of Y. Another example is the covariance between two dependent variables CovðY 1 ; Y 2 Þ; that is, computing the correlation R between the cluster-specific model-estimated covariance and the cluster-specific sample covariance. Yet another example that is particularly of interest for the two-level DSEM model is to compute the correlation R between the subject-specific sample autocorrelation for a variable Y and the subject-specific model estimated autocorrelation of Y across the subjects.</p><p>Because there are many variables and many different statistics, one can expect that the R and MSE statistics can potentially disagree about which model represents the data better. Empirical data applications will yield more insight on that topic and whether such a disagreement is common. Note also that the DSEM model offers many more subject-specific estimated quantities than the standard two-level SEM model without any random structural coefficients. For example, in the standard two-level SEM, estimated variances are not subject-specific so R would be zero for all models and no model comparison can be performed that way.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DYNAMIC STRUCTURAL EQUATION MODELS</head><p>In Mplus the correlations R can be obtained for the means and the variance statistics within the Mplus between-level scatter plots simply by plotting the estimated against the sample quantities. The MSE is not reported in those plots but can easily be computed by saving the data of the plots and computing it in a separate step. In the Mplus residual output, other estimated statistics, such as covariance and autocorrelations, can be found as well.</p><p>The model-estimated means, variances, and covariances for the DSEM model are not computed the way they are computed for the SEM model. The details on this computation are given in Appendix D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SIMULATION EXAMPLES</head><p>In this section we illustrate the new DSEM framework with several simulation studies to highlight diverse key aspects in this context. First, we focus on the core issue of centering variables per person, and using a latent rather than a sample mean for this purpose. Second, we discuss the option of subject-specific variance. Third, we present two related models: the AR(1) with measurement error and the ARMA(1,1) model. Fourth, we discuss how to include a time-varying covariate in autoregressive models. Fifth, we present dynamic factor analysis, which includes latent variables to capture the common variance of multiple indicators. Sixth, we discuss the issue of unequally spaced data, which result from certain popular sampling schemes that are based on random measurement occasions. Finally, we discuss how time-specific effects can be studied using the DSEM framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Centering</head><p>In this first simulation study we show that the DSEM framework can be used to eliminate the dynamic panel bias, also known as Nickell's bias <ref type="bibr" target="#b21">(Nickell, 1981)</ref>. The example we use for this illustration is inspired by a simulation study reported in <ref type="bibr" target="#b13">Hamaker and Grasman (2015)</ref>. The sample consists of N individuals observed at times t ¼ 1; :::; T . We focus on the univariate random autoregressive AR(1) model given by the following equation:</p><formula xml:id="formula_17">Y it ¼ μ i þ ϕ i ðY i;tÀ1 À μ i Þ þ it : (27)</formula><p>The variable it is assumed to be white noise with mean 0 and variance σ w . The variables μ i and ϕ i have a bivariate normal distribution with mean parameters μ and ϕ, variances σ 11 and σ 22 , and covariance σ 12 . In the DSEM framework this model can be estimated directly. The predictor Y i;tÀ1 À μ i in Equation 27 is centered using a latent variable that represents the true mean μ i ; hence, we call this centering the latent centering.</p><p>In contrast to the latent centering model, we also consider the observed centering model,</p><formula xml:id="formula_18">Y it ¼ μ i þ ϕ i ðY i;tÀ1 À Y iÃ Þ þ it ; (28)</formula><p>where the predictor is now centered by the sample mean instead of the true mean for individual i. Equation 28 can be estimated as a standard two-level regression model. However, that estimation produces Nickell's bias for the parameter ϕ because the model does not account for the error in the sample mean estimate of the true mean. <ref type="bibr" target="#b21">Nickell (1981)</ref> also produced the following formula that approximates the bias</p><formula xml:id="formula_19">À 1 þ ϕ T À 1 : (29)</formula><p>We conduct a simulation study to evaluate Nickell's bias, generating data according to the model in Equation <ref type="formula">27</ref>and using the following parameter values (based on <ref type="bibr" target="#b13">Hamaker &amp; Grasman, 2015)</ref>:</p><formula xml:id="formula_20">μ ¼ 0; ϕ ¼ 0:3; σ 11 ¼ σ w ¼ 3; σ 22 ¼ 0:01;</formula><p>and σ 12 ¼ 0. The variance σ 22 is small so that the autoregressive parameter ϕ i remains in the ðÀ1; 1Þ range, as it is a correlation parameter. If the parameter ϕ i exceeds that range, VarðY it Þ will increase with time to infinity. To generate time-series data according to the model in Equation <ref type="formula">27</ref>, we need a starting value for the first time point. The standard way to resolve this ambiguity is to start at 0 but generate and discard the first few observations. That way the generated values stabilize and the effect of the original starting value of 0 is removed. We discard the first 10 values for each person.</p><p>In Table <ref type="table">1</ref> we report the simulation results for various values of N and T using 100 simulated data sets for each combination of N and T.</p><p>The results show that the DSEM latent centering approach resolves Nickell's bias and that the latent centering is superior to the observed centering. We also see that the bias is quite small for T ! 100. The simulation study shows also that Nickell's formula predicts the bias quite accurately.</p><p>It was noted in <ref type="bibr" target="#b13">Hamaker and Grasman (2015)</ref> that not centering the covariate also produces very good results for Nickell's bias; that is, we can replace Equation 28 with</p><formula xml:id="formula_21">Y it ¼ c i þ ϕ i Y i;tÀ1 þ it :<label>(30)</label></formula><p>We call this model the uncentered model. This model can also be estimated as a standard two-level regression model. The uncentered approach resolves Nickell's bias, but it produces a random intercept c i that does not represent an individual's mean μ i , as the lagged predictor does not have a mean of zero if it is not centered. This also implies that the variance of this parameter (i.e., the variance of the random effect) will deviate from the actual variance of μ i . In Table <ref type="table">2</ref> we report the bias for the variance of the random mean or intercept σ 11 using the uncentered method and the DSEM method.</p><p>Regarding the bias in the DSEM method for estimating the between-level parameter σ 11 , we can see that it is driven by the number of subjects N, and seems to disappear for N ! 100. The DSEM bias is guaranteed to disappear asymptotically as the method is equivalent to the ML method for large N. It is also known that the variance of the between-level effect when N &lt; 100 can be fine-tuned by using proper priors (see <ref type="bibr" target="#b4">Browne &amp; Draper, 2006)</ref>. For N &lt; 100 the effect of the prior is not negligible and selecting a weakly informative prior can reduce the bias substantially. In this simulation study we used improper and uninformative priors. We can also conclude that the uncentered method yields distortion on the between level and the bias seen in Table <ref type="table">2</ref> does not disappear asymptotically.</p><p>Note also that in the preceding simple AR(1) model, the uncentered model is a reparameterization of the latent centering model: c i ¼ μ i ð1 À ϕ i Þ. Such a reparameterization, however, does not exist for more complex models. For example, the uncentered and the latent centering models are not a reparameterization of each other if we add a predictor for the random effects.</p><p>This simulation uses a very simple DSEM model. The biases that we illustrated here for the observed centering method and the uncentered method will be difficult to track in more complicated models, especially because Nickell's bias can interact with the bias described by <ref type="bibr" target="#b18">Lüdtke et al. (2008)</ref>. We can also see clearly that the perils of the uncentered method are disadvantageous from an estimation point of view, as they remain in the model even with large samples.</p><p>Furthermore, the latent centering method used with DSEM is the only method that accommodates missing data, whereas both the observed centering and the uncentered method are essentially not available when there are missing values in the data. The covariate (i.e., the lagged predictor) cannot be constructed when the data point is missing and that means that if Y i;tÀ1 is missing, the equation containing Y i;t would have to be removed as well. Thus if the data contain 20% missing data, we might have to remove another 20% because the lagged predictor is missing. If the model we estimate is a second-order autoregressive process (i.e., an AR(2)), we might have to remove another 20%, because the only data points that can be used for model estimation are the ones for which the preceding two observations are not missing. This problem comes in addition to the well-known problems that occur when listwise deletion is used for dealing with missing data, particularly when the missing data are not MCAR. In contrast, DSEM is not based on listwise deletion, and missing data in combination with autoregressive relationships do not result in having to remove additional cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Subject-Specific Variance</head><p>In regular multilevel analysis the within-level variance is generally estimated to be a cluster-invariant parameter. Even if that parameter is not cluster-invariant, the assumption of invariance generally does not affect the estimation of the structural parameters. However, for DSEM models that is not the case. <ref type="bibr" target="#b15">Jongerling, Laurenceau, and Hamaker (2015)</ref> showed that ignoring the subject-specific variance can distort the structural parameters of the model, particularly when the subject-specific variance is correlated with other random effects in the model. In this section we reproduce this finding in the DSEM framework and discuss the general implications for DSEM modeling.</p><p>Consider the following simulation study based on the random autoregressive AR(1) model given by the following equations</p><formula xml:id="formula_22">Y it ¼ μ i þ ε it (31) ε it ¼ ϕ i ε i;tÀ1 þ it ;<label>(32)</label></formula><p>where now we include subject-specific residual variance through the normally distributed random effect v i v i ¼ LogðVarð it ÞÞ:</p><p>The three random effects ðμ i ; ϕ i ; v i Þ in this model are assumed to have an unrestricted multivariate normal distribution with mean ν ¼ ð2; 0:2; 0Þ and variance-covariance Σ where σ 11 ¼ 0:7, σ 22 ¼ 0:05, σ 33 ¼ 0:5, σ 12 ¼ σ 13 ¼ 0. Because the covariance parameter between ϕ i and v i appears to be the  <ref type="bibr">meter)</ref>. The results of the simulation are presented in Table <ref type="table">3</ref>.</p><p>The results show that the DSEM model without the random variance effect leads to bias and low coverage rates, whereas the DSEM model with the random variance effect shows no bias and good coverage rates. Model parameter distortions are directly caused by the correlation between the random autoregressive parameter ϕ i and the random variance parameter v i . The higher that correlation is, the bigger the distortions.</p><p>The random autoregressive parameter and the random residual variance are directly related via the following equation:</p><formula xml:id="formula_24">VarðY it jiÞ ¼ Expðv i Þ 1 À ϕ 2 i :<label>(34)</label></formula><p>Because of that strict relationship one can expect in practical applications that v i and ϕ i are fairly highly correlated and therefore one can expect the DSEM model results with random variances to differ somewhat from the results without random variances. In that case we can assume that the DSEM model with random variances will yield the more accurate results. The effect of ignoring the random residual variance on the estimation of the individual autoregressive parameters is even more dramatic than the effect on the parameters reported in Table <ref type="table">3</ref>. To compare the estimated individual autoregressive parameters φi with their true values, we compute the square root of the mean squared error; that is,</p><formula xml:id="formula_25">SMSE ¼ ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi ffi ð1=N Þ X i φi À ϕ i 2 s (35)</formula><p>and the correlation between the estimated individual autoregressive parameters and the true values; that is,</p><formula xml:id="formula_26">correlation ¼ Corð φi ; ϕ i Þ<label>(36)</label></formula><p>These quantities are computed for each of the 100 replications and the average values are reported in Table <ref type="table" target="#tab_2">4</ref>. The results show that the distortions in the estimates caused by ignoring the random variance effect go beyond simple inflation or deflation of the random parameters and the errors appear to have doubled from what they are for the nonrandom parameters. However, the cause of the increase in the SMSE error is somewhat more complex because it is not just due to the misspecification of the random variance effect. Consider, for example, the fact that the DSEM model with the random variance effect extracted a lot more information from the data and created v i , which is essentially a very good predictor for ϕ i on the between level. This will undeniably result in precision improvement for the ϕ i estimates. Such a phenomenon exists, of course, not just for DSEM models, but for regular two-level models as well; that is, even when the nonrandom parameter estimates are not distorted, adding a random variance effect will improve the estimation of the other random effects particularly when the random variance effect is correlated with those other effects.</p><p>In multivariate DSEM models we can further consider modeling not just random variances but also random covariances and random correlations. The easiest way to model random covariance in this framework is to model the covariance through a random variance of a common factor (cf. <ref type="bibr" target="#b12">Hamaker et al., 2017)</ref>. However, it is not as easy to evaluate the effect of random covariance on the model estimates, because even if the factor covariance is not random, the correlation between the variables is random when the variances are random. Some preliminary simulation studies, not reported here, indicate that the effect of random covariances might be more muted than  those of random variances and might require much larger samples to detect. Further simulation studies are needed on this topic.</p><p>The DSEM framework can accommodate seamlessly a large number of random effects and thus using models with random variances and covariances in many situations should be the preferred choice as long as the MCMC convergence is unhindered. Because of the increase in the number of random effects, the likelihood of the model with these random variances and covariances will be less pronounced and in some cases the MCMC convergence will be much slower. This should be taken as an indication that there is not sufficient information in the data to identify the DSEM model with random variances and covariances. In such situations, using cluster-invariant variances and covariances is not a poor choice by any means and unless these random variances and covariances are highly correlated with other random parameters, we see that the effects are somewhat negligible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ARMA(1,1) and the Measurement Error AR(1) Model</head><p>The ARMA(1,1) time-series model is given by the following equation:</p><formula xml:id="formula_27">Y t ¼ ν þ ϕY tÀ1 þ ε t þ θε tÀ1 :<label>(37)</label></formula><p>The model has four parameters; that is, ν, ϕ, θ and σ ¼ Varðε t Þ. The ARMA(1,1) process is stationary and invertible when the two parameters ϕ and θ are within the interval ðÀ1; 1Þ <ref type="bibr" target="#b10">(Greene, 2014)</ref>, and generally when used in practical applications we expect these two parameters to be within that range. The model-implied mean is ν=ð1 À ϕÞ and the model-implied variance is</p><formula xml:id="formula_28">VarðY t Þ ¼ σ 1 þ ðϕ þ θÞ 2 1 À ϕ 2 ! :<label>(38)</label></formula><p>The model-implied first autocorrelation is given by</p><formula xml:id="formula_29">ρð1Þ ¼ ðθ þ ϕÞð1 þ θϕÞ 1 þ 2θϕ þ θ 2 (39)</formula><p>and for lag l &gt; 1 the autocorrelation is given by ρðlÞ ¼ ϕ lÀ1 ρð1Þ:</p><p>(40) <ref type="bibr" target="#b9">Granger and Morris (1976)</ref> showed that this model is equivalent to the following measurement error AR(1) model under certain parameter restrictions:</p><formula xml:id="formula_30">Y t ¼ μ þ f t þ t (<label>41</label></formula><formula xml:id="formula_31">)</formula><formula xml:id="formula_32">f t ¼ ϕ f tÀ1 þ ε t :<label>(42)</label></formula><p>We call this model the measurement error AR(1) model-that is, MEAR(1)-because the latent variable f t follows an AR(1) process but is not observed directly; rather, it is measured with error by the observed variable Y t . Alternatively, this model is also sometimes referred to as AR(1)+WN <ref type="bibr" target="#b9">(Granger &amp; Morris, 1976)</ref>, where WN stands for the white noise process representing the measurement error. The four parameters in the MEAR(1) model are μ, ϕ, σ 1 ¼ Varð t Þ, and σ 2 ¼ Varðε t Þ. The relationship between the parameters in the two models is as follows. The mean parameter μ can be obtained from the first model using the expression μ ¼ ν=ð1 À ϕÞ. The autoregressive parameter ϕ is the same across the two models. The MEAR(1) parameters σ 1 (i.e., measurement error variance) and σ 2 (i.e., innovation variance, also referred to as dynamic error variance) can be derived from the ARMA(1,1) parameters via the following equations:</p><formula xml:id="formula_33">σ 1 ¼ À θσ ϕ (43) σ 2 ¼ ð1 þ θ 2 Þσ þ ð1 þ ϕ 2 Þθσ ϕ :<label>(44)</label></formula><p>The equivalence of the two models is subject to the parameter constraints that arise from the inequalities σ 1 &gt; 0 and σ 2 &gt; 0.</p><p>Under the regularity conditions of ϕ and θ being in the interval (-1, 1) the constraints can be further simplified to</p><formula xml:id="formula_34">ϕθ &lt; 0 (45) ϕ þ θ &gt; 0:<label>(46)</label></formula><p>Every MEAR(1) model can be represented as an ARMA (1,1) model, and an ARMA(1,1) model can be represented by a MEAR(1) model when Equations 43 and 44 produce positive variances, or equivalently when the inequalities in Equations 45 and 46 hold. In the most common situation the autoregressive parameter ϕ will be positive. Let's assume for now the case of ϕ &gt; 0. In that case it is interesting to note for the MEAR(1,1) model that the autocorrelation parameters for the latent variable are always larger or equal to those for the observed variable</p><formula xml:id="formula_35">Corðf t ; f tÀl Þ ! CorðY t ; Y tÀl Þ:<label>(47)</label></formula><p>For the ARMA(1,1) model this is not the case and the corresponding statement</p><formula xml:id="formula_36">ϕ l &gt; ρðlÞ (48)</formula><p>is precisely equivalent to θ being negative, which is the necessary condition for the ARMA(1,1) models to be equivalent to the MEAR(1) model; that is, these DYNAMIC STRUCTURAL EQUATION MODELS constraints are not coincidences and have meaningful interpretations.</p><p>The MEAR(1) model is much easier to interpret than the ARMA(1,1) model, especially in the social sciences applications where measurement error is common (cf. <ref type="bibr" target="#b24">Schuurman, Houtveen, &amp; Hamaker, 2015)</ref>. In cross-sectional studies it is not possible to identify the measurement error model when there is only one measurement, but as the MEAR(1) model clearly illustrates it is possible to do that in dynamic timeseries models.</p><p>The MEAR(1)/ARMA(1,1) model is generally preferred to the AR(1) model in the econometrics literature, as it offers more flexible autoregressive representation. The AR(1) model has an exponential decay of the autocorrelation function and the ARMA(1,1) autocorrelation decays slower. This is particularly important if we have to change time scale as is done with continuous time dynamic modeling. An AR(1) hourly autocorrelation of 0.75 implies a daily autocorrelation of 0.001. With reliability of 0.8 the MEAR(1) model hourly autocorrelation of 0.75 implies a daily autocorrelation of 0.212. Thus, the AR(1) model implies that observations in two consecutive days would be approximately independent, whereas the MEAR(1) model implies that some lag relations will remain across consecutive days, which is a more realistic assumption. The difference in the decay of the autocorrelation is illustrated in Figures <ref type="figure" target="#fig_0">1</ref> and<ref type="figure" target="#fig_1">2</ref>, which show typical decay for the autocorrelation for the AR(1) and ARMA(1,1) models.</p><p>In practical settings one can compute the sample autocorrelations and check if the decay is exponential and use this as the basis to decide if the AR(1) model is sufficient or that the ARMA(1,1) should be explored. It should be noted, however, that there are many other possibilities, such as the AR(2) model, the more general ARMA(p,q) model, or a MEAR(p) model, which is a special case of the ARMA(p,p) model (see <ref type="bibr" target="#b9">Granger &amp; Morris, 1976)</ref>.</p><p>We conduct a brief simulation study to evaluate the performance of the estimation of the MEAR(1)/ARMA(1,1) model and to evaluate the sample size needed to obtain satisfactory estimates. We use the N = 1 case with T = 100, 200, 300, 500. We use 100 replications in all cases. The results are presented in Table <ref type="table">5</ref>. We use the MEAR(1) model formulation given in Equations 41 and 42.</p><p>We can make the following conclusions from these results. The estimation of the ARMA(1,1) model is more difficult than the estimation of the AR(1) model. Good estimation where the bias is small and the coverage is near or above 90% needs at least T ! 200. The estimates are biased at T = 100 and coverage dropped to 82%. We  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>372</head><p>ASPAROUHOV, HAMAKER, MUTHÉN can also conclude that to estimate a two-level ARMA (1,1) model, with all four of the parameters as random subject-specific parameters, at least T ! 200 is needed per person. If such a sample size is not available, then one or two of the four ARMA(1,1) parameters should be held equal across individuals (i.e., should be nonrandom parameters). Most suitable are the two variance parameters σ 1 and σ 2 , because the bias in Table <ref type="table">3</ref> is smaller than the bias in Table <ref type="table">5</ref> for T = 100. When parameters are held equal across individuals, essentially the sample size used for the estimation changes from T to N Á T , and we can estimate a two-level ARMA(1,1) model with much fewer observations per person than we need for a single-level ARMA(1,1) model.</p><p>Next, we consider a two-level MEAR(1) model for categorical data. The DSEM framework has one limitation when it comes to categorical variables. Such variables cannot be lagged on their own but only through a factor. The MEAR(1) model essentially resolves this problem as it includes such a factor already and thus we can estimate a univariate autoregressive model with an observed categorical variable. If we attempt to estimate a subject-specific autoregressive model, such as the one in Equation <ref type="formula">27</ref>, where the autoregressive parameter is subject-specific, we will need a substantial sample size. Simulation studies, not reported here, indicate that for the N = 1 case the MEAR(1) model needs a sample size of about 10,000 for the binary case and about 1,000 for an ordered polytomous case with six categories. This is the kind of sample size we would need per subject if we want to estimate a subject-specific two-level MEAR (1) model. Although such sample sizes are sometimes realized in the case of physiological measurements and certain observational studies in which ratings are made continuously, they are less common in self-report studies using daily diaries or experience sampling.</p><p>However, if we estimate a two-level MEAR(1) model where the autoregressive parameter is not subject-specific, then the data from the different subjects are combined and many fewer observations will be needed per subject. Our simulation study uses N = 100 individuals with T = 300 time points and 100 replications. Because the autoregressive coefficient is estimated at the population level, we essentially have 100 × 300 = 30,000 observations to estimate this model, which is sufficient.</p><p>The MEAR(1) model we estimate for the binary variable is given by</p><formula xml:id="formula_37">PðY it ¼ 1Þ ¼ Φðμ i þ f it Þ (49) f it ¼ ϕf i;tÀ1 þ it (50) μ i ,N ðμ; σ b Þ; it ,N ð0; σ w Þ (51)</formula><p>The function Φ is the standard normal distribution function.</p><p>Note that for identification purposes the residual variance in Equation 41 is now fixed to 1. The model has four parameters: the grand mean μ, the variance of the individual means σ b , the fixed autoregressive parameter ϕ, and the within-level residual variance σ w .</p><p>The results of the simulation study are presented in Table <ref type="table" target="#tab_4">6</ref>. Parameter estimates appear to have no bias, but some of the parameters have low coverage. This can usually be resolved by running longer MCMC chains. Here we used a minimum of 1,000 MCMC iterations and convergence is determined by the potential scale reduction convergence criterion (see <ref type="bibr" target="#b1">Asparouhov &amp; Muthén, 2010)</ref>. Mixing with categorical variables is somewhat slower than with normally distributed variables and might require much longer MCMC chains. The current simulation takes 1 min per replication.</p><p>Additionally, we also consider a two-level MEAR(1) model with ordered polytomous variables. Using ordered polytomous variables in practical applications is one way to deal with nonnormally distributed dependent variables. The model is given by the following equations   </p><formula xml:id="formula_38">PðY it ¼ jÞ ¼ Φðτ jþ1 À μ i À f it Þ À Φðτ j À μ i À f it Þ (52) f it ¼ ϕf i;tÀ1 þ it (53) μ i ,N ð0; σ b Þ; it ,N ð0; σ w Þ<label>(54)</label></formula><p>The first τ 0 ¼ À1 and the last threshold τ J ¼ 1, where J is the number of categories of the observed variable. We conduct a simulation study using a six-category variable, N = 100, T = 100, and 100 replications. The results are presented in Table <ref type="table">7</ref>.</p><p>The parameter bias appears to be small and again we see some standard error underestimation that could potentially be resolved with running much longer MCMC chains. Each replication takes 2 min here. Because the outcome is ordered polytomous, we were able to estimate the model with only T = 100, which is much smaller than what is needed for a binary outcome. This is due to the fact that the ordered polytomous variable carries more information than the binary variable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>How to Add a Covariate in the MEAR(1) and AR(1) Models</head><p>The AR(1) model is nested within the MEAR(1) model, because if we set the parameter Varð t Þ ¼ 0 in Equation <ref type="formula" target="#formula_30">41</ref>(i.e., if we set the measurement error to zero), the model becomes equivalent to the AR(1) model. The following discussion applies to both the AR(1) and the MEAR(1) models.</p><p>There are three ways to add a covariate to the MEAR(1) model defined in Equations 41 and 42. The covariate can be used in either of the two equations, but it can also be used in both equations. When it is included in the measurement equation, we call it the direct model; that is,</p><formula xml:id="formula_39">Y t ¼ μ þ f t þ β 1 X t þ t (55) f t ¼ ϕ f tÀ1 þ ε t :<label>(56)</label></formula><p>In this model, X has a direct effect on Y, and because Y itself is not characterized by sequential relationships, each occasional score of X only affects the concurrent Y.</p><p>When the covariate is included in the transition equation, we call it the indirect model; that is,</p><formula xml:id="formula_40">Y t ¼ μ þ f t þ t (57) f t ¼ ϕ f tÀ1 þ β 2 X t þ ε t : (58)</formula><p>Here, X has an indirect effect on Y, through the latent variable f, and because the latter is characterized by an autoregressive process, preceding X scores also affect current and later Y scores. When the covariate is included in both equations, we call it the full model; that is,</p><formula xml:id="formula_41">Y t ¼ μ þ f t þ β 1 X t þ t (59) f t ¼ ϕ f tÀ1 þ β 2 X t þ ε t : (60)</formula><p>The full model has a direct and an indirect effect from X on Y.</p><p>The first issue that we have to address is the fact that the full model is not always identified. When the autoregressive parameter ϕ ¼ 0, the model is a standard SEM model and the direct and indirect effects on Y are equivalent and therefore the full model, which includes both effects, is not identified. In fact, if ϕ ¼ 0, the measurement error model is also not identified, because a one-indicator factor model is not identified in standard SEM. A second case where the full model is not identified is the two-level MEAR(1) model where the covariate is time invariant. If the covariate is time invariant, then the indirect and the direct model become equivalent. The specific relationship between their parameters is</p><formula xml:id="formula_42">β 2 ¼ β 1 1 À ϕ :<label>(61)</label></formula><p>This relationship holds because when the covariate is time invariant it is essentially equivalent to the μ parameter. If the μ parameter is moved from Equation <ref type="formula">59</ref>to Equation <ref type="formula">60</ref>, it will also be divided by ð1 À ϕÞ. Because the indirect and the direct models are equivalent, the full model is not identified under these circumstances.</p><p>Another covariate for which the indirect and direct model become statistically equivalent is when X t ¼ t; that is, the linear growth model (see <ref type="bibr" target="#b11">Hamaker, 2005)</ref>. In this case, the relationship between the parameters from the direct and the indirect model as provided in Equation 61 also holds. We formulate this equivalence for the AR (1) model but the same holds for the MEAR(1) model. The direct linear growth AR(1) model is formulated as follows:</p><formula xml:id="formula_43">Y t ¼ γ 0 þ γ 1 t þ t (62) t ¼ ϕ tÀ1 þ ε t :<label>(63)</label></formula><p>In contrast, the indirect linear growth AR(1) model can be formulated as follows:</p><formula xml:id="formula_44">Y t ¼ β 0 þ β 1 t þ ϕY tÀ1 þ ε t :<label>(64)</label></formula><p>Simple algebraic manipulations show that the direct and indirect models are algebraically equivalent and</p><formula xml:id="formula_45">γ 0 ¼ β 0 1 À ϕ À ϕβ 1 ð1 À ϕÞ 2 (65) γ 1 ¼ β 1 1 À ϕ (66)</formula><p>and the parameters ϕ and Varðε t Þ remain unchanged. The difference between the two models is that in the direct model, the autoregressive structure is imposed on the residuals variable t , whereas in the indirect model it is imposed on the observed variable Y t . This results in a different parametrization, where the parameters of the direct model (i.e., γ 0 and γ 1 ) have intuitive interpretations as the intercept and slope of the regression line that describes the underlying linear trajectory of the time series over time, whereas the parameters that form the indirect model (i.e., β 0 and β 1 ) have a less intuitive appeal.</p><p>A result of the equivalence between the direct and the indirect model in case of X t ¼ t is that the full model is unidentified. However, the equivalence between the direct and the indirect linear growth models does not translate completely in two-level models with subject-specific random parameters, especially when there are covariates predicting the random effects β j and ϕ. A linear relationship between a covariate and subject-specific β j and ϕ will result in a nonlinear relationship of that covariate with γ j and vice versa. Thus the two-level indirect linear growth model is not equivalent to the two-level direct linear growth model, which estimates a linear relationship between the covariate and γ j .</p><p>The equivalence between the direct and indirect model can also be shown to hold in case of a quadratic growth AR(1) model. The direct quadratic growth AR(1) model is</p><formula xml:id="formula_46">Y t ¼ γ 0 þ γ 1 t þ γ 1 t 2 þ t (67) t ¼ ϕ tÀ1 þ ε t :<label>(68)</label></formula><p>The indirect quadratic growth AR(1) model is</p><formula xml:id="formula_47">Y t ¼ β 0 þ β 1 t þ β 2 t 2 þ ϕY tÀ1 þ ε t :<label>(69)</label></formula><p>Simple algebraic manipulations show that</p><formula xml:id="formula_48">γ 0 ¼ β 0 1 À ϕ À ϕβ 1 ð1 À ϕÞ 2 þ β 2 ϕð1 þ ϕÞ ð1 À ϕÞ 3 (70) γ 1 ¼ β 1 1 À ϕ À 2ϕβ 2 ð1 À ϕÞ 2 (71) γ 2 ¼ β 2 1 À ϕ (72)</formula><p>while the parameters ϕ and Varðε t Þ remain unchanged. In fact, similar algebraic equivalence can be constructed with any polynomial growth AR(1) model. Again, as a result of the equivalence between the direct and the indirect model, the full model is unidentified when the predictor is a polynomial of t. Another conclusion that we can make is that the simple relationship given in Equation <ref type="formula" target="#formula_42">61</ref>, where one simply divides the direct effect by 1 À ϕ to obtain the equivalent indirect effect, does not hold except for the two cases of linear growth model and a between-level covariate (i.e., a time-invariant predictor). The relationship shown in the quadratic growth case is more complex and we see that the relationship does not depend only on the covariate but also on what other covariates there are in the model. When X t ¼ t, the relationship between the direct and the indirect effect changed after we added another covariate t 2 . The fundamental difference between the direct, indirect, and the full model becomes clear when using the conditional expectation EðY t jX Þ. For the direct model we have</p><formula xml:id="formula_49">EðY t jX Þ ¼ μ þ β 1 X t :<label>(73)</label></formula><p>This shows that in the direct model the condition expectation depends only on the current value of X t . Although this value of X might depend on the prior values of X, this is not a part of the model, as we model only the conditional distribution of Y given X. Hence, regardless of what the X t process is, it is clear that the conditional expectation of Y t depends only on X t and if there is any dependence on X tÀ1 it is only indirect through the effect of X tÀ1 on X t . The conditional expectation for the indirect model is</p><formula xml:id="formula_50">EðY t jX Þ ¼ μ þ β 2 ðX t þ ϕX tÀ1 þ ϕ 2 X tÀ2 þ ϕ 3 X tÀ3 þ :::Þ:<label>(74)</label></formula><p>Here, there is an accumulation of the effect of all prior values of X with diminishing influence when the model is stationary (i.e., ϕ j j&lt;1). The power ϕ l will converge to 0 as l increases and so will the effect of X tÀl on Y t .</p><p>For the full model we have</p><formula xml:id="formula_51">EðY t jX Þ ¼ μ þ β 1 X t þ β 2 ðX t þ ϕX tÀ1 þ ϕ 2 X tÀ2 þ ϕ 3 X tÀ3 þ :::Þ: (75)</formula><p>This shows that there is an accumulated effect of X t as well as a special direct effect exceeding the accumulating effect for the current value X t . In practical applications we can determine the type of influence a covariate should have (i.e., accumulated vs. direct) by estimating the full model and considering the significance of the two effects β 1 and β 2 .</p><p>To illustrate the performance of the full model, we make use of a two-level MEAR(1) model, but we stress that the simulation results using the AR(1) model are similar. The full two-level MEAR(1) model is given by</p><formula xml:id="formula_52">Y it ¼ μ i þ f it þ β 1 X it þ it (76) f it ¼ ϕf i;tÀ1 þ β 2 X it þ ε it ; (77)</formula><p>where μ i is a between-level random effect with mean μ and variance σ b . We generate and analyze 100 samples, where </p><formula xml:id="formula_53">β 1 ¼ 0:3, β 2 ¼ 0:4, ϕ ¼ 0:5, μ ¼ 0, σ b ¼ 0:7, Varð it Þ ¼ Varðε it Þ ¼ 1.</formula><p>The covariate X it is generated from an AR(1) process with VarðX it Þ ¼ 1 and autoregression ϕ x . We use three different values for ϕ x : 0, 0.5, and 0.8. Table <ref type="table" target="#tab_6">8</ref> contains the results of the simulation study for the structural parameters and the various values of ϕ x . The results show that the parameter estimates are unbiased, the coverage is acceptable, and the model is well identified.</p><p>Next we analyze the same data using the two-level direct and indirect MEAR(1) models. The results are presented in Tables <ref type="table">9</ref> and<ref type="table">10</ref>. For the effect of the covariate in these tables we used β 1 þ β 2 ¼ 0:7. Note, however, that there is no true value, as the model is misspecified. The results show that for both the direct and the indirect model, the estimated effect is not near 0.7, and that it is highly dependent on autocorrelation parameter ϕ x . Furthermore, the estimation of the autoregressive parameter of Y (i.e., ϕ) is distorted, and coverage appears insufficient for both the indirect and the direct model. It appears that the level of distortion in the model parameters is directly related to how close the indirect or the direct model is to the full model. The further away these models are from the full model, the bigger the biases.</p><p>It is also possible to estimate random direct and indirect effects in the full two-level MEAR(1) model, in addition to a random autoregressive effect. Table <ref type="table">11</ref> shows the results of a small simulation study with 100 replications, with N = 200 and T = 100. Note that we are able to estimate this twolevel random MEAR(1) model only with T = 100 due to the fact that only two of the four MEAR(1) parameters are subject-specific, whereas the two variance parameters are not random. The results show that the parameter estimates are unbiased, the coverage is acceptable, and the model is well identified, which implies that the model with a covariate and two random effects does not appear to complicate the model estimation.</p><p>The difference between the direct model (Equations 55-56) and the indirect model (Equations 57-58) can also be viewed as the difference between the general RDSEM model and the general DSEM model. The direct model separates the autoregression and the regression parts of the model, which is at the core of the RDSEM model. In the indirect model the autoregression and the regression are within the same equation as it is in the general DSEM model. When using the MEAR(1) model, both the direct and the indirect models fit well within the DSEM framework. However, when using the AR(1) model (i.e., the measurement error is fixed to 0) the direct model fits well in the RDSEM framework and the indirect model fits well in the DSEM framework. That is, the direct AR(1) model is most efficiently estimated as an RDSEM model, which avoids specifying zero measurement error variance.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dynamic Factor Analysis</head><p>Most of the dynamic factor analysis models considered previously have been for the case of N = 1; that is, when single-subject multivariate time-series data are fitted with a factor analysis model across time. The DSEM framework described here includes dynamic factor analysis models for an entire population rather than a single subject only. The two most common dynamic factor models are the direct autoregressive factor score (DAFS) and the white noise factor score (WNFS) models (see <ref type="bibr" target="#b19">Molenaar, 1985;</ref><ref type="bibr" target="#b30">Zhang &amp; Nesselroade, 2007)</ref>.</p><p>The DAFS model is given by the following equations:</p><formula xml:id="formula_54">Y t ¼ ν þ Λη t þ ε t (78) η t ¼ X L l¼1 B l η tÀl þ t :<label>(79)</label></formula><p>The WNFS model is given by the following equation:</p><formula xml:id="formula_55">Y t ¼ ν þ X L l¼0 Λ l η tÀl þ ε t :<label>(80)</label></formula><p>Comparing these models shows that in the DAFS model only the current factor affects the observed variables, whereas in the WNFS model the observed variables are also affected directly by the preceding factor values. In addition, the factors in the DAFS model behave as an autoregressive process of order L, whereas the factors in the WNFS model are independent across time; that is, they behave as a white noise process. The implications for the observed variables are also different. Based on <ref type="bibr" target="#b9">Granger and Morris (1976)</ref>, it can be determined that the observed variables in the WNFS model follow an MA(L) process, whereas the observed variable in the DAFS model follows an ARMA(L,L) process. Furthermore, note that the DAFS model for L = 1 is the MEAR(1) model for each factor indicator.</p><p>In practical applications, the question might arise which one of these two factor analysis models should be used. <ref type="bibr" target="#b20">Molenaar (2017)</ref> considered a hybrid DAFS + WNFS model that is nested above both the DAFS and the WNFS models; that is,</p><formula xml:id="formula_56">Y t ¼ ν þ X L l¼0 Λ l η tÀl þ ε t (81) η t ¼ X L l¼1 B l η tÀl þ t :<label>(82)</label></formula><p>This model is referred to as a DFM(p,q,L,L,0), where p refers to the number of observed variables in the factor model and q refers to the number of factors in the model. It is interesting to note that the hybrid DAFS + WNFS model is equivalent to a DAFS model where the factor follows an ARMA(L,L) process if the loadings Λ l are proportional in the one-factor model, or can be rotated into the same loadings in the multivariate case. Such a model would be referred to in <ref type="bibr" target="#b20">Molenaar (2017)</ref> terminology as a DFM(p,q,0,L,L) model. The hybrid model is also interesting because it illustrates how DSEM models differ from SEM models. In SEM models it is not possible to identify a model where a factor predictor is also a direct predictor for all indicator variables, whereas in the hybrid DAFS + WNFS this is possible.</p><p>In the following simulation study we illustrate the performance of the estimation method for a two-level hybrid DAFS + WNFS model. We use an L = 1 model with five indicators and one factor. We generate and analyze 100 samples with N = 100 and T = 100. The two-level model also has a between-level factor model and the full model is given by the following equations:  Note. DIC = deviance information criterion; DAFS = direct autoregressive factor score; WNFS = white noise factor score.</p><formula xml:id="formula_57">Y it ¼ Y 1;it þ Y 2;i (83) Y 1;it ¼ Λ 0 η 1;t þ Λ 1 η 1;tÀ1 þ ε 1;t (84) η 1;t ¼ ϕη 1;tÀ1 þ t (85) Y 2;i ¼ ν þ Λ b η 2;t þ ε 2;t<label>(86)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DYNAMIC STRUCTURAL EQUATION MODELS</head><p>We generate the data using the following parameter values that, for simplicity, are identical across the five indicators. For j ¼ 1; :::; 5 we set the within-person lag 0 factor loadings λ 0;j ¼ 1, and the lag 1 factor loadings λ 1;j ¼ 0:6, the withinperson measurement error variances θ 1;j ¼ Varðε 1;t;j Þ ¼ 1, the autoregressive parameter ϕ ¼ 0:4, the innovation variance ψ 1 ¼ Varð t Þ ¼ 1, the between-person intercepts ν j ¼ 0, the between-person factor loadings λ b;j ¼ 0:5, the between-person factor variance ψ 2 ¼ Varðη 2;t Þ ¼ 1, and the betweenperson residual variances Varðε 2;t;j Þ ¼ 1.</p><p>For identification purposes, we fix the variance ψ 1 and ψ 2 to 1. Table <ref type="table" target="#tab_10">12</ref> contains the results of the simulation study for a selection of the model parameters. The estimates show no bias and good coverage is obtained.</p><p>Additionally, we illustrate how the DIC criterion can be used for model selection. We estimate the two-level DAFS model, the two-level WNFS model, and the two-level hybrid DAFS + WNFS model, using the same generated data. In all three models we use the correct one-factor model on the between level. The average DIC values across 100 replications are given in Table <ref type="table">13</ref>. In each replication we compare the DIC across the three models and select the model with smallest value. In 99 out of 100 replications the correct WNFS + DAFS model had the smallest DIC value; that is, the DIC performed well in identifying the correct model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Subject-Specific and Uneven Times of Observations</head><p>In this section we illustrate the quality of the estimation when the timing of observations varies across individuals, and when the observations are unevenly spaced. To this end, we conduct two different simulation studies. The first study is based on a two-level DAFS AR(1) model and the second is based on a two-level AR(1) model. The estimation algorithm described in Appendix A indicates that the quality of the estimation depends on the amount of missing data inserted between the observed values and how accurately the original times of observations are approximated by the integer grid that is used in the DSEM estimation. The more accurate the approximation, the more missing data will be inserted.</p><p>In the first simulation study we want to see how the percentage of missing data affects the parameter estimates, convergence rates, and speed of the estimation. We generate samples with 100 individuals using the same two-level AR (1) model we used in the previous section with the exception that we set Λ 1 to 0 so that the model is simply a DAFS AR(1) model rather than a hybrid model. We generate T observations for each individual and mark m percent of these observations as MAR. To be more precise, for each individual, each time point is marked as missing with probability m, and is removed from the data set. We consider four different values for m: .80, .85, .90, and .95; that is, the simulation study will have between 80% and 95% missing values. We also vary T as a function of m, and we set T ¼ 60=ð1 À mÞ, which implies that on average after the missing values are removed each individual will have 60 observations taken at various uneven and unequal times. For each value of m we generate and analyze 20 data sets.</p><p>The results of this simulation study are given in Table <ref type="table" target="#tab_2">14</ref>. We report the average estimates and coverage for the autoregressive parameter ϕ for the within-level factor, the convergence rate for the estimation, and the computational time per replication. The results show that in this model the quality of the estimation deteriorates as the amount of missing data reaches 90%. As the amount of missing data increases, the computational time increases, the number of convergence problems increases, and the quality of the estimates decreases in terms of bias and coverage. However, the results are acceptable for 80% or 85% missing values. Apparently, adding too many missing values between the observed data can destabilize the MCMC estimation.</p><p>In the second simulation study, we use the simpler twolevel AR(1) model</p><formula xml:id="formula_58">Y it ¼ μ i þ ϕðY i;tÀ1 À μ i Þ þ ε it (87) μ i ,N ðμ; vÞ:<label>(88)</label></formula><p>We use μ ¼ 0, v ¼ 0:5, Varðε it Þ ¼ 1, and ϕ ¼ :8 to generate the data. We again set N = 100 and T ¼ 60=ð1 À mÞ, where m is the percentage of missing data. In this simulation, we consider only two values for m, 0.80 and 0.95. The missing data are generated at random, and that generates subjectspecific times of observations; for example, when m = .95, T = 1,200, and each individual has approximately 60 observations that occur at times between 1 and 1,200.</p><p>In this simulation, we vary the interval δ used in Appendix A, which determines the grid fineness. This interval is specified in Mplus using the tinterval option. We estimate the two-level AR(1) model using different values of δ ¼ 1; 2; 3; 4; 5; 10. The case of δ ¼ 1 is the original time scale. As δ increases we use a more and more crude time scale, worsening the time scale approximation. Note also that we cannot directly compare the models using different vales of δ. Denote by ϕ j the estimated autoregression coefficient for δ ¼ j. This is also the autocorrelation at lag j for the original process, and therefore ϕ j ¼ ϕ j 1 . To compare the models, we use ϕ 1=j j , which is the implied estimate for ϕ 1 ¼ ϕ ¼ 0:8. Note here that when we use δ &gt; 1 the data will be rearranged and a different amount of missing data will be inserted. We denote these missing data as m 2 . These are the missing data that are being used in the analysis. For each of the values of m we generate 100 data sets and we analyze those with the various values of δ.</p><p>The results are presented in Table <ref type="table">15</ref>. In all cases the rate of convergence is 100%. This means that simpler models like the two-level AR(1) model can tolerate more missing data than the more complex models like the DAFS AR(1) model considered earlier. We can also see from the results that when using a cruder scale (i.e., a larger δ), the results are more biased. It is also somewhat clear that it will be impossible to establish a clear rule of thumb for δ and the amount of missing data that should be used. These quantities are probably going to remain specific to the particular examples. However, the overall trends are clear. The smaller δ is, the better the estimates are, but if δ is too small (and the inserted missing data are too big), there might be convergence problems for the MCMC algorithm.</p><p>In practical applications, when estimating an AR(1) model and we want to verify that a particular value of δ is sufficiently small, we can simply compare the results for the autoregressive parameter using δ and δ=2. If ϕ δ % ϕ 2 δ=2 we can conclude that δ is sufficiently small. If that is not approximately true, then we should consider this as evidence that δ should be decreased, or that the AR(1) model does not hold. When using this method with our simulated data for the case of m = .80, we obtain ϕ 1 ¼ 0:8002 and ϕ 2 0:5 ¼ 0:7998, which confirms that δ ¼ 1 is sufficiently refined as it yields the same model as the more precise δ ¼ 0:5. Note here that if the model is a more complicated time-series model, rather than a simple AR(1) model, the connection between the time series model for Y t and the model for Y 2t is much more complicated. This problem is somewhat compounded by the fact that such a question has not been of interest in the econometric literature, whereas it is of interest in the social sciences and this DSEM framework particularly for the purpose of addressing subjectspecific and uneven times of observations. In this regard, the RDSEM model has an advantage over the DSEM model because it is not as dependent on the time interval δ. In the RDSEM model the autoregressive equations involve only the residuals. Thus changing the time scale will affect only the residual model, and the structural part of the RDSEM model will remain the same.</p><p>Overall, it appears that the optimal amount of inserted missing data should be somewhere between 80% and 95%, depending on how complex the model is. This corresponds to 5% to 20% present data and covariance coverage as reported in the Mplus output. In a practical setting one should, of course, consider interpretability in the choice of δ. For instance, if times of observations are recorded in a "days" metric, choosing δ to represent 1 day is the most natural choice and it will preserve the interpretability of the model.</p><p>It is also worth noting here that when δ values increase to a sufficiently large value the amount of missing data converges to 0%, which means that the time scale is completely ignored and the times of observations are set to 1; 2; :::; that is, they are assumed to be consecutive. In our example this happened for m = 0.80 and δ ¼ 10. The estimate of the autocorrelation coefficient is 0.92 which is ϕ 0:1 10 ; that is, the raw estimate of the autoregression is ϕ 10 ¼ 0:45 % 0:92 10 . This is the autoregression that one would get by estimating the data and ignoring the subjectspecific and uneven times of observations. Such an estimate, of course, is quite different from the true value of 0.8.</p><p>There are many other ways to deal with subject-specific and uneven times of observations. For example, continuous time modeling can be performed using Brownian motion theory, or using dynamic models based on differential equations (e.g., <ref type="bibr" target="#b8">Deboeck &amp; Preacher, 2016;</ref><ref type="bibr" target="#b22">Oravecz, Tuerlinckx, &amp; Vandekerckhove, 2011;</ref><ref type="bibr" target="#b28">Voelkle &amp; Oud, 2013)</ref>. Another possible approach is to use the times between consecutive observations in the model to reflect the strength of the relationship between the observations; that is, having the autoregressive and cross-lagged parameters depend on the distance between the observations. Yet another method is to use the same approach of missing data insertion, but to change the algorithm described in Appendix A. As described there, the algorithm focuses on global time scale matching. A different algorithm that focuses on matching consecutive time differences could potentially yield more accurate results. Such alternative algorithms can easily be studied with Mplus by preprocessing the continuous times of observations before employing the DSEM analysis. Clearly, this is a vast research topic and there are many Note. Estimates and coverage for ϕ and amount of missing data m 2 during the analysis.</p><p>DYNAMIC STRUCTURAL EQUATION MODELS possibilities for improving the treatment described here. The main advantages of the method we chose are that it can fit smoothly in the general framework, it applies to all models, and it works fairly well, as the preceding simulations show.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Time-Specific Effects</head><p>In the models thus far, the parameters were allowed to differ across individuals, but were assumed to be stable over time.</p><p>Here we illustrate the TVEM feature that is implemented in the DSEM framework. To this end, we consider a MEAR(1) model with a covariate where the random random intercept and random slope evolve over time, and is given by</p><formula xml:id="formula_59">Y it ¼ μ t þ Y i þ β t X it þ f it þ ε it (89) f it ¼ ϕ f i;tÀ1 þ it<label>(90)</label></formula><p>In this model Y i is a subject-specific random effect, and μ t and β t are time-specific random effects. We generate a single data set with 500 individuals, each observed at Times 1, 2, …, 50, using the following parameters:</p><formula xml:id="formula_60">θ b ¼ VarðY i Þ ¼ 0:5, θ w ¼ Varðε it Þ ¼ 0:5, ϕ ¼ 0:5, and ψ ¼ Varð it Þ ¼ 1:2.</formula><p>We generate the covariate X it from a standard normal distribution.</p><p>The time-specific effects μ t and β t are generated from arbitrary functions of time. In this simulation we use a logarithmic function for μ t and a quadratic function for β t as follows:   use the DSEM framework to estimate this model as a first step in an exploratory fashion. Because there are 500 observations at each time point (because there are 500 persons), the prior assumptions, N ðμ; v μ Þ and N ðβ; v β Þ, for these two random effects will have only a minor (if any) effect on the estimates; that is, the estimates of μ t and β t will be dominated by the data.</p><formula xml:id="formula_61">μ t ¼ g 1 ðtÞ ¼ logðtÞ (91)</formula><formula xml:id="formula_62">β t ¼ g 2 ðtÞ ¼ a þ bt þ ct 2 ¼ 0:001 Á t Á</formula><p>Table <ref type="table" target="#tab_4">16</ref> contains the results for the nonrandom parameters of this analysis. The estimates are near the true values and the credibility intervals contain the true value for all four parameters. Figures <ref type="figure" target="#fig_3">3</ref> and<ref type="figure">4</ref> show the estimated values of μ t and β t compared with the true values given by g 1 ðtÞ and g 2 ðtÞ. It shows that the estimated values trace the true curves well. In fact, the correlation between the true and estimated values for μ t is 0.993 and for β t it is 0.953. The SMSE for μ t is 0.157 and for β t it is 0.057.</p><p>Based on the clear trends that are found in this exploratory TVEM-DSEM analysis, the next step of the analysis is to incorporate these trends in the DSEM model by creating predictors for μ t and β t that account for the trends. The predictors are essentially smoothing curves for the estimated values obtained in the exploratory analysis. Such curves can be constructed through a separate algorithm, using the estimated μ t and β t values, or using multiple imputed values for μ t and β t . The smoothing can be done through polynomial functions or splines as in <ref type="bibr" target="#b5">Buja, Hastie, and Tibshirani (1989)</ref>. These smoothed curves can be entered into the DSEM model as predictors of μ t and β t .</p><p>Alternatively the smoothing can be performed within the DSEM framework as follows. We add time-specific predictors of μ t and β t based on the shapes of the trends. Given the estimated values we add logðtÞ as the predictor for μ t , and t and t 2 as the predictors of β t so that it is modeled as a quadratic function. Thus, we augment the model given in   Equations 89 and 90 with the following two equations, with some added scaling for the predictors,</p><formula xml:id="formula_63">μ t ¼ a 1 þ a 2 logðtÞ þ 1;t (93) β t ¼ a 3 þ a 4 ð0:05tÞ þ a 5 ð0:001t 2 Þ þ 2;t :<label>(94)</label></formula><p>The results of this analysis are presented in </p><formula xml:id="formula_64">Y it ¼ a 1 þ a 2 logðtÞ þ Y i þ a 3 þ a 4 ð0:05tÞ ð þ a 5 ð0:001t 2 Þ Á X it þ f it þ ε it (95) f it ¼ ϕ f i;tÀ1 þ it :<label>(96)</label></formula><p>The coefficients a 4 and a 5 are the interaction effects of X it with t and t 2 . The results for this analysis are presented in Table <ref type="table" target="#tab_6">18</ref>. All parameter estimates are very close to the true values. Note that in this model the effects μ t and β t are now smooth curves with no error term, which is how we generated the data. Because the parameter estimates are so close to the true values, these curves are virtually indistinguishable from the true value curves. The correlation for both estimated effects and their corresponding true values is 1, and the SMSEs are now further reduced to 0.021 and 0.017.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CONCLUSION</head><p>The DSEM framework builds on the econometric literature and recent advancements in time-series modeling, single-level dynamic structural modeling, and multilevel SEM. The DSEM framework allows us to combine timeseries models for a population of subjects. One of the strengths of the framework is that it allows subject-specific structural and autoregressive parameters. These parameters can be used further for structural modeling on the population level; that is, they can be predicted by subjectspecific variables, or they can be used as predictors of other such variables.</p><p>Although the multilevel time-series models with random effects, which allow for individual differences in the parameters that describe the dynamics, are appealing to many researchers, we also want to stress the value of the opposite here. In the DSEM framework, autoregressive and structural parameters can also be chosen to be nonrandom (i.e., invariant across subjects in the population). When the number of time points is within the midlength range of 10 to 100, which is the most common range in the social sciences, parameters invariant across subjects are essential in allowing us to expand the model complexity beyond what is accessible with single-level DSEM models. The inclusion of nonrandom parameters gives us the ability to combine data across the population to obtain more accurate time-series and structural parameters. Perhaps the real strength of DSEM, though, is the fact that it seamlessly can accommodate random and nonrandom parameters at the same time, not just to improve the quality of the estimation and the quality of the statistical methodology effort to match the data and the models, but also to use data analysis to find answers to real-life questions hidden in the data. over integer and unequal values tij in most common situations. In some situations this algorithm will not quite minimize the preceding objective function, but it will come fairly close to minimizing it. Full minimization might be too intricate to accomplish in general because of the discrete optimization space. This algorithm as implemented in Mplus would report max tij À t ij =δ if this quantity is greater than 5, which means that an observation had to be shifted more than five intervals away from its original assignment. This would suggest that the discretized grid constructed for that value of δ is too crude to be considered a good approximation and a smaller value of δ should be used.</p><p>Step 2: Time Shift Transformation</p><p>The next step of the time transformation is a time shift transformation.</p><p>There is a fundamental difference between the cross-classified DSEM model and the two-level DSEM model that comes into play here. In cross-classified DSEM models we estimate time-specific effects and this can be meaningful only if the time scale is aligned between individuals. In cross-classified DSEM, time t for individual i = 1 should have the same meaning as as time t for individual i = 2, for example, the number of days since an intervention that both individuals received, so that the same timespecific random effect s t;3 applies. Such an alignment of time is not needed for the two-level DSEM model and this is why the time shift transformation is different for the two models.</p><p>For cross-classified DSEM models we compute T 0 ¼ min i;j ð tij Þ and we shift the time so that we start at 1, b tij ¼ tij À T 0 þ 1. At least one individual is observed at Time 1 and this is the earliest time an observation was made in the sample. Missing values are recorded for all individuals and time points not in the set b tij . For each individual, the missing values beyond the last observed value are not analyzed. This time shift is done differently for two-level DSEM models. We compute T 0i ¼ min j ð tij Þ; that is, we find the first observed value for each individual i and shift each individual by that value so that every individual starts at 1; that is, b tij ¼ tij À T 0i þ 1. This minimizes that amount of missing data that will have to be analyzed and imputed in the MCMC estimation. Again all missing data after the last observed value are not analyzed. The difference in the time shift transformation is that in the cross-classified model we shift the time uniformly across all individuals, whereas in the two-level model the time scale is shifted for each individual separately.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>How to Choose δ</head><p>The transformation is determined by time interval δ. The smaller this value is, the more precise the approximation. However, the smaller the value is, the more missing data will be interspersed between the observed data. This will cause the MCMC sequence to converge slower. It will also cause the model to lose some precision. Consider, for example, trying to estimate the daily autocorrelation ϕ d by first estimating the hourly autocorrelation ϕ h using an AR(1) model. The relationship between the two is given by ϕ d ¼ ϕ 24 h . If ϕ d ¼ 0:75 then ϕ h ¼ 0:988. A small error in the estimation of ϕ h , say 0:987, results in bigger error for ϕ d as it will be estimated to 0.73. Thus model imprecision is amplified for smaller δ.</p><p>The selection of δ should be driven by three principles. First is the interpretability. Using natural δ values such as an hour, a day, a 2-day interval, a week, or a month would improve the interpretability as opposed to, say, a time metric such as 1.3 days. The second consideration is the amount of missing data resulting in this process. The missing data should be no more than 90% to 95% of the data. More missing data than that will likely yield a slow converging MCMC estimation that potentially can produce bigger error in the estimation than the discrete time approximation for larger δ values. The third consideration should be that δ needs to be small enough so that the original times are well approximated. Using a large value of δ will result in b tij ¼ j in two-level DSEM models; that is, the information in the original times t ij is completely ignored and all observations are assumed equally spaced.</p><p>There is one further consideration that applies only to the cross-classified DSEM model. The smaller the δ value is, the more time periods there will be. Because the DSEM model estimates time-specific random effects for each interval, it is desirable that each period has at least several observations, which act as measurements for the time-specific effects. A simple rule of thumb would be to have at least three observations per random effect at each time point. Thus the δ value should not be chosen to be so small as to reduce the number of observations below that level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX B DSEM MODEL ESTIMATION</head><p>Here we describe the conditional distributions for each of the 13 blocks given in the text. These are used in the MCMC estimation to update each block. The conditional distributions we are interested in are the conditional distributions for each block conditional on all other blocks and the data.</p><p>Consider the conditional distribution of B1. Given that we condition on B3, the variables Y 3;t are considered known. All other random and nonrandom slopes and loadings are also considered known. Let Y 1;it 0 ¼ Y it À Y 3;t . Equation 6 can be expressed as</p><formula xml:id="formula_65">Y 0 1;it À Y 2;i ¼ ðI À R 0 Þ À1 ν 1 þ X L l¼0 ðI À R 0 Þ À1 Λ 1;l η 1;i;tÀl þ X L l¼1 ðI À R 0 Þ À1 R l ðY 0 1;i;tÀl À Y 2;i Þ þ X L l¼0 ðI À R 0 Þ À1 K 1;l X 1;i;tÀl þ ðI À R 0 Þ À1 ε 1;it (B:1) or equivalently Y 0 1;it À ðI À X L l¼1 ðI À R 0 Þ À1 R l ÞY 2;i ¼ ðI À R 0 Þ À1 ν 1 þ X L l¼0 ðI À R 0 Þ À1 Λ 1;l η 1;i;tÀl þ X L l¼1 ðI À R 0 Þ À1 R l Y 0 1;i;tÀl þ X L l¼0 ðI À R 0 Þ À1 K 1;l X 1;i;tÀl þ ðI À R 0 Þ À1 ε 1;it (B:2)</formula><p>where I denotes the identity matrix. The conditional distribution of Y 2;i is now determined by the log-likelihood of the preceding equation in conjunction with Equation 2. Denote FðY 2;i Þ FðY 2;i Þ ¼ X t LðY 0 1;it ÃÞ þ LðY 2;i ÃÞ;</p><p>(B:3)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>384</head><p>ASPAROUHOV, HAMAKER, MUTHÉN where LðY 0 1;it jÃÞ is the log-likelihood expression of Equation B.2 and LðY 2;i jÃÞ is the likelihood expression of Equation <ref type="formula">2</ref>. Because all these equations are for normal distributions, the conditional distribution of Y 2;i is given by Y 2;i ,N ðF 00 À1 F 0 ð0Þ; F 00 À1 Þ;</p><p>(B:4)</p><p>where F 0 and F 00 denote the first and the second derivative of the log-likelihood function F. Note that because F is a quadratic function of Y 2;i the second derivative is a constant matrix that does not depend on the value of Y 2;i : Another way to compute this is as follows. Let</p><formula xml:id="formula_66">M ¼ ðI À P L l¼1 ðI À R 0 Þ À1 R l Þ.</formula><p>The conditional distribution of MY 2;i can be computed as follows. The variable MY 2;i is the random intercept of a twolevel model where the within-level model is given by Equation B.2 and the between-level model is given by Equation 2 multiplied by M so that MY 2;i is the dependent variable on the between level as well. The conditional distribution of the random intercept in a standard two-level model is well known. If the conditional distribution of MY 2;i is N ðm; vÞ then the conditional distribution of Y 2;i is N ðM À1 m; M À1 vðM À1 Þ T Þ.</p><p>The conditional distribution of B2 is similar. Conditional on all other blocks, Y 2;i and Y 3;t are considered known, which means that Y 1;it is known, as well as η 1;it . The joint conditional distribution of all s 2;i come from Equations 9 and 10 as well as a reformulation of Equation 3 that expresses s 2;i as a dependent variable on the left side and other variables on the right side. Denote again by F the log-likelihood function Fðs</p><formula xml:id="formula_67">2;i Þ ¼ X t LðY 1;it jÃÞ þ X t</formula><p>Lðη 1;it jÃÞ þ Lðs 2;i jÃÞ;</p><p>(B:5)</p><p>where LðY 1;it jÃÞ is the log-likelihood contribution of Equation <ref type="formula">9</ref>, written directly as it is expressed in that equation, Lðη 1;it jÃÞis the log-likelihood contribution of Equation 10, also written directly as it is expressed in that equation and Lðs 2;i jÃÞ is the log-likelihood contribution of Equation 3. All these distributions are normal and thus the function F is again a quadratic function of s 2;i and the conditional distribution can be obtained as in Equation B.4. s 2;i ,N ðF 00 À1 F 0 ð0Þ; F 00 À1 Þ: (B:6)</p><p>There is a key assumption in this procedure, which can be viewed also as a model restriction. Equations 9 and 10 can be used directly to write the likelihood only under the assumption that there are no nonrecursive interactions in the model. That is to say that the dependent variables Y 1;it cannot appear in a cyclical fashion in these equations; that is, no two components of that vector, say, Y 1;it1 and Y 1;it2 , can simultaneously be predictors of each other. Also longer cyclical regressions involving three or more variables cannot appear in the model. Such a restriction is needed to preserve the quadratic form of F and to preserve the integrity of the likelihood obtained directly from these equations. If the equations are nonrecursive then F is not quadratic and those equations cannot be used directly to write the log-likelihood. When the equations are recursive they can be ordered in such a way that the ½Y 1;it1 Y 1;it2 ; Y 1;it3 :::½Y 1;it2 Y 1;it3 :::::: conditional distributions are expressed precisely by Equation <ref type="formula">9</ref>. The same applies to Equation <ref type="formula">10</ref>. The conditional distribution of block B3 is slightly more complicated than the conditional distribution of block B1. Let Y 1;it 0 ¼ Y it À Y 2;i . Equation 6 can be expressed as  <ref type="formula">4</ref>. Because all these equations are for normal distributions, the conditional distribution of Y 3;t is given by Y 3;t ,N ðF 00 À1 F 0 ð0Þ; F 00 À1 Þ;</p><formula xml:id="formula_68">Y 0 1;it À Y 3;t ¼ ðI À R 0 Þ À1 ν 1 þ X L l¼0 ðI À R 0 Þ À1 Λ 1;l η 1;i;tÀl þ X L l¼1 ðI À R 0 Þ À1 R l ðY 0 1;i;tÀl À Y 3;tÀl Þ þ X L l¼0 ðI À R 0 Þ À1 K 1;l X 1;i;tÀl þ ðI À R 0 Þ À1 ε 1;it (B:7) or equivalently Y 0 1;it À ðY 3;t À X L l¼1 ðI À R 0 Þ À1 R l Y 3;tÀl Þ ¼ ðI À R 0 Þ À1 ν 1 þ X L l¼0 ðI À R 0 Þ À1 Λ 1;l η 1;i;tÀl þ X L l¼1 ðI À R 0 Þ À1 R l Y 0 1;i;tÀl þ X L l¼0 ðI À R 0 Þ À1 K 1;l X 1;i;tÀl þ ðI À R 0 Þ À1</formula><p>(B:10)</p><p>where F 0 and F 00 denote the first and the second derivative of the loglikelihood function F.</p><p>Another way to compute this posterior distribution is as follows. Denote by B 0 ¼ I, B l ¼ ÀðI À R 0 Þ À1 R l . The random intercept of Equation B.8 is</p><formula xml:id="formula_69">A t ¼ P L l¼0</formula><p>B l Y 3;tÀl . Suppose that the conditional distribution of that random intercept A t computed from the data in that cluster is N ðm t ; v t Þ, excluding a between-level model. The conditional distribution of Y 3;t , conditional on all other Y 3;t 0 where t 0 Þt is given by DYNAMIC STRUCTURAL EQUATION MODELS of Y 2;i , which is estimated within the MCMC estimation, and the withinlevel estimated mean. Thus, we can focus on Equations 6 and 7.</p><p>Let Z represent the variables in these equations that are involved in an autoregressive model. Let's assume the following autoregressive model for Z t :</p><formula xml:id="formula_70">Z t ¼ μ þ X L l¼1 A l Z tÀl þ ζ;</formula><p>(D:3)</p><p>where Σ ¼ VarðζÞ. Assuming stationarity of this model, the mean of Z t is</p><formula xml:id="formula_71">EðZ t Þ ¼ ðI À X L l¼1 A l Þ À1 μ: (D:4) Let Γ j ¼ CovðZ t ; Z tÀj Þ.</formula><p>The variance of Z t , Γ 0 , is computed from the Yule-Walker equations (see <ref type="bibr" target="#b10">Greene, 2014)</ref>: These equations can been used to compute the model parameters A j from the sample autocovariances Γ j , however, we do the opposite. As the model parameters are known, we solve these equations for the model-implied Γ j , which have a total of Lp 2 þ pðp þ 1Þ=2 parameters, where p is the size of the vector Z. This system is overidentified, as it has ðL þ 1Þp 2 equations. To make it just identified we remove the pðp À 1Þ=2 upper diagonal of the first row. Note that this method yields not just the model-estimated variance for the dependent and latent variables, but also the model-estimated autocorrelations of lags 1; :::; L.</p><formula xml:id="formula_72">Γ 0 Γ T</formula><p>In Mplus the Yule-Walker computation is done within the residual output option. To be more clear, model estimation does not require the trend to be modeled outside of the autoregressive process. In fact in some cases, such as linear growth models, modeling the trend within the autoregressive process or outside of the autoregressive process makes no difference, and the models are equivalent reparameterizations of each other. However, the Yule-Walker computation just outlined does require model trends to be outside of the autoregressive process and the autoregressive part of the model is assumed stationary. If there is a trend in the data it should be modeled outside of the autoregressive part of the model. For example, the direct growth model (Equations 62-63) has the linear trend outside of the autoregressive process, whereas for the equivalent indirect growth model (Equation <ref type="formula" target="#formula_44">64</ref>) the trend is within the autoregressive part of the model. Therefore the Yule-Walker computation can be applied for the direct linear growth model, but should not be used with the indirect linear growth model. More generally, the RDSEM model has an advantage over the DSEM model when it comes to checking stationarity of the autoregressive part of the model. In the RDSEM model, the autoregressive part of the model includes only residual variables. All regression equations are by definition outside of the autoregressive part of the model, including any regressions on nonstationary covariates such as the time variable itself.</p><p>Three Mplus output options are based on the Yule-Walker equations: tech4, residual, and standardization, and therefore are only valid when the autoregressive part of the model is stationary. In some cases the Mplus program will automatically detect and report nonstationarity for some of the subjects in the population simply because the model-implied subject-specific variance estimates are negative or the model-implied subject-specific variance-covariance matrices are not positive definite. Even if the Yule-Walker equations produce positive definite variance-covariance matrices and the Mplus program does not produce nonstationarity warnings, the stationarity assumption might still not hold and that could result in incorrect model-implied estimates for the means and variances. Thus the stationarity assumption should be carefully inspected before the model-implied estimates are used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>388</head><p>ASPAROUHOV, HAMAKER, MUTHÉN</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>FIGURE 1</head><label>1</label><figDesc>FIGURE 1 AR(1) autocorrelation decay function.</figDesc><graphic coords="15,147.06,379.28,300.00,140.64" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>FIGURE 2</head><label>2</label><figDesc>FIGURE 2 AR(1,1) autocorrelation decay function.</figDesc><graphic coords="15,147.06,563.93,300.00,140.64" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>DYNAMIC STRUCTURAL EQUATION MODELS each sample consists of N = 200 individuals and T = 100 times. The parameter values we use are</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>FIGURE 3</head><label>3</label><figDesc>FIGURE 3 Estimated versus true value for μ t.</figDesc><graphic coords="23,147.06,413.86,300.00,137.04" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>FIGURE 6</head><label>6</label><figDesc>FIGURE 6 Estimated versus true value for β t accounting for the trends.</figDesc><graphic coords="24,146.89,599.64,300.24,104.88" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>FIGURE 5</head><label>5</label><figDesc>FIGURE 5 Estimated versus true value for μ t accounting for the trends.</figDesc><graphic coords="24,147.06,411.70,300.00,138.24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>::: :::Γ L Γ LÀ1 Γ LÀ2 ::: Γ 0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>important parameter in this simulation study, we use four different values for σ 23 ¼ 0:15; 0:1; 0:05; and 0. These four values correspond to the following correlation values: 0.95 (high), 0.63 (medium), 0.31 (small), and 0 (none). In the simulation we use 100 replications, N = 200 and T = 100 for each value of σ 23 . We generate data according to the model in Equations 31 to 33 and analyze the data with the same model and with a model based on Equations 31 and 32 but without random variance (i.e., assuming an invariant variance para-</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>DYNAMIC STRUCTURAL EQUATION MODELS</cell></row><row><cell></cell><cell></cell><cell>TABLE 1</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">Nickell's Bias for ϕ = 0.3</cell><cell></cell></row><row><cell></cell><cell></cell><cell>DSEM</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>(Latent</cell><cell>Observed</cell><cell>Nickell's</cell></row><row><cell>T</cell><cell>N</cell><cell>Centering)</cell><cell>Centering</cell><cell>Formula</cell></row><row><cell>10</cell><cell>100</cell><cell>0.025</cell><cell>-0.140</cell><cell>-0.144</cell></row><row><cell>20</cell><cell>50</cell><cell>0.006</cell><cell>-0.070</cell><cell>-0.068</cell></row><row><cell>30</cell><cell>30</cell><cell>0.008</cell><cell>-0.042</cell><cell>-0.045</cell></row><row><cell>50</cell><cell>50</cell><cell>0.000</cell><cell>-0.029</cell><cell>-0.027</cell></row><row><cell>100</cell><cell>100</cell><cell>-0.001</cell><cell>-0.014</cell><cell>-0.013</cell></row><row><cell></cell><cell cols="3">Note. DSEM = dynamic structural equation modeling.</cell><cell></cell></row><row><cell></cell><cell></cell><cell>TABLE 2</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Bias for σ 11 ¼ 3</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>DSEM</cell><cell></cell><cell></cell></row><row><cell>T</cell><cell>N</cell><cell cols="2">(Latent Centering)</cell><cell>Uncentered</cell></row><row><cell>10</cell><cell>100</cell><cell>-0.015</cell><cell></cell><cell>-1.637</cell></row><row><cell>20</cell><cell>50</cell><cell>0.217</cell><cell></cell><cell>-1.483</cell></row><row><cell>30</cell><cell>30</cell><cell>0.645</cell><cell></cell><cell>-1.256</cell></row><row><cell>50</cell><cell>50</cell><cell>0.378</cell><cell></cell><cell>-1.361</cell></row><row><cell>100</cell><cell>100</cell><cell>0.096</cell><cell></cell><cell>-1.508</cell></row><row><cell></cell><cell cols="3">Note. DSEM = dynamic structural equation modeling.</cell><cell></cell></row></table><note><p>most</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE 4 Square</head><label>4</label><figDesc>Root of the Mean Squared Error (SMSE) for the Random Autoregressive Parameters and the Correlation Between True and Estimated Random Autoregressive Parameters</figDesc><table><row><cell></cell><cell></cell><cell>DSEM Random</cell><cell>DSEM Invariant</cell></row><row><cell></cell><cell>Covðϕ i ; v i Þ</cell><cell>Variance</cell><cell>Variance</cell></row><row><cell>SMSE</cell><cell>High</cell><cell>.255</cell><cell>.346</cell></row><row><cell>SMSE</cell><cell>Medium</cell><cell>.293</cell><cell>.329</cell></row><row><cell>SMSE</cell><cell>Low</cell><cell>.300</cell><cell>.316</cell></row><row><cell>SMSE</cell><cell>None</cell><cell>.300</cell><cell>.310</cell></row><row><cell cols="2">Correlation High</cell><cell>.96</cell><cell>.87</cell></row><row><cell cols="2">Correlation Medium</cell><cell>.92</cell><cell>.89</cell></row><row><cell cols="2">Correlation Low</cell><cell>.91</cell><cell>.90</cell></row><row><cell cols="2">Correlation None</cell><cell>.91</cell><cell>.90</cell></row><row><cell cols="4">Note. DSEM = dynamic structural equation modeling.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE 6</head><label>6</label><figDesc>Two-Level ARMA(1,1) With Binary Variable, N = 100, T = 300</figDesc><table><row><cell>Parameter</cell><cell>True Value</cell><cell>Estimate (Coverage)</cell></row><row><cell>μ</cell><cell>0</cell><cell>0.00 (.95)</cell></row><row><cell>ϕ</cell><cell>0.5</cell><cell>0.50 (.78)</cell></row><row><cell>σ w</cell><cell>1</cell><cell>1.01 (.71)</cell></row><row><cell>σ b</cell><cell>0.5</cell><cell>0.52 (.94)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE 8</head><label>8</label><figDesc>Two-Level Full MEAR(1) With Covariate, N = 200, T = 100</figDesc><table><row><cell>Parameter</cell><cell>ϕ x</cell><cell>True Value</cell><cell>Estimate (Coverage)</cell></row><row><cell>β 1</cell><cell>0</cell><cell>.30</cell><cell>.30 (.87)</cell></row><row><cell>β 1</cell><cell>0.5</cell><cell>.30</cell><cell>.30 (.96)</cell></row><row><cell>β 1</cell><cell>0.8</cell><cell>.30</cell><cell>.31 (.89)</cell></row><row><cell>β 2</cell><cell>0</cell><cell>.40</cell><cell>.40 (.87)</cell></row><row><cell>β 2</cell><cell>0.5</cell><cell>.40</cell><cell>.40 (.93)</cell></row><row><cell>β 2</cell><cell>0.8</cell><cell>.40</cell><cell>.40 (.90)</cell></row><row><cell>ϕ</cell><cell>0</cell><cell>.50</cell><cell>.50 (.88)</cell></row><row><cell>ϕ</cell><cell>0.5</cell><cell>.50</cell><cell>.50 (.93)</cell></row><row><cell>ϕ</cell><cell>0.8</cell><cell>.50</cell><cell>.50 (.93)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE 12 Two</head><label>12</label><figDesc>-Level Hybrid DAFS + WNFS, N = 100, T = 100</figDesc><table><row><cell>Parameter</cell><cell>True Value</cell><cell>Estimate (Coverage)</cell></row><row><cell>λ 0;1</cell><cell>1</cell><cell>1.00 (.92)</cell></row><row><cell>λ 1;1</cell><cell>0.6</cell><cell>0.60 (.93)</cell></row><row><cell>θ 1;1</cell><cell>1.0</cell><cell>1.00 (.95)</cell></row><row><cell>ϕ</cell><cell>0.4</cell><cell>0.40 (.95)</cell></row><row><cell>ν 1</cell><cell>0</cell><cell>0.00 (.95)</cell></row><row><cell>λ b;1</cell><cell>0.5</cell><cell>0.51 (.94)</cell></row><row><cell>θ 2;1</cell><cell>0.2</cell><cell>0.21 (.97)</cell></row><row><cell cols="3">Note. DAFS = direct autoregressive factor score; WNFS = white noise</cell></row><row><cell>factor score.</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head></head><label></label><figDesc>If we expect parameters to vary over time, but are unsure of the shape these changes will take on, we can estimate this model in the DSEM framework assuming that μ t and β t are normally distributed random effects with distributions N ðμ; v μ Þ and N ðβ; v β Þ. This is technically an incorrect assumption because the distributions of μ t and β t are not time invariant (i.e., Eðμ t jtÞ ¼ g 1 ðtÞ, Varðμ t jtÞ ¼ 0, Eðβ t jtÞ ¼ g 2 ðtÞ, and Varðβ t jtÞ ¼ 0). Nevertheless, we can</figDesc><table><row><cell>ð50 À tÞ:</cell><cell>(92)</cell></row></table><note><p>FIGURE 4 Estimated versus true value for β t.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 17 .</head><label>17</label><figDesc>All parameters are estimated well and the true values are within the credibility intervals. The only exception is the Varð 1;t Þ parameter where the lower end of the credibility interval is 0.002, slightly above the true value of 0, but clearly there is no support for a meaningful nonzero variance. The estimated random effects for μ t and β t are plotted against the true values in Figures5 and 6. Clearly the estimates are improved particularly for the β t values. The correlation between the true and estimated values for μ t is 0.999 and for β t it is 0.997. The SMSE for μ t is 0.133 and for β t it is 0.019.Given that the time-specific random effects have nearly zero residual variance, we can remove the random effects 1;t and 2;t from Equations 93 and 94. If we do so, the model can be estimated as a two-level DSEM model, rather than a cross-classified DSEM model, as follows</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head></head><label></label><figDesc>ε 1;it(B:8)    It is clear from this equation that the conditional distribution of Y 3;t is determined not just by this equation at time t (i.e., the Level 3 cluster at time t), but also by the preceding equation at times t þ 1, …, t þ L. It is also clear that the conditional distribution of Y 3;t1 is not independent of the conditional distribution of Y 3;t2 . Therefore computing the joint distribution of all Y 3;t becomes computationally infeasible. We resolve this problem by breaking down block B3 into separate blocks, one for each time t and we consider the conditional distribution of Y 3;t not just conditioned on all other blocks but also on all other Y 3;t 0 where t 0 Þt. Denote by FðY 3;t Þ LðY 0 1;it jÃÞ is the log-likelihood expression of Equation B.8 and LðY 3;t jÃÞ is the likelihood expression of Equation</figDesc><table><row><cell>FðY 3;t Þ ¼</cell><cell>X L</cell><cell>X</cell><cell>LðY 0 1;i;tþl ÃÞ þ LðY 3;t</cell><cell>ÃÞ;</cell><cell>(B:9)</cell></row><row><cell></cell><cell>l¼0</cell><cell>i</cell><cell></cell><cell></cell><cell></cell></row></table><note><p>where</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>ASPAROUHOV, HAMAKER, MUTHÉN</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX A CONTINUOUS TIME DSEM MODELING</head><p>In this appendix we describe the algorithm implemented in Mplus for approximating a continuous time DSEM model with a discrete time DSEM model. Every continuous function f ðtÞ can be approximated by a step function. Let δ be a small number. The function f ðtÞ can be approximated by a step function f 0 ðtÞ ¼ f j ¼ f ðj Á δÞ when j Á δ À δ=2&lt;t j Á δ þ δ=2. The smaller the step interval δ the better the approximation. Based on this same principle we can approximate a continuous time DSEM model with a discrete time DSEM model.</p><p>Step 1: Rescaling the Time Variable Suppose that individual i is observed at times t ij , for j ¼ 1; :::; T i . We replace the value t ij with an integer value tij ¼ ½t ij =δ, where ½t denotes the smallest integer value not smaller than t; that is, tij is the integer value for which ð tij À 1Þδ &lt; t ij tij δ:</p><p>(A:1)</p><p>Essentially, we first rescale the time variable by multiplying it by 1=δ and then rounding it up to the nearest integer. Thus for all t ij falling in the interval ð0; δ, tij ¼ 1, for all t ij falling in ðδ; 2δ, tij ¼ 2and so on. Using this approach we convert any real time values t ij to the integer time values tij . At that point the standard DSEM modeling can be used. For all integer values that are not observed, missing data is assumed; that is, for individual i and integer time value t that is not equal to any of the tij , we assume that the data are missing or not recorded. This is not really an assumption, but is a way to properly record the data so that the observations are recorded for every integer.</p><p>If the δ value in the preceding algorithm is not sufficiently small, it is very likely that two or more t ij values for individual i will appear in the interval ððn À 1Þδ; nδ. This will result in several values tij being assigned the value n, which is not an acceptable outcome as we can use just one observation for time n. To resolve this problem we apply the following algorithm. For individual i all t ij are placed in the intervals ððn À 1Þδ; nδ following Equation A.1. Starting with the smallest n for which the interval ððn À 1Þδ; nδ contains multiple values, we determine the closest empty interval to that interval and we shift one of the overflow values toward that interval, preserving the original order of t ij . That means that each interval from the overflow interval to the empty interval shifts one value in the direction of the overflow interval. This algorithm approximately minimizes DYNAMIC STRUCTURAL EQUATION MODELS Y 3;t ,N ðDd; DÞ;</p><p>where N ðμ 3 ; Σ 3 Þ is the implied distribution for Y 3;t from Equation <ref type="formula">4</ref>. These equations apply for t T À L where T ¼ maxðT i Þ. When t &gt; T À L the equations get reduced by L À T þ t because the index of the equation is greater than the largest t in the model; that is, equations with time index greater than T do not exist, as no data are observed beyond time T.</p><p>The conditional distributions of block B4 is obtained the same way as the conditional distribution of block B2. Level 2 and Level 3 simply reverse roles. The conditional distribution of block B5 is as in Step 1 in Section 2.4 in <ref type="bibr" target="#b1">Asparouhov and Muthén (2010)</ref>. Conditional on blocks B1 through B4 and the generated values for these variables, the Level 2 and Level 3 models become essentially like multiple groups in single-level modeling. The two levels are independent of each other, the within-level model, and the observed data Y it . Therefore the single-level approach in <ref type="bibr" target="#b1">Asparouhov and Muthén (2010)</ref> applies. We reproduce this step here for completeness. Consider the single-level SEM model</p><p>The conditional distribution is given by ½ηjÃ,N ðDd; DÞ;</p><p>where B 0 ¼ I À B, Iis the identity matrix, Θ ¼ VarðεÞ, and</p><p>The conditional distribution of block B6 requires some additional computations. Because η 1;it are not independent across time they cannot be generated simultaneously in an efficient manner, as that will require computing the large joint conditional distribution of η 1;it for all t. Therefore block B6 is essentially split into separate blocks, one for each t. Thus we update the within-level latent variable one at a time starting at η 1;i;1ÀL , η 1;i;2ÀL ,…, η 1;i;Ti , where T i is the last observation for individual i. We need to construct the conditional distribution of η 1;it conditional on all the other blocks and all the other η 1;it , for times different from t. Given all the other blocks, Y 1;it is observed. The conditional distribution of η 1;it is somewhat different at the end and at the beginning of that sequence so first we consider the case where t is in the middle, more specifically 0&lt;t&lt;T i À L. The latent variable η 1;it conditional distribution is determined by Equations 6 and 7 at time t, t þ 1, …, t þ L. In total there are 2L + 2 equations that affect the conditional distribution. We combine all these equations into one big model for η 1;it that consists of one structural equation: Equation 7 at time t, and 2L + 1 measurement equations for η 1;it : Equation 6 at time t and Equations 6 and 7 at times t þ 1, …, t þ L. Using this larger model the conditional distribution is obtained again as in Equation B.16. For t &gt; T i À L the conditional distribution is obtained similarly. However, because there are no observations beyond time T i there will be only 2ðT i À t þ 1Þ equations in the enlarged model, again one structural equation and 2ðT i À tÞ þ 1 measurement equations. For t 0 we also have fewer equations due to the fact that there are no observations before t ¼ 1. There are only 2ðL þ tÞ measurement equations in the model, and the prior specification for η 1;it for t 0 takes the role of the structural equation.</p><p>In block B7 similar considerations are taken into account. Missing values are imputed one at a time and in a sequential order. Block B8 is actually done at the same time as block B7 because one can interpret the initial conditions as missing values; that is, at times t 0, Y 1;it and X 1;it can be viewed as missing values. Note here that conditional on all other blocks, the missing values of Y it are essentially the missing values of Y 1;it ; that is, once the missing values for Y 1;it are imputed, the values of Y it are obtained by Equation 1 because Y 2;i and Y 3;t are known (i.e., are conditioned on). Let's first consider the missing value Y 1;it in the middle of the sequence 0&lt;t&lt;T i À L. In non-time-series analysis we impute the missing value from the univariate conditional normal distribution obtained from the within-level model (see Section 4 in <ref type="bibr" target="#b1">Asparouhov &amp; Muthén, 2010)</ref>. Because conditional on X 1;it the multivariate joint distribution of Y 1;it and η 1;it is normal, then one of these variables conditional on all other variables has a univariate normal distribution, and that distribution is used for missing value imputation. However, in the time-series model we consider here Y 1;it variable is used in 2L þ 2 Equations 6 and 7 at times t, t þ 1, …, t þ L. A missing variable in this context is nothing more than an unobserved latent variable. Therefore the procedure we outlined earlier for conditional distribution for block B6 applies here as well. As in block B6 at the end of the sequence for t &gt; T i À L or at the beginning of the sequence for t 0, the number of equations used for the computation decreases and for t 0 the structural equation, where the missing value is the dependent variable, is replaced by the prior specification. This applies both for Y 1;it and X 1;it when t 0. Note that the missing data treatment is likelihood based and thus will guarantee consistent estimation as long as the missing data are MAR.</p><p>Blocks B9 through B12 are all implemented as in <ref type="bibr" target="#b1">Asparouhov and Muthén (2010)</ref>. Conditional on all latent variables, the DSEM model is essentially a three-group single-level structural equation model and the procedures for single-level SEM apply directly. Finally let's consider block B13. The conditional distribution of the random effects from Equation <ref type="formula">11</ref>is not explicit and we use the Metropolis-Hastings algorithm to generate values from that distribution. Suppose that Y 1;it1 has a random residual variance σ i ¼ Expðs 2;i Þ, where s 2;i is a normally distributed random effect. Suppose that the current value of that random effect is s 0 . A new proposed value s 1 is drawn from a normal distribution N ðs 0 ; V Þ where V is referred to as the proposal distribution variance. We then compute the acceptance ratio as follows: R ¼ Pðs</p><p>where Pðs 2;i ¼ s j jÃÞ is the likelihood of s 2;i obtained from Equation <ref type="formula">3</ref>conditional on all other variables in that equation and PðY 1;it1 jσ i ¼ Expðs j ÞÞ is the likelihood of Y 1;it1 obtained from Equation <ref type="formula">6</ref>conditional on all other variables in that equation. The proposed value s 1 is accepted with probability minð1; RÞ. If the value is rejected the old value s 0 is retained. The proposal distribution variance V is chosen to be a small value such as 0.1 and is adjusted during a burn-in stage of the estimation to obtain optimal mixing; that is, optimal acceptance rate in the Metropolis-Hastings algorithm. The optimal acceptance rate is considered to be between .25 and .50. To preserve the integrity of the MCMC chain the jumping distribution variance is not changed beyond the burn-in iterations and those iterations are discarded and not used in the posterior distribution. Under these conditions the preceding Metropolis-Hastings algorithm generates s 2;i from the correct conditional distribution. Random variances for latent factors are estimated similarly. This concludes the description of the MCMC estimation of the DSEM model. What is hidden in the preceding description of the estimation is the computational times to estimate the model. Depending on the particular details of the model, the conditional distributions might or might not be invariant across subject or invariant across time. The more random structural parameters there are that vary across subject and time, the less invariance there is in the conditional distributions just described. Generally speaking, for the two-level DSEM model most of the conditional distributions are invariant across time. Thus the conditional means and variances depend only on sufficient statistics of the data and are easily computed. For the cross-classified DSEM model even when a single structural parameter varies across time and subject, the structural SEM model given in Equations 9 and 10 changes for every i and t and a separate computation is required. This generally results in substantial increase in the computational time. Paired with the slower convergence, that stems from the fact that the model is more flexible and the cross-classified DSEM model can become substantially more computationally intensive than a twolevel DSEM model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX C RDSEM MODEL ESTIMATION</head><p>Here we describe some of the complexities encountered in the estimation of the RDSEM model. For simplicity let's consider the single-level AR(1) RDSEM model:</p><p>We can assume that the covariance vector X t includes the constant 1 so that the intercept of the regression Equation C.1 is also included in this model.</p><p>To obtain the conditional distribution of B given all other parameters, note that</p><p>From here we can obtain the conditional distribution of B in two steps. First consider the conditional distribution of B 1 and B 2 for this model: </p><p>and the second derivative is computed as follows: </p><p>Note that in the MCMC estimation of the RDSEM model the autoregressive parameters R in the residual model for Ŷt and the regression equation parameters B are updated in two separate steps (i.e., are in two separate blocks). This is needed because the joint conditional distribution of B and R is not normal (due to the multiplication term RB seen in Equation C.4), and both ½BjR; Ã and ½RjB; Ã are conditionally normal distributions. This is in contrast to the estimation of the DSEM model where R and B are updated simultaneously. In addition to the complications just described for the conditional distribution of B, the RDSEM model estimation requires similar modifications for the conditional distribution of the latent variables η 1;it , the missing values for the lagged variables Y 1;it , and the between-level random effects. All other conditional distributions are identical to those described in the DSEM model estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX D COMPUTING THE MODEL-ESTIMATED SUBJECT-SPECIFIC MEANS AND VARIANCES</head><p>In this section we provide details on how model-estimated subjectspecific means and variances can be computed for the DSEM model. The main assumption in such a computation is the assumption of stationarity. Any autoregressive process in the model has to be stationary; that is, over time the distribution of the variables in the autoregressive process stabilizes.</p><p>To compute the subject-specific model-estimated mean and variance implied by the two-level DSEM model we start with Equation 1 assuming no time-specific component Y 3;t ; that is, EðY it iÞ ¼ EðY 1;it iÞ þ Y 2;i (D:1)</p><p>VarðY it iÞ ¼ VarðY 1;it iÞ: (D:2)</p><p>The estimated subject-specific variance is simply the estimated within-level subject-specific variance and the estimated subject-specific mean is the sum DYNAMIC STRUCTURAL EQUATION MODELS</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A Bayesian approach to nonlinear latent variable models using the Gibbs sampler and the Metropolis-Hastings algorithm</title>
		<author>
			<persName><forename type="first">G</forename><surname>Arminger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Muthen</surname></persName>
		</author>
		<idno type="DOI">10.1007/BF02294856</idno>
	</analytic>
	<monogr>
		<title level="j">Psychometrika</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="page" from="271" to="300" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Bayesian analysis using Mplus: Technical implementation</title>
		<author>
			<persName><forename type="first">T</forename><surname>Asparouhov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Muthén</surname></persName>
		</author>
		<ptr target="http://statmodel.com/download/Bayes3.pdf" />
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">General random effect latent variable modeling: Random subjects, items, contexts, and parameters</title>
		<author>
			<persName><forename type="first">T</forename><surname>Asparouhov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Muthén</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in multilevel modeling for educational research: Addressing practical issues found in real-world applications</title>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Harring</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Stapleton</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><forename type="middle">N</forename><surname>Beretvas</surname></persName>
		</editor>
		<meeting><address><addrLine>Charlotte, NC</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="163" to="192" />
		</imprint>
	</monogr>
	<note>Information Age</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Intensive longitudinal methods: An introduction to diary and experience sampling research</title>
		<author>
			<persName><forename type="first">N</forename><surname>Bolger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Laurenceau</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>Guilford</publisher>
			<pubPlace>New York, NY</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A comparison of Bayesian and likelihood-based methods for fitting multilevel models</title>
		<author>
			<persName><forename type="first">W</forename><surname>Browne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Draper</surname></persName>
		</author>
		<idno type="DOI">10.1214/06-BA117</idno>
	</analytic>
	<monogr>
		<title level="j">Bayesian Analysis</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="473" to="514" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Linear smoothers and additive models</title>
		<author>
			<persName><forename type="first">A</forename><surname>Buja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
		<idno type="DOI">10.1214/aos/1176347115</idno>
	</analytic>
	<monogr>
		<title level="j">Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="453" to="510" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deviance information criteria for missing data models</title>
		<author>
			<persName><forename type="first">G</forename><surname>Celeux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Forbes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">P</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Titterington</surname></persName>
		</author>
		<idno type="DOI">10.1214/06-BA122</idno>
	</analytic>
	<monogr>
		<title level="j">Bayesian Analysis</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="651" to="673" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Accelerating Monte Carlo Markov Chain Convergence for Cumulative-Link Generalized Linear Models</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Cowles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistics and Computing</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="101" to="111" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">No need to be discrete: A method for continuous time mediation analysis</title>
		<author>
			<persName><forename type="first">P</forename><surname>Deboeck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Preacher</surname></persName>
		</author>
		<idno type="DOI">10.1080/10705511.2014.973960</idno>
	</analytic>
	<monogr>
		<title level="j">Structural Equation Modeling</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="61" to="75" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Time series modelling and interpretation</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">W J</forename><surname>Granger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Morris</surname></persName>
		</author>
		<idno type="DOI">10.2307/2345178</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society, Series A</title>
		<imprint>
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page" from="246" to="257" />
			<date type="published" when="1976">1976</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">H</forename><surname>Greene</surname></persName>
		</author>
		<title level="m">Econometric analysis</title>
		<meeting><address><addrLine>Upper Saddle River, NJ</addrLine></address></meeting>
		<imprint>
			<publisher>Prentice Hall</publisher>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note>th ed.</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Conditions for the equivalence of the autoregressive latent trajectory model and a latent growth curve model with autoregressive disturbances</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">L</forename><surname>Hamaker</surname></persName>
		</author>
		<idno type="DOI">10.1177/0049124104270220</idno>
	</analytic>
	<monogr>
		<title level="j">Sociological Methods &amp; Research</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="404" to="416" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">At the frontiers of modeling intensive longitudinal data: Dynamic structural equation models for the affective measurements from the COGITO study</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">L</forename><surname>Hamaker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Asparouhov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Brose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Schmiedek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Muthén</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>Manuscript submitted for publication</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">L</forename><surname>Hamaker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P P</forename><surname>Grasman</surname></persName>
		</author>
		<idno type="DOI">10.3389/fpsyg.2014.01492</idno>
	</analytic>
	<monogr>
		<title level="m">To center or not to center? Investigating inertia with a multilevel autoregressive model</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">1492</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Analysis of affective instability in ecological momentary assessment: Indices using successive difference and group comparison via multilevel modeling</title>
		<author>
			<persName><forename type="first">S</forename><surname>Jahng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">K</forename><surname>Wood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Trull</surname></persName>
		</author>
		<idno type="DOI">10.1037/a0014173</idno>
	</analytic>
	<monogr>
		<title level="j">Psychological Methods</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="354" to="375" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<author>
			<persName><forename type="first">J</forename><surname>Jongerling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Laurenceau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hamaker</surname></persName>
		</author>
		<idno type="DOI">10.1080/00273171.2014.1003772</idno>
	</analytic>
	<monogr>
		<title level="m">A multilevel AR(1) model: Allowing for inter-individual differences in trait-scores, inertia, and innovation variance</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page" from="334" to="349" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A new approach to linear filtering and prediction problems</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Kalman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Basic Engineering</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="page" from="35" to="45" />
			<date type="published" when="1960">1960</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Structural equation modelling: A Bayesian approach</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Y</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
			<publisher>Wiley</publisher>
			<pubPlace>London UK</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The multilevel latent covariate model: A new, more reliable approach to group-level effects in contextual studies</title>
		<author>
			<persName><forename type="first">O</forename><surname>Lüdtke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">W</forename><surname>Marsh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Robitzsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Trautwein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Asparouhov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Muthén</surname></persName>
		</author>
		<idno type="DOI">10.1037/a0012869</idno>
	</analytic>
	<monogr>
		<title level="j">Psychological Methods</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="203" to="229" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A dynamic factor model for the analysis of multivariate time series</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">C M</forename><surname>Molenaar</surname></persName>
		</author>
		<idno type="DOI">10.1007/BF02294246</idno>
	</analytic>
	<monogr>
		<title level="j">Psychometrika</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page" from="181" to="202" />
			<date type="published" when="1985">1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Equivalent dynamic models</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">C M</forename><surname>Molenaar</surname></persName>
		</author>
		<idno type="DOI">10.1080/00273171.2016.1277681</idno>
	</analytic>
	<monogr>
		<title level="j">Multivariate Behavioral Research</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page" from="242" to="258" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Biases in dynamic models with fixed effects</title>
		<author>
			<persName><forename type="first">S</forename><surname>Nickell</surname></persName>
		</author>
		<idno type="DOI">10.2307/1911408</idno>
	</analytic>
	<monogr>
		<title level="j">Econometrica: Journal of the Econometric Society</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="page" from="1417" to="1426" />
			<date type="published" when="1981">1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A hierarchical latent stochastic differential equation model for affective dynamics</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Oravecz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Tuerlinckx</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vandekerckhove</surname></persName>
		</author>
		<idno type="DOI">10.1037/a0024375</idno>
	</analytic>
	<monogr>
		<title level="j">Psychological Methods</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="468" to="490" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">W</forename><surname>Raudenbush</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Bryk</surname></persName>
		</author>
		<title level="m">Hierarchical linear models: Applications and data analysis methods</title>
		<meeting><address><addrLine>Thousand Oaks, CA</addrLine></address></meeting>
		<imprint>
			<publisher>Sage</publisher>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
	<note>nd ed.</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Incorporating measurement error in n = 1 psychological autoregressive modeling</title>
		<author>
			<persName><forename type="first">N</forename><surname>Schuurman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Houtveen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hamaker</surname></persName>
		</author>
		<idno type="DOI">10.3389/fpsyg.2015.01038</idno>
	</analytic>
	<monogr>
		<title level="j">Frontiers in Psychology</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">1038</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Bayesian measures of model complexity and fit</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Spiegelhalter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">G</forename><surname>Best</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">P</forename><surname>Carlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Van Der Linde</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society, Series B</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="page" from="583" to="639" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">The role of ambulatory assessment in psychological science</title>
		<author>
			<persName><forename type="first">T</forename><surname>Trull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Ebner-Priemer</surname></persName>
		</author>
		<idno type="DOI">10.1177/0963721414550706</idno>
	</analytic>
	<monogr>
		<title level="j">Current Directions in Psychological Science</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="466" to="470" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Conditional Akaike information for mixedeffects models</title>
		<author>
			<persName><forename type="first">F</forename><surname>Vaida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Blanchard</surname></persName>
		</author>
		<idno type="DOI">10.1093/biomet/92.2.351</idno>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">92</biblScope>
			<biblScope unit="page" from="351" to="370" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Continuous time modelling with individually varying time intervals for oscillating and non-oscillating processes</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Voelkle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H L</forename><surname>Oud</surname></persName>
		</author>
		<idno type="DOI">10.1111/j.2044-8317.2012.02043.x</idno>
	</analytic>
	<monogr>
		<title level="j">British Journal of Mathematical and Statistical Psychology</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="page" from="103" to="126" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Comparisons of four methods for estimating a dynamic factor model</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hamaker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Nesselroade</surname></persName>
		</author>
		<idno type="DOI">10.1080/10705510802154281</idno>
	</analytic>
	<monogr>
		<title level="j">Structural Equation Modeling</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="377" to="402" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Bayesian estimation of categorical dynamic factor models</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Nesselroade</surname></persName>
		</author>
		<idno type="DOI">10.1080/00273170701715998</idno>
	</analytic>
	<monogr>
		<title level="j">Multivariate Behavioral Research</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="729" to="756" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
