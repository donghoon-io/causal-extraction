<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">blavaan: Bayesian Structural Equation Models via Parameter Expansion</title>
				<funder ref="#_6nKEssu">
					<orgName type="full">National Science Foundation</orgName>
					<orgName type="abbreviated">NSF</orgName>
				</funder>
				<funder ref="#_B4CuszD">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Edgar</forename><forename type="middle">C</forename><surname>Merkle</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Missouri</orgName>
								<orgName type="institution" key="instit2">Ghent University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yves</forename><surname>Rosseel</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Missouri</orgName>
								<orgName type="institution" key="instit2">Ghent University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">blavaan: Bayesian Structural Equation Models via Parameter Expansion</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.18637/jss.v085.i04</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-14T17:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Bayesian SEM</term>
					<term>structural equation models</term>
					<term>JAGS</term>
					<term>MCMC</term>
					<term>lavaan</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This article describes blavaan, an R package for estimating Bayesian structural equation models (SEMs) via JAGS and for summarizing the results. It also describes a novel parameter expansion approach for estimating specific types of models with residual covariances, which facilitates estimation of these models in JAGS. The methodology and software are intended to provide users with a general means of estimating Bayesian SEMs, both classical and novel, in a straightforward fashion. Users can estimate Bayesian versions of classical SEMs with lavaan syntax, they can obtain state-of-the-art Bayesian fit measures associated with the models, and they can export JAGS code to modify the SEMs as desired. These features and more are illustrated by example, and the parameter expansion approach is explained in detail.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The intent of blavaan is to implement Bayesian structural equation models (SEMs) that harness open source MCMC samplers (in JAGS; Plummer 2003) while simplifying model specification, summary, and extension. Bayesian SEM has received increasing attention in recent years, with MCMC samplers being developed for specific priors (e.g., Lee 2007; <ref type="bibr" target="#b43">Scheines, Hoijtink, and Boomsma 1999)</ref>, specific models being estimated in JAGS and BUGS <ref type="bibr" target="#b26">(Lunn, Jackson, Best, Thomas, and Spiegelhalter 2012;</ref><ref type="bibr" target="#b27">Lunn, Thomas, Best, and Spiegelhalter 2000)</ref>, factor analysis samplers being included in R packages bfa <ref type="bibr" target="#b33">(Murray 2014)</ref> and MCMCpack <ref type="bibr" target="#b29">(Martin, Quinn, and Park 2011)</ref>, and multiple samplers being implemented in Mplus <ref type="bibr" target="#b34">(Muthén and Asparouhov 2012)</ref>. These methods have notable advantages over analogous frequentist methods, including the facts that estimation of complex models is typically easier (e.g., <ref type="bibr" target="#b28">Marsh, Wen, Hau, and Nagengast 2013)</ref> and that estimates of parameter uncertainty do not rely on asymptotic arguments. Further, in addition to allowing for the inclusion of existing knowledge into model estimates, prior distributions can be used to inform underidentified models, to average across multiple models in a single run (e.g., <ref type="bibr" target="#b25">Lu, Chow, and Loken 2016)</ref>, and to avoid Heywood cases <ref type="bibr" target="#b30">(Martin and McDonald 1975)</ref>.</p><p>The traditional SEM framework (e.g., <ref type="bibr" target="#b5">Bollen 1989</ref>) assumes an n × p data matrix Y = (y 1 y 2 . . . y n ) with n independent cases and p continuous manifest variables. Using the LISREL "all y" notation (e.g., <ref type="bibr" target="#b20">Jöreskog and Sörbom 1997)</ref>, a structural equation model with m latent variables may be represented by the equations y = ν + Λη +</p><p>(1)</p><formula xml:id="formula_0">η = α + Bη + ζ,<label>(2)</label></formula><p>where η is an m × 1 vector containing the latent variables; is a p × 1 vector of residuals; and ζ is an m × 1 vector of residuals associated with the latent variables. Each entry in these three vectors is independent of the others. Additionally, ν and α contain intercept parameters for the manifest and latent variables, respectively; Λ is a matrix of factor loadings; and B contains parameters that reflect directed paths between latent variables (with the assumption that (I -B) is invertible).</p><p>In conventional SEMs, we assume multivariate normality of the and ζ vectors. In particular,</p><formula xml:id="formula_1">∼ N p (0, Θ)<label>(3)</label></formula><formula xml:id="formula_2">ζ ∼ N m (0, Ψ),<label>(4)</label></formula><p>with the latter equation implying multivariate normality of the latent variables. Taken together, the above assumptions imply that the marginal distribution of y (integrating out the latent variables) is multivariate normal with parameters µ = ν + Λα (5)</p><formula xml:id="formula_3">Σ = Λ(I -B) -1 Ψ(I -B ) -1 Λ + Θ. (<label>6</label></formula><formula xml:id="formula_4">)</formula><p>If one is restricted to conjugate priors, then the <ref type="bibr" target="#b45">Song and Lee (2012)</ref> or <ref type="bibr" target="#b34">Muthén and Asparouhov (2012)</ref> procedures are often available for simple MCMC estimation of the above model. However, if one wishes to use general priors or to estimate novel models, then there is the choice of implementing a custom MCMC scheme or using a general MCMC program like BUGS, JAGS, or Stan <ref type="bibr" target="#b6">(Carpenter, Gelman, Hoffman, Lee, Goodrich, Betancourt, Brubaker, Guo, Li, and Riddell 2017)</ref>. These options are often time-consuming and difficult to extend to further models. This is where blavaan is intended to be helpful: it allows for simple specification of Bayesian SEMs while also allowing the user to extend the original model. In addition to easing Bayesian SEM specification, the package includes a novel approach to JAGS model estimation that allows us to handle models with correlated residuals. This approach, further described below, builds on the work of many previous researchers: the approach of <ref type="bibr" target="#b22">Lee (2007)</ref> for estimating models in WinBUGS; the approach of <ref type="bibr" target="#b35">Palomo, Dunson, and Bollen (2007)</ref> for handling correlated residuals; and the approaches of <ref type="bibr" target="#b2">Barnard, McCulloch, and Meng (2000)</ref> and <ref type="bibr" target="#b34">Muthén and Asparouhov (2012)</ref> for specifying prior distributions of covariance matrices.</p><p>Package blavaan is potentially useful for the analyst that wishes to estimate Bayesian SEMs and for the methodologist that wishes to develop novel extensions of Bayesian SEMs. The package is a series of bridges between several existing R packages, with the bridges being used to make each step of the modeling easy and flexible. Package lavaan <ref type="bibr" target="#b41">(Rosseel 2012)</ref> serves as the starting point and ending point in the sequence: the user specifies models via lavaan syntax, and objects of class blavaan make use of many lavaan functions. Consequently, writings on lavaan (e.g., <ref type="bibr" target="#b4">Beaujean 2014;</ref><ref type="bibr" target="#b41">Rosseel 2012)</ref> give the user a good idea about how to use blavaan.</p><p>Following model specification, blavaan examines the details of the specified model and converts the specification to JAGS syntax. Importantly, this conversion often makes use of the parameter expansion ideas presented below, resulting in MCMC chains that quickly converge for many models. We employ package runjags <ref type="bibr" target="#b10">(Denwood 2016)</ref> for sampling parameters and summarizing the MCMC chains. Once runjags is finished, blavaan organizes summaries and computes a variety of Bayesian fit measures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Bayesian SEM</head><p>As noted above, the approach of Lee (2007; see also <ref type="bibr" target="#b45">Song and Lee 2012)</ref> involves the use of conjugate priors on the parameters from Equations 1 to 4. Specific priors include inverse gamma distributions on variance parameters, inverse Wishart distributions on unrestricted covariance matrices (typically covariances of exogenous latent variables), and normal distributions on other parameters.</p><p>Importantly, Lee assumes that the manifest variable covariance matrix Θ and the endogenous latent variable covariance matrix Ψ are diagonal. This assumption of diagonal covariance matrices is restrictive in some applications. For example, researchers often wish to fit models with correlated residuals, and it has been argued that such models are both necessary and under-utilized <ref type="bibr" target="#b9">(Cole, Ciesla, and Steiger 2007)</ref>. Correlated residuals pose a difficult problem for MCMC methods because they often result in covariance matrices with some (but not all) off-diagonal entries equal to zero. In this situation, we cannot assign an inverse Wishart prior to the full covariance matrix because this does not allow us to fix some off-diagonal entries to zero. Further, if we assign a prior to each free entry of the covariance matrix and sample the parameters separately, we can often obtain covariance matrices that are not positive definite. This can lead to numerical problems, resulting in an inability to carry out model estimation.</p><p>Finally, it is possible to specify an equivalent model that possesses the required, diagonal matrices. However, the setting of prior distributions can be unclear here: if the analyst specifies prior distributions for her model of interest, then it may be cumbersome to translate these into prior distributions for the equivalent model.</p><p>To address the issue of non-diagonal covariance matrices, <ref type="bibr" target="#b34">Muthén and Asparouhov (2012)</ref> implemented a random walk method that is based on work by <ref type="bibr" target="#b7">Chib and Greenberg (1998)</ref>. This method samples free parameters of the covariance matrix via Metropolis-Hastings steps. While the implementation is fast and efficient, it does not allow for some types of equality constraints because parameters are updated in blocks (either all parameters in a block must be constrained to equality, or no parameters in a block can be constrained). Further, the method is unreliable for models involving many latent variables. Consequently, <ref type="bibr" target="#b34">Muthén and Asparouhov (2012)</ref> implemented three other MCMC methods that are suited to different types of models.</p><p>In our initial work on package blavaan, we developed methodology for fitting models with free residual covariance parameters. We sought methodology that (i) would work in JAGS, (ii) was reasonably fast and efficient (by JAGS standards), and (iii) allowed for satisfactory specification of prior distributions. In the next section, we describe the resulting methodology.</p><p>The methodology can often be used to reliably estimate posterior means to the first decimal place on the order of minutes (often, less than two minutes on desktop computers, but it also depends on model complexity). This will never beat compiled code, but it is typically better than alternative JAGS parameterizations that rely on multivariate distributions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Parameter expansion</head><p>General parameter expansion approaches to Bayesian inference are described by <ref type="bibr" target="#b13">Gelman (2004</ref><ref type="bibr" target="#b14">Gelman ( , 2006))</ref>, with applications to factor analysis models detailed by <ref type="bibr" target="#b17">Ghosh and Dunson (2009)</ref>. Our approach here is related to that of <ref type="bibr" target="#b35">Palomo et al. (2007)</ref>, who employ phantom latent variables (they use the term pseudo-latent variable) to simplify the estimation of models with non-diagonal Θ matrices. This results in a working model that is estimated, with the working model parameters being transformed to the inferential model parameters from Equations 1 and 2. The use of phantom latent variables in SEM has long been known (e.g., <ref type="bibr" target="#b40">Rindskopf 1984)</ref>, though the Bayesian approach involves new issues associated with prior distribution specification. This is further described below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Overview</head><p>Assuming v nonzero entries in the lower triangle of Θ (i.e., v residual covariances), <ref type="bibr" target="#b35">Palomo et al. (2007)</ref> take the inferential model residual vector and reparameterize it as</p><formula xml:id="formula_5">= Λ D D + * D ∼ N v (0, Ψ D ) * ∼ N p (0, Θ * ),</formula><p>where Λ D is a p × v matrix with many zero entries, D is a v × 1 vector of phantom latent variables, and * is a p × 1 residual vector. This approach is useful because, by carefully choosing the nonzero entries of Λ D , both Ψ D and Θ * are diagonal. This allows us to employ an approach related to Lee (2007), avoiding high-dimensional normal distributions in favor of univariate normals.</p><p>Under this working model parameterization, the inferential model covariance matrix Θ can be re-obtained via</p><formula xml:id="formula_6">Θ = Λ D Ψ D Λ D + Θ * .</formula><p>We can also handle covariances between latent variables in the same way, as necessary. Assuming m latent variables with w covariances, we have</p><formula xml:id="formula_7">ζ = B E E + ζ * E ∼ N w (0, Ψ E ) ζ * ∼ N m (0, Ψ * ),</formula><p>where the original covariance matrix Ψ is re-obtained via:</p><formula xml:id="formula_8">Ψ = B E Ψ E B E + Ψ * .</formula><p>This approach to handling latent variable covariances can lead to slow convergence in models with many (say, &gt; 5) correlated latent variables. In these cases, we have found it better (in JAGS) to sample the latent variables from multivariate normal distributions. Package blavaan attempts to use multivariate normal distributions in situations where it can, reverting to the above reparameterization only when necessary.</p><p>To choose nonzero entries of Λ D (with the same method applying to nonzero entries of B E ), we define two v × 1 vectors r and c. These vectors contain the respective row numbers and column numbers of the nonzero, lower-triangular entries of Θ. For j = 1, . . . , v, the nonzero entries of Λ D then occur in column j, rows r j and c j . <ref type="bibr" target="#b35">Palomo et al. (2007)</ref> set these nonzero entries equal to 1, which can be problematic if the covariance parameters' posteriors are negative or overlap with zero. This is because the only remaining free parameters are variances, which can only be positive. Instead of 1s, we free the parameters in Λ D so that they are functions of inferential model parameters. This is further described in the next section.</p><p>First, however, we give an example of the approach.  </p><formula xml:id="formula_9">            1 0 0 λ 2 0 0 λ 3 0 0 0 1 0 0 λ 5 0 0 λ 6 0 0 λ 7 0 0 0 1 0 0 λ 5 0 0 λ 6 0 0 λ 7              ind60 dem60 dem65 +             </formula><formula xml:id="formula_10">             ind60 dem60 dem65 = 0 0 0 + 0 0 0 b 1 0 0 b 2 b 3 0 ind60 dem60 dem65 + ζ 1 ζ 2 ζ 3 1 0 0 λ 2 0 0 λ 3 0 0 0 1 0 0 λ 5 0 0 λ 6 0 0 λ 7 0 0 0 1 0 0 λ 5 0 0 λ 6 0 0 λ 7              ind60 dem60 dem65 +              0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 λ D1 0 0 0 0 0 0 λ D2 λ D3 0 0 0 0 0 0 λ D4 0 0 0 λ D5 0 0 λ D6 0 λ D7 0 0 0 0 0 0 0 λ D8 0 0 λ D9 0 0 0 λ D10 0 0 0 0 0 0 λ D11 λ D12                   D1 D2 D3 D4 D5 D6      +              e *</formula><formula xml:id="formula_11">           ind60 dem60 dem65 = 0 0 0 + 0 0 0 b 1 0 0 b 2 b 3 0 ind60 dem60 dem65 + ζ 1 ζ 2 ζ 3 .</formula><p>The covariance matrix associated with * , Θ * , is now diagonal, which makes the model easier to estimate via MCMC. The latent variable residual, ζ, is maintained as before because its covariance matrix was already diagonal. In general, we only reparameterize residual covariance matrices that are neither diagonal nor unconstrained.</p><p>As mentioned earlier, the difference between our approach and that of <ref type="bibr" target="#b35">Palomo et al. (2007)</ref> is that we estimate the loadings with D subscripts, whereas <ref type="bibr" target="#b35">Palomo et al. (2007)</ref> fix these loadings to 1. This allows us to obtain posteriors on residual covariances that overlap with zero or that become negative, as necessary. Estimation of these loadings comes with a cost, however, in that the prior distributions of the inferential model do not immediately translate into prior distributions of the working model. This problem and a solution are further described in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Priors on covariances</head><p>Due to the reparameterization of covariances, prior distributions on the working model parameters require care in their specification. Every inferential model covariance parameter is potentially the product of three parameters: two representing paths from a phantom latent variable to the variable of interest, and one precision (variance) of the phantom latent variable. These three parameters also impact the inferential model variance parameters. We carefully restrict these parameters to arrive at prior distributions that are both meaningful and flexible.</p><p>We place univariate prior distributions on model variance and correlation parameters, with the distributions being based on the inverse Wishart or on other prior distributions described in the literature. This is highly related to the approach of <ref type="bibr" target="#b2">Barnard et al. (2000)</ref>, who separately specify prior distributions on the variance and correlation parameters comprising a covariance matrix (for an overview and related approaches, see <ref type="bibr" target="#b0">Alvarez, Niemi, and Simpson 2014)</ref>. Our approach is also related to <ref type="bibr" target="#b34">Muthén and Asparouhov (2012)</ref>, who focus on marginal distributions associated with the inverse Wishart. A notable difference is that we always use proper prior distributions, whereas <ref type="bibr" target="#b34">Muthén and Asparouhov (2012)</ref> often use improper prior distributions.</p><p>To illustrate the approach, we refer to Figure <ref type="figure" target="#fig_2">2</ref>. This figure uses path diagrams to display two observed variables with correlated residuals (left panel) along with their parameter-expanded representation in the working model (right panel). Our goal is to specify a sensible prior distribution for the inferential model's variance/covariance parameters in the left panel, using the parameters in the right panel. The covariance of interest, θ 12 , has been converted to a normal phantom latent variable with variance ψ D and two directed paths, λ 1 and λ 2 . To implement an approach related to that of <ref type="bibr" target="#b2">Barnard et al. (2000)</ref> in JAGS, we set</p><formula xml:id="formula_12">ψ D = 1 λ 1 = |ρ 12 |θ 11 λ 2 = sign(ρ 12 ) |ρ 12 |θ 22 θ * 11 = θ 11 -|ρ 12 |θ 11 θ * 22 = θ 22 -|ρ 12 |θ 22 ,</formula><p>where ρ 12 is the correlation associated with the covariance θ 12 , and sign(a) equals 1 if a &gt; 0 and -1 otherwise.</p><p>Using this combination of parameters, we need only set prior distributions on the inferential model parameters θ 11 , θ 22 , and ρ 12 , with θ 12 being obtained from these parameters in the usual way. If we want our priors to be analogous to an inverse Wishart prior with d degrees of freedom and diagonal scale matrix S, we can set univariate priors of</p><formula xml:id="formula_13">θ 11 ∼ IG((d -p + 1)/2, s 11 /2) θ 22 ∼ IG((d -p + 1)/2, s 22 /2) ρ 12 ∼ Beta (-1,1) ((d -p + 1)/2, (d -p + 1)/2),</formula><p>where Beta (-1,1) is a beta distribution with support on (-1, 1) instead of the usual (0, 1) and p is the dimension of S (typically, the number of observed variables). These priors are related to those used by <ref type="bibr" target="#b34">Muthén and Asparouhov (2012)</ref>, based on results from <ref type="bibr" target="#b2">Barnard et al. (2000)</ref>. They are also the default priors for variance/correlation parameters in blavaan, with d = (p + 1) and S = I. We refer to this parameterization option as "srs", reflecting the fact that we are dissecting the covariance parameters into standard deviation and correlation parameters. <ref type="bibr" target="#b2">Barnard et al. (2000)</ref> avoid the inverse gamma priors on the variance parameters, instead opting for log-normal distributions on the standard deviations that they judged to be more numerically stable. Package blavaan allows for custom priors, so that the user can employ the log-normal or others on precisions, variances, or standard deviations (this is illustrated later). This approach is generally advantageous because it allows for flexible specification of prior distributions on covariance matrices, including those with fixed zeros or those where we have different amounts of prior information on certain variance/correlation parameters within the matrix.</p><p>While we view the "srs" approach as optimal for dealing with covariance parameters here, there exist similar alternative approaches in JAGS. For example, we can treat the phantom latent variables similarly to the other latent variables in the model, assigning prior distributions in the same manner. This alternative approach, which we call "fa" (because the priors resemble traditional factor analysis priors), involves the following default prior distributions on the working model from Figure <ref type="figure" target="#fig_2">2</ref>:</p><formula xml:id="formula_14">X 1 θ 11 X 2 θ 22 θ 12 Inferential model X 1 θ * 11 X 2 θ * 22 D ψ D λ 1 λ 2 Working model</formula><formula xml:id="formula_15">ψ D ∼ IG(1, .5) λ 1 ∼ N (0, 10 4 ) λ 2 ∼ N (0, 10 4 ) θ * 11 ∼ IG(1, .5) θ * 22 ∼ IG(1, .5),</formula><p>with the inferential model parameters being obtained in the usual way:</p><formula xml:id="formula_16">θ 11 = θ * 11 + λ 2 1 ψ D θ 22 = θ * 22 + λ 2 2 ψ D θ 12 = λ 1 λ 2 ψ D .</formula><p>The main disadvantage of the "fa" approach is that the implied prior distributions on the inferential model parameters are not of a common form. Thus, it is difficult to introduce informative prior distributions for the inferential model variance/covariance parameters. For example, the prior on the inferential covariance (θ 12 ) is the product of two normal prior distributions and an inverse gamma, which can be surprisingly informative. To avoid confusion here, we do not allow the user to directly modify the priors on the working parameters λ 1 , λ 2 , and ψ D under the "fa" approach. The priors chosen above are approximately noninformative for most applications, and the user can further modify the exported JAGS code if he/she desires. Along with the prior distribution issue, the working model parameters are not identified by the likelihood because each inferential covariance parameter is the product of the three working parameters. This can complicate Laplace approximation of the marginal likelihood (which, as described below, is related to the Bayes factor), because the approximation works best when the posterior distributions are unimodal.</p><p>In blavaan, the user is free to specify "srs" or "fa" priors for all covariance parameters in the model. In the example section, we present a simple comparison of these two options' relative speeds and efficiencies. Beyond these options, the package attempts to identify when it can sample latent variables directly from a multivariate normal distribution, which can improve sampling efficiency. This most commonly happens when we have many exogenous latent variables that all covary with one another. In this case, we place an inverse Wishart prior distribution on the latent variable covariance matrix.</p><p>While we prefer the "srs" approach to covariances between observed variables, the "fa" approach may be useful for complex models that run slowly. In these cases, it could be useful to sacrifice precise prior specification in favor of speed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Model fit and comparison</head><p>Package blavaan supplies a variety of statistics for model evaluation and comparison, available via the fitMeasures() function. This function is illustrated later in the examples, and specific statistics are described in Appendix A. For model evaluation, blavaan supplies posterior predictive checks of the model's log-likelihood (e.g., <ref type="bibr" target="#b15">Gelman, Carlin, Stern, and Rubin 2004)</ref>. For model comparision, it supplies a variety of statistics including the Deviance Information Criterion (DIC; <ref type="bibr" target="#b46">Spiegelhalter, Best, Carlin, and Van Der Linde 2002)</ref>, the (Laplace-approximated) log-Bayes factor, the Widely Applicable Information Criterion (WAIC; Watanabe 2010), and the leave-one-out cross-validation statistic (LOO; e.g., <ref type="bibr" target="#b11">Gelfand 1996)</ref>. The latter two statistics are computed by R package loo (Vehtari, Gelman, and Gabry 2015), using output from blavaan.</p><p>Calculation of the information criteria is somewhat complicated by the fact that, during JAGS model estimation, we condition on the latent variables η i , i = 1, . . . , n. That is, our model utilizes likelihoods of the form L(ϑ, η i |y i ), where ϑ is the vector of model parameters and η i is a vector of latent variables associated with individual i. Because the latent variables are random effects, we must integrate them out to obtain L(ϑ|Y ), the likelihood by which model assessment statistics are typically computed. This is easy to do for the models considered here, because the integrated likelihood continues to be multivariate normal. However, we cannot rely on JAGS to automatically calculate the correct likelihood, so that blavaan calculates the likelihood separately after parameters have been sampled in JAGS. Now that we have provided background information on the models implemented in blavaan, the next section further describes how the package works. We then provide a series of examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Overview of blavaan</head><p>Readers familiar with lavaan will also be familiar with blavaan. The main functions are the same as the main lavaan functions, except that they start with the letter 'b'. For example, confirmatory factor analysis models are estimated via bcfa() and structural equation models are estimated via bsem(). These two functions call the more general blavaan() function with a specific arrangement of arguments. The blavaan model specification syntax is nearly the same as the lavaan model specification syntax, and many functions are used by both packages.</p><p>As compared to lavaan, there are a small number of new features in blavaan that are specific to Bayesian modeling: prior distribution specification, export/use of JAGS syntax, convergence diagnostics, and specification of initial values. We discuss these topics in the context of a simple, one-factor confirmatory model. The model uses the <ref type="bibr" target="#b19">Holzinger and Swineford (1939)</ref> data included with lavaan, with code for model specification and estimation appearing below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>R&gt; model &lt;-</head><p>visual =~x1 + x2 + x3 R&gt; fit &lt;-bcfa(model, data = HolzingerSwineford1939, jagfile = TRUE)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Prior distribution specification</head><p>Package blavaan defines a default prior distribution for each type of model parameter (see Equations 1 and 2). These default priors can be viewed via R&gt; dpriors() nu alpha lambda beta itheta "dnorm(0,1e-3)" "dnorm(0,1e-2)" "dnorm(0,1e-2)" "dnorm(0,1e-2)" "dgamma(1,.5)" ipsi rho ibpsi tau delta "dgamma(1,.5)"</p><p>"dbeta(1,1)" "dwish(iden,3)" "dnorm(0,.1)" "dgamma(1,.5)"</p><p>which includes a separate default prior for ten types of parameters. Default prior distributions are placed on precisions instead of variances, so that the letter i in itheta and ipsi stands for "inverse." Most of these parameters correspond to notation from Equations 1 to 4, with the exception of rho and ibpsi. The rho prior is used only for the "srs" option, as it is for correlation parameters associated with covariances in Θ or Ψ. The ibpsi prior is used for the covariance matrix of latent variables that are all allowed to covary with one another (commonly, blocks of exogenous latent variables). This block prior can improve sampling efficiency and reduce autocorrelation.</p><p>For most parameters, the user is free to declare prior distributions using any prior distribution that is defined in JAGS. Changes to default priors can be supplied with the dp argument. Further, the modifiers [sd] and [var] can be used to put a prior on a standard deviation or variance parameter, instead of the corresponding precision parameter. For example, R&gt; fit &lt;-bcfa(model, data = HolzingerSwineford1939, + dp = dpriors(nu = "dnorm(4,1)", itheta = "dunif(0,20)[sd]"))</p><p>sets the default priors for (i) the manifest variables' intercepts to normal with mean 4 and precision 1, and (ii) the manifest variables' standard deviations to uniform with bounds of 0 and 20.</p><p>Priors associated with the rho and ibpsi parameters are less amenable to change. The default rho prior is a beta(1,1) distribution with support on (-1, 1); beta distributions with other parameter values could be used, but this distribution must be beta for now. The default prior on blocks of precision parameters is the Wishart with identity scale matrix and degrees of freedom equal to the dimension of the scale matrix plus one. The user cannot easily make changes to this prior distribution via the dp argument. Changes can be made, however, by exporting the JAGS syntax and manually editing it. This is further described in the next section.</p><p>In addition to priors on classes of model parameters, the user may wish to set the prior of a specific model parameter. This is accomplished by using the prior() modifier within the model specification. For example, R&gt; model &lt;-visual =~x1 + prior("dnorm(1,1)")*x2 + x3 sets a specific prior distribution for the loading parameter from the visual factor to x2. Priors set in this manner (via the model syntax) take precedent over the default priors set via the dp argument. Additionally, if one specifies a covariance parameter in the model syntax, the prior() modifier is for the correlation associated with the covariance, as opposed to the covariance itself. The prior should be a distribution with support on (0, 1) (typically the beta distribution), which is automatically converted to an analogous distribution that has support on (-1, 1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">JAGS syntax</head><p>Users may find that some desired features are not currently implemented in blavaan; such features may be related to, e.g., the use of specific types of priors or the handling of discrete variables. In these situations, we allow the user to export the JAGS syntax so that they can implement new features themselves. We believe this will be especially useful for researchers who wish to develop new types of models: these researchers can specify a traditional model in blavaan that is related to the one that they want to implement, and they can then obtain the JAGS syntax and data for the traditional model. This JAGS syntax should ease implementation of the novel model.</p><p>To export the JAGS syntax, users can employ the jagfile argument that was used in the bcfa command above. When jagfile is set to TRUE, the syntax will be written to the lavExport folder; if a character string is supplied, blavaan will use the character string as the folder name.</p><p>When the syntax is exported, two files are written within the folder. The first, sem.jag, contains the model specification in JAGS syntax. The second, semjags.rda, is a list named jagtrans that contains the data and initial values in JAGS format, as well as labels associated with model parameters. These pieces can be used to run the model "manually" via, e.g., package rjags <ref type="bibr" target="#b38">(Plummer 2016)</ref> or runjags <ref type="bibr" target="#b10">(Denwood 2016)</ref>. The example below illustrates how to load the JAGS data (along with initial values and parameter labels) and estimate the exported model via runjags.</p><p>R&gt; load("lavExport/semjags.rda") R&gt; fit &lt;-run.jags("lavExport/sem.jag", monitor = jagtrans$coefvec$jlabel, + data = jagtrans$data, inits = jagtrans$inits)</p><p>If the user modifies the JAGS file, then the monitor and inits arguments might require modification (or, for automatic initial values from JAGS, the user could omit the inits argument). The data should not require modification unless the user adds or removes observed variables from the model.</p><p>We also note that the dimensions of the parameter vectors/matrices in the JAGS syntax generally match the dimensions that we would expect from Equations 1 and 2, with a third dimension being added for multiple group models. These matrix dimensions (and the specific nonzero entries within each matrix) are obtained directly from lavaan.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Convergence diagnostics</head><p>Package blavaan offers two methods for monitoring chain convergence, specified as the "auto" or "manual" options to the convergence argument. Regardless of which option is used, the default number of chains is three and can be modified via the n.chains argument.</p><p>Under "auto" convergence, chains are sampled for an initial period in order to (i) achieve convergence as determined by the potential scale reduction factor (PSRF; Gelman and Rubin 1992) and (ii) determine the number of samples necessary to obtain precise posterior estimates. Following this initial period, the chains are further sampled for the determined number of iterations. This automatic assessment comes directly from the autorun.jags() function of package runjags <ref type="bibr" target="#b10">(Denwood 2016)</ref>; see there for further detail.</p><p>Under "manual" convergence, the user can specify the desired number of adaptation, burnin, and sample iterations via arguments of the same name (with defaults of 1,000 adaptation iterations, 4,000 burnin iterations, and 10,000 sample iterations). Adaptation iterations are those where samplers can modify themselves to increase sampling efficiency, whereas burnin iterations are discarded samples obtained from "fixed" samplers (e.g., <ref type="bibr" target="#b37">Plummer 2015)</ref>. A warning is issued if any PSRF is greater than 1.2, though some users may desire a more stringent criterion that is closer to 1.0. Access to the parameters' PSRF values is obtained via R&gt; blavInspect(fit, "psrf")</p><p>Additionally, there is a plot method that includes the familiar time series plots, autocorrelation plots, and more. For example, autocorrelation plots for the first four free parameters are obtained via R&gt; plot(fit, 1:4, "autocorr")</p><p>where parameter numbers correspond to the ordering of coef(fit). This plot functionality comes from package runjags, with plotting options being found in the help file for plot.runjags().</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Initial values</head><p>In many Bayesian SEMs, poorly-chosen initial values can lead to extremely slow convergence.</p><p>Package blavaan provides multiple options for supplying initial values that vary from chain to chain and/or that improve chain convergence. These are obtained via the inits argument. Under option "prior" (default), starting values are random draws from each parameter's prior distribution. However, to aid in convergence, loading and regression parameters are all required to be positive and close to 1, and correlation parameters are required to be close to zero. Under option "jags", starting values are automatically set by JAGS.</p><p>Additional options for starting values are obtained as byproducts of lavaan. For example, the options "Mplus" and "simple" are analogous to the options for the lavaan start argument, where the same initial values are used for all chains. Individual initial values can also be set via the start() modifier to the lavaan syntax, and these initial values will be used for all chains.</p><p>Finally, it is also possible to supply a full set of user-defined starting values for each chain. This is most easily accomplished by estimating the model once in order to obtain the list of initial values. The initial values of a fitted model can be obtained via blavInspect(); i.e.,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>R&gt; myinits &lt;-blavInspect(fit, "inits")</head><p>The object myinits is a list whose length equals n.chains, with each entry being another list that contains initial values for the model's parameter vector. It may be helpful to call blavInspect() with the "jagnames" argument, in order to examine correspondence between blavaan parameter names and JAGS parameter names. Finally, after editing, a call to blavaan with the argument inits = myinits will estimate a model using the custom starting values.</p><p>Now that we have seen some features that are novel to blavaan, we will further illustrate the package by example.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Applications</head><p>In the applications below, we first illustrate some general features involving prior distribution specification and model fit measures. We then illustrate the study of measurement invariance via Bayesian models, which involves multiple across-group parameter constraints. Finally, we provide some advanced examples involving the direct modification of exported JAGS code and the use of informative prior distributions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Political democracy</head><p>We begin with the <ref type="bibr" target="#b5">Bollen (1989)</ref>   sampled parameters can be obtained immediately; for example, the command below provides trace plots for the first four free parameters, with the resulting plot displayed in Figure <ref type="figure" target="#fig_3">3</ref>.</p><p>R&gt; plot(fit, 1:4, "trace")</p><p>As described earlier in the paper, users can also control the handling of residual covariances via the cp argument. For the particular model considered here, we compared the sampling speed and efficiency (excluding posterior predictive checks) of the "srs" and "fa" options using a four-core Dell Precision laptop running Ubuntu linux. We observed that the "srs" option took 76 seconds and the "fa" option took 65 seconds, illustrating the speed advantage of the "fa" approach. However, the ratio of effective sample sizes for the two approaches averaged 1.49 in favor of the "srs" approach (where the average is taken across parameters), implying that "srs" leads to more efficient sampling. In our experience, these results can vary depending on the specific model of interest, but it generally appears that "fa" is somewhat faster and "srs" somewhat more efficient.</p><p>Once we have sampled for the desired number of iterations, we can make use of lavaan functions in order to summarize and study the fitted model. Users may primarily be interested in summary(), which organizes results in a manner similar to the classical lavaan models. While the results look similar to those that one would see in lavaan, there are a few differences that require explanation. First, the top of the output includes two model evaluation measures: a Laplace approximation of the marginal log-likelihood and the posterior predictive p value (see Appendix A). Second, the "Parameter Estimates" section contains many new columns. These are the posterior mean (Post.Mean), the posterior standard deviation (Post.SD), a 95% highest posterior density interval (HPD.025 and HPD.975), the potential scale reduction factor for assessing chain convergence (PSRF; Gelman and Rubin 1992), and the prior distribution associated with each model parameter (Prior). Users can optionally obtain posterior medians, posterior modes, and effective sample sizes (number of posterior samples drawn, adjusted for autocorrelation). These can be obtained by supplying the logical arguments postmedian, postmode, and neff to summary(). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>R&gt; summary(fit)</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Measurement invariance</head><p>Verhagen and Fox (2013; see also <ref type="bibr" target="#b52">Verhagen, Levy, Millsap, and Fox 2016)</ref> describe the use of Bayesian methods for studying the measurement invariance of sets of items or scales. Briefly, measurement invariance means that the items or scales measure diverse groups of individuals in a fair manner. For example, two individuals with the same level of mathematics ability should receive the same score on a mathematics test (within random noise), regardless of the individuals' backgrounds and demographic variables. In this section, we illustrate a Bayesian measurement invariance approach using the <ref type="bibr" target="#b19">Holzinger and Swineford (1939)</ref> data. This section also illustrates how we can estimate a model in blavaan using pieces of a fitted lavaan object. This may be of interest to advanced users who would prefer to write/edit a lavaan "parameter table" instead of using model specification syntax. Additionally, the use of a lavaan object provides a path from Mplus to blavaan, via the mplus2lavaan() function from lavaan.</p><p>We first fit three increasingly-restricted models to the <ref type="bibr" target="#b19">Holzinger and Swineford (1939)</ref> data via classical methods. This is accomplished via the lavaan cfa() function (though we could also immediately use bcfa() here):</p><formula xml:id="formula_17">R&gt; HS.model &lt;- visual =~x1 + x2 + x3 + textual =~x4 + x5 + x6 + speed =~x7 + x8 + x9 R&gt; fit1 &lt;-</formula><p>cfa(HS.model, data = HolzingerSwineford1939, group = "school") R&gt; fit2 &lt;-cfa(HS.model, data = HolzingerSwineford1939, group = "school", + group.equal = "loadings") R&gt; fit3 &lt;-cfa(HS.model, data = HolzingerSwineford1939, group = "school", + group.equal = c("loadings", "intercepts"))</p><p>The resulting objects each include a "parameter table," which contains all model details necessary for lavaan estimation. To fit these models via Bayesian methods, we can merely pass the classical model's parameter table and data to blavaan. This is accomplished via R&gt; bfit1 &lt;-bcfa(parTable(fit1), data = HolzingerSwineford1939, + group = "school") R&gt; bfit2 &lt;-bcfa(parTable(fit2), data = HolzingerSwineford1939, + group = "school") R&gt; bfit3 &lt;-bcfa(parTable(fit3), data = HolzingerSwineford1939, + group = "school")</p><p>As mentioned above, we could also supply the group.equal argument directly to the bcfa() calls.</p><p>Following model estimation, we can immediately compare the models via fitMeasures().</p><p>For example, focusing on the first two models, we obtain As judged by the posterior predictive p value (ppp), neither of the two models provide a good fit to the data. In practice, we might stop our measurement invariance study there. However, for the purpose of demonstration, we can also examine the information criteria. Based on these, we would prefer the second model (due to the smaller DIC, WAIC, and LOOIC values).</p><formula xml:id="formula_18">R&gt;</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Informative prior distributions</head><p>We have seen that blavaan allows users to specify prior distributions for individual model parameters and for classes of model parameters. In this section, we illustrate how informative prior distributions can be set for factor analysis parameters in the context of a specific dataset, where priors are set based on the parameters' interpretations.</p><p>We consider a one-factor, three-indicator model fit to a subset of data from a stereotype threat study <ref type="bibr" target="#b54">(Wicherts, Dolan, and Hessen 2005)</ref>. The data are available in R package psychotools <ref type="bibr" target="#b55">(Zeileis, Strobl, Wickelmaier, Komboz, and Kopf 2018)</ref>, and the model specifies a general "ability" factor underlying three academic tests (of verbal ability, numerical ability, and abstract reasoning). The test scores are sums of the number of items answered correctly, so they are bounded from below at 0 and from above at 16 (verbal), 14 (numerical), and 18 (abstract reasoning). While these bounds technically violate the normality assumption of the factor analysis model, it is customary to apply this model to test scores (especially in the current situation, where the test scores are unimodal with modes in the middle of the score ranges). A factor analysis model with default (non-informative) prior distributions and automatic convergence assessment can be fit to the data via R&gt; library("psychotools") R&gt; data("StereotypeThreat") R&gt; st &lt;-subset(StereotypeThreat, ethnicity == "majority") R&gt; model &lt;-ability =~abstract + verbal + numerical R&gt; bfit &lt;-bcfa(model, data = st, convergence = "auto") Instead of using the default prior distributions, we can tailor prior distributions to this particular dataset by simultaneously considering the data and each model parameter's interpretation. First, because the test scores are all bounded, we know the maximum possible standard deviation on each test (obtained if half the participants scored a 0 and half score the maximum). These are 8 for the verbal test, 7 for the numerical test, and 9 for the abstract reasoning test. Thus, we place uniform priors on the manifest variables' standard deviations with lower bounds at 0 and upper bounds at these maxima: R&gt; model &lt;-c(model, + abstract ~~prior("dunif(0,9)[sd]")*abstract + verbal ~~prior("dunif(0,8)[sd]")*verbal + numerical ~~prior("dunif(0,7)[sd]")*numerical ) Next, we consider the factor variance. Because the loading associated with the abstract reasoning test will be fixed at 1, the factor variance can be interpreted as the variability in the abstract reasoning test that can be attributed to the ability factor. Because the three tests were chosen to be highly correlated (and because the total standard deviation of abstract reasoning is 9 or less), we use a uniform prior with an upper bound of 6 here. Thus, even in the case where we observe the maximum standard deviation of 9, the factor could still account for 2/3 of the variability in the ability test.</p><formula xml:id="formula_19">Parameter Prior λ 2 λ 3 θ 1 θ 2 θ 3 ψ 1 ν 1 ν 2 ν 3 Default Est 1.</formula><p>R&gt; model &lt;-c(model, ability ~~prior("dunif(0,4.5)[sd]")*ability )</p><p>Next, we consider the manifest variables' intercept parameters, which reflect average scores on each of the three tests. These tests are known historically to result in average scores near the middle of the range <ref type="bibr" target="#b54">(Wicherts et al. 2005)</ref>, so our prior distributions are truncated normals centered at the middle of each test's range (with the truncation points at the minimum and maximum possible scores on each test):</p><p>R&gt; model &lt;-c(model, + abstract ~prior("dnorm(9,.25) T(0,18)")*1 + verbal ~prior("dnorm(8,.25) T(0,16)")*1 + numerical ~prior("dnorm(7,.25) T(0,14)")*1 )</p><p>Finally, we consider the factor loadings. We have no knowledge that the ability factor will influence one test more than others, so we might expect the two free loadings to be around 1 (because the fixed loading equals 1). We also expect positive loadings, because ability should positively influence all three tests. Thus, we supply uniform(0,3) priors to the dp argument of bcfa():</p><p>R&gt; bfitInform &lt;-bcfa(model, data = st, + dp = dpriors(lambda = "dunif(0,3)"), convergence = "auto")</p><p>The two models' fit measures are generally similar and not shown. We compare the two models' posterior means and standard deviations in Table <ref type="table" target="#tab_5">1</ref>. The differences are unlikely to impact substantive conclusions, but two of them are noteworthy. First, the factor variance ψ 1 is larger under the model with informative priors, likely because the informative prior (uniform(0,4.5) on the standard deviation) placed more density on larger values of the standard deviation. We observe a similar phenomenon with the residual variance of the numerical test (θ 3 ). Second, the posterior means of the loadings (λ 2 and λ 3 ) are somewhat smaller under the informative priors. This is likely related to the larger factor variance estimates. This example could be used as the start of a larger analysis of posterior sensitivity to prior distributions. For the purpose of automation, prior distributions could be entered directly into the model's parameter table <ref type="bibr">(obtained via, e.g., parTable(bfit)</ref>) and the model subsequently re-estimated, similar to what was done in the measurement invariance example.</p><formula xml:id="formula_20">model { for(i in 1:N) { abstract[i] ~dt(mu[i,1], 1/theta[1,1,g[i]], df) verbal[i] ~dt(mu[i,2], 1/theta[2,2,g[i]], df) numerical[i] ~dt(mu[i,3], 1/theta[3,3,g[i]], df) # lvs eta[i,1] ~dt(mu_eta[i,1], 1/psi[1,1,g[i]], df) } df &lt;-1/dfinv dfinv ~dunif(1/200, 1) # mu definitions</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Extensions of JAGS syntax</head><p>Finally, we provide an example involving use of the JAGS syntax to estimate novel models. Consider the robust factor analysis model of <ref type="bibr" target="#b56">Zhang, Li, and Liu (2014)</ref>, which essentially replaces the factor analysis model's normal distributions with t distributions. In particular, each factor η arises from a t df (0, 1) distribution, and each residual j arises from a t df (0, ψ j ) distribution. The degrees of freedom, df, is a free parameter and is shared by all the t distributions. To our knowledge, there exists no software to readily estimate this model, and <ref type="bibr" target="#b56">Zhang et al. (2014)</ref> implemented their own Gibbs sampler (making use of analytic posterior distributions under conjugate priors). Here, we show how the JAGS syntax from blavaan can be modified to easily estimate this model. For illustration, we again apply a one-factor model to the <ref type="bibr" target="#b54">Wicherts et al. (2005)</ref> data from the previous section.</p><p>We begin by using blavaan to export JAGS syntax for a simple, one-factor model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>R&gt; model &lt;-</head><p>ability =~abstract + verbal + numerical R&gt; bfit &lt;-bcfa(model, data = st, jagfile = TRUE) By default, the exported JAGS code is written to the file lavExport/sem.jag. Upon opening that file, we must make two edits (see Figure <ref type="figure" target="#fig_4">4</ref>). First, the normal distributions (dnorm()) are replaced with t distributions (dt()). Second, we need to specify a prior distribution for the degrees of freedom parameter. Similar to <ref type="bibr" target="#b56">Zhang et al. (2014)</ref>, we place a flat prior on the inverse degrees of freedom. The lower bound is nonzero to aid in convergence; this is justified by realizing that, as the degrees of freedom increase, the t distribution becomes the normal distribution. Once we surpass, say, 200 degrees of freedom, we can accept that larger values are practically equivalent to 200.</p><p>The modified syntax from Figure <ref type="figure" target="#fig_4">4</ref> could then be estimated manually, making use of the exported data and adding a monitor for the df parameter. For example, if the exported files are saved as lavExport/robustfa.jag and lavExport/robustfa.rda, the model can be estimated via R&gt; load("lavExport/robustfa.rda") R&gt; fit &lt;-run.jags("lavExport/robustfa.jag", + monitor = c(jagtrans$monitors, "df"), data = jagtrans$data, + inits = jagtrans$inits)</p><p>While many model extensions will be more complicated than the one considered here, this example illustrates blavaan's potential use for statistical researchers who are developing new models: instead of writing JAGS syntax entirely from scratch, these researchers may use blavaan to obtain syntax for a basic model that is similar to the desired model. In many cases, this will simplify implementation of the desired model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>As described throughout the paper, package blavaan combines many existing tools so that it is easy to estimate multivariate normal SEMs via open-source software. The package can be useful to applied researchers who need an expanded set of prior distributions at their disposal, freeing them from the need to learn the intricacies of an MCMC package. It can also be useful to methodological researchers, freeing them from the need to code their own JAGS models from scratch. The idea of separating model specification from MCMC coding seems generally useful beyond SEM, and others are indeed making progress in this direction. For example, package rstanarm (Stan Development Team 2018) allows users to estimate generalized linear mixed models via Stan, using lme4 (Bates, Mächler, Bolker, and Walker 2015) syntax. The coding of complex models in BUGS, JAGS, or Stan is often tedious even for experienced users, so that progress here could improve most users' workflows.</p><p>There are a variety of additional models that we plan to support in future versions of blavaan, including models with latent interactions, with ordinal variables, and with mixture and multilevel components. These models have been addressed in the literature (e.g., <ref type="bibr" target="#b1">Asparouhov and Muthén 2010;</ref><ref type="bibr">Song and</ref><ref type="bibr">Lee 2001, 2012)</ref>, and JAGS examples for estimating specific instances of these models is available (e.g., <ref type="bibr" target="#b8">Cho, Preacher, and Bottge 2015;</ref><ref type="bibr" target="#b31">Merkle and Wang 2018)</ref>. Work remains, however, to sample from these models efficiently and to specify the models via lavaan's syntax. The sampling efficiency of Stan may be useful for at least some of these models, and we plan to explore this in the future.</p><p>In summary, blavaan is currently useful for estimating many types of multivariate normal SEMs, and the JAGS export feature allows researchers to extend the models in any fashion desired. As additional features are added, we hope that the package keeps pace with lavaan as an open set of tools for SEM estimation and study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Details on model assessment statistics</head><p>In the subsections below, we review the model evaluation and comparison metrics that blavaan supplies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Posterior predictive checks</head><p>As a measure of the model's absolute fit, blavaan computes a posterior predictive p value that compares observed likelihood ratio test (χ 2 ) statistics to likelihood ratio test statistics generated from the model's posterior predictive distribution (also see <ref type="bibr" target="#b34">Muthén and Asparouhov 2012;</ref><ref type="bibr" target="#b43">Scheines et al. 1999)</ref>. The likelihood ratio test statistic is generally computed via</p><formula xml:id="formula_21">LRT(Y , ϑ) = -2 log L(ϑ|Y ) + 2 log L(ϑ sat |Y ),</formula><p>where ϑ sat is a "saturated" parameter vector that perfectly matches the observed mean and covariance matrix.</p><p>For each posterior draw ϑ s (s = 1, . . . , S), computation of the posterior predictive p value includes four steps:</p><p>1. Compute the observed LRT statistic as LRT(Y , ϑ s ). For use in practice, <ref type="bibr" target="#b34">Muthén and Asparouhov (2012)</ref> use a threshold of 0.05 (analogous to a frequentist α level) and present some evidence that, as compared to the frequentist likelihood ratio test, the posterior predictive p is less sensitive to minor model misspecifications and exhibits better performance at small sample sizes. On the other hand, Hoijtink and Van de Schoot (2018) show that the posterior predictive p value misbehaves in situations involving highly informative prior distributions (as are sometimes used for shrinkage/regularization). They recommend a prior-posterior predictive p value that may be implemented in future versions of blavaan.</p><p>In blavaan, the posterior predictive p value is currently impractical when there are missing data. This is because it is impractical to compute the saturated log-likelihood in the presence of missing data. We need to use, e.g., the EM algorithm to do this, and this needs to be done (S + 1) times (once for the saturated likelihood of the observed data and once for the saturated likelihood of each of the S artificial datasets). With complete data, we can more simply calculate the saturated log-likelihood using the sample mean vector and covariance matrix. The argument test = "none" can be supplied to blavaan in order to bypass these slow calculations when there are missing data, and future versions may sample the missing observations in order to avoid the issue.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DIC</head><p>The DIC is given as DIC = -2 log L( θ|Y ) + 2 efp DIC , (7</p><formula xml:id="formula_22">)</formula><p>where θ is a vector of posterior parameter means (or other measure of central tendency), L( θ|Y ) is the model's likelihood at the posterior means, and efp DIC is the model's effective number of parameters, calculated as efp DIC = 2 log L( θ|Y ) -log L(ϑ|Y ) .</p><p>(8)</p><p>The latter term, log L(ϑ|Y ), is obtained by calculating the log-likelihood for each posterior sample and then averaging.</p><p>Many readers will be familiar with the automatic calculation of DIC within programs like BUGS and JAGS. As we described above, the automatic DIC is not what users typically desire because the likelihood conditions on the latent variables η i . This greatly increases the effective number of parameters and may result in poor inferences (for further discussion of this issue, see <ref type="bibr" target="#b32">Millar 2009)</ref>. As previously mentioned, blavaan avoids the automatic DIC computation in JAGS, calculating its own likelihoods after model parameters have been sampled.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>WAIC and LOO</head><p>The WAIC and LOO are asymptotically equivalent measures that are advantageous over DIC while also being more difficult to compute than DIC. Many computational difficulties are overcome in developments by <ref type="bibr" target="#b50">Vehtari, Gelman, and Gabry (2016)</ref>, with their methodology being implemented in package loo <ref type="bibr" target="#b49">(Vehtari et al. 2015)</ref>. As input, loo requires casewise loglikelihoods associated with a set of posterior samples: log L(ϑ s |y i ), s = 1, . . . , S; i = 1, . . . , n.</p><p>Like DIC, both of these measures seek to characterize a model's predictive accuracy (generalizability). The definition of WAIC looks similar to that of DIC:</p><formula xml:id="formula_23">WAIC = -2 lppd + 2 efp WAIC ,</formula><p>where the first term is related to log-likelihoods of observed data and the second term involves an effective number of parameters. Both terms now involve log-likelihoods associated with individual observations, however, which help us estimate the model's expected log pointwise predictive density (a measure of predictive accuracy).</p><p>The first term, the log pointwise predictive density of the observed data (lppd), is estimated via</p><formula xml:id="formula_24">lppd = n i=1 log 1 S S s=1 f (y i |ϑ s ) , (<label>9</label></formula><formula xml:id="formula_25">)</formula><p>where S is the number of posterior draws and f (y i |ϑ s ) is the density of observation i with respect to the parameters sampled at iteration s. The second term, the effective number of parameters, is estimated via</p><formula xml:id="formula_26">efp WAIC = n i=1</formula><p>var s (log f (y i |ϑ)),</p><p>where we compute a separate variance for each observation i across the S posterior draws.</p><p>The LOO measure estimates the predictive density of each individual observation from a cross-validation standpoint; that is, the predictive density when we hold out one observation at a time and use the remaining observations to update the prior. We can use analytical results to estimate this quantity based on the full-data posterior, removing the need to reestimate the posterior while sequentially holding out each observation. These results lead to the estimate We can also estimate the effective number of parameters under the LOO measure by comparing LOO to the lppd that was used in the WAIC calculation. This gives us efp LOO = lppd + LOO/2, where these terms come from Equations 9 and 10, respectively. Division of the latter term by two (and addition, vs. subtraction) offsets the multiplication by -2 that occurs in Equation <ref type="formula">10</ref>. For further detail on all these measures, see <ref type="bibr" target="#b50">Vehtari et al. (2016)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bayes factor</head><p>The Bayes factor between two models is defined as the ratio of the models' marginal likelihoods (e.g., <ref type="bibr" target="#b21">Kass and Raftery 1995)</ref>. Candidate model 1's marginal likelihood can be written as</p><formula xml:id="formula_27">f 1 (Y ) = f (ϑ 1 )f (Y |ϑ 1 )∂ϑ 1 , (<label>11</label></formula><formula xml:id="formula_28">)</formula><p>where ϑ 1 is a vector containing candidate model 1's free parameters (excluding the latent variables η i ) and f () is a probability density function. The marginal likelihood of candidate model 2 may be written in the same manner. The Bayes factor is then</p><formula xml:id="formula_29">BF 12 = f 1 (Y ) f 2 (Y ) ,</formula><p>with values greater than 1 favoring Model 1.</p><p>Because the integral from Equation 11 is generally difficult to calculate, <ref type="bibr" target="#b23">Lewis and Raftery (1997)</ref> describe a Laplace-Metropolis estimator of the Bayes factor (also see <ref type="bibr" target="#b39">Raftery 1993)</ref>. This estimator relies on the Laplace approximation to integrals that involve a natural exponent. Such integrals can be written as exp(h(u))∂u ≈ (2π) Q/2 |H * | 1/2 exp(h(u * )),</p><p>where h() is a function of the vector u, Q is the length of u, u * is the value of u that maximizes h, and H * is the inverse of the information matrix evaluated at u * . As applied to marginal likelihoods, we take h(ϑ) = log(f (ϑ)f (Y |ϑ)) so that our solution to Equation <ref type="formula" target="#formula_27">11</ref>is</p><formula xml:id="formula_30">f 1 (Y ) ≈ (2π) Q/2 |H * | 1/2 f (ϑ * )f (Y |ϑ * ),<label>(12)</label></formula></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Political democracy path diagrams, original (top) and parameter-expanded version (bottom). Variance parameters are omitted.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Example of phantom latent variable approach to covariances.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Trace plots of the first four parameters from the political democracy model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Illustration of JAGS modifications necessary to implement the Zhang et al. (2014) robust factor analysis model. The original JAGS code exported from blavaan is in black font, with modifications and additions in green. Additional code requiring no modification (extending below the "mu definitions" comment) is omitted.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>2.</head><label></label><figDesc>Generate artificial data Y rep from the model, assuming parameter values equal to ϑ s . 3. Compute the posterior predictive LRT statistic LRT(Y rep , ϑ s ). 4. Record whether or not LRT(Y rep , ϑ s ) &gt; LRT(Y , ϑ s ). The posterior predictive p value is then the proportion of the time (out of S draws) that the posterior predictive LRT statistic is larger than the observed LRT statistic. Values closer to 0.5 indicate a model that fits the observed data, while values closer to 0 indicate the opposite.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>LOO</head><label></label><figDesc>s i are importance sampling weights based on the relative magnitude of individual i's density function across the S posterior samples. Package loo smooths these weights via a generalized Pareto distribution, improving the estimate.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Finally, we can obtain various Bayesian model assessment measures via fitMeasures()</figDesc><table><row><cell cols="2">R&gt; fitMeasures(fit)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>npar</cell><cell>logl</cell><cell>ppp</cell><cell>bic</cell><cell>dic</cell><cell>p_dic</cell><cell>waic</cell></row><row><cell cols="2">39.000 -1550.509</cell><cell>0.525</cell><cell>3268.876</cell><cell>3174.329</cell><cell>36.656</cell><cell>3179.278</cell></row><row><cell>p_waic</cell><cell>looic</cell><cell cols="2">p_loo margloglik</cell><cell></cell><cell></cell><cell></cell></row><row><cell>38.836</cell><cell>3179.962</cell><cell cols="2">39.178 -1658.937</cell><cell></cell><cell></cell><cell></cell></row></table><note><p>where, as previously mentioned, the WAIC and LOO statistics are computed with the help of package loo. Other lavaan functions, including parameterEstimates() and parTable(), similarly work as expected.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 1 :</head><label>1</label><figDesc>Comparison of posterior means and standard deviations under default prior distributions and informative prior distributions.</figDesc><table><row><cell>1 1.58</cell><cell cols="2">7.9 8.43 1.49 2.06 9.84 6.97 5.43</cell></row><row><cell cols="2">SD 0.33 0.49 1.11</cell><cell>1.2 1.04 0.94 0.25 0.26 0.19</cell></row><row><cell cols="3">Inform. Est 1.06 1.37 7.73 8.39 2.03 2.51 9.83 6.99 5.44</cell></row><row><cell cols="3">SD 0.33 0.48 1.24 1.23 1.13 1.18 0.25 0.26 0.19</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This research was partially supported by <rs type="funder">NSF</rs> grants <rs type="grantNumber">SES-1061334</rs> and <rs type="grantNumber">1460719</rs>. The authors thank <rs type="person">Herbert Hoijtink</rs>, <rs type="person">Ross Jacobucci</rs>, two anonymous reviewers, and the editor for comments that helped to improve the paper. The authors are solely responsible for all remaining errors.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_6nKEssu">
					<idno type="grant-number">SES-1061334</idno>
				</org>
				<org type="funding" xml:id="_B4CuszD">
					<idno type="grant-number">1460719</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Bayesian Inference for a Covariance Matrix</title>
		<author>
			<persName><forename type="first">I</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Niemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Simpson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th Annual Conference on Applied Statistics in Agriculture</title>
		<meeting>the 26th Annual Conference on Applied Statistics in Agriculture</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="71" to="82" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Asparouhov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Muthén</surname></persName>
		</author>
		<ptr target="http://www.statmodel.com/download/Bayes3.pdf" />
		<title level="m">Bayesian Analysis Using Mplus: Technical Implementation. Version</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Modeling Covariance Matrices in Terms of Standard Deviations and Correlations, with Application to Shrinkage</title>
		<author>
			<persName><forename type="first">J</forename><surname>Barnard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mcculloch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">L</forename><surname>Meng</surname></persName>
		</author>
		<idno type="DOI">10.5705/ss.2010.253a</idno>
	</analytic>
	<monogr>
		<title level="j">Statistica Sinica</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="1281" to="1311" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Fitting Linear Mixed-Effects Models Using lme4</title>
		<author>
			<persName><forename type="first">D</forename><surname>Bates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mächler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Bolker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Walker</surname></persName>
		</author>
		<idno type="DOI">10.18637/jss.v067.i01</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Statistical Software</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="48" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Beaujean</surname></persName>
		</author>
		<title level="m">Latent Variable Modeling Using R. Routledge</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Bollen</surname></persName>
		</author>
		<idno type="DOI">10.1002/9781118619179</idno>
		<title level="m">Structural Equations with Latent Variables</title>
		<imprint>
			<publisher>John Wiley &amp; Sons</publisher>
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Stan: A Probabilistic Programming Language</title>
		<author>
			<persName><forename type="first">B</forename><surname>Carpenter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gelman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Goodrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Betancourt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Brubaker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Riddell</surname></persName>
		</author>
		<idno type="DOI">10.18637/jss.v076.i01</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Statistical Software</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="32" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Analysis of Multivariate Probit Models</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chib</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Greenberg</surname></persName>
		</author>
		<idno type="DOI">10.1093/biomet/85.2.347</idno>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">85</biblScope>
			<biblScope unit="page" from="347" to="361" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Detecting Intervention Effects in a Cluster-Randomized Design Using Multilevel Structural Equation Modeling for Binary Responses</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J</forename><surname>Preacher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Bottge</surname></persName>
		</author>
		<idno type="DOI">10.1177/0146621615591094</idno>
	</analytic>
	<monogr>
		<title level="j">Applied Psychological Measurement</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="627" to="642" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The Insidious Effects of Failing to Include Design-Driven Correlated Residuals in Latent-Variable Covariance Structure Analysis</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Ciesla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Steiger</surname></persName>
		</author>
		<idno type="DOI">10.1037/1082-989x.12.4.381</idno>
	</analytic>
	<monogr>
		<title level="j">Psychological Methods</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="381" to="398" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">runjags: An R Package Providing Interface Utilities, Model Templates, Parallel Computing Methods and Additional Distributions for MCMC Models in JAGS</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Denwood</surname></persName>
		</author>
		<idno type="DOI">10.18637/jss.v071.i09</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Statistical Software</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="page" from="1" to="25" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Model Determination Using Sampling-Based Methods</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Gelfand</surname></persName>
		</author>
		<editor>WR Gilks, S Richardson, DJ Spiegelhalter</editor>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="145" to="162" />
		</imprint>
	</monogr>
	<note>Markov Chain Monte Carlo in Practice</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title/>
		<author>
			<persName><surname>Chapman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">London</forename><surname>Hall</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Parameterization and Bayesian Modeling</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gelman</surname></persName>
		</author>
		<idno type="DOI">10.1198/016214504000000458</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="page" from="537" to="545" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Prior Distributions for Variance Parameters in Hierarchical Models</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gelman</surname></persName>
		</author>
		<idno type="DOI">10.1214/06-ba117a</idno>
	</analytic>
	<monogr>
		<title level="j">Bayesian Analysis</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="515" to="534" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Bayesian Data Analysis. 2nd edition</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gelman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Carlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">S</forename><surname>Stern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rubin</forename><surname>Db</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
			<publisher>Chapman &amp; Hall</publisher>
			<pubPlace>London</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Inference from Iterative Simulation Using Multiple Sequences</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gelman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Rubin</surname></persName>
		</author>
		<idno type="DOI">10.1214/ss/1177011136</idno>
	</analytic>
	<monogr>
		<title level="j">Statistical Science</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="457" to="511" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Default Prior Distributions and Efficient Posterior Computation in Bayesian Factor Analysis</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Dunson</surname></persName>
		</author>
		<idno type="DOI">10.1198/jcgs.2009.07145</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Computational and Graphical Statistics</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="306" to="320" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Testing Small Variance Priors Using Prior-Posterior Predictive P-Values</title>
		<author>
			<persName><forename type="first">H</forename><surname>Hoijtink</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Van De Schoot</surname></persName>
		</author>
		<idno type="DOI">10.1037/met0000131</idno>
	</analytic>
	<monogr>
		<title level="j">Psychological Methods</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">A Study of Factor Analysis: The Stability of a Bi-Factor Solution. Number 48 in Supplementary Educational Monograph</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J</forename><surname>Holzinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">A</forename><surname>Swineford</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1939">1939</date>
			<publisher>University of Chicago Press</publisher>
			<pubPlace>Chicago</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">G</forename><surname>Jöreskog</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sörbom</surname></persName>
		</author>
		<title level="m">LISREL 8 User&apos;s Reference Guide. Scientific Software International</title>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Bayes Factors</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Kass</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Raftery</surname></persName>
		</author>
		<idno type="DOI">10.2307/2291091</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="page" from="773" to="795" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">Lee</forename><surname>Sy</surname></persName>
		</author>
		<title level="m">Structural Equation Modeling: A Bayesian Approach</title>
		<imprint>
			<publisher>John Wiley &amp; Sons</publisher>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Estimating Bayes Factors via Posterior Simulation with the Laplace-Metropolis Estimator</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Raftery</surname></persName>
		</author>
		<idno type="DOI">10.2307/2965712</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">92</biblScope>
			<biblScope unit="page" from="648" to="655" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Bayes Factors: Prior Sensitivity and Model Generalizability</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Aitkin</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.jmp.2008.03.002</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Mathematical Psychology</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page" from="362" to="375" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Bayesian Factor Analysis as a Variable-Selection Problem: Alternative Priors and Consequences</title>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Chow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Loken</surname></persName>
		</author>
		<idno type="DOI">10.1080/00273171.2016.1168279</idno>
	</analytic>
	<monogr>
		<title level="j">Multivariate Behavioral Research</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page" from="519" to="539" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">The BUGS Book: A Practical Introduction to Bayesian Analysis</title>
		<author>
			<persName><forename type="first">D</forename><surname>Lunn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Jackson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Best</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Spiegelhalter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>Chapman &amp; Hall/CRC</publisher>
			<pubPlace>Boca Raton</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Lunn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Best</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Spiegelhalter</surname></persName>
		</author>
		<idno type="DOI">10.1023/a:1008929526011</idno>
	</analytic>
	<monogr>
		<title level="m">WinBUGS -A Bayesian Modelling Framework: Concepts, Structure, and Extensibility</title>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="325" to="337" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Structural Equation Models of Latent Interaction and Quadratic Effects</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">W</forename><surname>Marsh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">T</forename><surname>Hau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Nagengast</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Structural Equation Modeling: A Second Course, 2nd edition</title>
		<editor>
			<persName><surname>Hancock</surname></persName>
		</editor>
		<editor>
			<persName><surname>Mueller</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="267" to="308" />
		</imprint>
	</monogr>
	<note>Information Age Publishing</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">MCMCpack: Markov Chain Monte Carlo in R</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Quinn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Park</surname></persName>
		</author>
		<idno type="DOI">10.18637/jss.v042.i09</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Statistical Software</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="1" to="21" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Bayesian Estimation in Unrestricted Factor Analysis: A Treatment for Heywood Cases</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Mcdonald</surname></persName>
		</author>
		<idno type="DOI">10.1007/bf02291552</idno>
	</analytic>
	<monogr>
		<title level="j">Psychometrika</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="505" to="517" />
			<date type="published" when="1975">1975</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Bayesian Latent Variable Models for the Analysis of Experimental Psychology Data</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">C</forename><surname>Merkle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.3758/s13423-016-1016-7</idno>
	</analytic>
	<monogr>
		<title level="j">Psychonomic Bulletin &amp; Review</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="256" to="270" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Comparison of Hierarchical Bayesian Models for Overdispersed Count Data Using DIC and Bayes&apos; Factors</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Millar</surname></persName>
		</author>
		<idno type="DOI">10.1111/j.1541-0420.2008.01162.x</idno>
	</analytic>
	<monogr>
		<title level="j">Biometrics</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="page" from="962" to="969" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Murray</surname></persName>
		</author>
		<ptr target="https://CRAN.R-project.org/package=bfa" />
		<title level="m">bfa: Bayesian Factor Analysis. R package version 0.3.1</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Bayesian Structural Equation Modeling: A More Flexible Representation of Substantive Theory</title>
		<author>
			<persName><forename type="first">B</forename><surname>Muthén</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Asparouhov</surname></persName>
		</author>
		<idno type="DOI">10.1037/a0026802</idno>
	</analytic>
	<monogr>
		<title level="j">Psychological Methods</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="313" to="335" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Bayesian Structural Equation Modeling</title>
		<author>
			<persName><forename type="first">J</forename><surname>Palomo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Dunson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Bollen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Handbook of Latent Variable and Related Models</title>
		<editor>
			<persName><surname>Sy Lee</surname></persName>
		</editor>
		<imprint>
			<publisher>Elsevier</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="163" to="188" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">JAGS: A Program for Analysis of Bayesian Graphical Models Using Gibbs Sampling</title>
		<author>
			<persName><forename type="first">M</forename><surname>Plummer</surname></persName>
		</author>
		<ptr target="https://www.R-project.org/conferences/DSC-2003/Proceedings/Plummer.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd International Workshop on Distributed Statistical Computing (DSC 2003)</title>
		<editor>
			<persName><forename type="first">F</forename><surname>Hornik</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Leisch</surname></persName>
		</editor>
		<editor>
			<persName><surname>Zeileis</surname></persName>
		</editor>
		<meeting>the 3rd International Workshop on Distributed Statistical Computing (DSC 2003)<address><addrLine>Vienna, Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
		<respStmt>
			<orgName>Technische Universität Wien</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Plummer</surname></persName>
		</author>
		<title level="m">JAGS Version 4.0.0 User Manual</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">rjags: Bayesian Graphical Models Using MCMC</title>
		<author>
			<persName><forename type="first">M</forename><surname>Plummer</surname></persName>
		</author>
		<ptr target="http://CRAN.R-project.org/package=rjags" />
	</analytic>
	<monogr>
		<title level="m">R package version 4-6</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Bayesian Model Selection in Structural Equation Models</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Raftery</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Testing Structural Equation Models</title>
		<editor>
			<persName><surname>Ka Bollen</surname></persName>
		</editor>
		<editor>
			<persName><surname>Long</surname></persName>
		</editor>
		<meeting><address><addrLine>Beverly Hills</addrLine></address></meeting>
		<imprint>
			<publisher>Sage Publications</publisher>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="163" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Using Phantom and Imaginary Latent Variables to Parameterize Constraints in Linear Structural Models</title>
		<author>
			<persName><forename type="first">D</forename><surname>Rindskopf</surname></persName>
		</author>
		<idno type="DOI">10.1007/bf02294204</idno>
	</analytic>
	<monogr>
		<title level="j">Psychometrika</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="page" from="37" to="47" />
			<date type="published" when="1984">1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">lavaan: An R Package for Structural Equation Modeling</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Rosseel</surname></persName>
		</author>
		<idno type="DOI">10.18637/jss.v048.i02</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Statistical Software</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="36" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Is There a Free Lunch in Inference?</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">N</forename><surname>Rouder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Morey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Verhagen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Province</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Wagenmakers</surname></persName>
		</author>
		<idno type="DOI">10.1111/tops.12214</idno>
	</analytic>
	<monogr>
		<title level="j">Topics in Cognitive Science</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="520" to="547" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Bayesian Estimation and Testing of Structural Equation Models</title>
		<author>
			<persName><forename type="first">R</forename><surname>Scheines</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hoijtink</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Boomsma</surname></persName>
		</author>
		<idno type="DOI">10.1007/bf02294318</idno>
	</analytic>
	<monogr>
		<title level="j">Psychometrika</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="page" from="37" to="52" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Bayesian Estimation and Test for Factor Analysis Model with Continuous and Polytomous Data in Several Populations</title>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lee</forename><surname>Sy</surname></persName>
		</author>
		<idno type="DOI">10.1348/000711001159546</idno>
	</analytic>
	<monogr>
		<title level="j">British Journal of Mathematical and Statistical Psychology</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page" from="237" to="263" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lee</forename><surname>Sy</surname></persName>
		</author>
		<idno type="DOI">10.1002/9781118358887</idno>
		<title level="m">Basic and Advanced Bayesian Structural Equation Modeling: With Applications in the Medical and Behavioral Sciences</title>
		<imprint>
			<publisher>John Wiley &amp; Sons</publisher>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Bayesian Measures of Model Complexity and Fit</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Spiegelhalter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">G</forename><surname>Best</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">P</forename><surname>Carlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Van</forename><surname>Der Linde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename></persName>
		</author>
		<idno type="DOI">10.1111/1467-9868.00353</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society B</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="page" from="583" to="639" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">rstanarm: Bayesian Applied Regression Modeling via Stan. R package version 2</title>
		<ptr target="https://CRAN.R-project.org/package=rstanarm" />
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>Stan Development Team</publisher>
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Prior Sensitivity in Theory Testing: An Apologia for the Bayes Factor</title>
		<author>
			<persName><forename type="first">W</forename><surname>Vanpaemel</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.jmp.2010.07.003</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Mathematical Psychology</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page" from="491" to="498" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Vehtari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gelman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gabry</surname></persName>
		</author>
		<ptr target="https://github.com/jgabry/loo" />
		<title level="m">loo: Efficient Leave-One-Out Cross-Validation and WAIC for Bayesian Models</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Efficient Implementation of Leave-One-Out Cross-Validation and WAIC for Evaluating Fitted Bayesian Models</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vehtari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gelman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gabry</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1507.04544</idno>
		<ptr target="http://arxiv.org/abs/1507.04544" />
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note>stat.CO</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Bayesian Tests of Measurement Invariance</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Verhagen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Fox</surname></persName>
		</author>
		<idno type="DOI">10.1111/j.2044-8317.2012.02059.x</idno>
	</analytic>
	<monogr>
		<title level="j">British Journal of Mathematical and Statistical Psychology</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="page" from="383" to="401" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Evaluating Evidence for Invariant Items: A Bayes Factor Applied to Testing Measurement Invariance in IRT Models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Verhagen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Millsap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Fox</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.jmp.2015.06.005</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Mathematical Psychology</title>
		<imprint>
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="page" from="171" to="182" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Asymptotic Equivalence of Bayes Cross Validation and Widely Applicable Information Criterion in Singular Learning Theory</title>
		<author>
			<persName><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
		<idno type="DOI">10.1109/scis-isis.2012.6505025</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="3571" to="3594" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Stereotype Threat and Group Differences in Test Performance: A Question of Measurement Invariance</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Wicherts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">V</forename><surname>Dolan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Hessen</surname></persName>
		</author>
		<idno type="DOI">10.1037/0022-3514.89.5.696</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Personality and Social Psychology</title>
		<imprint>
			<biblScope unit="volume">89</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="696" to="716" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">psychotools: Infrastructure for Psychometric Modeling</title>
		<author>
			<persName><forename type="first">A</forename><surname>Zeileis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Strobl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wickelmaier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Komboz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kopf</surname></persName>
		</author>
		<ptr target="https://CRAN.R-project.org/package=psychotools" />
	</analytic>
	<monogr>
		<title level="m">R package version 0.4-3</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Robust Factor Analysis Using the Multivariate t-Distribution</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.5705/ss.2012.022</idno>
	</analytic>
	<monogr>
		<title level="j">Statistica Sinica</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="291" to="312" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Vanpaemel 2010), so researchers using the Bayes factor are advised to carefully consider their priors. Kass and Raftery (1995) provide popular rules of thumb for interpreting the log-Bayes factor, though the extent to which these rules are meaningful in any specific application is unclear. In general, as the log-Bayes factor increases from 0, we gain increasing support for the first model. The Bayes factor can also be motivated as the extent to which, after observing data, we should revise the prior odds of model 1 being correct versus model 2 being correct; see</title>
		<author>
			<persName><surname>Rouder</surname></persName>
		</author>
		<author>
			<persName><surname>Morey</surname></persName>
		</author>
		<author>
			<persName><surname>Verhagen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wagenmakers</forename><surname>Province</surname></persName>
		</author>
		<idno type="DOI">10.18637/jss.v085.i04</idno>
		<ptr target="http://www.foastat.org" />
	</analytic>
	<monogr>
		<title level="m">obtained from the MCMC output. In package blavaan, we follow Lewis and Raftery</title>
		<imprint>
			<date type="published" when="1995">1997. 2008. 1995. 2016. June 2018</date>
			<biblScope unit="volume">85</biblScope>
			<biblScope unit="page" from="2015" to="2026" />
		</imprint>
	</monogr>
	<note>Liu and Aitkin. Submitted</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
