<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Regularized Structural Equation Modeling</title>
				<funder ref="#_er8Yn8v">
					<orgName type="full">National Institute on Aging</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2016-07-08">2016 July 08.</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Ross</forename><surname>Jacobucci</surname></persName>
							<email>jacobucc@usc.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Southern</orgName>
								<address>
									<country>California</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kevin</forename><forename type="middle">J</forename><surname>Grimm</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Psychology</orgName>
								<orgName type="institution" key="instit1">Arizona State University</orgName>
								<orgName type="institution" key="instit2">University of Southern California</orgName>
								<address>
									<addrLine>SGM 501 3620 South McClintock Avenue</addrLine>
									<postCode>90089</postCode>
									<settlement>Los Angeles</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">John</forename><forename type="middle">J</forename><surname>Mcardle</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Southern</orgName>
								<address>
									<country>California</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Regularized Structural Equation Modeling</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2016-07-08">2016 July 08.</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1080/10705511.2016.1154793</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-14T17:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>factor analysis</term>
					<term>lasso</term>
					<term>penalization</term>
					<term>regularization</term>
					<term>ridge</term>
					<term>shrinkage</term>
					<term>structural equation modeling</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A new method is proposed that extends the use of regularization in both lasso and ridge regression to structural equation models. The method is termed regularized structural equation modeling (RegSEM). RegSEM penalizes specific parameters in structural equation models, with the goal of creating easier to understand and simpler models. Although regularization has gained wide adoption in regression, very little has transferred to models with latent variables. By adding penalties to specific parameters in a structural equation model, researchers have a high level of flexibility in reducing model complexity, overcoming poor fitting models, and the creation of models that are more likely to generalize to new samples. The proposed method was evaluated through a simulation study, two illustrative examples involving a measurement model, and one empirical example involving the structural part of the model to demonstrate RegSEM's utility.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="bibr" target="#b17">(Hsu, Troncoso Skidmore, Li, &amp; Thompson, 2014;</ref><ref type="bibr" target="#b44">Muthén &amp; Asparouhov, 2012)</ref><p>. In a confirmatory factor analysis (CFA), biased parameter estimates manifest as inflated covariances between latent factors caused by not allowing cross-loadings or residual covariances. It is not that the goal of simple structure is a poor choice, but that blind adherence to this strategy can lead to bias and misinterpretation in model evaluation and selection. Although more complex models typically fit better, improved fit comes at the loss of interpreting the meaning of each factor and relations between factors. In addition to the increased interpretability of simpler models, parameter estimates often have smaller standard errors, allowing for more precise substantive conclusions <ref type="bibr" target="#b50">(Raykov &amp; Marcoulides, 1999)</ref>.</p><p>The question that remains is this: When we conduct a purely exploratory search or wish to improve on a poor fitting model, how can we conduct this search, and how do we do it well? Specification search (e.g., <ref type="bibr" target="#b22">Kaplan, 1988;</ref><ref type="bibr" target="#b29">MacCallum, 1986)</ref> refers to the modification of a model to improve parsimony or fit. This exploratory phase of model modification in structural equation modeling (SEM) has traditionally been conducted using modification indices (for overview, see <ref type="bibr" target="#b6">Chou &amp; Huh, 2012)</ref>. Conducting specification search with the use of modification indices is fraught with problems, including capitalizing on chance <ref type="bibr" target="#b30">(MacCallum, Roznowski, &amp; Necowitz, 1992)</ref>, as well as a low probability of finding the best global solution <ref type="bibr" target="#b5">(Chou &amp; Bentler, 1990)</ref>. Model modification with the use of modification indexes might work well if the number of misspecifications is small, but when there is a higher degree of uncertainty regarding the model structure, global search strategies are needed. This has led to the proposal of heuristic search algorithms such as ant colony optimization <ref type="bibr" target="#b27">(Leite, Huang, &amp; Marcoulides, 2008;</ref><ref type="bibr" target="#b33">Marcoulides &amp; Drezner, 2003)</ref>, genetic algorithm <ref type="bibr" target="#b32">(Marcoulides &amp; Drezner, 2001)</ref>, and tabu search <ref type="bibr" target="#b34">(Marcoulides, Drezner, &amp; Schumacker, 1998</ref>; for overview, see <ref type="bibr" target="#b35">Marcoulides &amp; Ing, 2012)</ref>.</p><p>Outside the field of psychometrics and SEM, there has been an increasing focus on developing methods for creating sparse models, particularly in regression and graphical modeling. Although it is beyond the scope of this article to provide a detailed account of this work (for overview, see <ref type="bibr" target="#b11">Hastie, Tibshirani, &amp; Wainwright, 2015;</ref><ref type="bibr" target="#b51">Rish &amp; Grabarnik, 2014)</ref>, there are specific developments with latent variable models that are noteworthy. These methods for sparsity have focused on the development of alternative cost functions that impose a penalty on the size of certain parameters. This method for penalizing the size of certain parameters is generally referred to as regularization, shrinkage, or penalization. The drawback of these alternative cost functions is that they induce model misfit and a reduction in explained variance. In the context of latent variable models, sparse estimation techniques were first applied to PCA through the use of penalization to create sparse loadings <ref type="bibr" target="#b61">(Zou et al., 2006)</ref> along with alternative methods of rotation <ref type="bibr">(Trendafilov &amp; Adachi, 2014)</ref>. Similar methods have also been applied to EFA through the use of penalized maximum likelihood estimation <ref type="bibr" target="#b4">(Choi, Zou, &amp; Oehlert, 2010;</ref><ref type="bibr">Hirose &amp; Yamamoto, 2014a;</ref><ref type="bibr">Ning &amp; Georgiou, 2011)</ref>.</p><p>Although regularization has been introduced in the context of PCA and EFA, less research has been done in the context of CFA, or SEM in general, where the researcher has more influence in constraining the model parameters based on theory. An example is when a researcher is creating a psychometric scale that contains a mixture of items known to be good items of the construct of interest and items that have been recently developed. Factor loadings for the previously validated items could be left free and penalties could be added to the factor loadings of the newly created items. This allows the researcher to test the inclusion of the new items while leaving the loadings for the previously validated items unpenalized to stabilize the estimation of the factor(s) of interest. This is precisely the area between EFA and CFA, between purely unrestricted and restricted model specification, in which a void exists in options for model creation in the frequentist framework. Model creation is very rarely conducted in a purely exploratory or confirmatory manner. It is this middle ground that has led to problems, where researchers use CFA for exploratory purposes, when EFA would be best <ref type="bibr" target="#b3">(Browne, 2001)</ref>.</p><p>In an attempt to extend the use of regularization to SEM, a new method is proposed, termed regularized SEM (RegSEM). RegSEM adds penalties to specific model parameters set by the researcher, allowing greater flexibility in model specification and selection. Traditionally, model selection is a categorical choice; selection is between a small number of models that were either hypothesized or created as a result of poor fit. Adding a sequence of small penalties to specific parameters turns model selection into a continuous choice, where the resulting models lie somewhere on the continuum between the most unrestricted and restricted models. For instance, a researcher could start with a completely unconstrained factor model, selecting three latent factors, and allowing every item to load on every factor. Penalties for each factor loading parameter could be gradually increased until every loading equals zero. An external criterion such as performance in a holdout data set, or an information criterion that takes into account complexity, could then be used to choose one of the models. By building penalties directly into the estimation, the researcher has more flexibility in testing various models, while building in safeguards to prevent overfitting. Model selection of this nature is common in the data mining literature but in psychometrics it is rarely, if ever, used.</p><p>The objective in developing RegSEM is not necessarily to replace existing methods in psychometrics, but instead to tackle two different goals. One is to transfer the benefits of regularization to SEM. As demonstrated in the studies that follow, RegSEM can be applied in ways that no traditional method in psychometrics could be applied. The second goal is to show the general applicability of RegSEM for creating parsimonious SEMs.</p><p>The purpose of this article is to give a detailed overview of RegSEM, demonstrating its utility in a simulation study, two illustrative examples involving a measurement model, and one empirical example involving the structural part of the model. To set the stage for RegSEM we cover the matrix formulation of SEMs, as well as previous work on regularization in both regression and EFA. RegSEM is then discussed in detail, followed by a simulation study to test its ability to choose the correct model and evaluate various fit indexes in conjunction with RegSEM. More detail on choosing a final model and when RegSEM is and is not appropriate is then covered with two illustrative examples. Finally, an empirical example demonstrating the use of regularization at the structural level of an structural equation model is provided. We conclude with recommendations on the use of RegSEM across different contexts and directions for future applications and extensions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BACKGROUND Reticular Action Model</head><p>To provide the background on general SEM and make clear what matrices are available for regularization, we briefly detail Reticular Action Model <ref type="bibr">(McArdle, 2005;</ref><ref type="bibr" target="#b9">RAM;</ref><ref type="bibr" target="#b39">McArdle &amp; McDonald, 1984)</ref> notation as it provides a 1:1 correspondence between the graphical and matrix specifications <ref type="bibr" target="#b1">(Boker, Neale, &amp; Rausch, 2004)</ref>, as well as only requiring three matrices to specify the full SEM. Additionally, the current implementation of RegSEM only uses the RAM matrices, making it necessary to understand the notation. One of the main benefits in using RAM notation is that only one matrix is needed to capture the direct effects. As direct effects might be the most natural parameters for regularization, this makes formulation simpler than having to add penalties to more than one matrix.</p><p>To illustrate the matrix specification with RAM notation, we use a one-factor CFA model. In this model, depicted in Figure <ref type="figure">1</ref>, there is a single latent factor, f 1 . For identification, we fixed the variance of f 1 to 1. This allows us to estimate factor loadings (λ 1 -λ 4 ) and unique variances (u 1 -u 4 ) for each of the manifest variables (X 1 -X 4 ).</p><p>In RAM, the three matrices are the filter (F), the asymmetric (A), and the symmetric (S). The F matrix is a p × (p + l) matrix, where p is the number of manifest variables, and l is the number of latent variables. This matrix contains ones in the positions corresponding to each manifest variable, and zeros elsewhere. In this case, with four observed variables (X 1 -X 4 ) and one latent variable (f 1 ), the F matrix has four rows and five columns, such that Directional relationships, such as directed paths or regressions between variables (factor loadings), are contained in the A matrix. In Figure <ref type="figure">1</ref>, the only directed paths are the four factor loadings, each originating from the factor f 1 and going to each of the observed variables. This results in an A matrix of In A, there are no relationships between the observed variables (Columns 1-4), and the only entries are in Column 5 for the factor f 1 . If we wished to incorporate the mean structure (necessary for longitudinal models), we would add an additional column to A to include the unit constant. Because none of the models tested in the remainder of this article include mean structures, details regarding their inclusion in matrices and the resultant expectations are omitted (see <ref type="bibr" target="#b8">Grimm &amp; McArdle, 2005;</ref><ref type="bibr">McArdle, 2005)</ref>.</p><p>The S matrix is a square matrix that contains all two-headed arrows, which can be either variances or covariances. In Figure <ref type="figure">1</ref>, there is a unique variance for each of the manifest variables and one variance for f 1 . This results in In this case, the factor variance has been constrained to 1. If, for instance, we wished to add a covariance between X 1 and X 2 , we would add a free parameter to position [2,1] and <ref type="bibr">[1,</ref><ref type="bibr">2]</ref> in S.</p><p>After having specified these three matrices, we can now calculate the expected covariance matrix <ref type="bibr" target="#b39">(McArdle &amp; McDonald, 1984)</ref>,</p><p>(1)</p><p>where -1 refers to the inverse of a matrix and ′ is the transpose operator. Once the expected covariance matrix is calculated, this can then be inserted into a loss function such as maximum likelihood <ref type="bibr" target="#b19">(Jöreskog, 1969;</ref><ref type="bibr">ML;</ref><ref type="bibr" target="#b25">Lawley, 1940)</ref></p><formula xml:id="formula_0">, (<label>2</label></formula><formula xml:id="formula_1">)</formula><p>where C is the sample covariance matrix and | • | is the determinant. Given the brief overview of RAM and its reference to ML, we can now incorporate regularization. For further detail underlying matrix specification and algebra see <ref type="bibr">McArdle (2005)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Regularized Regression</head><p>The two most common procedures for regularization in regression are the ridge <ref type="bibr" target="#b15">(Hoerl &amp; Kennard, 1970)</ref> and the least absolute shrinkage and selection operator (lasso; <ref type="bibr" target="#b56">Tibshirani, 1996)</ref>; however, there are various alternative forms that can be seen as subsets or generalizations of these two procedures. Both methods minimize a penalized residual sum of squares. Given an outcome vector y and predictor matrix X ε R n×p , the ridge estimates are defined as</p><p>(3) where β 0 is the intercept, β j is the coefficient for x j , and λ is the penalty that controls the amount of shrinkage. Note that when λ = 0, Equation 3 reduces to ordinary least squares regression. As λ is increased, the β j parameters are shrunken towards zero. The lasso estimates are defined as (4)</p><p>In lasso regression, the l 1 -norm is used, instead of l 2 -norm as in ridge, which also shrinks the β parameters, but additionally drives the parameters all the way to zero, thus performing a form of subset selection (for more detail, see <ref type="bibr" target="#b10">Hastie, Tibshirani, &amp; Friedman, 2009;</ref><ref type="bibr" target="#b56">Tibshirani, 1996)</ref>.</p><p>Although these methods for regularization are intuitively appealing, the question of how to best choose λ remains. Two different methods dominate: using a fit index that takes into account complexity, such as the Akaike information criteria (AIC; <ref type="bibr" target="#b0">Akaike, 1973)</ref> or Bayesian information criteria (BIC; <ref type="bibr" target="#b53">Schwarz, 1978)</ref>, or using cross-validation (CV). The simplest form of CV, and the type that has been used widely in SEM applications <ref type="bibr" target="#b2">(Browne, 2000)</ref>, is splitting the sample into two: a training and test data set. A large number of prespecified λ values can be utilized (e.g., 40) with each of these models run on the training data set. Instead of examining the fit of the model at this point, the parameter estimates are then treated as fixed parameters and model fit is re-estimated on the test data set. This almost invariably results in worse model fit, but ultimately gives a more accurate estimate of the generalization of the model. After all of these models have generated fit on both the training and test data sets, it is common to pick the model that results in the best fit on the test data set, or choose the most penalized model (most sparse) within one standard error of the best fitting model <ref type="bibr" target="#b10">(Hastie et al., 2009)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Regularized EFA and PCA</head><p>First proposed in the context of PCA <ref type="bibr" target="#b18">(Jolliffe, Trendafilov, &amp; Uddin, 2003;</ref><ref type="bibr" target="#b61">Zou et al., 2006)</ref>, sparse estimation procedures for dimension reduction have flourished with a number of different methods proposed for both PCA and EFA <ref type="bibr" target="#b4">(Choi et al., 2010;</ref><ref type="bibr">Hirose &amp; Yamamoto, 2014b;</ref><ref type="bibr" target="#b20">Jung &amp; Takane, 2008;</ref><ref type="bibr">Ning &amp; Georgiou, 2011)</ref>. <ref type="bibr" target="#b20">Jung and Takane (2008)</ref> proposed penalizing the diagonal of the observed covariance or correlation matrix to overcome the propensity for EFA to yield improper solutions (negative unique variances). This is in contrast to other sparse EFA methods that explicitly use different forms of regularization as a method of achieving a sparse structure, similar to the original goal of achieving simple structure. A sparse structure denotes a factor loading matrix with a large number of zeroes, leading to simpler interpretations of each latent factor. This is in contrast to what is typically done in EFA (PCA), where an orthogonal factor (component) structure is estimated and a specific number of factors (components) are retained. This solution then undergoes rotation, either oblique or orthogonal, to achieve simple structure. An alternative method is that of target or procrustean rotation <ref type="bibr" target="#b3">(Browne, 2001)</ref>, where a mechanical rotation is replaced with researcher-specified target loadings of zero. This represents a more flexible approach, allowing for input from the researcher on producing a sparse structure. Typically, this rotation does not lead to loadings of exactly zero. Instead, loadings below some prespecified threshold are either omitted from display, or are replaced by truncated values. Needless to say, this is a nonoptimal solution to obtaining a sparse structure <ref type="bibr">(Trendafilov &amp; Adachi, 2014)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>REGULARIZED STRUCTURAL EQUATION MODELING</head><p>As an attempt to generalize forms of regularization from the regression framework to that of SEMs a new general cost function is proposed for regularization. Although regularization could be added to any form of estimation, ML will be the only estimation method detailed. As previously detailed in Equation <ref type="formula" target="#formula_0">2</ref>, the ML cost function can be written as ( <ref type="formula">5</ref>)</p><p>Using the ML cost function as a base, the general form of RegSEM estimation is (6) where λ is the regularization parameter, and takes some value between zero and infinity. When λ is zero, ML estimation is performed. P(•) is a general function for summing the values of one or more matrices. The two most common forms of P(•) include both the lasso (‖•‖ 1 ), which penalizes the sum of the absolute values of the parameters and ridge (‖•‖ 2 ), which penalizes the sum of the squared values of the parameters. The norm in this case can take any form of either vector or matrix, and allows any combination of parameters or matrices. The general form of RegSEM is no different from other regularization equations. The distinction between RegSEM and other forms of regularization comes in the specification of which parameters to penalize. RegSEM allows the regularization of parameters from general SEM models. Regularized parameters can come from either the A or S matrices in RAM notation; however, it could easily be generalized to include matrices from other SEM matrix configurations (e.g., LISREL). Practically, because the value of λ is the same for every parameter, only parameters from either A or S, not both, should be penalized in a model. The penalization of parameters from one matrix in a sense pushes the parameter values into the other matrix (penalizing the A matrix (factor loadings) pushes the values into the corresponding entries of the S matrix (residual variances), and vice versa), therefore, estimation problems could occur if dependent parameters were both penalized. One important detail, with regard to implementation, is to standardize variables prior to estimation because of the summation of parameter estimates in the penalty term. By standardizing the variables, we ensure that each penalized parameter is given an equal weight in contributing to model fit.</p><p>The most obvious application of Equation 6 is to penalize the A matrix. As ML and ordinary least squares can be shown to be equivalent in regression under certain assumptions, the exact same estimates from lasso regression can be produced by adding lasso penalties to the direct effects in a regression model specified as a SEM. It is worth noting that to get an exact equivalence, the residual variance for the outcome variable needs to be fixed in structural equation model as this is not a freely estimated parameter in regression (instead a by-product of summing the squared residuals). As a result of this equivalence, RegSEM can be seen as a more general form of regularization, with lasso and ridge regression viewed as subsets.</p><p>As in regularized regression, an optimal value of λ is chosen by using a large number of values (typically 20-100) and running the model for each value of the penalty. In RegSEM, the initial penalty should be zero and the penalty should increase thereafter. This is done because of the propensity for estimation problems in SEMs with latent variables. Furthermore, instead of increasing the penalty until all penalized parameters reach zero, testing can be halted once estimation problems occur. This might be most relevant in penalizing factor loadings, where after a high enough penalty is reached, a latent factor no longer has strong enough indicators to reach a stable solution.</p><p>RegSEM is implemented as a package in R (R Core Team, 2015), termed regsem. The regsem package makes it easy for the researcher to fit a model with the lavaan package <ref type="bibr" target="#b52">(Rosseel, 2012)</ref>, and use this model as the basis for regularization with regsem. The regsem package uses the RAM matrices, and has built in capabilities for specifying a large number of penalties with and without cross-validation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Degrees of Freedom</head><p>The degrees of freedom for a model using RAM matrices is <ref type="bibr">(7)</ref> where r(C) is the rank of the sample covariance matrix and r(Ȧ, Ṡ) is the rank of the matrix of first derivatives for A and S at the solution. When an element of A or S is shrunken to zero, the first derivative necessarily goes to zero, resulting in an increase in degrees of freedom. This concurs with work done on degrees of freedom for lasso regression, as <ref type="bibr" target="#b62">Zou, Hastie, and Tibshirani (2007)</ref> proved that the number of nonzero coefficients is an unbiased estimate of the degrees of freedom for regression. Practically, the degrees of freedom changes only when the beta coefficient for a predictor is shrunken to zero. In the context of SEM, this translates to an increase in the degrees of freedom when a factor loading or covariance is shrunken to zero. It is worth noting that this only occurs for lasso penalties, as ridge parameters are not shrunken all the way to zero. Practically speaking, changing the degrees of freedom as parameters are shrunken to zero can have a large impact on assessing model fit. For instance, when assessing the fit on the training sample, introducing penalties without changing the degrees of freedom will only result in a decrement in fit (e.g., when using the root mean square error of approximation [RMSEA], Tucker-Lewis index [TLI], or comparative fit index [CFI]).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>STUDY 1: SIMULATION</head><p>The purpose of Study 1 was to determine the ability of RegSEM with lasso penalties to perform a form of subset selection through choosing a final model with selected factor loadings specified as zero. To choose a final model, and it is unknown which fit index is best to use, across either the training or test data set. To examine this question, a number of different fit indices were used to choose a final model. Selection involved choosing the model that fit best according to each fit index. A CFA model was used, with the population parameters simulated at the values in Table <ref type="table">1</ref>. Dash marks refer to fixed values of zero in the population model. Performance was judged through measuring the percentage of false positives (number divided by 9), or choosing a final model that concluded factor loadings that were simulated as zero were in fact nonzero.</p><p>Each model had nine factor loadings simulated with a true value of zero, but were in fact allowed to be freely estimated. The simulateData() function from the lavaan package <ref type="bibr" target="#b52">(Rosseel, 2012)</ref> in R was used to simulate both training and test data sets that conformed to the factor structure and at the specified sample size. Four different sample sizes were used, 100, 400, 2,000, and 10,000. It was expected that performance would increase as sample size increased due to less random noise in the simulated factor loadings. The results of the simulation are displayed in Table <ref type="table" target="#tab_0">2</ref>.</p><p>The fit indices used to judge performance included the ML cost function F train and F test , non-centrality parameter (NCP) NCP train and NCP test , and the root mean square error of approximation (RMSEA) RMSEA train and RMSEA test . In the test versions of each fit index, the covariance matrix from the test data set was used in Equation 7 instead of the train covariance matrix. Finally, both the AIC (AIC train ) and BIC (BIC train ) were only used with the training data set. This was due to the degree of penalty for extra parameters in Equation <ref type="formula">9</ref>, which was thought to be enough to increase the generalizability to alternative data sets.</p><p>It is important to note that because both the NCP and RMSEA take into account degrees of freedom, their values could decrease (get better) even as the F ML becomes worse. This is due to the degrees of freedom increasing as additional factor loadings are set to zero. Because of this, both the NCP and RMSEA might do well when using the train data despite F ML monotonically increasing as λ is increased. The results of Study 1 are displayed in Table <ref type="table" target="#tab_0">2</ref>. False negatives (how often the final model over-penalized the true factor loading of 0.2 and concluded that it was zero) are not displayed because the percentage of errors was very low. Across the sample sizes, the BIC performed the best, especially for larger sample sizes. For N = 100, the RMSEA test had the lowest percentage of error. This could be due to how sample size is factored into the BIC, whereas the RMSEA is not affected by sample size.</p><p>The results suggest that sample size played a large role in the use of regularization in SEM. Across all of the fit indices, model selection improved as sample size increased. Additionally, the results of all fit indices were better in the test sample versus the training. This is not surprising given the amount of research on regularization with cross-validation. It is interesting that even when combining a fit index that penalizes complexity, such as the NCP or RMSEA, none of the results suggest that RegSEM over-penalized models when examining the results from the test sample. The lack of false negatives, combined with none of the false positives reaching zero, provides evidence for this conclusion and might lend support to choosing the least complex model within one standard error of the best fit as in regression <ref type="bibr" target="#b10">(Hastie et al., 2009)</ref>. Additionally, with the RMSEA and NCP proving more accurate in model selection in comparison to F ML , the importance of taking into account the change in degrees of freedom is highlighted.</p><p>Overall, the results are very promising. Even though error rates for some of the fit indexes were above 20%, this is not cause for concern. The resulting erroneous factor loadings were on the order of 0 to 0.1, and might be up for elimination simply on the grounds of their small magnitude. When the goal of regularization is to achieve simple structure or subset selection in the context of factor analysis, the BIC was best, with the RMSEA test coming in a close second. More research is needed with other forms of structural equation models to determine if the same results hold. The purpose of this simulation was not to show that performing subset selection with RegSEM completely negates the need for human judgement, but instead that RegSEM can identify models that are closer to the truth and might indicate further model modification that could bring researchers closer to the right or best answer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>STUDY 2: FIT ILLUSTRATION</head><p>The question of when regularization can improve model fit, and when using non-penalized estimation might be best still remains. In some cases, the initial model might fit well, without too complex of a representation. Further, the addition of regularization might not improve the fit of the model on a holdout sample. The use of regularization does not guarantee improved model fit with respect to generalizability. In these scenarios, the starting structure of the model might be incorrect; either a different factor structure is needed or other model assumptions are violated. This can only be determined with a thorough examination, assessing multiple fit indices across a spectrum of penalties. In the first illustrative example, data were simulated using the simulateData() function from the lavaan package. Using a sample size of 500, the population model was simulated to have three factors, with three manifest variables as indicators for each factor, with no crossloadings. However, for demonstration purposes, RegSEM was used with a model that allowed three cross-loadings for each factor in addition to the three true loadings. To illustrate a situation where little was known regarding the structure of the factor loadings, other than that there were six plausible indicators for each factor, penalties were applied to all factor loadings (factor variances were constrained to be one). The values of F ML for both the train and test data sets are displayed in Figure <ref type="figure" target="#fig_1">2</ref>.</p><p>The addition of shrinkage moved the cross-loading parameters closer to zero, resulting in a better fit on the test data set despite also penalizing the true factor loadings. This is evidenced by the slight convexity to the test fit line, with a minimum achieved at a value of λ greater than zero. Note that fit will always be worse on the training data set when using a fit index that does not account for model complexity. In this situation, a two-step strategy might be advisable. The first step penalizes all of the factor loadings to determine which loadings might be zero. Once the zero loadings are determined, re-fitting the model with only the non-zero factor loadings and no regularization.</p><p>However, the use of penalties does not always improve test model fit, which is demonstrated in Figure <ref type="figure" target="#fig_2">3</ref>. In Figure <ref type="figure" target="#fig_2">3</ref>, the Holzinger-Swineford data set <ref type="bibr" target="#b16">(Holzinger &amp; Swineford, 1939)</ref> was used to test a one-factor model with a subset of nine cognitive scales. This initial fit was not good (χ 2 (27)= 348.7, p &lt;.001), and the increase of penalties only made the train and test model fit become worse (monotonic increase in each fit line). In this case, it would be necessary to start with a different factor structure. The best fitting structure to these data is generally thought to be a bi-factor model with one general factor and three specific factors. In situations like this where the ML fit is poor and the use of regularization on the factor loadings only makes it worse, it might be advisable to conduct a search for the best model. To use RegSEM to search among different factor structures, not just for which indicators are necessary within a single factor structure, it is generally best to start with the most complex model and then add penalties to see which factors, loadings, and so on are unnecessary.</p><p>To get a feel for how regularization influences parameter estimates, the estimates for the nine factor loadings in the Holzinger-Swineford sample were plotted in Figure <ref type="figure">4</ref> for lasso penalties and in Figure <ref type="figure">5</ref> for ridge. In Figure <ref type="figure">4</ref>, some factor loadings are quickly set to zero, whereas the estimates in Figure <ref type="figure">5</ref> all plateau at a point greater than zero. If the goal of regularization was to remove factor loadings (set to 0), then plotting the estimates across the range of λ values can be particularly helpful. This facilitates the inclusion of domain or theoretical knowledge into the decision-making process. For instance, in Figure <ref type="figure">4</ref>, the factor loadings that were set to zero rather quickly could be the most natural candidates for removal, even without looking at the fit indices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>STUDY 3: EMPIRICAL EXAMPLE</head><p>To demonstrate the utility of RegSEM at the structural part of a multiple indicators multiple causes (MIMIC) SEM, we analyzed data from the 2006 Health and Retirement Study (HRS; <ref type="bibr" target="#b21">Juster &amp; Suzman, 1995)</ref>. The motivating model, depicted in Figure <ref type="figure" target="#fig_5">6</ref>, was a one-factor CFA model using 7 items (rc1-rc7) from the Immediate Recall task. The latent variable, conceptualized as short-term memory (STM), was predicted by five covariates: age, gender, race, hispanic, and years of education (Ed). Fitting the non-regularized model (Model 1) resulted in the estimates for the regressions that are displayed in Table <ref type="table">3</ref>.</p><p>All of the regression coefficients in Model 1 were significant at p &lt;.001. The question we were trying to answer in this analysis was not what covariates have a significant relationship in predicting the latent variable, but instead, which regressions were likely to generalize beyond this sample. With a sample size of over 18,000, it was expected that every regression coefficient would be significant. One alternative is that we could have chosen which covariates to keep based on effect sizes, although this again does not get to the question of what is likely to generalize beyond the HRS sample. To get at generalizability, we turned to regularization.</p><p>Using RegSEM, we could apply either ridge or lasso penalties. We chose to use lasso penalties to stabilize the estimates and perform subset selection. Lasso penalties tend to over-penalize parameters that were not set to zero <ref type="bibr" target="#b10">(Hastie et al., 2009)</ref>. Away to overcome this is to use lasso penalties to first identify the nonzero paths, and then run an unconstrained model (i.e., linear regression with no penalties) using only these variables in the model. This is known as the relaxed lasso <ref type="bibr" target="#b41">(Meinshausen, 2007)</ref>. Therefore, in addition to using lasso penalties (Model 2), we kept the non-zero regressions from this model and re-estimated the model without penalties (Model 3). The factor loading estimates from Model 1 were used as fixed estimates for both Models 2 and 3. Imposing this form of invariance is necessary for both ensuring that the formulation of the latent construct stays the same across penalties, and also preventing the pushing of parameter estimates from the structural regression estimates to the factor loadings. Constraining the factor loadings ensures that the regularization only affects the regression parameters.</p><p>To choose the optimal value of penalty (λ), we used the RMSEA with cross-validation. The HRS sample was randomly split into two: a training and test data set. RegSEM was conducted on the train data set with twenty values of λ, and the RMSEA was tested on the test data set to get a better understanding of how the model will fit beyond the current sample.</p><p>The lowest value of RMSEA occurred at a λ of 0.7. Rerunning the RegSEM model at this penalty value produced the estimates in the column for Model 2 in Table <ref type="table">3</ref>. The only nonzero regression was for age. Removing the penalty and only keeping age as a predictor of STM for Model 3, we can see in Table <ref type="table">3</ref> that the resulting parameter estimate is almost the same as in Model 1. As a result, using RegSEM with lasso penalties pared away all but one covariate, age, suggesting that going forward this might be the only important variable to use as a covariate for conditioning the model estimates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DISCUSSION</head><p>The application of RegSEM in the above two studies reported here represents a small subset of the possible ways to use regularization with SEM models. These studies demonstrated that RegSEM makes model choice nearly continuous, and allows for a more flexible application of finding the model structure that increases both interpretability and generalizability. Study 1 showed RegSEM had a high propensity for choosing the correct model in a scenario where non-penalized estimation would result in choosing a model with a large number of non-zero factor loadings that are in reality zero. Study 2 provided a demonstration of scenarios where RegSEM can and cannot improve the fit of the initial model, and how plotting the fit across the range of λ values can assist in determining the best tool for the data. Finally, Study 3 demonstrated the application of RegSEM with lasso penalties on the structural part of a MIMIC model with the HRS data set. This example can be seen as a direct translation of lasso regression to the use of lasso penalties for the regression parameters in a structural equation model.</p><p>As it currently stands, we are unaware of any method to regularize specific parameters at the structural model of an SEM. Of course, one could test the measurement model, and then generate a factor score to be entered into one of the many pieces of software for conducting regularized regression. However, creating factor scores is fraught with problems (e.g., <ref type="bibr" target="#b7">Grice, 2001)</ref>, and this strategy does not generalize to more complicated models that might have more latent factors, for instance. Although Study 3 included only a small number of predictors in the structural model, this empirical example demonstrates an additional way that RegSEM can be used. With the advent of Big Data, it will become increasingly common to have extremely large models with possibly hundreds or thousands of covariates. To make sense out of this high level of complexity, using regularization with lasso penalties, for instance, will become increasingly useful to pare away unnecessary and uninformative parameters, allowing the researcher a clearer picture of what the model says about reality. RegSEM gives the researcher the flexibility to use theoretical input in shaping the model while exploring alternative specifications of both the measurement and structural parts of complex SEMs to enhance either interpretability or generalizability.</p><p>The purpose of introducing a new form of estimation for SEMs was not necessarily to show how it is better in certain scenarios, but instead to show that one general method can accomplish several tasks that would have required using separate methods. As previously mentioned, the contexts examined in Studies 1 through 3 represent a small subset of the possible applications of RegSEM. In addition to the aforementioned benefits, regularization can provide stable solutions in the case of multicollinearity among predictors and when the number of predictors is larger than the number of cases (p &gt; n; for overview, see <ref type="bibr" target="#b10">Hastie, et al. 2009;</ref><ref type="bibr" target="#b40">McNeish, 2015)</ref>. Additionally, given the aforementioned advent of Big Data, regularization has provided more accurate solutions in the case of recovering highdimensional signals in areas such as image reconstruction and genomic data, to name a few (for overview, see <ref type="bibr" target="#b11">Hastie et al., 2015;</ref><ref type="bibr" target="#b51">Rish &amp; Grabarnik, 2014)</ref>. Given that this article has focused on using RegSEM mainly for model search and modification, future research should investigate other applications of RegSEM outside this context.</p><p>The question of whether ridge or lasso penalties (or other forms of penalty) should be used and in what circumstances remains. If subset selection is the aim, lasso penalties are clearly better. Lasso penalties shrink parameters all the way to zero, whereas ridge estimates never reach zero. RegSEM with ridge penalties can be seen as an extension of ridge SEM <ref type="bibr" target="#b59">(Yuan &amp; Chan, 2008;</ref><ref type="bibr" target="#b60">Yuan, Wu, &amp; Bentler, 2011)</ref>, where a ridge penalty is added to the diagonal of a covariance or correlation matrix. In contrast, RegSEM can be seen as a method for adding various types of penalties to any part of the SEM.</p><p>Although a limited number of applications of RegSEM are detailed, there are a wide array of possible applications not covered, particularly in more complex SEMs. One possibility is applying regularization to longitudinal models specifying multiple curves. This might be understood easiest with latent-growth curve models <ref type="bibr" target="#b38">(McArdle &amp; Epstein, 1987;</ref><ref type="bibr" target="#b42">Meredith &amp; Tisak, 1990)</ref>, where models can be tested with anywhere from a linear model with fixed factor loadings from the slope to the latent basis growth model, which can be seen as an exploratory approach to determining the optimal shape of development <ref type="bibr" target="#b9">(Grimm, Steele, Ram, &amp; Nesselroade, 2013)</ref>. One could penalize the deviation in the slope from a linear curve, to produce a slope on the continuum between the most constrained, fixing all slope parameters as in the case of linear basis, to the least constrained, in the case of the latent basis growth model. Particularly through the use of cross-validation, the final model could be chosen on the potential for generalizability. RegSEM allows for more robust testing of models and their potential to generalize, overcoming the problems with only testing models that differed largely in their model specification. RegSEM turns model selection into a more continuous decision.</p><p>An area for future research is the comparison of frequentist and Bayesian methods for regularization in SEM. Although not marketed as a method for performing regularization or more broadly for achieving simple structure, Bayesian SEM (BSEM; <ref type="bibr" target="#b23">Kaplan &amp; Depaoli, 2012;</ref><ref type="bibr" target="#b26">Lee, 2007;</ref><ref type="bibr" target="#b28">Levy, 2011;</ref><ref type="bibr" target="#b43">Muthén &amp; Asparouhov, 2011)</ref> allows the researcher a high level of flexibility in specifying model structure. Specifically, the Mplus implementation of BSEM <ref type="bibr" target="#b43">(Muthén &amp; Asparouhov, 2011)</ref> using small variance priors allows for parameter estimates to occupy the area between unconstrained and fully constrained, yielding a hybrid approach. Changing the prior variance on a parameter can either shrink or inflate the parameter estimates, resulting in a more continuous form of model selection. Setting the prior variance of some parameters to be small to restrict the likely range of estimates, allows the researcher to experiment with allowing more influence of some parameters and negating the influence of others. Although the equivalence or similarity between regularization in a frequentist and Bayesian framework has been established in the context of regression <ref type="bibr" target="#b47">(Park &amp; Casella, 2008;</ref><ref type="bibr" target="#b56">Tibshirani, 1996)</ref>, this has not been the case in the context of SEM. Future research should compare the parameter estimates between RegSEM and BSEM with both Normal (ridge) and Laplace (lasso) distribution priors.</p><p>The applications discussed in this article have not addressed uncertainty regarding the number of factors in a model. This is a next step of examination, where the number of factors are allowed to vary in addition to the factor loading estimates. One could imagine starting with the largest number of factors possible (given identification constraints; e.g., <ref type="bibr" target="#b12">Hayashi &amp; Marcoulides, 2006)</ref>, and through increasing the penalties to each factor loading, eliminate latent factors from the model when either the number of non-zero loadings or sum of loadings drops below a threshold. Although more complicated in its implementation, this form of testing of models is necessary to create a form of automatic model creation. The goal of testing both the number of factors and their respective loadings is more in line with the search algorithms described in the introduction (e.g., <ref type="bibr" target="#b35">Marcoulides &amp; Ing, 2012)</ref>. It is worth testing whether these methods can produce similar results to RegSEM, and in what circumstances one might be preferred. One possibility is to incorporate one of the many genetic algorithm or tabu search R packages into regsem to provide a comparison, allowing the researcher greater flexibility in choosing the final model.</p><p>The accuracy of the final parameter estimates in Study 1 was not detailed. The reason for this is simple as in many cases a two-step strategy to deriving final model parameter estimates might be warranted. In lasso regression, it has been found that the lasso has a propensity to over shrink the coefficients, with these estimates biased toward zero <ref type="bibr" target="#b10">(Hastie et al., 2009)</ref>. Testing different forms of obtaining the final parameter estimates is an additional area for future study with regard to RegSEM, as it is unclear whether conducting a two-stage approach would be beneficial and in what circumstances.</p><p>Despite the lack of applications using regularization in the psychological and behavioral sciences, this will soon change. A recent overview of using the lasso for regression <ref type="bibr" target="#b40">(McNeish, 2015)</ref>, along with applications in differential item functioning <ref type="bibr" target="#b31">(Magis, Tuerlinckx, &amp; Boeck, 2014;</ref><ref type="bibr" target="#b58">Tutz &amp; Schauberger, 2015)</ref>, highlight this movement. It is fitting that regularization will start its proliferation at the same time that we face a crisis of replication (e.g., <ref type="bibr" target="#b48">Pashler &amp; Wagenmakers, 2012)</ref>. This fact, in and of itself, is enough evidence that we need to find ways to measure the propensity to replicate or generalize. RegSEM, seen as a family of tools for regularization with psychological and behavioral data, offers a flexible approach to model generation and modification, with an emphasis on generalizability. This article represents a first step in the application and evaluation of RegSEM. Example one factor confirmatory factor analysis model.   Example analysis using 9 scales from the Holzinger-Swinefored data set. A one-factor model was used, penalizing the absolute the absolute value of each of the 9 factor loadings. The top axis denotes the number of nonzero loadings at every five iterations. The bottom axis details the sum of of the absolute values of the factor loadings at every 5 iterations. Example analysis using 9 scales from the Holzinger-Swinefored data set. A one-factor model was used, penalizing the 12 normal of each the 9 factor loadings. The top axis denotes the number of nonzero loadings at every five iterations. The bottom axis details the sum of the absolute values of the factor loadings at every 5 iterations.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>FIGURE 1.</figDesc><graphic coords="18,196.32,62.00,279.36,503.35" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>FIGURE 2 .</head><label>2</label><figDesc>FIGURE 2.Example analysis using 9 scales from a simulated data set demonstrating how fit of both the training and test data sets changes across increasing penalization.</figDesc><graphic coords="19,84.00,62.00,504.00,328.08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>FIGURE 3 .</head><label>3</label><figDesc>FIGURE 3.Example analysis using 9 scales from the Holzinger -Swinefored data set demonstrating how fit of both the training and test data sets changes across increasing penalization.</figDesc><graphic coords="20,84.00,62.00,504.00,328.08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>FIGURE 4.</figDesc><graphic coords="21,84.00,62.00,504.00,354.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>FIGURE 5.</figDesc><graphic coords="22,84.00,62.00,504.00,354.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>FIGURE 6 .</head><label>6</label><figDesc>FIGURE 6.Health and Retirement study structural equation model. Note. STM=short-tem memory.</figDesc><graphic coords="23,84.07,62.00,503.86,443.52" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE 2</head><label>2</label><figDesc>Performance of Fit indexes in Choosing the Simulated ModelNote. Each run was replicated 500 times. NCP=noncentrality parameter RMSEA=root mean square error of approximation; BIC=Bayesian information criterion; AIC=Akaike information criterion. Values in bold represent the lowest average false positive percentage for each sample size.</figDesc><table><row><cell></cell><cell></cell><cell cols="3">Average False Positive Percentage</cell></row><row><cell>N</cell><cell>100</cell><cell>400</cell><cell>1,000</cell><cell>10,000</cell></row><row><cell>F train</cell><cell>52.4</cell><cell>46.0</cell><cell>41.1</cell><cell>27.2</cell></row><row><cell>NCP train</cell><cell>43.3</cell><cell>40.9</cell><cell>37.0</cell><cell>26.9</cell></row><row><cell cols="2">RMSEA train 42.8</cell><cell>38.1</cell><cell>34.2</cell><cell>24.3</cell></row><row><cell>BIC train</cell><cell>22.3</cell><cell>10.6</cell><cell>5.5</cell><cell>4.2</cell></row><row><cell>A1C train</cell><cell>29.0</cell><cell>19.6</cell><cell>23.1</cell><cell>13.6</cell></row><row><cell>F test</cell><cell>28.1</cell><cell>30.0</cell><cell>31.9</cell><cell>24.7</cell></row><row><cell>NCP test</cell><cell>23.2</cell><cell>22.6</cell><cell>22.6</cell><cell>17.7</cell></row><row><cell>RMSEA test</cell><cell>22.2</cell><cell>12.6</cell><cell>12.4</cell><cell>10.6</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>Struct Equ Modeling. Author manuscript; available in PMC 2016 July 08.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head></div>
<div><head>FUNDING</head><p><rs type="person">Ross Jacobucci</rs> was supported by funding through the <rs type="funder">National Institute on Aging</rs> Grant Number <rs type="grantNumber">T32AG0037</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_er8Yn8v">
					<idno type="grant-number">T32AG0037</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Simulated Factor Structure: Each Factor was Esimated With 6 Factor Loadings, 3 "True" Loadings and 3 "False" Loadings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Latent Factors</head><p>Note. Loading entries with a dash were not included in the estimated structure.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Author Manuscript</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Information theory and an extension of the maximum likelihood principle</title>
		<author>
			<persName><forename type="first">H</forename><surname>Akaike</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Second international symposium on information theory</title>
		<editor>
			<persName><forename type="first">B</forename><forename type="middle">N</forename><surname>Petrov</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Csaki</surname></persName>
		</editor>
		<meeting><address><addrLine>Budapest, Hungary</addrLine></address></meeting>
		<imprint>
			<publisher>Akademiai Kiado</publisher>
			<date type="published" when="1973">1973</date>
			<biblScope unit="page" from="267" to="281" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Latent differential equation modeling with multivariate multi-occasion indicators</title>
		<author>
			<persName><forename type="first">S</forename><surname>Boker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Neale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rausch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Recent developments on structural equation models</title>
		<editor>
			<persName><forename type="first">K</forename><surname>Van Montfort</surname></persName>
		</editor>
		<meeting><address><addrLine>New york, NY</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="151" to="174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Cross-validation methods</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Browne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Mathematical Psychology</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="108" to="132" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
	<note>PubMed: 10733860</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">An overview of analytic rotation in exploratory factor analysis</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Browne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multivariate Behavioral Research</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="111" to="150" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A penalized maximum likelihood approach to sparse factor analysis</title>
		<author>
			<persName><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Oehlert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistics and Its Interface</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="429" to="436" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Model modification in covariance structure modeling: A comparison among likelihood ratio, Lagrange multiplier, and Wald tests</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">P</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M</forename><surname>Bentler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multivariate Behavioral Research</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="115" to="136" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
	<note>PubMed: 26741976</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Model modification in structural equation modeling</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">P</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Handbook of Structural Equation Modeling</title>
		<editor>
			<persName><forename type="first">R</forename><surname>Hoyle</surname></persName>
		</editor>
		<meeting><address><addrLine>New York, NY</addrLine></address></meeting>
		<imprint>
			<publisher>Guilford</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="232" to="246" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Computing and evaluating factor scores</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Grice</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Methods</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">430</biblScope>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
	<note>PubMed: 11778682</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A note on the computer generation of mean and covariance expectations in latent growth curve analysis</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J</forename><surname>Grimm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Mcardle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Multi-level issues in strategy and methods</title>
		<editor>
			<persName><forename type="first">F</forename><surname>Dansereau</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><forename type="middle">J</forename><surname>Yammarino</surname></persName>
		</editor>
		<meeting><address><addrLine>Amsterdam</addrLine></address></meeting>
		<imprint>
			<publisher>Emerald</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="335" to="364" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Exploratory latent growth models in the structural equation modeling framework</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J</forename><surname>Grimm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Steele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Nesselroade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Structural Equation Modeling: A Multidisciplinary Journal</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="568" to="591" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">The elements of statistical learning</title>
		<author>
			<persName><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Friedman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>Springer</publisher>
			<pubPlace>New York, NY</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Statistical learning with sparsity: The lasso and generalizations</title>
		<author>
			<persName><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wainwright</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>CRC</publisher>
			<pubPlace>Boca Raton, FL</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Teacher&apos;s corner: Examining identification issues in factor analysis</title>
		<author>
			<persName><forename type="first">K</forename><surname>Hayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Marcoulides</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Structural Equation Modeling</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="631" to="645" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Estimation of an oblique structure via penalized likelihood factor analysis</title>
		<author>
			<persName><forename type="first">K</forename><surname>Hirose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yamamoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Statistics &amp; Data Analysis</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="page" from="120" to="132" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Sparse estimation via nonconcave penalized likelihood in factor analysis model</title>
		<author>
			<persName><forename type="first">K</forename><surname>Hirose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yamamoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistics and Computing</title>
		<imprint>
			<biblScope unit="volume">2014</biblScope>
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Ridge regression: Biased estimation for nonorthogonal problems</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Hoerl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">W</forename><surname>Kennard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Technometrics</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="55" to="67" />
			<date type="published" when="1970">1970</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A study in factor analysis: The stability of a bi-factor solution</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J</forename><surname>Holzinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Swineford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Supplementary Educational Monographs</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<date type="published" when="1939">1939</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Forced zero cross-loading misspecifications in measurement component of structural equation models: Beware of even &quot;small&quot; misspecifications</title>
		<author>
			<persName><forename type="first">H-Y</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Troncoso</forename><surname>Skidmore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Thompson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Methodology: European Journal of Research Methods for the Behavioral and Social Sciences</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">138</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A modified principal component technique based on the lasso</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">T</forename><surname>Jolliffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">T</forename><surname>Trendafilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Uddin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computational and Graphical Statistics</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="531" to="547" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A general approach to confirmatory maximum likelihood factor analysis</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">G</forename><surname>Jöreskog</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychometrika</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="183" to="202" />
			<date type="published" when="1969">1969</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Regularized common factor analysis</title>
		<author>
			<persName><forename type="first">S</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Takane</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">New trends in psychometrics</title>
		<editor>
			<persName><forename type="first">K</forename><surname>Shigemasu</surname></persName>
		</editor>
		<meeting><address><addrLine>Tokyo</addrLine></address></meeting>
		<imprint>
			<publisher>Universal Academy Press</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="141" to="149" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">An overview of the health and retirement study</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">T</forename><surname>Juster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Suzman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Human Resources</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="7" to="S56" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The impact of specification error on the estimation, testing, and improvement of structural equation models</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kaplan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multivariate Behavioral Research</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="69" to="86" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
	<note>PubMed: 26782258</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Handbook of structural equation modeling</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Depaoli</surname></persName>
		</author>
		<editor>Hoyle, R.</editor>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>Guilford</publisher>
			<biblScope unit="page" from="650" to="673" />
			<pubPlace>New York, NY</pubPlace>
		</imprint>
	</monogr>
	<note>Bayesian structural equation modeling</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Sparsistency and rates of convergence in large covariance matrix estimation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fan</forename><forename type="middle">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">6B</biblScope>
			<biblScope unit="page">4254</biblScope>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
	<note>PubMed: 21132082</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">-the estimation of factor loadings by the method of maximum likelihood</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">N</forename><surname>Lawley</surname></persName>
		</author>
		<author>
			<persName><surname>Vi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the Royal Society of Edinburgh</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">01</biblScope>
			<biblScope unit="page" from="64" to="82" />
			<date type="published" when="1940">1940</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Structural equation modeling: A bayesian approach</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Y</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
			<publisher>John Wiley</publisher>
			<biblScope unit="volume">711</biblScope>
			<pubPlace>New York, NY</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Item selection for the development of short forms of scales using an ant colony optimization algorithm</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Leite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I-C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Marcoulides</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multivariate Behavioral Research</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="411" to="431" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
	<note>PubMed: 26741203</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Bayesian data-model fit assessment for structural equation modeling</title>
		<author>
			<persName><forename type="first">R</forename><surname>Levy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Structural Equation Modeling</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="663" to="685" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Specification searches in covariance structure modeling</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Maccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Bulletin</title>
		<imprint>
			<biblScope unit="volume">100</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">107</biblScope>
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Model modifications in covariance structure analysis: the problem of capitalization on chance</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Maccallum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Roznowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">B</forename><surname>Necowitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Bulletin</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="page">490</biblScope>
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
	<note>PubMed: 16250105</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Detection of differential item functioning using the lasso approach</title>
		<author>
			<persName><forename type="first">D</forename><surname>Magis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Tuerlinckx</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">D</forename><surname>Boeck</surname></persName>
		</author>
		<idno type="DOI">10.3102/1076998614559747</idno>
		<ptr target="http://dx.doi.org/10.3102/1076998614559747" />
	</analytic>
	<monogr>
		<title level="j">Journal of Educational and Behavioral Statistics</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="111" to="135" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Specification searches in structural equation modeling with a genetic algorithm</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Marcoulides</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Drezner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">New developments and techniques in structural equation modeling</title>
		<editor>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Marcoulides</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Schumacker</surname></persName>
		</editor>
		<meeting><address><addrLine>Mahwah, NJ</addrLine></address></meeting>
		<imprint>
			<publisher>Erlbaum</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="247" to="268" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Model specification searches using ant colony optimization algorithms</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Marcoulides</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Drezner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Structural Equation Modeling</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="154" to="164" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Model specification searches in structural equation modeling using tabu search</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Marcoulides</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Drezner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Schumacker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Structural Equation Modeling: A Multidisciplinary Journal</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="365" to="376" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Automated structural equation modeling strategies</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Marcoulides</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ing</surname></persName>
		</author>
		<editor>Hoyle, R.</editor>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>Guilford</publisher>
			<biblScope unit="page" from="690" to="704" />
			<pubPlace>New York, NY</pubPlace>
		</imprint>
	</monogr>
	<note>Handbook of structural equation modeling</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Assessing goodness of fit: Is parsimony always desirable?</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">W</forename><surname>Marsh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K-T</forename><surname>Hau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Experimental Education</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="page" from="364" to="390" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">The development of the ram rules for latent variable structural equation modeling</title>
		<author>
			<persName><forename type="first">Jj ;</forename><surname>Mcardle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Roderick</surname></persName>
		</author>
		<author>
			<persName><surname>Mcdonald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Contemporary psychometrics: A festschrift for</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Maydeu-Olivares</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Mcardle</surname></persName>
		</editor>
		<meeting><address><addrLine>Mahwah, NJ</addrLine></address></meeting>
		<imprint>
			<publisher>Lawrence Erlbaum</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="225" to="273" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Latent growth curves within developmental structural equation models</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Mcardle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Epstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Child development</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page" from="110" to="133" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
	<note>PubMed: 3816341</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Some algebraic properties of the reticular action model for moment structures</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Mcardle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Mcdonald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">British Journal of Mathematical and Statistical Psychology</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page">6509005</biblScope>
			<date type="published" when="1984">1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Using lasso for predictor selection and to assuage overfitting: A method long overlooked in behavioral sciences</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Mcneish</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multivariate Behavioral Research</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page" from="471" to="484" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note>PubMed: 26610247</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Relaxed lasso</title>
		<author>
			<persName><forename type="first">N</forename><surname>Meinshausen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Statistics &amp; Data Analysis</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="374" to="393" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Latent curve analysis</title>
		<author>
			<persName><forename type="first">W</forename><surname>Meredith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tisak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychometrika</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="107" to="122" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Bayesian SEM: A more flexible representation of substantive theory</title>
		<author>
			<persName><forename type="first">B</forename><surname>Muthén</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Asparouhov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Methods</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="313" to="335" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
	<note>PubMed: 22962886</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Bayesian structural equation modeling: A more flexible representation of substantive theory</title>
		<author>
			<persName><forename type="first">B</forename><surname>Muthén</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Asparouhov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Methods</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="313" to="335" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note>PubMed: 22962886</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Sparse factor analysis via likelihood and I 1-regularization</title>
		<author>
			<persName><forename type="first">L</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">T</forename><surname>Georgiou</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">L</forename><surname>Orlando</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Usa</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2011">December 12-15, 2011; 2011</date>
			<biblScope unit="page" from="5188" to="5192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">The Bayesian lasso</title>
		<author>
			<persName><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Casella</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">103</biblScope>
			<biblScope unit="issue">482</biblScope>
			<biblScope unit="page" from="681" to="686" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Editors&apos; introduction to the special section on replicability in psychological science a crisis of confidence?</title>
		<author>
			<persName><forename type="first">H</forename><surname>Pashler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E-J</forename><surname>Wagenmakers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Perspectives on Psychological Science</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="528" to="530" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note>PubMed: 26168108</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">R: A language and environment for statistical computing</title>
		<author>
			<persName><forename type="first">Team</forename><surname>Core</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Computer software manual</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">On desirability of parsimony in structural equation model selection</title>
		<author>
			<persName><forename type="first">T</forename><surname>Raykov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Marcoulides</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Structural Equation Modeling: A Multidisciplinary Journal</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="292" to="300" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Sparse modeling: Theory, algorithms, and applications</title>
		<author>
			<persName><forename type="first">I</forename><surname>Rish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Grabarnik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>CRC</publisher>
			<pubPlace>Boca Raton, FL</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Lavaan: An R package for structural equation modeling</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Rosseel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Statistical Software</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="36" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Estimating the dimension of a model</title>
		<author>
			<persName><forename type="first">G</forename><surname>Schwarz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="461" to="464" />
			<date type="published" when="1978">1978</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">The vectors of mind</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">L</forename><surname>Thurstone</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1935">1935</date>
			<publisher>University of Chicago Press</publisher>
			<pubPlace>Chicago, IL</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Multiple factor analysis</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">L</forename><surname>Thurstone</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1947">1947</date>
			<publisher>University of Chicago Press</publisher>
			<pubPlace>Chicago, IL</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Regression shrinkage and selection via the lasso</title>
		<author>
			<persName><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society: Series B (Methodological)</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="267" to="288" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Sparse versus simple structure loadings</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">T</forename><surname>Trendafilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Adachi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychometrika</title>
		<imprint>
			<biblScope unit="volume">2014</biblScope>
			<biblScope unit="page" from="1" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">A penalty approach to differential item functioning in rasch models</title>
		<author>
			<persName><forename type="first">G</forename><surname>Tutz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Schauberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychometrika</title>
		<imprint>
			<biblScope unit="page" from="21" to="43" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note>PubMed: 24297435</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Structural equation modeling with near singular covariance matrices</title>
		<author>
			<persName><forename type="first">K-H</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chan</forename><forename type="middle">W</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Statistics &amp; Data Analysis</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page" from="4842" to="4858" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Ridge structural equation modelling with correlation matrices for ordinal and continuous data</title>
		<author>
			<persName><forename type="first">K-H</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M</forename><surname>Bentler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">British Journal of Mathematical and Statistical Psychology</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="107" to="133" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
	<note>PubMed: 21506947</note>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Sparse principal component analysis</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computational and Graphical Statistics</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="265" to="286" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">On the &quot;degrees of freedom&quot; of the lasso</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="2173" to="2192" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
