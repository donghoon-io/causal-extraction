<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Identifiability of Gaussian structural equation models with equal error variances</title>
				<funder ref="#_e9af7KT">
					<orgName type="full">European Union</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2018-10-29">October 29, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jonas</forename><surname>Peters</surname></persName>
							<email>peters@stat.math.ethz.ch</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Seminar for Statistics ETH Zurich Switzerland</orgName>
								<orgName type="department" key="dep2">Seminar for Statistics ETH Zurich Switzerland</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Peter</forename><surname>Bühlmann</surname></persName>
							<email>buhlmann@stat.math.ethz.ch</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Seminar for Statistics ETH Zurich Switzerland</orgName>
								<orgName type="department" key="dep2">Seminar for Statistics ETH Zurich Switzerland</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Identifiability of Gaussian structural equation models with equal error variances</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2018-10-29">October 29, 2018</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:1205.2536v3[stat.ML]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-14T17:51+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We consider structural equation models in which variables can be written as a function of their parents and noise terms, which are assumed to be jointly independent. Corresponding to each structural equation model, there is a directed acyclic graph describing the relationships between the variables. In Gaussian structural equation models with linear functions, the graph can be identified from the joint distribution only up to Markov equivalence classes, assuming faithfulness. In this work, we prove full identifiability if all noise variables have the same variances: the directed acyclic graph can be recovered from the joint Gaussian distribution. Our result has direct implications for causal inference: if the data follow a Gaussian structural equation model with equal error variances and assuming that all variables are observed, the causal structure can be inferred from observational data only. We propose a statistical method and an algorithm that exploit our theoretical findings.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction 1.Graphical and structural equation models</head><p>For random variables X 1 , . . . , X p , we define a graphical model as a pair {G, L(X)}, where L(X) = L(X 1 , . . . , X p ) is a joint probability distribution that is Markov with respect to a directed acyclic graph G <ref type="bibr">[Lauritzen, 1996, Chapter 3.2]</ref>. Structural equation models, also referred to as a functional models, are related to graphical models. They are specified by a collection S = {S 1 , . . . , S p } of p equations S j : X j = f j (X PA j , N j ) (j = 1, . . . , p)</p><p>(1) and a joint distribution L(N) = L(N 1 , . . . , N p ) of the noise variables. Here, PA j ⊂ {1, . . . , p} \ {j} denotes the parents of j. We require the noise terms to be jointly independent, so L(N) is a product distribution. The graph G of a structural equation model is obtained by drawing directed edges from each variable X k , k ∈ PA j , occurring on the right-hand side of equation ( <ref type="formula">1</ref>) to X j . The graph G is required to be acyclic. Furthermore, given a structural equation model, the joint distribution L(X) is fully determined and L(X) is Markov with respect to the graph G <ref type="bibr">[Pearl, 2009, Theorem 1.4.1]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Identifiability from the distribution</head><p>We address the following problem. Given the joint distribution L(X) = L(X 1 , . . . , X p ) from a graphical model or from a structural equation model with directed acyclic graph G 0 , can we recover the graph G 0 ? By first considering graphical models one can easily see that the answer is negative: the joint distribution L(X) is Markov with respect to different directed acyclic graphs, e.g., to all fully connected directed acyclic graphs. Thus, there are many possible graphical models {G, L(X)} for the same distribution L(X). Similarly, there are structural equation models with different structures that could have generated the distribution L(X). By making additional assumptions one obtains restricted graphical models and restricted structural equation models for which the graph is identifiable from the joint distribution. It is precisely here that the difference between graphical and functional models becomes apparent. Given a graphical model, the distribution L(X) is faithful with respect to the directed acyclic graph G 0 if each conditional independence found in L(X) is implied by the Markov condition. If faithfulness holds, one can obtain the Markov equivalence graph of the true directed acyclic graph G 0 <ref type="bibr" target="#b12">[Spirtes et al., 2000]</ref>. But the Markov equivalence class may still be large [cf. <ref type="bibr" target="#b0">Andersson et al., 1997]</ref> and the directed acyclic graph G 0 is not identifiable. Furthermore, faithfulness in its full generality cannot be tested from data <ref type="bibr" target="#b16">[Zhang and Spirtes, 2008]</ref>. Since both the Markov condition and faithfulness only restrict the conditional independences in the joint distribution, it is not surprising that two graphs entailing the same conditional independences cannot be distinguished.</p><p>Structural equation models enable us to exploit a different type of restriction. First, a general Gaussian structural equation model is equivalent to a Gaussian graphical model {G 0 , L(X)}, so the structure G 0 is not identifiable from L(X). Recently, however, it has been shown that this case is exceptional: (i) if we consider linear functions and non-Gaussian noise, one can identify the underlying directed acyclic graph G 0 <ref type="bibr" target="#b11">[Shimizu et al., 2006]</ref>; (ii) if one restricts the functions to be additive in the noise component and excludes the linear Gaussian case, as well as a few other pathological function-noise combinations, one can show that G 0 is identifiable from L(X) <ref type="bibr" target="#b6">[Hoyer et al., 2009</ref><ref type="bibr" target="#b10">, Peters et al., 2011]</ref>. In this work, we prove that there is a third way to deviate from the general linear Gaussian case: (iii) Gaussian structural equation models where all functions are linear, but the normally distributed noise variables have equal variances σ 2 , are again identifiable. The identifiability results (i) and (ii) require a condition called causal minimality. In its original form, <ref type="bibr" target="#b16">Zhang and Spirtes [2008]</ref> define causal minimality as follows: for the true causal graph G 0 , L(X) is not Markov to any proper subgraph of G 0 . Causal minimality is therefore a weak form of faithfulness. Remark 3 shows that for proving (iii) we assume causal minimality.</p><p>It may come as a surprise that for a class of Gaussian structural equation models the underlying directed acyclic graph is identifiable. The assumption of equal error variances seems natural for applications with variables from a similar domain and is commonly used in time series models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3">Causal interpretation</head><p>Our result has implications for causal inference. If G 0 is interpreted as the causal graph of the data generating process for X 1 , . . . , X p , the problem considered here is to infer the causal structure from the joint distribution. This is particularly interesting when the causal graph is of interest but interventional experiments are too expensive, unethical or even impossible to perform. In the causal setting, our result reads as follows. If the observational data are generated by a Gaussian structural equation model that represents the causal relationships and has equal error variances, then the causal graph is identifiable from the joint distribution. Despite the potentially important application in causal inference, we present the main statement and its proof without causal terminology; in particular, equations ( <ref type="formula">1</ref>) and (2) can be interpreted as holding in distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Identifiability for Gaussian models with equal error variances</head><p>We first introduce some notation. The index set J = {1, . . . , p} corresponds to a set of vertices in a graph. Associated with j ∈ J are random variables X j from X = (X 1 , . . . , X p ). Given a directed acyclic graph G, we denote the parents of a node j by PA G j , the children by CH G j , the descendants by DE G j and the non-descendants by</p><formula xml:id="formula_0">ND G j . L Y S 1 S 2 Graph G L Y S 1 S 2</formula><p>Graph G We consider a structural equation model with directed acyclic graph G 0 of the form</p><formula xml:id="formula_1">X j = k∈PA G 0 j β jk X k + N j (j = 1, . . . , p) ,<label>(2)</label></formula><p>where all N j are independent and identically distributed according to N (0, σ 2 ) with σ 2 &gt; 0. Additionally, for each j ∈ {1, . . . , p} we require β jk = 0 for all k ∈ PA G0 j .</p><p>Theorem 1 Let L(X) be generated from model (2). Then G 0 is identifiable from L(X) and the coefficients β jk can be reconstructed for all j and k ∈ PA G0 j .</p><p>Problem 2 The idea of the proof is to assume that there are two structural equation models with distinct graphs G and G that lead to the same joint distribution. We exploit the Markov condition and causal minimality, see Remark 3, in order to find variables L and Y that have the same set of parents S = {S 1 , S 2 } in both graphs, but reversed edges between each other in G and G , as shown in Fig. <ref type="figure" target="#fig_0">1</ref>. Defining L * = L | S=s for some value s ∈ R 2 , we can use the equal error variances to show that L * has different variances in both graphs. This leads to a contradiction.</p><p>Problem 3 Theorem 1 assumes that the coefficients β jk = 0 do not vanish for any k ∈ PA G0 j . Lemma 8 below and Proposition 2 in <ref type="bibr" target="#b10">Peters et al. [2011]</ref> show that this condition implies causal minimality. From our point of view, causal minimality is a natural condition and in accordance with the intuitive understanding of a causal influence between variables.</p><p>Problem 4 Theorem 1 can be generalized to the case where the error covariance matrix has the form Cov(N 1 , . . . , N p ) = σ 2 diag(α 1 , . . . , α p ) with pre-specified α 1 , . . . , α p and unknown σ 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Penalized maximum likelihood estimator</head><p>Consider data which are independent and identically distributed realizations of X (1) , . . . , X (n) from model (2) with true coefficients β 0 jk . The representation in vector form is X = BX + N, where B is the p × p matrix with entries B jk = β jk . To make the manuscript easier to read we write B or β whenever we think of a matrix or a vector of parameters, respectively. As estimator for the coefficients B 0 = (β 0 jk ) j,k and the error variance σ 2 , we consider</p><formula xml:id="formula_2">{ β(λ), σ2 (λ)} = argmin β∈B,σ 2 ∈R + -(β, σ 2 ; X (1) , . . . , X (n) ) + λ β 0 ,<label>(3)</label></formula><p>where</p><formula xml:id="formula_3">-(β, σ 2 ; X (1) , . . . , X (n) ) = np 2 log(2πσ 2 ) + n 2σ 2 tr{(I -B) T (I -B) Σ} ,</formula><p>with sample covariance matrix Σ, is the negative log-likelihood assuming equal error variances σ 2 and β 0 = |{j, k : β jk = 0}|. Furthermore, B = {B ∈ R p×p : Adj(B) has only zero eigenvalues} contains only those coefficient matrices whose corresponding graphs do not have cycles <ref type="bibr">[Cvetković et al., 1995, p.81</ref>]. Here, Adj(B) jk = 1 β jk =0 is the adjacency matrix. Minimizing over all β ∈ B includes optimizing over all directed acyclic graphs, see Section 4. The induced directed acyclic graph from β(λ) is denoted by Ĝ. For λ = log(n)/2 the objective function in equation ( <ref type="formula" target="#formula_2">3</ref>) is the bic score.</p><p>The convergence rate and consistency of the penalized maximum likelihood estimator for the true coefficients β 0 jk and the true structure G 0 follow from an analysis in van de Geer and Bühlmann [2013, Theorem 5.1], under regularity conditions. More precisely, for λ n = log(n)/2 we have</p><formula xml:id="formula_4">p j,k=1 { βjk (λ n ) -β 0 jk } 2 = O P {log(n)n -1 } (n → ∞) , pr( Ĝn = G 0 ) → 1 (n → ∞) .</formula><p>The results in van de Geer and Bühlmann [2013, Section 5] also cover the high-dimensional sparse setting where p = p n = O{n/ log(n)}.</p><p>One could use a combination of the PC-algorithm and minimization of the penalized likelihood in equation (3): the former, which is computationally very efficient, could be used for estimating the Markov equivalence class and the latter for orienting remaining undirected edges. A related approach has been suggested by <ref type="bibr" target="#b13">Tillman et al. [2010]</ref>. For consistency in the first step one necessarily requires a version of the strong faithfulness assumption, which can be very restrictive <ref type="bibr" target="#b14">[Uhler et al., 2013]</ref>. Penalized maximum likelihood estimation does not need such an assumption [van de Geer and Bühlmann, 2013] but pays a price in terms of computational complexity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Greedy search algorithm</head><p>Because the optimization in equation ( <ref type="formula" target="#formula_2">3</ref>) is over the space of all directed acyclic graphs, the estimator is hard to compute. Already for p = 20, there are 2.3 × 10 72 directed acyclic graphs <ref type="bibr">[OEIS Foundation Inc., 2011]</ref>, which makes an exhaustive search infeasible. Instead, we propose a greedy procedure that we call greedy directed acyclic graph search with equal error variance. At each iteration t we are given a directed acyclic graph G t and move to the neighbouring directed acyclic graph with the largest drop in the bic score. If all neighbours have a higher bic score in equation ( <ref type="formula" target="#formula_2">3</ref>) than G t , the algorithm terminates. Here, we say that two directed acyclic graphs are neighbours if they can be transformed into each other by one edge addition, removal or reversal. <ref type="bibr" target="#b3">Chickering [2002]</ref> proposes a similar search strategy but with the search done in the space of Markov equivalence classes rather than over directed acyclic graphs.</p><p>In order to shorten the runtime, we randomly search through neighbouring directed acyclic graphs until we find a directed acyclic graph with a better score than G t and use this directed acyclic graph for G t+1 . We consider at least k neighbours; if there are several directed acyclic graphs among the first k with better scores than G t , we take the best one. The whole procedure further improves if we increase the probability of changing edges pointing into nodes whose residuals have a high variance. This modification and the score function are the only parts of the algorithm that make use of the equal error variances. Additionally, we restart the method five times starting from a random sparse graph with k = p, k = 2p, k = 3p, k = 5p and k = 300. This choice is ad hoc but works well in practice, as it decreases the risk of getting stuck in a local optimum. R code for this method is available as Supplementary Material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Existing methods</head><p>We compare our method against the PC-algorithm <ref type="bibr" target="#b12">[Spirtes et al., 2000]</ref> and greedy equivalence search <ref type="bibr" target="#b3">[Chickering, 2002]</ref>. The latter approximates the BIC-regularized maximum likelihood estimator for nonrestricted Gaussian structural equation models. Both methods can only recover the Markov equivalence class, see Section 1.2, and therefore leave some arrows undirected. The Markov equivalence class can be represented by a completed partially directed acyclic graph. In the experiments, we report the structural Hamming distance between the true and estimated partially directed acyclic graphs; this assigns a distance of two for each pair of reversed edges, for example, → in the true and ← in the estimated graph; all other edge mistakes count as one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Random graphs</head><p>For varying n and p we compare the three methods. For a given value p, we randomly choose an ordering of the variables with respect to the uniform distribution and include each of the p(p -1)/2 possible edges with a probability of p edge . All noise variances are set to 1 since scaling all noise variables with a common factor yields exactly the same estimates β and Ĝ. The coefficients β 0 jk are uniformly chosen from [-1, -0.1] ∪ [0.1, 1]. We consider a sparse setting with p edge = 3/(2p -2), which results in an expected number of 3p/4 edges, and a dense setting with p edge = 0.3. Table <ref type="table">5</ref>.2 shows the average structural Hamming distance to the true directed acyclic graph and to the true completed partially directed acyclic graph over 100 simulations for the sparse setting. Except for p = 40 and n = 100, the graphs estimated by the proposed method are closer to the true directed acyclic graph than the resulting graphs from state of the art methods, who can only recover the true Markov equivalence class; greedy directed acyclic graph search also performs better when comparing the distance to the true completed partially directed acyclic graph. Table <ref type="table">5</ref>.2 shows the analogous results for the dense setting, in which the improvement with greedy directed acyclic graph search with equal error variances is even larger. As a proof of concept, we also simulate data with n = 500 from a non-faithful distribution:</p><formula xml:id="formula_5">X 1 = N 1 , X 2 = -X 1 + N 2 and X 3 = X 1 + X 2 + N 3 .</formula><p>As stated by the theory, the PC-algorithm and greedy equivalent search fail here: in all 100 experiments, they output X 1 → X 2 ← X 3 , which is not the correct Markov equivalence class. Greedy directed acyclic graph search always identified the correct directed acyclic graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Deviation from equal error variances</head><p>When the data are generated by a Gaussian structural equation model with different error variances, the method is not guaranteed to find the correct directed acyclic graph or the correct Markov equivalence   class. When the true data generating process follows such a Gaussian structural equation model with different variances, we can always represent it as a model with equal error variances if we apply a finetuned rescaling of the variables X i → a i X i with a i equal to the inverse of the standard deviation of the error in the ith structural equation. Of course, such a rescaling is only possible when knowing the error variances, hence the word fine-tuned. In the hypothetical case where the data would be scaled with such a deceptive fine-tuned standardization, the graph identified by our method would belong to the correct Markov equivalence class. We emphasize, however, that this is for an artificial scenario which is different from having raw data from a Gaussian structural equation model with different error variances. An important question is how sensitive our method is to deviations from the assumption of equal error variances. We investigate this empirically. For p = 10 and n = 500, we sample the noise variances uniformly from [1 -a, 1 + a] and vary a between 0 and 0.9. Theorem 1 establishes identifiability of the graph only for a = 0. As before, the coefficients β 0 jk are uniformly chosen from [-1, -0.1] ∪ [0.1, 1]. The parameter p edge is chosen to be 2/(p -1), on average resulting in p edges; this is in between the sparse and the dense setting. Figure <ref type="figure">5</ref>.3 shows that the performance of greedy directed acyclic graph search is relatively robust as the parameter a changes. Even for large values of a, the method does not perform worse than the PC-algorithm. The best-score method reports the result of greedy directed acyclic graph search or greedy equivalence search depending on which method obtained the better score. Greedy directed acyclic graph search was chosen in 100%, 100%, 88%, 36%, 7%, 1%, 2%, 0%, 0% and 0% of the cases, for a ranging between 0 and 0.9, respectively.</p><formula xml:id="formula_6">• • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • 0 0.</formula><formula xml:id="formula_7">GDS_SEV GES PC BEST_SCORE • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • 0 0.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Real data</head><p>We now apply the greedy equivalence search and greedy directed acyclic graph search to seven data sets containing microarray data, described by <ref type="bibr" target="#b5">Dettling and Bühlmann [2003]</ref> and <ref type="bibr" target="#b1">Bühlmann et al. [2013]</ref>, and compare their bic scores. When greedy equivalence search obtains the better score, this indicates that the assumption of equal error variances is not justified. In Figure <ref type="figure">5</ref>.3 we have seen that even then it might sometimes be useful to look at the greedy directed acyclic graph search solution. If, on the other hand, greedy directed acyclic graph search obtains a better score than greedy equivalence search, we prefer the solution obtained by greedy directed acyclic graph search, which furthermore is a graph rather than a Markov equivalence class. To avoid a high-dimensional setting with p &gt; n, we always chose the 0.8n genes with the highest variance.  Proof of Theorem 1.</p><p>If we assumed faithfulness, we could recover the correct Markov equivalence class, which itself implies the existence of an L and Y shown in Remark 2 <ref type="bibr">[Chickering, 1995, Theorem 2</ref>]. Since we are not assuming faithfulness, proving existence of a situation similar to that in Fig. <ref type="figure" target="#fig_0">1</ref> requires more work. This part of the proof, due to not assuming faithfulness, is taken from <ref type="bibr" target="#b10">Peters et al. [2011]</ref> and remains almost the same. The difference to <ref type="bibr" target="#b10">Peters et al. [2011]</ref> is that we can prove causal minimality and need not assume it. New are also Lemmata 5 and 8, as well as the proof's main argument given in the second part of case (ii).</p><p>Proof . We assume that there are two structural equation models as in equation ( <ref type="formula" target="#formula_1">2</ref>) that both induce L(X), one with graph G, the other with graph G . We will show that G = G . Since directed acyclic graphs do not contain any cycles, we always find nodes that have no descendants. To see this start a directed path at some node; after at most #X -1 steps we reach a node without a child. Eliminating such a node from the graph leads to a directed acyclic graph, again; we can discard further nodes without children in the new graph. We repeat this process for all nodes that have no children in both G and G and have the same parents in both graphs. If we end up with no nodes left, the two graphs are identical and the result is proved. Otherwise, we end up with a smaller set of variables that we again call X, two smaller graphs that we again call G and G and a node L that has no children in G and either PA G L = PA G L or CH G L = ∅. We will show that this leads to a contradiction. Importantly, because of the Markov property of the distribution with respect to G, all other nodes are independent of L given PA G L :</p><formula xml:id="formula_8">L ⊥ ⊥ X \ (PA G L ∪ {L}) | PA G L .<label>(4)</label></formula><p>To make the arguments easier to understand, we introduce the following notation, see also Fig. <ref type="figure" target="#fig_2">3</ref>. We partition G-parents of L into Y, Z and W. Here, Z are also G -parents of L, Y are G -children of L and W are not adjacent to L in G . Let D be the G -parents of L that are not adjacent to L in G and by E the G -children of L that are not adjacent to L in G. Thus:  <ref type="figure" target="#fig_4">4</ref>, must contain a collider V 1 that is an ancestor of a Z with V 1 , . . . , V m , Z / ∈ S and corresponding nodes U i for a W node. Choose V 1 and U 1 on the given path so close to each other such that there is no such collider in between. If there is no since det{cov(X)} = 0. Equations ( <ref type="formula">6</ref>) and ( <ref type="formula">7</ref>) contradict each other.</p><formula xml:id="formula_9">PA G L = Y ∪ Z ∪ W, CH G L = ∅, PA G L = Z ∪ D, CH G L = Y ∪ E. Consider T = W ∪ Y.</formula><formula xml:id="formula_10">• • • V 1 • • • U 1 • • • W , see Fig.</formula><formula xml:id="formula_11">V 1 , choose U 1 close to L, if there is no U 1 , choose V 1 close to W . Now the path L ← Z • • • V 1 • • • U 1 • • • W → W is unblocked given S,</formula><p>To prove Remark 4, replace var(X) by var(X)/α X in (5) and σ 2 by σ 2 α X in ( <ref type="formula">6</ref>) and ( <ref type="formula">7</ref>).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The situation dealt with in the second part of case (ii) of the proof of Theorem 1, with S = {S 1 , S 2 } and D = ∅. It contains the proof's main argument.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Box plots for the structural Hamming distance of greedy directed acyclic graph search (white), greedy equivalence search (light grey), PC-algorithm (grey) and a best-score method (dark grey) to the true directed acyclic graph, DAG, (top) and to the true partially directed acyclic graph, CPDAG, (bottom). The graph shows various values of a measuring perturbation a of equal error variances; only a = 0 corresponds to equal error variances.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Nodes adjacent to L in G and G</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>We distinguish two cases. Case (i): T = ∅. Then there must be a node D ∈ D or a node E ∈ E, otherwise L would have been discarded. If there is aD ∈ D then (4) implies L ⊥ ⊥ D | S for S = Z ∪ D \ {D}, which contradicts Lemma 8 applied to G . If D = ∅ and there is E ∈ E then E ⊥ ⊥ L | S holds for S = Z ∪ PA G E \ {L}, which also contradicts Lemma 8; to avoid cycles it is necessary that Z ⊆ ND G E . Case (ii): T = ∅.Then T contains a G -youngest node with the property that there is no directed G -path from this node to any other node in T. This node may not be unique.Suppose that W ∈ W is such a youngest node. Consider the directed acyclic graph G that equals G with additional edges Y → W and W → W for all Y ∈ Y and W ∈ W \ {W }. In G , L and W are not adjacent. Thus we find a set S such that S d-separates L and W in G ; indeed, one can take S = PA S = S ∪ {Y, Z, W \ {W }} d-separates L and W in G .We now prove this claim. All Y ∈ Y are already in S in order to block L → Y → W . Suppose there is a G -path that is blocked by S and unblocked if we add Z and W nodes to S. How can we unblock a path by including more nodes? The path L</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Assume the path L• • • V 1 • • • U 1 • • • W is blocked by S, but unblocked if we include Z and W .Then the dashed path is unblocked given S.Therefore, the G -youngest node in T must be some Y ∈ Y. It holds thatσ 2 G = σ 2 G = min X∈X var(X) = σ 2 .(5)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell cols="2">n = 100</cell><cell></cell><cell cols="2">n = 500</cell><cell></cell><cell></cell><cell cols="3">n = 1000</cell></row><row><cell>p</cell><cell></cell><cell>gds eev</cell><cell>pc</cell><cell cols="2">ges gds eev</cell><cell>pc</cell><cell cols="3">ges gds eev</cell><cell>pc</cell><cell>ges</cell></row><row><cell>5</cell><cell>dag cpdag</cell><cell>1.5 1.5</cell><cell>3.9 2.9</cell><cell>3.6 2.3</cell><cell>0.5 0.5</cell><cell>2.9 1.4</cell><cell>2.8 1.2</cell><cell>0.4 0.3</cell><cell></cell><cell>3.0 1.0</cell><cell>2.5 0.7</cell></row><row><cell>20</cell><cell>dag cpdag</cell><cell>12.2 13.9</cell><cell cols="2">14.1 18.0 10.9 17.0</cell><cell>4.5 5.2</cell><cell cols="2">11.1 10.3 7.7 7.6</cell><cell>2.7 3.0</cell><cell></cell><cell cols="2">10.1 8.7 6.9 5.6</cell></row><row><cell>40</cell><cell>dag cpdag</cell><cell>44.7 50.0</cell><cell cols="2">29.6 53.0 24.4 53.1</cell><cell>15.7 18.9</cell><cell cols="2">22.6 26.1 15.9 23.4</cell><cell cols="2">10.7 13.4</cell><cell cols="2">20.1 21.9 13.3 17.5</cell></row><row><cell></cell><cell></cell><cell></cell><cell>n = 100</cell><cell></cell><cell></cell><cell cols="2">n = 500</cell><cell></cell><cell></cell><cell cols="2">n = 1000</cell></row><row><cell>p</cell><cell></cell><cell>gds eev</cell><cell>pc</cell><cell>ges</cell><cell>gds eev</cell><cell>pc</cell><cell>ges</cell><cell cols="3">gds eev</cell><cell>pc</cell><cell>ges</cell></row><row><cell>5</cell><cell>dag cpdag</cell><cell>1.2 1.3</cell><cell>2.9 2.1</cell><cell>3.0 1.9</cell><cell>0.6 0.5</cell><cell>2.4 1.2</cell><cell>2.2 0.7</cell><cell></cell><cell>0.3 0.2</cell><cell></cell><cell>2.1 0.8</cell><cell>2.1 0.5</cell></row><row><cell>20</cell><cell>dag cpdag</cell><cell>30.0 31.0</cell><cell>56.6 56.1</cell><cell>63.9 63.2</cell><cell>12.5 13.1</cell><cell>55.7 55.5</cell><cell>66.3 66.2</cell><cell></cell><cell>8.2 8.8</cell><cell></cell><cell>57.6 57.5</cell><cell>69.1 68.5</cell></row><row><cell>40</cell><cell>dag cpdag</cell><cell>216.1 217.1</cell><cell cols="2">242.8 323.1 242.4 323.0</cell><cell>185.2 185.7</cell><cell cols="3">247.2 430.4 247.0 430.1</cell><cell cols="2">172.0 172.2</cell><cell>248.9 470.6 248.5 470.4</cell></row></table><note><p>Structural Hamming distance between estimated and true directed acyclic graph and estimated and true Markov equivalence class, for sparse graphs with p nodes and sample size n. dag, directed acyclic graph; cpdag, completed partially directed acyclic graph; gds eev , greedy directed acyclic graph search with equal error variances; pc, PC-algorithm; ges, greedy equivalence search.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Structural Hamming distance between estimated and true directed acyclic graph and estimated and true Markov equivalence class, for dense graphs with p nodes and sample size n. dag, directed acyclic graph; cpdag, completed partially directed acyclic graph; gds EEV , greedy directed acyclic graph search with equal error variances; pc, PC-algorithm; ges, greedy equivalence search.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Table 5.4 shows that in two out of the seven data sets, greedy directed acyclic graph search obtained a better score than greedy equivalence search. For the Colon example, bic scores of greedy equivalent search and greedy directed acyclic graph search on different type of microarray data; smaller is better. ges, greedy equivalence search; gds EEV , greedy directed acyclic graph search with equal error variances. greedy directed acyclic graph search proposes a directed acyclic graph with 192 edges, greedy equivalence search a graph with 217 edges. There are 91 edges in both solutions, 61 with the same orientation. The graphs therefore differ on roughly half of the edges.</figDesc><table><row><cell></cell><cell cols="7">Prostate Lymphoma Riboflavin Leukemia Brain Cancer Colon</cell></row><row><cell>ges</cell><cell>4095</cell><cell>4560</cell><cell>2711</cell><cell>5456</cell><cell>1411</cell><cell>5891</cell><cell>3224</cell></row><row><cell>gds eev</cell><cell>6057</cell><cell>5404</cell><cell>3236</cell><cell>5481</cell><cell>1343</cell><cell>6288</cell><cell>3201</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgement</head><p>We thank <rs type="person">R. Tanase</rs> for fruitful discussions. The research leading to these results received funding from the <rs type="funder">European Union</rs>'s <rs type="programName">Seventh Framework Programme</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_e9af7KT">
					<orgName type="program" subtype="full">Seventh Framework Programme</orgName>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix Some lemmata</head><p>In the following two sections we consider different subsets of the set of variables X: to simplify notation we do not distinguish between indices and variables, since the context should clarify the meaning. This way, we can also speak of the parents PA G B of a variable B ∈ X. We also consider sets of variables S ⊂ X to be a single multivariate variable.</p><p>The following four statements are all plausible and their proofs mostly involve technicalities. The reader may skip to the next section and use the lemmata whenever needed.</p><p>Lemma 5 Let (A 1 , . . . , A m ) ∼ N {(µ 1 , . . . , µ m ) T , Σ} with strictly positive definite Σ and define A * 1 = A 1 | (A2,...,Am)=(a2,...,am) , in distribution. Then var(A * 1 ) ≤ var(A 1 ) for all (a 2 , . . . , a m ) ∈ R m-1 .</p><p>We use the notation of conditional variables rather than conditional distributions to improve readability.</p><p>Proof . Let us decompose Σ into</p><p>Lemma 6 <ref type="bibr" target="#b10">[Peters et al., 2011]</ref> Let Y, N, Q and R be random variables taking values in Y, N , Q and R, respectively, whose joint distribution is absolutely continuous with respect to some product measure; we denote the densities by p Y,Q,R,N (y, q, r, n). The variables Q and R can be multivariate. Let f :</p><p>Lemma 7 <ref type="bibr" target="#b10">(Peters et al. [2011]</ref>) Let L(X) be generated by a structural equation model as in (2) with corresponding directed acyclic graph G and consider a variable X ∈ X. If S ⊆ ND G X then N X ⊥ ⊥ S.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A characterization of Markov equivalence classes for acyclic digraphs</title>
		<author>
			<persName><forename type="first">A</forename><surname>Andersson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Madigan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Perlman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="505" to="541" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">High-dimensional statistics with a view towards applications in biology</title>
		<author>
			<persName><forename type="first">P</forename><surname>Bühlmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kalisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Meier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annual Review of Statistics and its Applications</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note>to appear</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A transformational characterization of equivalent Bayesian network structures</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Chickering</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th Conference on Uncertainty in Artificial Intelligence (UAI 1995)</title>
		<meeting>the 11th Conference on Uncertainty in Artificial Intelligence (UAI 1995)<address><addrLine>San Francisco, California</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="87" to="98" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Optimal structure identification with greedy search</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Chickering</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="507" to="554" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Cvetković</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Doob</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sachs</surname></persName>
		</author>
		<title level="m">Spectra of Graphs: Theory and Application</title>
		<meeting><address><addrLine>Barth, Heidelberg</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
	<note>third and enlarged edition</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Boosting for tumor classification with gene expression data</title>
		<author>
			<persName><forename type="first">M</forename><surname>Dettling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bühlmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1061" to="1069" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Nonlinear causal discovery with additive noise models</title>
		<author>
			<persName><forename type="first">P</forename><surname>Hoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Janzing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mooij</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 21 (NIPS 2008)</title>
		<meeting><address><addrLine>Red Hook, New York</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="689" to="696" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Graphical Models</title>
		<author>
			<persName><forename type="first">S</forename><surname>Lauritzen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996">1996</date>
			<publisher>Oxford University Press</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">The on-line encyclopedia of integer sequences</title>
		<ptr target="http://oeis.org/A003024" />
		<imprint>
			<date type="published" when="2011">2011</date>
			<publisher>OEIS Foundation Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Causality: Models, Reasoning, and Inference</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>Cambridge University Press</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
	<note>2nd edition</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Identifiability of causal graphs using functional models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Mooij</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Janzing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">27th Conference on Uncertainty in Artificial Intelligence (UAI 2011)</title>
		<meeting><address><addrLine>Corvallis, Oregon</addrLine></address></meeting>
		<imprint>
			<publisher>AUAI Press</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="589" to="598" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A linear non-Gaussian acyclic model for causal discovery</title>
		<author>
			<persName><forename type="first">S</forename><surname>Shimizu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">O</forename><surname>Hoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hyvärinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kerminen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<date type="published" when="2003">2003-2030, 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Causation, Prediction, and Search</title>
		<author>
			<persName><forename type="first">P</forename><surname>Spirtes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Glymour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Scheines</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, Massachusetts</pubPlace>
		</imprint>
	</monogr>
	<note>2nd edition</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Nonlinear directed acyclic structure learning with weakly additive noise models</title>
		<author>
			<persName><forename type="first">R</forename><surname>Tillman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Spirtes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 22 (NIPS 2009)</title>
		<meeting><address><addrLine>Red Hook, New York</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1847" to="1855" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Geometry of the faithfulness assumption in causal inference</title>
		<author>
			<persName><forename type="first">C</forename><surname>Uhler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Raskutti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bühlmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="436" to="463" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">0 -penalized maximum likelihood for sparse directed acyclic graphs</title>
		<author>
			<persName><forename type="first">S</forename><surname>Van De Geer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bühlmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="536" to="567" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Detection of unfaithfulness and robust causal inference</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Spirtes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Minds and Machines</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="239" to="271" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
