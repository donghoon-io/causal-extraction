<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Sample Size Requirements for Structural Equation Models: An Evaluation of Power, Bias, and Solution Propriety</title>
				<funder ref="#_n7NqgMU">
					<orgName type="full">National Institute on Alcohol Abuse and Alcoholism</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2015-02-19">2015 February 19</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Erika</forename><forename type="middle">J</forename><surname>Wolf</surname></persName>
							<email>erika.wolf@va.gov</email>
							<affiliation key="aff0">
								<orgName type="department">National Center for PTSD</orgName>
								<orgName type="institution">Boston Healthcare System</orgName>
								<address>
									<settlement>Boston</settlement>
									<region>VA MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Boston University School of Medicine</orgName>
								<address>
									<settlement>Boston</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kelly</forename><forename type="middle">M</forename><surname>Harrington</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">National Center for PTSD</orgName>
								<orgName type="institution">Boston Healthcare System</orgName>
								<address>
									<settlement>Boston</settlement>
									<region>VA MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Boston University School of Medicine</orgName>
								<address>
									<settlement>Boston</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shaunna</forename><forename type="middle">L</forename><surname>Clark</surname></persName>
							<affiliation key="aff2">
								<orgName type="department" key="dep1">Center for Biomarker Research and Personalized Medicine</orgName>
								<orgName type="department" key="dep2">School of Pharmacy</orgName>
								<orgName type="institution">Virginia Commonwealth University</orgName>
								<address>
									<settlement>Richmond</settlement>
									<region>VA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mark</forename><forename type="middle">W</forename><surname>Miller</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">National Center for PTSD</orgName>
								<orgName type="institution">Boston Healthcare System</orgName>
								<address>
									<settlement>Boston</settlement>
									<region>VA MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Boston University School of Medicine</orgName>
								<address>
									<settlement>Boston</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">National Center for PTSD</orgName>
								<orgName type="institution">Boston Healthcare System</orgName>
								<address>
									<addrLine>116B-2) 150 South Huntington Avenue</addrLine>
									<postCode>02130</postCode>
									<settlement>Boston</settlement>
									<region>VA MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Sample Size Requirements for Structural Equation Models: An Evaluation of Power, Bias, and Solution Propriety</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2015-02-19">2015 February 19</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1177/0013164413495237</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-14T17:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>structural equation modeling</term>
					<term>confirmatory factor analysis</term>
					<term>sample size</term>
					<term>statistical power</term>
					<term>Monte Carlo simulation</term>
					<term>bias</term>
					<term>solution propriety</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Determining sample size requirements for structural equation modeling (SEM) is a challenge often faced by investigators, peer reviewers, and grant writers. Recent years have seen a large increase in SEMs in the behavioral science literature, but consideration of sample size requirements for applied SEMs often relies on outdated rules-of-thumb. This study used Monte Carlo data simulation techniques to evaluate sample size requirements for common applied SEMs. Across a series of simulations, we systematically varied key model properties, including number of indicators and factors, magnitude of factor loadings and path coefficients, and amount of missing data. We investigated how changes in these parameters affected sample size requirements with respect to statistical power, bias in the parameter estimates, and overall solution propriety. Results revealed a range of sample size requirements (i.e., from 30 to 460 cases), meaningful patterns of association between parameters and sample size, and highlight the limitations of commonly cited rules-of-thumb. The broad "lessons learned" for determining SEM sample size requirements are discussed.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>One of the strengths of SEM is its flexibility, which permits examination of complex associations, use of various types of data (e.g., categorical, dimensional, censored, count variables), and comparisons across alternative models. However, these features of SEM also make it difficult to develop generalized guidelines regarding sample size requirements <ref type="bibr" target="#b22">(MacCallum, Widaman, Zhang, &amp; Hong, 1999)</ref>. Despite this, various rules-of-thumb have been advanced, including (a) a minimum sample size of 100 or 200 <ref type="bibr" target="#b3">(Boomsma, 1982</ref><ref type="bibr" target="#b4">(Boomsma, , 1985))</ref>, (b) 5 or 10 observations per estimated parameter <ref type="bibr" target="#b1">(Bentler &amp; Chou, 1987</ref>; see also <ref type="bibr" target="#b2">Bollen, 1989)</ref>, and (c) 10 cases per variable <ref type="bibr" target="#b33">(Nunnally, 1967)</ref>. Such rules are problematic because they are not model-specific and may lead to grossly over-or underestimated sample size requirements. <ref type="bibr" target="#b22">MacCallum et al. (1999)</ref> demonstrated that model characteristics such as the level of communality across the variables, sample size, and degree of factor determinacy all affect the accuracy of the parameter estimates and model fit statistics, which raises doubts about applying sample size rules-of-thumb to a specific SEM.</p><p>There has been a sharp increase in the number of SEM-based research publications that evaluate the structure of psychopathology and the correlates and course of psychological disorders and symptoms, yet applied information on how to determine adequate sample size for these studies has lagged behind. The primary aim of this study was to evaluate sample size requirements for SEMs commonly applied in the behavioral sciences literature, including confirmatory factor analyses (CFAs), models with regressive paths, and models with missing data. We also sought to explore how systematically varying parameters within these models (i.e., number of latent variables and indicators, strength of factor loadings and regressive paths, type of model, degree of missing data) affected sample size requirements. In so doing, we aimed to demonstrate the tremendous variability in SEM sample size requirements and the inadequacy of common rules-of-thumb. Although statisticians have addressed many of these concerns in technical papers, our impression from serving as reviewers, consultants, and readers of other articles is that this knowledge may be inaccessible to many applied researchers and so our overarching objective was to communicate this information to a broader audience.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sample Size Considerations</head><p>When contemplating sample size, investigators usually prioritize achieving adequate statistical power to observe true relationships in the data. Statistical power is the probability of rejecting the null hypothesis when it is false; it is the probability of not making a Type II error (i.e., 1 -beta; see <ref type="bibr" target="#b6">Cohen, 1988)</ref>. Power is dependent on (a) the chosen alpha level (by convention, typically α = .05), (b) the magnitude of the effect of interest, and (c) the sample size. However, power is not the only consideration in determining sample size as bias 1 in the parameter estimates and standard errors also have bearing. Bias refers to conditions in which an estimated parameter value differs from the true population value (see <ref type="bibr" target="#b17">Kelley &amp; Maxwell, 2003;</ref><ref type="bibr" target="#b28">Maxwell, Kelley, &amp; Rausch, 2008)</ref>. A standard error may also be biased if it is under-or overestimated (which increases the risk of Type I and II errors, respectively). A 1 Throughout this article, we refer to the term bias in its broadest sense (i.e., a systematic misrepresentation of a parameter estimate or statistic due to methodological problems such as insufficient sample size or poor measurement psychometric characteristics). We also use this term to describe bias in the standard errors and parameter estimates due to insufficient sample size in order to maintain consistency with <ref type="bibr">Muthén and Muthén (2002)</ref>.</p><p>third element of SEMs that is affected by sample size is solution propriety (see <ref type="bibr" target="#b13">Gagné &amp; Hancock, 2006)</ref>. This is whether there are a sufficient number of cases for the model to converge without improper solutions or impossible parameter estimates. Models based on larger samples <ref type="bibr" target="#b3">(Boomsma, 1982;</ref><ref type="bibr" target="#b13">Gagné &amp; Hancock, 2006;</ref><ref type="bibr" target="#b36">Velicer &amp; Fava, 1998)</ref>, with more indicators per factor <ref type="bibr" target="#b13">(Gagné &amp; Hancock, 2006;</ref><ref type="bibr" target="#b26">Marsh, Hau, Balla, &amp; Grayson, 1998)</ref>, and with larger factor loadings <ref type="bibr">(Gagneé &amp; Hancock, 2006)</ref> are more likely to converge properly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Monte Carlo Analyses for Sample Size Determinations</head><p>Three major approaches to evaluating sample size requirements in SEMs have been proposed: (a) the <ref type="bibr" target="#b35">Satorra and Saris (1985)</ref> method, which estimates power based on the noncentrality parameter (i.e., the amount of model misspecification); (b) the MacCallum, Browne, and Sugawara (1996) method, which is based on the power of the model to obtain a root mean square error of approximation value that is consistent with good model fit; and (c) the Monte Carlo simulation method <ref type="bibr" target="#b29">(Muthén &amp; Muthén, 2002)</ref>, the focus of this research. With this method, associations among variables are set by the user based on a priori hypotheses. The specified associations are akin to the population estimates of the true relationships among the variables. A large number of data sets are then generated to match the population values; each individual data set is akin to a sample and is based on a userdetermined number of cases. The hypothesized model is then evaluated in the generated data sets and each parameter estimate is averaged across the simulations to determine if the specified number of cases is sufficient for reproducing the population values and obtaining statistically significant parameter estimates. This approach does not address the power of the overall model-it provides estimates of power and bias for individual effects of interest (i.e., individual factor loadings, correlations, or regressive paths). This is important as an investigator may only be interested in having sufficient sample size for select aspects of a given model. Additional details and script for conducing Monte Carlo analyses in MPlus can be found in <ref type="bibr">Muthén and Muthén (2002)</ref>, <ref type="bibr" target="#b29">Muthén and</ref><ref type="bibr" target="#b30">Asparouhov (2002), and</ref><ref type="bibr" target="#b29">Muthén (2002)</ref>. Readers are also referred to <ref type="bibr">Paxton, Curran, Bollen, Kirby, and Chen (2001)</ref>, who provide a useful step-by-step discussion of conducting Monte Carlo analyses.</p><p>It is important to distinguish between two overarching approaches to the use of Monte Carlo analyses. The first, termed proactive (see <ref type="bibr">Marcoulides &amp; Chin, in press;</ref><ref type="bibr" target="#b25">Marcoulides &amp; Saunders, 2006)</ref>, involves conducting simulation studies of a hypothesized model with relationships among the variables specified based on the available research literature; this approach is the focus of this investigation, as described above (see also <ref type="bibr" target="#b34">Paxton et al., 2001)</ref>. The second approach, termed reactive <ref type="bibr">(Marcoulides &amp; Chin, in press;</ref><ref type="bibr" target="#b25">Marcoulides &amp; Saunders, 2006)</ref>, involves analyzing existing data after a study has been completed to evaluate the hypothesized model many times over by taking repeated random draws of cases from the larger sample (see also <ref type="bibr" target="#b23">Marcoulides, 1990)</ref>. The model is fit in each subsample and the resulting parameter estimates and fit statistics are then examined across all the random draws. The former approach is a prospectively designed one that has its basis in theory and the relevant literature. The latter approach is a post hoc method that is limited by the quality of the existing data and may lead to unwarranted confidence in the stability of the results or the appropriateness of the sample for the planned analyses. Throughout this article, we describe the importance of conducting proactive Monte Carlo simulation studies for the purposes of sample size planning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Aims</head><p>The primary aim of this study was to provide applied behavioral science researchers with an accessible evaluation of sample size requirements for common types of latent variable models and to demonstrate the range of sample sizes that may be appropriate for SEM. We hope this will help researchers better understand the factors most relevant to SEM sample size determinations and encourage them to conduct their own Monte Carlo analyses rather than relying on rules-of-thumb. A second aim was to examine how sample size requirements change as a function of elements in an SEM, such as number of factors, number of indicators, strength of indicator loadings, strength of regressive paths, degree of missing data, and type of model. 2 We evaluated these aspects of structural models in one study and included several permutations and combinations of each model characteristic to have broad applicability. Finally, this study also afforded the opportunity to compare sample size requirements for SEMs versus for similar models based on single indicators (i.e., traditional path models).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method Procedure</head><p>We conducted Monte Carlo simulation studies for several types of CFAs and SEMs following the guidelines described by <ref type="bibr">Muthén and Muthén (2002)</ref>. For each model, we systematically varied the number of indicators of the latent variable(s) and the strength of the factor loadings and structural elements in the model to examine how these characteristics would affect statistical power, the precision of the parameter estimates, and the overall propriety of the results. We examined models that we thought would have broad applicability and included models with the absolute minimum number of indicators required to yield overidentified measurement models (i.e., models with positive degrees of freedom, meaning that the total number of variances and covariances in the data set exceeded the number of parameter estimates in the analysis).</p><p>CFA models-Figure <ref type="figure" target="#fig_0">1</ref> provides an overview of the one-, two-, and three-factor CFAs evaluated. For illustrative purposes, suppose that the one-factor model tests the latent construct of depression, the two-factor model tests correlated latent constructs of depression and anxiety, and the three-factor model tests latent constructs of depression, anxiety, and substance use. We varied the number of indicators of these factors such that the one-factor model was indicated by four, six, or eight indicators and the two-and three-factor models were indicated by three, six, or eight indicators. We did not evaluate a three-indicator, onefactor model because it would be just-identified (i.e., have 0 df and hence would estimate all the associations among the data perfectly and yield perfect fit; see <ref type="bibr" target="#b5">Brown, 2006)</ref>. We also varied the factor loadings (and hence the unreliability of the indicators) using standardized 2 These characteristics of a model have also been shown to influence other important aspects of model output that are not under consideration in this study, such as the performance and stability of fit indices (see <ref type="bibr" target="#b16">Jackson, 2001;</ref><ref type="bibr" target="#b18">Kim, 2005;</ref><ref type="bibr" target="#b36">Velicer &amp; Fava, 1998)</ref>. loadings of .50, .65, or .80. All loadings and structural paths in the models were completely standardized (factor variances were fixed to 1.0, factor means fixed to 0, and the squared indicator loading plus the residual indicator variance equaled 1.0). Within a model, factor loadings were held constant across indicators. For models with more than one factor, the factor intercorrelations were set to r = .30; for some models, we also evaluated the effect of a stronger factor intercorrelation (r = .50). We did so because determining the sample size required to observe a factor correlation is often an important consideration. For example, this is crucial when evaluating convergent or discriminant validity, in determining the longitudinal stability of a trait, or in determining the similarity of scores across nonindependent observations (e.g., married couples, twins, etc). Appendix A provides sample Mplus script for the first CFA model that was evaluated (i.e., the one-factor model with four indicators, each loading at .50).</p><p>Structural path models-We also evaluated a three-factor latent variable mediation model with regressive direct and indirect paths between factors (i.e., implying directional relationships) and varied the variance explained in the dependent variable. We focused on the mediation model because of its popularity in the behavioral science literature. This model is shown in Figure <ref type="figure" target="#fig_1">2</ref>. As an applied example, suppose this model tested if chronic stress (the independent variable) predicted functional impairment (the dependent variable) via symptoms of depression (the mediator). In many mediation models, the researcher is primarily interested in the size and significance of the indirect effect (i.e., stress → depression → impairment) because this effect informs the understanding of the mechanism of action. Therefore, we focused these analyses on the minimum sample size required to observe this indirect effect. In these models, each factor was indicated by three observed variables, and all factor loadings were set to .65. This model was structurally saturated (i.e., no unanalyzed relationships among the latent variables), although the measurement portion of it remained overidentified because all indicators loaded on only one factor and there were no cross-loadings. We varied the total variance explained in the latent dependent variable from 16% to 45% to 75% (corresponding to standardized indirect effects of β = .06, .16, and .25, respectively), kept the magnitude of all structural relationships in the model equal to one another, and held the total variance of all latent and observed variables at 1.0. 3 Sample Mplus script for the first SEM that was evaluated (i.e., the model explaining 16% of the variance in the dependent variable) is included in Appendix A.</p><p>Missing data-We next evaluated the effects of missing data on sample size requirements for one CFA and the latent mediation model. Prior work has evaluated the effects of missing data on statistical power analyses in structural equation models (i.e., <ref type="bibr">Davey &amp; Salva, 2009a</ref><ref type="bibr">, 2009b;</ref><ref type="bibr" target="#b10">Dolan, van der Sluis, &amp; Grasman, 2005)</ref>, but to our knowledge, the specific models under investigation in this study have not been evaluated under missing data conditions with respect to effects on power, parameter bias, and solution propriety. The CFA that was 3 The model which explained 16% of the variance was based on standardized direct structural paths of β = .25; the model with 45% variance explained included standardized structural paths of β = .40, and the model with 75% of the variance explained in the latent dependent variable was based on standardized structural paths of β = .50. The size of the unstandardized paths was determined through the following equation: total variance explained in dependent variable = c 2 * Variance X + b 2 * Variance M + 2bc* Covariance (X, M) + Residual Variance Y , where c = the parameter estimate for the dependent variable (Y) regressed on the independent variable (X); b = the parameter estimate for the dependent variable regressed on the mediator (M) and variance explained plus residual variance = 1.0. evaluated was a two-factor model with each factor indicated by three observed variables loading on their respective factors at .65 and a correlation of .30 between the factors, permitting direct comparison to the same model without missing data. We also evaluated the effects of missing data on the mediation model (Figure <ref type="figure" target="#fig_1">2</ref>), where each factor was indicated by three observed variables loading at .65, and all direct paths in the model were set to .40. We systematically varied the percentage of missing data on each indicator in the models such that each indicator was missing 2%, 5%, 10%, or 20% of its data. We did not model a covariate as a predictor of missingness; thus, in accord with <ref type="bibr">Muthén and Muthén (2002)</ref>, we assumed the data were missing completely at random <ref type="bibr" target="#b19">(Little &amp; Rubin, 1989)</ref>.</p><p>Single indicators-Finally, we also compared power, bias, and solution propriety of path models that were based on single indicators versus latent variables (i.e., we evaluated the effects of unspecified measurement error in single indicator designs). To do so, we specified a path analysis in which the association between the observed X and Y variables was mediated by an observed third variable (i.e., a single indicator version of the model depicted in Figure <ref type="figure" target="#fig_1">2</ref>). The standardized magnitude of the three direct paths in the model was set to . 40, and the total variance of each variable was set to 1.0 (and all means to 0). In total, 45% of the variance in the dependent variable was explained by the direct paths in this model. The reliability of each of the variables was specified in the data simulation phase of the analysis. Specifically, we set observed variable reliability to .42, .81, or .90, which, in the case of the use of completely standardized factor loadings, is equivalent to factor loadings of .65, .90, and .95, respectively. 4 After specifying the degree of indicator reliability for data generation, we then purposefully misspecified the model by setting the reliability of the indicators to 1.0 in the data analysis phase. Essentially, this is the assumption of a single indicator analysis, as measurement error is not removed from the variance of the measure and the indicator is treated as if it is a perfect measure of the latent construct.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data Analysis</head><p>Overview-All models were evaluated using Mplus version 5.2 <ref type="bibr">(Muthen &amp; Muthen, 1998</ref><ref type="bibr">-2008)</ref>. All models were based on a single group with 10,000 replications of the simulated data. We set the sample size of a given model and then adjusted it (upwards or downwards) based on whether the results met our criteria for acceptable precision of the estimates, statistical power, and overall solution propriety, as detailed below. In the first model, we started with a sample size of 200 because that has previously been suggested as the minimum for SEMs <ref type="bibr" target="#b3">(Boomsma, 1982)</ref>; starting sample sizes for subsequent models were determined based on the results of prior models. To determine the minimum sample size required, we tested the effects of increasing or decreasing the sample size of each model by n = 10 (or n = 20 when it was clear that increments of 10 were too fine to yield meaningful differences among the models). Next, we continued to increase the sample size by a unit of 10 or 20 to test stability of the solution, as defined by both the minimum sample size and the next largest sample size meeting all a priori criteria. Finally, as recommended by <ref type="bibr">Muthén and Muthén (2002)</ref>, we tested the stability of the results by running the analyses again with a 4 Reliability = the proportion of true score variance (i.e., the squared factor loading) to total score variance (i.e., true score + error variance). new, randomly selected seed number (i.e., so that data generation began at a different point, resulting in a different set of 10,000 data sets compared to the analysis with the initial seed number).</p><p>All analyses were conducted using the maximum likelihood (ML) estimator. The simulated data were dimensional (i.e., continuous) and normally distributed. Models with missing data invoked full information ML estimation for cases with missingness as this approach performs well when data are missing completely at random or missing at random <ref type="bibr" target="#b12">(Enders &amp; Bandalos, 2001)</ref>.</p><p>Criteria for evaluation of sample size requirements-We used several criteria to evaluate the minimum sample size required to achieve minimal bias, adequate statistical power, and overall propriety of a given model, following recommendations by <ref type="bibr">Muthén and Muthén (2002)</ref>. All criteria had to be met in order to accept a given N as the minimum sample size.</p><p>Bias-First, we evaluated (a) the degree of bias in the parameter estimates, (b) the degree of bias in the standard error estimate, and (c) the 95% confidence interval for the parameter estimates. Specifically, we examined the mean parameter estimates (i.e., factor loadings and regressive paths) across simulations in comparison to the specified population values. This index of parameter bias was derived by subtracting the population value from the mean estimated value and dividing by the population value, following <ref type="bibr">Muthén and Muthén (2002)</ref>. We specified that the most discrepant mean parameter estimate in a model had to be within 5% of the population value. We also evaluated the potential for standard error bias in an analogous way, by comparing mean standard errors in the generated data sets to the SD for each parameter estimate (the SD of the estimates over many replications is akin to the population standard error; <ref type="bibr" target="#b29">Muthén &amp; Muthén, 2002)</ref>. Acceptable standard error estimates also had to be within 5% of the population standard error. Finally, we specified that the 95% confidence interval for each parameter estimate had to include the population value in at least 90% of the analyses of the simulated data (i.e., achieve adequate accuracy).</p><p>Power-Second, we specified that estimated power had to be 80% or greater (with α = .05) for all parameters of interest (in this case, factor loadings, correlations, regressive paths) in the model. Thus, a model in which any one of the parameters of interest fell below 80% power would be rejected (this approach is conceptually similar to the "all-pairs power" approach described by <ref type="bibr" target="#b27">Maxwell, 2004)</ref>.</p><p>Overall solution propriety-Third, we specified that the analysis could not yield any errors (i.e., improper solutions or failures to converge). This is a conservative criterion as a single error in the analysis of 10,000 data sets is not particularly worrisome, assuming that all other criteria are met. However, we adopted this rule for several reasons. First, Mplus provides a description of any errors in the analysis of the generated data but omits these cases from the overall statistics summarized across replications. Depending on the number and type of errors, this can potentially yield overly optimistic summary statistics and also raises the possibility that a similarly sized sample of real data might produce improper solutions. Second, as is evident in this study, the number of errors in the analysis of the generated data increased with decreasing sample size, which further suggests the need to consider such instances when evaluating sample size requirements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Orientation to Figures and Tables</head><p>Figure <ref type="figure" target="#fig_2">3</ref> shows the minimum sample size (meeting all a priori criteria) required for each of the models. Supplementary tables appear online. Although not shown in the figures or tables, all models yielded acceptable coverage (i.e., the 95% confidence interval for each parameter estimate included the population value in at least 90% of the replications), with the exception of the single indicator models (discussed below).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CFAs</head><p>Effect of number of factors-Within the CFAs, increasing the number of latent variables in a model resulted in a significant increase in the minimum sample size when moving from one to two factors, but this effect plateaued, as the transition from two to three factors was not associated with a concomitant increase in the sample size. For example, as shown in Figure <ref type="figure" target="#fig_2">3 (Panels A-C</ref>) and Supplemental Tables <ref type="table">1</ref><ref type="table">2</ref><ref type="table">3</ref>, sample size requirements at least doubled when comparing the simplest possible one-factor, four-indicator model, which required a sample of 190, 90, and 60 participants at factor loadings of .50, .65, and .80, respectively, relative to the simplest possible two-factor model (with three indicators per factor), which required a minimum sample of 460, 200, and 120, respectively. However, this was not true for comparisons between two-and three-factor models. In these cases, sample sizes were relatively unchanged, or in some cases decreased, with the addition of the third factor.</p><p>Effect of number of indicators-Overall, models with fewer indicators required a larger sample relative to models with more indicators. A one-factor, four-indicator model with loadings of .50, .65, and .80 required sample sizes of 190, 90, and 60, respectively, while a one-factor, six-indicator model required sample sizes of 90, 60, and 40, respectively (see Figure <ref type="figure" target="#fig_2">3</ref>, Panels A-C). However, this effect also leveled as the transition from six to eight indicators did not result in as dramatic a decrease in sample size (i.e., this dropped from 60 to 50 when increasing from six to eight indicators in the one-factor model with factor loadings of .65) and, in some instances, yielded no reduction in minimum sample size (i.e., both the six-and the eight-indicator one-factor model with loadings of .50 were associated with a minimum sample size of 90). The same general pattern held for the twoand three-factor models at factor loadings of .50 and .65 but the effect of increasing the number of indicators was less evident in the models with factor loadings of .80 (see Figure <ref type="figure" target="#fig_2">3</ref>, Panel C, and Supplemental Table <ref type="table">3</ref>).</p><p>Effect of magnitude of factor loadings-Models with stronger factor loadings required dramatically smaller samples relative to models with weaker factor loadings. For example, the one-factor, four-indicator model with loadings of .50, .65, and .80 required a minimum of 190, 90, and 60 observations, respectively; decreasing the strength of the factor loadings from .80 to .50 necessitated a threefold increase in the sample size. On average, factor loadings of .50 were associated with nearly 2.5-fold increases in required sample size relative to an identical model with loadings of .80.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effect of magnitude of factor correlations-As described in the notes to</head><p>Supplementary Tables <ref type="table">1,</ref><ref type="table">2</ref>, and 3, we also evaluated the effect of increasing the factor intercorrelation from .30 to .50. On average, the increased factor relationship was associated with 78 fewer participants in the minimum acceptable sample size for the CFA models with .50 factor loadings, 55 fewer participants for CFA models with .65 factor loadings, and 37 fewer participants for CFA models with .80 factor loadings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SEMs</head><p>Effect of magnitude of regressive paths-Mediation models with larger effects tended to achieve adequate statistical power for the direct and indirect effects in the model with smaller sample sizes. For example, the model in which the direct effects accounted for 16% of the variance in the dependent variable required greater than 2.4 times more participants to achieve adequate statistical power relative to the model in which 45% of the variance was explained (i.e., n = 440 vs. 180). However, the model that explained 75% of the variance in the dependent variable actually required more participants than the model that explained 45% of the variance. This was not due to statistical power (which was sufficient even at relatively small sample sizes) but instead primarily due to bias of the parameter estimates in the model with larger effects. This likely reflects larger standard errors (evidenced by larger standard error estimates for the regressive paths) as the true value becomes more extreme, thus necessitating increased sample size.</p><p>Effect of Missing Data-Greater amounts of missing data in the two-factor CFA and mediation models generally necessitated larger sample sizes (see Figure <ref type="figure" target="#fig_2">3</ref>, Panel E, and Supplemental Table <ref type="table">5</ref>). This was primarily due to problems with errors in the analysis of generated data sets. For example, the two-factor CFA model with three indicators per factor loading at .65 and a factor intercorrelation of .30 was associated with a minimum sample size of 200 when there were no missing data or only a small amount of missing data (i.e., 2% per indicator). However, the minimum sample size increased to 260 with 5% and 10% missing data per indicator and to 320 with 20% missing data per indicator because of errors that occurred at smaller sample sizes. In contrast, power was not dramatically affected by the addition of missingness. The same basic pattern emerged with respect to the effect of missing data in the mediation models.</p><p>Effect of Latent Variables-The comparison of latent versus observed variables in the mediation model demonstrated that no amount of increasing the sample size of single indicator models could account for bias in those results; the attenuation in the magnitude of the parameter estimates was directly related to the degree of unspecified unreliability in the measure (see <ref type="bibr" target="#b7">Cole &amp; Maxwell, 2003)</ref>. Specifically, the results showed that a measure with 42% reliability (equivalent to a factor loading of .65) would estimate a direct path parameter estimate of .17 when the true population estimate was .40 (biased by 58%), whereas a measure with 90% reliability (equivalent to a factor loading of .95) would estimate a direct path parameter estimate of .36 when the true estimate was .40 (biased by 10%). Statistical power was also affected because the model with reliability of 42% estimated the direct effect as .17 (instead of the true effect of .40) and a sample size of 180, which was sufficient for the equivalent latent variable mediation model, was inadequate (power = 68%) to detect an effect of .17. The single-indicator models with more reasonable amounts of indicator reliability (.81 and .90) achieved at least 99% power for all direct and indirect paths because the parameter estimates in these models, while still attenuated, were not so small as to affect statistical power. If the three observed variables in the single-indicator mediation model contained only true score variance (i.e., unreliability = 0), then a sample size of 50 would achieve 86% power to detect the smallest direct effect, and a sample size of 70 would be the minimum required to detect the projected indirect effect (and would achieve 81% power).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Stability of Results</head><p>-With a few exceptions, solutions that met all a priori criteria at a given sample size were stable relative to the results of the analysis at the next largest sample size. We observed somewhat greater instability of results, however, with respect to the reanalysis of the best solutions using a new seed number. Specifically, in 34% of the analyses, the minimum sample size increased (average increase in sample size = 24 cases, range = 10-50); in 16% of the analyses, the minimum sample size decreased (average decrease in sample size = 18 cases, range = 10-30); and in 50% of the analyses, the minimum necessary sample size was equal across analyses using the two seed numbers. When reevaluated with the new seed number, 41% of the CFA models, 67% of the SEM models (note that there were only three total), and 75% of the missingness models required a change in sample size. In the majority of the cases, errors caused the increase in sample size in the models with the new seed number; however, in a minority of cases, power (when it was just on the threshold of .80 using the first seed number), and bias were the unstable factors that necessitated increased sample size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion</head><p>In this study, we systematically evaluated sample size requirements for common types of SEMs by performing Monte Carlo analyses that varied by type of model, number of factors, number of indicators, strength of the indicator loadings and regressive paths, and the amount of missing data per indicator. We evaluated the extent to which statistical power, parameter estimate bias, and the overall propriety of the results affected sample size requirements for these different models. In so doing, we aimed to demonstrate to applied researchers the broad variability in sample size requirements for latent variable models and show how the sample size estimates vary greatly from model to model. This is important as the results of methods-focused articles are not always accessible and interpretable to applied researchers, thus misinformation about sample size requirements persists in the literature and in reviewer feedback on manuscripts and grant applications. In the paragraphs that follow, we describe our results with respect to the broad "lessons learned" from this study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Optimizing Model Characteristics</head><p>Lesson 1A: One size does not fit all-The results of this study demonstrate the great variability in SEM sample size requirements and highlight the problem with a "one size fits all" approach, consistent with the observations of <ref type="bibr" target="#b22">MacCallum et al. (1999)</ref>. More specifically, required sample sizes ranged from 30 cases (for the one-factor CFA with four indicators loading at .80) to 460 (for the two-factor CFA with three indicators loading at . 50). In comparison, the 10 cases per variable rule-of-thumb would have led to sample size recommendations ranging from 40 to 240, respectively. Furthermore, rather than increasing linearly with number of estimated parameters or number of variables, we found that sample size requirements actually decreased when the number of indicators of a factor increased. This was likely a result of the increase in information available for use in solving the simultaneous regression equations. This effect was particularly evident in moving from three or four indicators to six, but less so when transitioning from six to eight indicators. This is consistent with prior work suggesting that increasing the number of indicators per factor may be one way to compensate for an overall small sample size and preserve statistical power <ref type="bibr" target="#b26">(Marsh et al., 1998)</ref>. This suggests that researchers may wish to design models in which the number of indicators per factor is greater than the minimum number of indicators required for model identification. Investigators should consider the tradeoff between the simplicity of having fewer indicators per factor versus the benefits of enhanced power and precision that come with having more than the minimum number of indicators per factor.</p><p>Lesson 1B: More is not always better-While the number of indicators in a model had an inverse effect on sample size requirements (i.e., models with more indicators per factor required fewer cases), other types of increases to the complexity of the model necessitated increased sample size requirements. For example, increasing the number of factors yielded a commensurate increase in the minimum sample size required. This was likely a function of the need to have adequate power to detect the correlation between the factors. This is an important point as it is not uncommon to have a measurement model in which the correlation between the factors is the focus of investigation and is expected to be small to moderate in magnitude (i.e., when evaluating the discriminant validity of constructs). Thus, evaluating a multifactor measurement model in which the factors are thought to be distinct but correlated may require larger samples compared to a model in which the factor correlation is not of interest and the researcher is focused only on obtaining adequate sample size to estimate the factor loadings.</p><p>While models with stronger effects generally required fewer cases, this effect was nonlinear, as SEM models with large path coefficients actually required substantially more cases relative to the same models with more moderate strengths of association due to bias in the parameter estimates. In general, models with indicators with strong relationships to the latent variables yielded more problems with biased and error-prone solutions, even after power reached acceptable levels. In contrast, models with weaker factor loadings tended to have problems with power, even when bias and errors were not a problem.</p><p>The broad take home message is that, with respect to the strength of the factor loadings and the magnitude of regressive effects in the model, effects that are very weak or very strong may require larger samples relative to effects that are more moderate in magnitude; albeit this effect is more pronounced in models with very weak effects. This implies that researchers need to carefully consider these parameters when planning an SEM and to keep in mind that having large effects, while associated with generally improved statistical power, yields new concerns about parameter bias that may ultimately result in increased sample size requirements in order to obtain sufficiently accurate estimates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The Perils of Ignoring Bias, Errors, Small Effects, and Missing Data</head><p>Lesson 2A: Power is not enough-This study highlights how attending only to statistical power is problematic when contemplating sample size requirements. In many models, statistical power was not the limiting factor driving sample size requirements, but rather, bias or errors (i.e., solution propriety) were the culprit. It is our impression that applied researchers often consider sample size needs only in relationship to achieving adequate statistical power.</p><p>Lesson 2B: Latent variable models have benefits-Results of this study also reveal the advantages associated with the use of latent variables and the inclusion of error theory in the model. Although a path model composed of only single indicators certainly requires fewer participants than the equivalent model evaluated with latent variables, this is based on the almost certainly false tenet that the single indicators are perfect measures of the underlying construct (i.e., in the behavioral sciences, most phenomena of interest are not directly observable and thus are measured with error). To the extent that a single indicator is not reliable, then it is clear from these results that bias in the parameter estimates is introduced (in direct relation to the degree of error in the indicator). Prior work has arrived at this same general conclusion <ref type="bibr" target="#b7">(Cole &amp; Maxwell, 2003)</ref>, although to our knowledge, no prior study has systematically evaluated and compared sample size requirements with respect to statistical power, bias, and solution propriety under varying parameter conditions across latent-variable and single-indicator models. No amount of increasing the sample size in a single-indicator model will compensate for inadequate reliability of the measures. In most single-indicator designs, concerns regarding bias are not considered when evaluating minimum sample size as only statistical power is typically evaluated. If an investigator has no choice but to use single indicators of a given construct, then it is generally advisable to attempt to account for measurement error in the indicator by estimating it (see <ref type="bibr" target="#b5">Brown, 2006)</ref>. Doing so will likely improve the strength and accuracy of the parameter estimates, and hence, statistical power to detect them.</p><p>Lesson 2C: Don't ignore the weakest link-In addition, these results demonstrate the need to attend to sample size requirements for the smallest effect of interest in the model. For example, in a mediation model, the strength of the indirect effect (often the effect of greatest interest to researchers) is always weaker than its component parts and this indirect effect may not be a focus of the Monte Carlo modeling. In fact, the Monte Carlo simulation could proceed without inclusion of the line in the model script that estimates the size and significance of the indirect effect, and a researcher might erroneously determine sample size requirements based only on the weakest projected direct effect. For example, note that in the simulations in which an impressive 75% of the variance in the dependent variable was explained, the magnitude of the indirect effect was still modest at best (β = .25). Failing to focus on the magnitude of the indirect effect could lead to a study with insufficient sample size.</p><p>Lesson 2D: Missing data matter-Results of this study also highlight problems with not accounting for missing data in determining sample size requirements. Models with 20% missing data per indicator necessitated, on average, nearly 50% increase in sample size requirements (due to problems with error and bias). This is a substantial relative increase in the sample size requirement and implies that failure to account for missing data when determining sample size requirements may ultimately lead to sample size benchmarks that are not sufficient. Prior work suggests that researchers must consider if missing data are missing at random versus missing completely at random, as this distinction will likewise affect statistical power and sample size requirements <ref type="bibr">(Davey &amp; Salva, 2009a;</ref><ref type="bibr" target="#b10">Dolan et al., 2005)</ref>. Although judging the effects of missing data may add an additional layer of complexity and time to the planning phase of research, the results of this study (and prior work) suggest that attending to the relationship between missing data and sample size requirements is time well spent.</p><p>To summarize, failure to consider potential measurement error, the effects of parameter bias, unevaluated weak effects, and the amount of missing data is likely to lead to sample size estimates that are inappropriate for the planned analyses. Participant and investigator time and effort, and research dollars, may be wasted if these contributors to sample size needs are overlooked. Attending to potential sources of bias and error is important for ensuring the validity of the results and their interpretation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ensuring Stability of Sample Size Requirements</head><p>Lesson 3A: Sample size estimates may vary-This study also demonstrated that the minimum sample required for a given model was not always stable, with respect to (a) having the next largest sample size yield acceptable results and (b) evaluating the same model with a new seed number. This problem was particularly evident in models that included missing data. Researchers may want to assess the cause of the instability of sample size requirements before determining how to respond to it. If reanalysis with a new seed number reveals bias in a primary parameter estimate, it may warrant choosing the most conservative sample size estimate, whereas a single error resulting from reanalysis may not warrant increasing the sample size. The difference in sample size requirements when evaluated with different seed numbers was sometimes quite meaningful-in one instance, there was a discrepancy of 50 cases across the two simulations. Depending on the participant pool, 50 additional cases may be unattainable. Investigators may want to combine across sample size estimates, for example, by taking the mean minimum sample size based on analyses with several different seed numbers. The lesson here is that, rather than focusing on a single minimum sample size requirement, it is preferable for investigators to think of acceptable sample size estimates as falling within a range.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Limitations and Conclusion</head><p>This study was limited in that we did not evaluate all of the possible factors that could have bearing on sample size requirements. For example, we did not evaluate the effects of nonnormal data (see <ref type="bibr" target="#b11">Enders, 2001)</ref>, or of the use of categorical, count, censored, or item parceled data (see <ref type="bibr" target="#b0">Alhija &amp; Wisenbaker, 2006)</ref>. We did not evaluate models in which indicators loaded very weakly on the latent variables. In addition, for the sake of simplicity and consistency, we held the size of all equivalent parameter estimates in a model consistent, but of course this pattern of associations is unlikely in real data sets. We did not evaluate the sample size necessary to obtain overall good model fit (e.g., <ref type="bibr" target="#b18">Kim, 2005)</ref> and there are many other types of models including longitudinal designs (see <ref type="bibr" target="#b15">Hertzog, von Oertzen, Ghisletta, &amp; Lindenberger, 2008)</ref>, multigroup designs (see <ref type="bibr" target="#b14">Hancock, Lawrence, &amp; Nevitt, 2000)</ref>, and power equivalence designs (see MacCallum, Lee, &amp; Browne, 2010) that we did not evaluate. We limited the models under consideration to those that we thought would have the widest appeal and that would be most readily interpretable.</p><p>We hope that researchers will find the examples in this study relevant to their own work and use them to help arrive at initial estimates for sample size that are then evaluated fully through additional analyses matched to the researcher's specific study. Use of popular rulesof-thumb for such purposes may be quick and handy, but they are insufficient. Just as clinicians must take an individualized approach to the assessment and treatment of the client in the therapy office, so too, must researchers individualize their sample size planning to the specific model under consideration. The final lesson learned is that determining sample size requirements for SEM necessitates careful, deliberate evaluation of the specific model at hand.   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Schematic diagram of CFA model permutations Note. The figure shows a representation of the model characteristics that were varied in the Monte Carlo analyses of the CFAs. We evaluated models with one to three factors and each factor in the model was indicated by three to eight indicators, which loaded on their respective factors at .50, .65, or .80. The factor correlation(s) was set to r = .30 or .50. CFA = confirmatory factor analysis; mag = magnitude.</figDesc><graphic coords="19,191.37,62.00,289.25,103.68" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Schematic diagram of mediation model permutationsNote. The strength of all the direct regressive paths was varied from .25 to .40 to .50 to account for 16%, 45%, and 75% of the variance in the dependent variable, respectively. All variables were set to have a variance of 1.0 and mean of 0. Arrows pointing toward the dependent variable show the amount of residual variance in each variable.</figDesc><graphic coords="20,186.24,62.00,299.52,256.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Minimum sample size required for CFA, SEM, and missingness models Note. Ind = indicator; CFA = confirmatory factor analysis. Panels A to E show the results of the Monte Carlo simulation studies. Each panel shows the minimum sample size required for each permutation of each type of model that was evaluated. For panel E, Model A is the CFA model and Model B is the SEM model. The permutations of the regressive model (i.e., mediation model) differed in both the magnitude of the structural parameters and the total variance explained in the dependent variable (as shown in parentheses).</figDesc><graphic coords="21,162.48,62.00,347.04,380.86" type="bitmap" /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>Educ Psychol Meas. Author manuscript; available in PMC 2015 February 19.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>We would like to thank <rs type="person">Drs. Dan</rs> and <rs type="person">Lynda King</rs> for their helpful review and commentary on a draft of this article.</p></div>
<div><head>Funding</head><p>The author(s) disclosed receipt of the following financial support for the research, authorship, and/or publication of this article: This research was supported by a <rs type="grantName">VA Career Development Award</rs> to <rs type="person">Erika J. Wolf</rs>, and by <rs type="funder">National Institute on Alcohol Abuse and Alcoholism</rs> grant <rs type="grantNumber">K01AA021266</rs> awarded to <rs type="person">Shaunna L. Clark</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_n7NqgMU">
					<idno type="grant-number">K01AA021266</idno>
					<orgName type="grant-name">VA Career Development Award</orgName>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material</head><p>Refer to Web version on PubMed Central for supplementary material. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A Monte Carlo study investigation the impact of item parceling strategies on parameter estimates and their standard errors in CFA</title>
		<author>
			<persName><forename type="first">Fna</forename><surname>Alhija</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wisenbaker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Structural Equation Modeling</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="204" to="228" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Practical issues in structural modeling</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M</forename><surname>Bentler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Chou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sociological Methods &amp; Research</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="78" to="117" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Structural equations with latent variables</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Bollen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989">1989</date>
			<publisher>John Wiley</publisher>
			<pubPlace>New York, NY</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Robustness of LISREL against small sample sizes in factor analysis models</title>
		<author>
			<persName><forename type="first">A</forename><surname>Boomsma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Systems under indirection observation: Causality, structure, prediction (Part I)</title>
		<editor>
			<persName><forename type="first">K</forename><forename type="middle">G</forename><surname>Joreskog</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Wold</surname></persName>
		</editor>
		<meeting><address><addrLine>Amsterdam, Netherlands</addrLine></address></meeting>
		<imprint>
			<publisher>North Holland</publisher>
			<date type="published" when="1982">1982</date>
			<biblScope unit="page" from="149" to="173" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Nonconvergence, improper solutions, and starting values in LISREL maximum likelihood estimation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Boomsma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychometrika</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page" from="229" to="242" />
			<date type="published" when="1985">1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Confirmatory factor analysis for applied research</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">A</forename><surname>Brown</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>Guilford Press</publisher>
			<pubPlace>New York, NY</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Statistical power analysis for the behavioral sciences</title>
		<author>
			<persName><forename type="first">J</forename><surname>Cohen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1988">1988</date>
			<publisher>Erlbaum</publisher>
			<biblScope unit="volume">2</biblScope>
			<pubPlace>Hillsdale, NJ</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Testing meditational models with longitudinal data: Questions and tips in the use of structural equation modeling</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Maxwell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Abnormal Psychology</title>
		<imprint>
			<biblScope unit="volume">112</biblScope>
			<biblScope unit="page" from="558" to="577" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
	<note>PubMed: 14674869</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Estimating statistical power with incomplete data</title>
		<author>
			<persName><forename type="first">A</forename><surname>Davey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Salva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Organizational Research Methods</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="320" to="346" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Statistical power analysis with missing data: A structural equation modeling approach</title>
		<author>
			<persName><forename type="first">A</forename><surname>Davey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Salva</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>Routledge</publisher>
			<pubPlace>New York, NY</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A note on normal theory power calculation in SEM with data missing completely at random</title>
		<author>
			<persName><forename type="first">C</forename><surname>Dolan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Van Der Sluis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Grasman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Structural Equation Modeling</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="245" to="262" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The impact of nonnormality on full information maximum-likelihood estimation for structural equation models with missing data</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K</forename><surname>Enders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Methods</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="352" to="370" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
	<note>PubMed: 11778677</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The relative performance of full information maximum likelihood estimation for missing data in structural equation models</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K</forename><surname>Enders</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Bandalos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Structural Equation Modeling</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="430" to="457" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Measurement model quality, sample size, and solution propriety in confirmatory factor models</title>
		<author>
			<persName><forename type="first">P</forename><surname>Gagné</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">R</forename><surname>Hancock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multivariate Behavioral Research</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="65" to="83" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Type I error and power of latent mean methods and MANOVA in factorially invariant and noninvariant latent variable systems</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">R</forename><surname>Hancock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">R</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Nevitt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Structural Equation Modeling</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="534" to="556" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Evaluating the power of latent growth curve models to detect individual differences in change</title>
		<author>
			<persName><forename type="first">C</forename><surname>Hertzog</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Von Oertzen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ghisletta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Lindenberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Structural Equation Modeling</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="541" to="563" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Sample size and number of parameter estimates in maximum likelihood confirmatory factor analysis: A Monte Carlo investigation</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Jackson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Structural Equation Modeling</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="205" to="223" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Sample size for multiple regression: Obtaining regression coefficients that are accurate, not simply significant</title>
		<author>
			<persName><forename type="first">K</forename><surname>Kelley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Maxwell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Methods</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="305" to="321" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
	<note>PubMed: 14596493</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The relation among fit indexes, power, and sample size in structural equation modeling</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">H</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Structural Equation Modeling</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="368" to="390" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The analysis of social science data with missing values</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Little</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rubin</forename><surname>Db</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sociological Methods and Research</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="292" to="326" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Power analysis and determination of sample size for covariance structural modeling</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Maccallum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Browne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">M</forename><surname>Sugawara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Methods</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="130" to="149" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The issue of isopower in power analysis for tests of structural equation models</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Maccallum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Browne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Structural Equation Modeling</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="23" to="41" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Sample size in factor analysis</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Maccallum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">F</forename><surname>Widaman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Methods</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="84" to="99" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Evaluation of confirmatory factor analytic and structural equation models using goodness-of-fit indices</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Marcoulides</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Reports</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="page" from="669" to="670" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">You write, but others read: Common methodological misunderstandings in PLS and related methods</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Marcoulides</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Abdi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">E</forename><surname>Vinzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Russolillo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Trinchera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">New perspectives in partial least squares and related methods</title>
		<meeting><address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
		</imprint>
	</monogr>
	<note>in press</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">PLS: A silver bullet?</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Marcoulides</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Saunders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">MIS Quarterly</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="iii" to="ix" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Is more ever too much? The number of indicators per factor in confirmatory factor analysis</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">W</forename><surname>Marsh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">T</forename><surname>Hau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Balla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Grayson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multivariate Behavioral Research</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="181" to="220" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">The persistence of underpowered studies in psychological research: Causes, consequences, and remedies</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Maxwell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Methods</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="147" to="163" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
	<note>PubMed: 15137886</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Sample size planning for statistical power and accuracy in parameter estimation</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Maxwell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kelley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Rausch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annual Review of Psychology</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="page" from="537" to="563" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Using Mplus Monte Carlo simulations in practice: A note on assessing estimation quality and power in latent variable models</title>
		<author>
			<persName><forename type="first">B</forename><surname>Muthén</surname></persName>
		</author>
		<ptr target="http://www.statmodel.com/download/webnotes/mc1.pdf" />
	</analytic>
	<monogr>
		<title level="s">Mplus Web Notes</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Using Mplus Monte Carlo simulations in practice: A note on non-normal missing data in latent variable models</title>
		<author>
			<persName><forename type="first">B</forename><surname>Muthén</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Asparouhov</surname></persName>
		</author>
		<ptr target="http://www.statmodel.com/download/webnotes/mc2.pdf" />
	</analytic>
	<monogr>
		<title level="s">Mplus Web Notes</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Mplus user&apos;s guide</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">K</forename><surname>Muthén</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">O</forename><surname>Muthén</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998-2008</date>
			<publisher>Muthén &amp; Muthén</publisher>
			<biblScope unit="volume">5</biblScope>
			<pubPlace>Los Angeles, CA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">How to use a Monte Carlo study to decide on sample size and determine power</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">K</forename><surname>Muthén</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">O</forename><surname>Muthén</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Structural Equation Modeling</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="599" to="620" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Psychometric theory</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Nunnally</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1967">1967</date>
			<publisher>McGraw-Hill</publisher>
			<pubPlace>New York, NY</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Monte Carlo experiments: Design and implementation</title>
		<author>
			<persName><forename type="first">P</forename><surname>Paxton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Curran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Bollen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kirby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bhen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Structural Equation Modeling</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="287" to="312" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Power of the likelihood ratio test in covariance structure analysis</title>
		<author>
			<persName><forename type="first">A</forename><surname>Satorra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">E</forename><surname>Saris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychometrika</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page" from="83" to="90" />
			<date type="published" when="1985">1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Effects of variable and subject sampling on factor pattern recovery</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">F</forename><surname>Velicer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Fava</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Methods</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="231" to="251" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">ON F1*.25; ! sets regressive path F3 ON F2</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">F2 ON F1*</title>
		<imprint>
			<biblScope unit="page">25</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Model Indirect: F3 IND F2 F1*.06; ! sets magnitude of indirect effect MODEL: F1 BY A1-A3</title>
		<imprint>
			<biblScope unit="page">65</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m">Model Indirect: F3 IND F2 F1*.06; OUTPUT: TECH9</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
