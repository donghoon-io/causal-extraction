<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Theory-Guided Exploration With Structural Equation Model Forests</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Andreas</forename><forename type="middle">M</forename><surname>Brandmaier</surname></persName>
							<email>brandmaier@mpib-berlin.mpg.de</email>
						</author>
						<author>
							<persName><forename type="first">John</forename><forename type="middle">J</forename><surname>Prindle</surname></persName>
						</author>
						<author>
							<persName><forename type="first">John</forename><forename type="middle">J</forename><surname>Mcardle</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Max Planck</orgName>
								<orgName type="institution">Max Planck Institute for Human Development</orgName>
								<address>
									<settlement>Berlin</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">UCL Centre for Computational Psychiatry and Ageing Research</orgName>
								<address>
									<settlement>Berlin</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Max Planck Institute for Human Development</orgName>
								<address>
									<settlement>Berlin</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">University of Southern California Ulman Lindenberger Max Planck Institute for Human Development</orgName>
								<address>
									<settlement>Berlin</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="department">Center for Lifespan Psychology</orgName>
								<orgName type="institution">European University Institute</orgName>
								<address>
									<settlement>San Domenico di Fiesole</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="department">Max Planck Institute for Human Development</orgName>
								<address>
									<settlement>Berlin</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff6">
								<orgName type="department">Max Planck</orgName>
								<orgName type="institution">UCL Centre for Computational Psychiatry and Ageing Research</orgName>
								<address>
									<settlement>Berlin</settlement>
									<country>Germany;</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff7">
								<orgName type="department" key="dep1">Center for Lifespan Psychology</orgName>
								<orgName type="department" key="dep2">Max Planck Institute for Human Development</orgName>
								<orgName type="department" key="dep3">Department of Psychology</orgName>
								<orgName type="department" key="dep4">Center for Lifespan Psy- chology</orgName>
								<orgName type="institution" key="instit1">John J. McArdle</orgName>
								<orgName type="institution" key="instit2">Univer- sity of Southern California</orgName>
								<orgName type="institution" key="instit3">Ulman Lindenberger</orgName>
								<orgName type="institution" key="instit4">Max Planck Institute for Human Development, and European Uni- versity Institute</orgName>
								<address>
									<settlement>San Domenico di Fiesole</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff8">
								<orgName type="department" key="dep1">School of Social Work</orgName>
								<orgName type="department" key="dep2">Center for Lifespan Psychology</orgName>
								<orgName type="department" key="dep3">Max Planck Institute for Human Development</orgName>
								<orgName type="institution">University of Southern California</orgName>
								<address>
									<addrLine>Lentzeallee 94</addrLine>
									<postCode>14195</postCode>
									<settlement>Berlin</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Theory-Guided Exploration With Structural Equation Model Forests</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1037/met0000090</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-14T17:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>SEM forest</term>
					<term>model-based tree</term>
					<term>recursive partitioning</term>
					<term>variable importance</term>
					<term>case proximity</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Structural equation model (SEM) trees, a combination of SEMs and decision trees, have been proposed as a data-analytic tool for theory-guided exploration of empirical data. With respect to a hypothesized model of multivariate outcomes, such trees recursively find subgroups with similar patterns of observed data. SEM trees allow for the automatic selection of variables that predict differences across individuals in specific theoretical models, for instance, differences in latent factor profiles or developmental trajectories. However, SEM trees are unstable when small variations in the data can result in different trees. As a remedy, SEM forests, which are ensembles of SEM trees based on resamplings of the original dataset, provide increased stability. Because large forests are less suitable for visual inspection and interpretation, aggregate measures provide researchers with hints on how to improve their models: (a) variable importance is based on random permutations of the out-of-bag (OOB) samples of the individual trees and quantifies, for each variable, the average reduction of uncertainty about the model-predicted distribution; and (b) case proximity enables researchers to perform clustering and outlier detection. We provide an overview of SEM forests and illustrate their utility in the context of cross-sectional factor models of intelligence and episodic memory. We discuss benefits and limitations, and provide advice on how and when to use SEM trees and forests in future research.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The privileged unit of analysis in psychology is the individual <ref type="bibr" target="#b47">(Nesselroade, Gerstorf, Hardy, &amp; Ram, 2007)</ref>. Nevertheless, many data-analytic approaches coarsely aggregate data and tacitly assume group-average models to hold and to be interpreted in lieu of more fine-grained and, ultimately, person-specific models. For example, when a group of persons show an average increase of performance in a learning task, this does not mean that all persons follow a pattern of change similar to this average. In fact, none of the persons may be well represented by the average trend. In a similar vein, <ref type="bibr" target="#b67">Tucker (1966)</ref> argued that the consideration of differences instead of averages will allow us to gain more information about the nature of basic functions underlying behavior. Ever since, researchers have been questioning coarse aggregation of data across persons (e.g., <ref type="bibr" target="#b31">Lamiell, 1981;</ref><ref type="bibr" target="#b48">Nesselroade &amp; Molenaar, 1999)</ref> as the estimates of averaged effects may not be representative of any single individual. In fact, strong inference about intraindividual variation from interindividual variation is only possible under the ergodic assumption <ref type="bibr" target="#b45">(Molenaar, 2004)</ref>, which assumes that the group model represents each individual's dynamics (homogeneity) and that those dynamics have constant characteristics in time (stationarity). In the same vein, <ref type="bibr" target="#b60">Simpson (1951)</ref> pointed out that a statistical relationship observed in a population could be reversed within subgroups that form the population. For instance, "It may be universally true that drinking coffee increases one's level of neuroticism; then it may still be the case that people who drink more coffee are less neurotic" <ref type="bibr">(Borsboom, Kievit, Cervone, &amp; Hood, 2009, p. 72</ref>). Simpson's paradox may arise whenever inferences are drawn across different explanatory levels, for example, from populations to the individual, or from cross-sectional data to intraindividual change over time (see <ref type="bibr" target="#b30">Kievit, Frankenhuis, Waldorp, &amp; Borsboom, 2013</ref>, for further illustrations). Hence, there still is a need for focusing on individuals or subgroups of individuals to more accurately model individual process idiosyncrasies and similarities across persons. Particularly, in light of large-scale empirical data sets, aggregation is more likely to lead to models with low informative value about individual underlying processes as it is often difficult to expand prior hypotheses to account for the large number of potential explanatory variables.</p><p>Researchers with an awareness of the aforementioned problem face two challenges: they need to (a) determine whether there is a substantially relevant amount of heterogeneity that needs to be accounted for, and (b) account for this variability in their hypothesis-driven model. Researchers can turn to a variety of approaches to account for heterogeneity in their sample. A principled approach is to explicitly account for subgroups in the data. One can consider two distinct venues to approach the issue of heterogeneity in such multiple group models: Either heterogeneity (i.e., group membership) is assumed to be latent or assumed to be observed. If group membership is latent, researchers can rely on latent class cluster analysis (spanning latent profile analysis, latent class analysis, finite mixture models; see <ref type="bibr" target="#b70">Vermunt &amp; Magidson, 2002)</ref> which has the goal of finding similar data patterns when group membership is unobserved and probabilistic. That is, observed data are assumed to be drawn from a mixture of underlying probability distributions. On the other hand, if group membership is observed through a measured variable, then it can be included in the resulting model. Probably the most frequent type of analysis asks whether observed group membership predicts mean differences in a continuous outcome and comes in many flavors, such as the t test, paired t test, or the analysis of variance in its many variants. A multivariate extension encompassing these techniques as special cases are structural equation models (SEM). Multiple group SEM allow the specification of a latent variable model with differences in parameters (but also constraints if dictated by theoretical considerations) across groups. This allows for testing factor structures, growth curves, and other types of SEMs across groups, and ultimately provides a framework for directly testing the homogeneity assumption for the observed subgroups. But these approaches are faced with new challenges when confronted with large-scale empirical data. We believe that researchers typically venture out with a good theory to start with. Such a theory formalized as a statistical model typically applies to only a subset of observed variables. However, it may be unclear how much heterogeneity in a large sample could be further explained. In large-scale data sets, multiple questionnaires with even more items, genetic data, and biomarkers may form large sets of covariates with the potential to explain heterogeneity. How can a researcher decide between those variables? Which of these covariates need to be accounted for to reduce heterogeneity and improve predictive performance of a model? If a mixture model yields a statistically plausible result of dozens of subgroups, then what do these subgroups mean for the empirical questions of the researcher? How are they discernible? Which variables distinguish among them? SEM trees, a multivariate instance of decision trees, provide a formal venue to approach these questions.</p><p>Decision trees (also known as recursive partitioning methods) became prominent through the seminal work of <ref type="bibr" target="#b62">Sonquist and Morgan (1964)</ref>; <ref type="bibr" target="#b15">Breiman, Friedman, Olshen, and Stone (1984)</ref> and <ref type="bibr" target="#b50">Quinlan (1986)</ref>, who extended the statistical paradigm to include general classification and regression problems (for an extensive overview, see <ref type="bibr" target="#b37">McArdle, 2013;</ref><ref type="bibr" target="#b65">Strobl, Malley, &amp; Tutz, 2009)</ref>. They provide a data-analytic method to find subgroups in a sample with similar response patterns when group membership is observed by a subset of potential predictor variables and their interactions. This feature gave name to one of the earlier representatives of this group of methods, automatic interaction detection. Decision trees can be seen as a nonparametric approach to selection among a set of potential predictors to best predict univariate or multivariate outcomes. Modern decision tree approaches come in many flavors but they all share a common algorithmic theme: They partition the sample at hand into subsets that differ in their observed data patterns across groups but are similar within groups. Only partitions of the sample are considered that are described by a set of potential predictor variables. For example, if gender and age are included as potential predictors, partitions based on these variables will be considered. Once the best partition is found, the data set is permanently split into independent partitions and the algorithm proceeds recursively in each of the resulting partitions, that is, it finds the next best partition in each subset. A tree structure obtained from a dataset with age and gender as potential predictors may thus describe partitions based on either or a combination of both variables, or none at all. The result of a decision tree analysis can be visualized in a hierarchical binary tree structure. A binary tree is made of nodes, where each node has either no successors (or children) and is called a leaf or has exactly two successors and is called an inner node. The first node of the tree, which has no parent, is called the root. In a tree, each inner node corresponds to a decision, that is, to a variable and a decision rule about this variable that determines what subsets will be associated with its children. Each leaf node is describing an outcome, typically as a prediction about the outcome. To determine which node a person belongs to, one simply follows the path from a root node down to a leaf as it is determined by comparing the person's predictors with the decision nodes encountered.</p><p>In psychological research, SEMs are general modeling techniques that encompass a variety of models for testing hypotheses in cross-sectional and longitudinal studies including both observed and latent variables. In particular, latent variable SEMs allow modeling measurement errors, thereby offering greater validity and generalizability of research designs than methods purely based on observed variables <ref type="bibr" target="#b33">(Little, Lindenberger, &amp; Nesselroade, 1999;</ref><ref type="bibr" target="#b41">McArdle &amp; Nesselroade, 2014)</ref>. One special appeal of this method is the correspondence of the underlying linear equations to a graphical representation (see von Oertzen, <ref type="bibr" target="#b72">Brandmaier, &amp; Tsang, 2015)</ref>. Conventionally, SEM is used as a confirmatory technique. <ref type="bibr">Brandmaier, von Oertzen, McArdle, and Lindenberger (2013b)</ref> introduced SEM trees as a technique that combines the benefits of both SEMs and decision trees into a single method of data analysis. The method allows for theory-guided exploration of models, with structural equation modeling providing a formal framework for implementing a prior theory and the decision tree approach enabling an exploratory analysis and refinement of the initial theory. The goal of decision tree methods is essentially to discover how potential predictor variables are linked to an outcome. Trees start off by undertaking an exhaustive search among the set of potential predictors to find a predictor that best describes a partition of the sample into groups with similar patterns of the outcome. Once a partition is found, this search is recursively continued in the resulting partitions. Different evaluation functions for assessing the necessity of a further split have been proposed. For classifica-This document is copyrighted by the American Psychological Association or one of its allied publishers.</p><p>This article is intended solely for the personal use of the individual user and is not to be disseminated broadly.</p><p>tion of univariate outcomes, typically, GINI impurity <ref type="bibr" target="#b15">(Breiman et al., 1984)</ref> or information gain <ref type="bibr" target="#b50">(Quinlan, 1986</ref><ref type="bibr" target="#b51">(Quinlan, , 1993) )</ref>  Similar to a mixture model analysis, SEM trees assume that the observed data is not homogeneous (that is, drawn from a single underlying probability distribution) but heterogeneous (drawn from multiple underlying probability distributions). When analyzing data with SEM trees, we assume that an initially hypothesized model is the correct model for the population but that the population is made of subpopulations, which differ in the parameters of the generating model. Thus, the population can be said to be heterogeneous with respect to the original SEM. In contrast to latent mixture models and related approaches, SEM trees not only retrieve a clustering structure of cases, but also predictors of the structure. SEM trees use the likelihood ratio as criterion to evaluate the necessity of a split <ref type="bibr">(Brandmaier, von Oertzen, McArdle, &amp; Lindenberger, 2013b)</ref>. Whenever we evaluate a particular split of the sample into multiple groups, the resulting multiple-group model (postsplit model) is nested in the original (presplit) model. If we introduced equality constraints for corresponding parameters across groups in the postsplit model, we would obtain the presplit model. This property makes the presplit model and the postsplit model nested and the likelihood ratio test applicable. Because the search across multiple potential predictors gives rise to a multiple testing problem, one cannot simply rely on the maximally selected p value but needs to adjust for multiple testing (e.g., by a simple Bonferroni correction). It can be obtained that the likelihood ratio is equivalent to scaled information gain <ref type="bibr">(Brandmaier et al., 2013a)</ref> and has, as such, both statistical and information-theoretic appeal. By default, the postsplit model is obtained by simply freeing all parameters of the original SEM across groups, that is, any differences in freely estimated regressions, means, variances, and covariances of the hypothesized SEM contribute to the decision whether a partition of the sample is to be made or not. As mentioned previously, it is possible to restrict the likelihood ratio criterion to only test selected differences with respect to selected parameters that are of particular interest to the researcher. The purpose of such designs is twofold, first, allowing for more focused hypotheses, and second, strengthening the likelihood ratio test at each node by restricting the degrees of freedom for each partition tested.</p><p>A SEM tree describes an exhaustive partition of the original sample into multiple groups (each described by one of the leafs), and is ultimately a multiple group model. The important difference between multiple group models and model trees is the fact that in SEM trees group membership is not prespecified but chosen in a data-driven manner to recursively optimize the log-likelihood criterion.</p><p>The SEM tree approach allows for the detection of subgroup heterogeneity using measured variables. In principal, the method can uncover variables and their interactions that predict differences in multivariate observed data patterns according to a specified model. For example, if a theory-driven model presumes linear change in a cognitive score, trees may find a hierarchy of subgroups with different change trends due to training dosage.</p><p>Conceptualizing different change profiles is a crucial ontologic and diagnostic activity in psychological research. For example, <ref type="bibr" target="#b46">Muthén (2004)</ref> explored subgroups of longitudinal trajectories in high school mathematics achievement, or <ref type="bibr" target="#b28">Josefsson, de Luna, Pudas, Nilsson, and Nyberg (2012)</ref> identified differential trajectories of episodic memory development. SEM trees and forests allow for the detection of covariate-specific subgroups, for example, in regression models, factor analytic models <ref type="bibr" target="#b26">(Jöreskog, 1969)</ref>, autoregressive models <ref type="bibr" target="#b27">(Jöreskog, 1970)</ref>, latent growth curve models <ref type="bibr" target="#b38">(McArdle &amp; Epstein, 1987)</ref>, latent change score models <ref type="bibr" target="#b36">(McArdle, 2009;</ref><ref type="bibr" target="#b40">McArdle &amp; Hamagami, 2001)</ref>, or latent differential equations <ref type="bibr" target="#b5">(Boker, Neale, &amp; Rausch, 2004)</ref>. However, a concern of trees is their potential instability (e.g., <ref type="bibr" target="#b1">Berk, 2006;</ref><ref type="bibr" target="#b24">Hastie, Tibshirani, &amp; Friedman, 2001)</ref>. In each inner node of a tree, both the associated predictor variable and its split point are chosen to be locally optimal and, thus, can easily be influenced by small perturbations of the sample at hand. A slightly different choice of a split point may lead to a different choice of the subsequent split in the children of a node; in this way, small perturbations are typically magnified down the tree. Instability is usually observed when predictors are highly correlated or variables are equally informative about different subpopulations.</p><p>In machine learning, ensemble methods were proposed to improve the robustness and accuracy of individual models. An ensemble refers to a set of predictive models that are, typically, each based on a random sample of the original data set. Ensemble methods are metalearning algorithms specifying (a) the sampling scheme for generating the data sets for the individual models, and (b) a combination scheme for aggregating the predictions of the individual models into a final prediction. Currently, the most widely known ensemble method for decision trees is random forest. The rationale of random forests is to rely on a variety of trees each based on a random sample of the original data to account for the potential instability of each individual tree. First, bootstrap aggregating (bagging) has been proposed as a means to create forests of decision trees. Bagging generates random samples of the original data by sampling cases uniformly and with replacement (bootstrapping). Bagged forests make predictions by aggregating predictions of the individual trees <ref type="bibr" target="#b11">(Breiman, 1996)</ref>, that is, by predicting a continuous outcome as the average over the individual trees' predictions, or by predicting a dichotomous outcome with the majority vote over the individual tree's predictions. As an extension to bagging, <ref type="bibr">Breiman (2001a)</ref> suggested to randomly draw both cases and predictors for each tree in a random forest. The former disadvantage of trees, their susceptibility to random fluctuations in the sample, becomes an advantage in an ensemble of trees. Increasing diversity can allow an ensemble to represent a better approximation to the potentially complex true partition of the sample. If cases and predictors are randomly sampled, then the diversity of the resulting trees is increased. Formal results show that increased diversity is beneficial for the performance of ensemble methods <ref type="bibr" target="#b16">(Bühlmann &amp; Yu, 2002)</ref>. In an empirical comparison of various classification methods over 176 data sets, random forests outperformed other classification approaches including generalized linear models and support vector machines <ref type="bibr" target="#b19">(Fernández-Delgado, Cernadas, Barro, &amp; Amorim, 2014)</ref>.</p><p>In this article, we extend SEM trees to SEM forests following the seminal work on random forests by <ref type="bibr">Breiman (2001a)</ref>. We present the procedure for growing a forest of SEM trees and describe two aggregate measures that allow researchers to obtain useful information about heterogeneity in their datasets: (a) variable importance, which quantifies the extent to which variables predict differences with respect to the initial SEM, and (b) case proximity, which enables researchers to perform case-based clustering based on a measure of similarity in predictor space. The benefit of forests is to provide more robust and effective measures of yet unmodeled<ref type="foot" target="#foot_1">foot_1</ref> information that may help to explain observed heterogeneity. Below, we will present two applications to illustrate the approach, (a) a factor model of intelligence, and (b) a factor model of episodic memory. The first data set may not be seen as particular "big" but was chosen to complement our earlier tree analyses <ref type="bibr">(Brandmaier et al., 2013b)</ref> and to highlight the added benefits of forest analyses. The second analysis is included to provide a data set that may be considered closer to a big data type of application.</p><p>The SEM forest program is freely available to researchers. We have implemented SEM forests within the semtree package <ref type="bibr" target="#b7">(Brandmaier, 2015</ref>; see also <ref type="bibr">Brandmaier et al., 2013b)</ref> for the statistical programming language R (R Core Team, 2013) including computation and plotting facilities for variable importance and case-based proximity. Researchers with access to computing networks have the possibility to profit from parallel growing of trees within a forest.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>SEM forests extend single SEM trees to ensembles of SEM trees, just as random forests extend decision trees to ensembles of decision trees. SEM trees are hierarchical structures of decision rules that describe differences between recursive partitions of a sample with respect to a SEM. More explicitly, SEM trees extend univariate decision trees to decision trees modeling multivariate outcomes.</p><p>A single SEM tree, potentially one of many<ref type="foot" target="#foot_2">foot_2</ref> in a SEM forest, is created as follows (see <ref type="bibr">Brandmaier et al., 2013b</ref> for a detailed description of nonrandom SEM trees):</p><p>Let M be a hypothesized SEM and let D be a data set containing variables in M and further variables as potential predictors of differences of observations in D with respect to the model M.</p><p>1. Among all potential predictors in D, randomly sample a subset of c candidate predictors.</p><p>2. Choose variable among all candidate predictors that is most informative about heterogeneity with respect to M, that is, choose the variable that finds the largest differ-ence between the resulting partitions of the sample. This is formalized as choosing the variable maximizing a likelihood ratio criterion.</p><p>3. Stop searching for further splits if a stopping criterion is reached. Stopping criteria may include (a) a minimum number of cases is reached in the current partition such that fitting a SEM is unreasonable; (b) a user-specified maximum height of the tree is reached, or (c) a statistical criterion is reached, for instance, none of the potential predictors has a significant p value when assessing the splits.</p><p>4. Else, permanently partition the dataset according to the split point of with the largest likelihood ratio, create two new nodes of the tree, and restart this algorithm with each of the resulting partitions as D.</p><p>The SEM forest algorithm is as follows:</p><p>Let t be the number of trees in a forest, also called the forest size or ensemble size. For i ϭ 1, . . . , t, create data set D i train by randomly sampling cases (sampling is done by bootstrapping or preferably subsampling; <ref type="bibr" target="#b64">Strobl, Boulesteix, Zeileis, &amp; Hothorn, 2007)</ref>. Assemble the remaining cases for each i in the out-of-bag sample D i OOB . Then grow a tree for each resampled dataset D i train .</p><p>Only a subset of all potential predictors, which we refer to as candidate predictors, is tested at each node in a random forest. The size of the set of candidate predictors, c, at each node is typically heuristically chosen as either 1, 2, c ϭ &lt;log 2 ͑c͒ ϩ 1=, c ϭ ͙m, or c ϭ m⁄3 (e.g., <ref type="bibr" target="#b14">Breiman &amp; Cutler, 2014;</ref><ref type="bibr" target="#b69">Verikas, Gelzinis, &amp; Bacauskiene, 2011)</ref> with m being the total number of potential predictors. Note that if c equals m, then the random forest algorithm reduces to bootstrap aggregating (also referred to as bagging) of trees since only cases but not variables are subsampled. The predictive accuracy<ref type="foot" target="#foot_3">foot_3</ref> of forests is generally higher with lower correlations among trees <ref type="bibr">(Breiman, 2001a)</ref>. A smaller c increases the variability between trees, and reduces the chance of suppression, that is, the chance that predictor influences remain undetected through the effect of variables with a stronger influence. On the other hand, a very small c reduces the chance to generate enough trees containing true important predictors and to detect higher-order interactions <ref type="bibr" target="#b17">(Díaz-Uriarte &amp; De Andres, 2006)</ref>. The parameter c is a so-called hyperparameter<ref type="foot" target="#foot_4">foot_4</ref> that can either be set manually or be automatically tuned, for example, by employing a hold-out data set or cross-validation; however, it seems a less critical hyperparameter than in many other analysis approaches <ref type="bibr" target="#b65">(Strobl et al., 2009)</ref>. An empirical test of c's criticality is the parallel construction of multiple SEM forests with slight variations in c and possibly the forest size t. If results of aggregate statistics as described below change considerably, hyperparameters need to be adapted.</p><p>Random sampling is typically either performed by bootstrapping or by subsampling (see <ref type="bibr" target="#b64">Strobl et al., 2007)</ref>. In a bootstrap sample, a new sample is obtained by drawing cases with replacement with the same sample size of the original data set. A subsampled data set is obtained by drawing a smaller data set than the original data set without replacement. In both instances, undrawn cases remain and form the out-of-bag sample that proves useful as a validation sample for hypotheses that were created with the former random sample. Because randomness is an essential part of the method, it is necessary to manually control and store the seed of the computer's random number generator to make analyses reproducible.</p><p>Partitioning of ordinal, continuous, and categorical variables is performed by internally dichotomizing each variable type <ref type="bibr">(Brandmaier et al., 2013b)</ref>. This approach is known as an exhaustive split search <ref type="bibr" target="#b51">(Quinlan, 1993)</ref>. Exhaustive split search simplifies the resulting tree as it enforces binary trees as outcome, that is, trees that branch at each node into either none or two children. Furthermore, this approach allows for testing effects of potential predictors on the SEM independent of any monotonic transformation, such as, normalization, logarithmic transformation, or polynomial transformation. Still, the dichotomization of categorical variables is exponential in the number of categories and prohibitive already for small numbers of categories. For example, a variable with 12 categories requires performing more than 4,000 model estimations in each node of the tree. To alleviate this problem, only a random sample of dichotomized candidate splits is examined <ref type="bibr">(Breiman, 2001a)</ref>. We adopted this procedure such that, when log 2 (n) Ͻ k for a variable with k categories and n remaining cases, only log (n) split candidates from a random uniform distribution are chosen. Asymptotically, the search for the optimal split in categorical variables is then in the order of magnitude of the search in continuous variables.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Variable Importance</head><p>Measures of variable importance quantify the impact a variable has on the overall prediction of a response, which in SEM forests typically takes the form of a multivariate means and covariance structure. For instance, counting the frequency with which a variable was selected in a forest is a naive implementation of variable importance. However, this does not necessarily reflect the impact a variable has in the prediction of a model-based response. Instead, recent variable importance measures are based on permutation accuracy importance. The intuition being if an important predictor is randomly permuted, and, thus, its functional relation with the model-predicted distribution is broken, then a considerable decrease of the model's goodness-of-fit for the scrambled data is expected. In other words, permuting the variable is seen as a proxy for removing the effect of that variable. By averaging the decrease in fitness across all trees of the forest on the out-of-bag (OOB) samples, an estimate of importance is obtained for each variable. For classification trees, the Gini criterion is often employed as an optimization criterion <ref type="bibr" target="#b34">(Loh &amp; Shih, 1997)</ref>. In SEM forests, Gini importance is replaced with log-likelihood importance, and, thus, the decrease of log-likelihood is averaged over the forest to obtain variable importance estimates. As described above, randomly sampling from both cases and predictors in the process of growing trees allows potentially influential variables to play out their effects in different, randomly sampled interactions without constantly being outrivaled by stronger competitors. The computation of variable importance in SEM forests follows from variable importance computation in conventional random forests <ref type="bibr">(Breiman, 2001a)</ref>:</p><p>For each potential predictor c, 1. Calculate the log-likelihood of the OOB sample for each tree, LL(D i OOB | T i ).</p><p>2. Obtain a scrambled OOB sample D ˜i OOB by randomly permuting the column of D i OOB corresponding to c.</p><p>3. Calculate the likelihood of the scrambled OOB samples,</p><formula xml:id="formula_0">LL͑D ˜i OOB Խ T i ͒.</formula><p>4. Obtain the estimate of variable importance by averaging over the log-likelihood-ratios, c ϭ 1</p><formula xml:id="formula_1">t ͚iϭ1 t LL͑D i OOB Խ T i ͒ Ϫ LL͑D ˜i OOB Խ T i ͒.</formula><p>The above algorithm yields variable importance as an average absolute increase in model misfit due to randomization of each variable. We recommend reporting importance as increase in model misfit relative to baseline fit because it is independent of the sample size. Alternatively, <ref type="bibr" target="#b14">Breiman and Cutler (2014)</ref> calculate standardized importance scores (z-scores) from the raw importance scores i as ⌿ i ϭ i i ͙t with i being the standard deviation across each of the i . This approach allows for a significance test of variable importance but is overpowered, if not meaningless, due to the typically large number of trees in a forest <ref type="bibr">(Strobl &amp; Zeileis, 2008)</ref>. <ref type="bibr" target="#b23">Hapfelmeier and Ulm (2013)</ref> reviewed different approaches to variable selection based on variable importance. They calculate p values for each variable derived from an empirical null distribution in a permutation test framework. When mixed variable types are contained in a dataset, the use of an unbiased variable selection criterion is recommended when SEM forests are estimated. Otherwise importance analyses were biased, typically in favor of variables with larger number of categories; under the null hypothesis, continuous variables were typically selected over ordinal variables, and among ordinal variables, those with more categories were selected over those with fewer. SEM trees offer unbiased variable selection methods <ref type="bibr">(Brandmaier et al., 2013b)</ref> and, thus, remove this bias.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proximity Measures</head><p>Proximity measures quantify the pairwise similarities of cases in a sample and, thus, provide a measure of the internal structure of the data. The calculation of proximity follows the ideas of <ref type="bibr" target="#b14">Breiman and Cutler (2014)</ref>. Clustering is a technique to find hidden structure in data. Clustering based on a proximity matrix can be used to uncover heterogeneity in the dataset, detect outliers, find prototypes, or impute missing values. The intuition behind case-based proximity is that similar cases should end up in the same leafs of trees across the forest. Vice versa, each pair of cases in a terminal node shares similar values of the variables along the path back to the root node. Because the variables were chosen to locally maximize the information about the model predictions, the similarity measures take into account the differential weighting of importance for the prediction as it is encoded in the forest. The proximity matrix is symmetric, and has rows and columns each correspond-This document is copyrighted by the American Psychological Association or one of its allied publishers.</p><p>This article is intended solely for the personal use of the individual user and is not to be disseminated broadly.</p><p>ing to one case of the original data. To compute the proximity matrix, we count the relative frequency of each pair of cases appearing in the same terminal nodes across the forest and store this result in the corresponding entry of the proximity matrix. This notion of proximity in covariate space is then based on the relevance of variables to explain differences in the model-predicted distribution. Particularly, variables that are less important contribute less to the proximity. By construction, the proximity matrix is symmetric and bounded by 1. For large data sets, the proximity matrix is too large to be useful for inspection because the contained information grows quadratically with the number of cases.</p><p>Beyond clustering, projections of the proximities on coordinates in a lower number of dimensions, for example, via multidimensional scaling (MDS), can shed light on the case-by-case similarity in a dataset. Given a dissimilarity matrix, MDS arranges objects in a low-dimensional space such that the projected proximities have maximal fidelity to the original proximities. This is usually achieved by a principal components analysis of the dissimilarity matrix. The resulting components are then referred to as principal coordinates and the coordinate system is defined by principal axes.</p><p>Then, the cases can be visually represented by plotting their principal coordinates. Because MDS requires a dissimilarity matrix, the proximity matrix, P, needs to be transformed into a dissimilarity matrix, D, before MDS can be applied:</p><formula xml:id="formula_2">D ϭ 1 Ϫ P</formula><p>A heuristic measure of "outlyingness" or "novelty" for classification was proposed by <ref type="bibr" target="#b14">Breiman and Cutler (2014)</ref> as the reciprocal of the sum of squared proximities between a case and all other cases conditional on the same class. In settings with multivariate continuous responses, no class information is available and the novelty is calculated as unconditional average: Let P be the proximity matrix derived as described above. The novelty of an observation i is its average dissimilarity to all other cases: <ref type="bibr" target="#b14">Breiman and Cutler (2014)</ref> suggested normalizing this score using robust statistics for location (median; MED) and dispersion (median of absolute deviation; MAD):</p><formula xml:id="formula_3">d i ϭ N \ ͚ j N P 2 (i, j)</formula><formula xml:id="formula_4">MED ϭ median i ͑d i ͒ MAD ϭ median i ͑ | d i Ϫ MED | ͒ d ˜i ϭ d i Ϫ MED MAD</formula><p>Intuitively, novelty of a case is large when its proximity to all other cases is on average small. In other words, the less often a case is consistently allocated to similar cases in the same leaf node across the forest, the greater its novelty value.</p><p>Proximity and importance complement each other. Variable importance is a nonparametric estimator of the information a variable can convey about the model-predicted distribution in the interaction with other potential predictors. The aggregation of importances across the trees hides the partitional information of single trees. Case-based proximity recovers parts of this information by defining a similarity between cases that is implicitly weighted by the importance of their variables in the forest. In addition to the predictive ranking of variable importance, proxim-ity may provide structural insights into potential extreme groups and outliers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Application</head><p>To illustrate the utility of SEM forests, we present analyses of two empirical data sets, of which one was investigated with SEM trees before <ref type="bibr">(Brandmaier et al., 2013b)</ref>. In Appendix A, we provide a worked R example to demonstrate how SEM Forests can be practically used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Factor Analysis of the Wechsler Adult Intelligence-Revised</head><p>We investigated variable importance for a cross-sectional factor model of verbal cognitive ability and showed how model modification based on importance may be performed. The underlying research question is concerned with exploring how we can better explain individual differences in verbal ability in a cross-sectional sample. This sample of the Wechsler Adult Intelligence-Revised (WAIS-R) was previously analyzed by others (e.g., <ref type="bibr" target="#b25">Horn &amp; McArdle, 1992;</ref><ref type="bibr" target="#b39">McArdle &amp; Hamagami, 1992;</ref><ref type="bibr" target="#b42">McArdle &amp; Prescott, 1992)</ref>. The sample was collected during 1976 and 1980 by the Psychological Corporation and includes the scores of N ϭ 1,880 individuals (age from 16 to 74 years) on a total of 11 WAIS-R subscales. A rich set of demographic variables is available for this dataset, which we selected as potential predictors, including age group (in nine ordinal categories, hence eight implied dichotomous variables), geographical information about the place of residence (four nominal categories, hence seven implied variables), urban/rural place of residence (dichotomous), born in the U.S. (dichotomous), marital status (again four nominal categories, hence seven variables), race (three nominal categories), Hispanic heritage (dichotomous), handedness (dichotomous), education (six ordinal categories, hence five variables), occupation (six nominal categories, hence 31 variables), sex (dichotomous) and birth order (nine ordinal categories, hence eight variables).</p><p>Following <ref type="bibr">Brandmaier et al. (2013b)</ref>, we set up a single-factor model that hypothesizes one latent factor F for verbal cognitive ability, Y ϭ ⌳F ϩ ⑀, with ⌳ ϭ (1, 2 , 3 , 4 ), Y being the vector of indicator scores, and ⑀ the vector of residuals (⑀ 1 , ⑀ 2 , ⑀ 3 , ⑀ 4 ). The four indicator variables for verbal performance were information, comprehension, similarities, and vocabulary. In addition to fixing the loading of information, the expectation of this variable was constrained to 0, allowing the mean and variance of the factor to be estimated.</p><p>We randomly split the sample (N ϭ 1,880) in two halves of equal size, a training set and a hold-out set, to allow confirmation of a forest-based exploration. The forest analysis was performed on the training set only. The resulting variable importance served as basis of a modification of the original SEM, which was in turn evaluated on the hold-out set.</p><p>A forest analysis with 1,000 trees (see Figure <ref type="figure" target="#fig_0">1</ref>), subsampling of cases across the tree, and m ϭ [log 2 (12)] ϭ 4 (the dataset contains 12 potential predictors, so we randomly sample four split candidates at each node) yields "education" as the most important variable, with an average absolute increase of Ϫ2LL of 363.22, which corresponds to an average decrease in loglikelihood of about 12.5%. To reiterate, this is the drop in likelihood averaged This document is copyrighted by the American Psychological Association or one of its allied publishers.</p><p>This article is intended solely for the personal use of the individual user and is not to be disseminated broadly.</p><p>over the forest when the variable "education" is randomly permuted. Second and third runner-up are "age" with only a fifth of the effect of "education" and "occupation" with 13.7%, respectively. Based on this result, we propose a modification of the model to include the effect of the most important predictor, "education," on the factor structure. Because "education" was coded as an ordinal variable,<ref type="foot" target="#foot_7">foot_7</ref> we created a single tree with only a single split at the root node to determine the maximally informative split into two groups. The resulting tree partitioned the sample into participants who graduated from high school (12ϩ years) or not (0 -11 years) as the most informative split ( 2 ϭ 372.16, df ϭ 12, p Ͻ .001). We hypothesize education to predict differences in mean verbal performance and, thus, created a variation of the previous factor model including high school graduation as an exogenous predictor of the latent verbal ability. If the null hypothesis were true that education were uncorrelated with our model variables, this model should have reasonable fitness when the influence of graduation was restricted to zero. The model fit including the zero constraint was unacceptable (RMSEA ϭ 0.24, CFI ϭ 0.89, SRMR ϭ 0.25). When freeing the regression of verbal ability on graduation, model fit was fine (RMSEA ϭ 0.069, CFI ϭ 0.99, SRMR ϭ 0.01). We formally tested the inclusion of the regression with a likelihood ratio test and could significantly reject the removal of the regression path ( 2 ϭ 317.6, df ϭ 1, p Ͻ 0.001). This model modification serves as an example of how a SEM forest can inform the amelioration of a theory-driven, explanatory model. Note that the strength of variable importance is the quantification of the overall predictive effect in the interaction with all other predictors whereas our proposed modification was limited to the inclusion of a single variable. For an example of a model modification using the continuous variable "age," see <ref type="bibr" target="#b35">McArdle (1994)</ref> or interactions of "education" and "age," see <ref type="bibr" target="#b42">McArdle and Prescott (1992)</ref>. Previous studies have also shown the verbal IQ construct to be related to education, age, and sex differences <ref type="bibr" target="#b29">(Kaufman, Reynolds, &amp; McLean, 1989;</ref><ref type="bibr" target="#b54">Reynolds, Chastain, Kaufman, &amp; McLean, 1987)</ref>.</p><p>The single SEM tree reported by Brandmaier et al. (2013b) provided one way to look at potential predictors and their interaction but was already at the verge of interpretability due to its large number of splits. The authors concluded that the variable "education" was the most important since the first two splits in a singletree analysis contained this variable; these splits separated the extreme groups with very high and very low education from the midfield that was further partitioned by the tree. Further partitions in the single-tree analysis were made with respect to the variable "occupation." A more rigorous SEM forest analysis supported the observed importances in the single-tree solution but added a robust quantification of their relative strength.</p><p>As further means of exploring structure in the data set, we calculated proximity between each pair of the 940 training cases and projected each case onto the first two principal coordinates of their dissimilarity matrix <ref type="bibr" target="#b21">(Gower, 1966)</ref>, that is, a two-dimensional representation that minimizes loss of information. As a result, we obtain a two-cluster structure that reflects the most important variables from the preceding analysis (see Figure <ref type="figure">2</ref>). The cluster structure along the vertical principal axis clearly shows two sets of three clusters discriminating education (as shown by the symbols encoding level of education). The horizontal principal axis differentiates between people in and out of the workforce. Within each cluster a third principal axis encodes age as shown by the colorcoding of the symbols. We followed up on this result by applying This document is copyrighted by the American Psychological Association or one of its allied publishers. This article is intended solely for the personal use of the individual user and is not to be disseminated broadly.</p><p>a k-medoids 6 clustering algorithm to the proximity matrix. The number of clusters was chosen in a data-driven way by choosing k to maximize silhouette width <ref type="bibr" target="#b55">(Rousseeuw, 1987)</ref>, a measure of clustering consistency, which relates the average distance of each case to members of its cluster to the average distance of each case to all members of the next closest cluster. From these two clusters, we calculated the means of each predictor and of the modeled variables on a z-scale. The result is shown in Figure <ref type="figure" target="#fig_1">3</ref>. The plot can serve as a means to inspect the homogeneity of the predictor structure across participants. As can be seen from the average deviation of the modeled variables, one homogeneous cluster that was identified here (shown in blue color) comprises mostly unemployed (99.7% of the cluster) older (m ϭ 54 years) women (73% of the cluster) whereas the other larger cluster representing the remaining sample does not conspicuously deviate from the whole sample average. The choice to interpret two clusters was based on a data-driven, exploratory step and may be repeated with larger number of clusters for further exploration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Exploring Predictors of Differences in a Factor of Episodic Memory</head><p>As a further practical example of how trees and forests can be used to explore heterogeneity in empirical data, we analyzed data from the Berlin Aging Study II (BASE-II; <ref type="bibr" target="#b2">Bertram et al., 2014)</ref>. Here, the exploratory analysis focused on the question of how much a set of diverse predictors spanning psychosocial, demographic, and health-related indicators may, potentially in their interaction, help to predict individual differences in episodic memory. BASE-II is an interdisciplinary study investigating physical, cognitive, and social conditions associated with successful aging. The analysis was based on the complete sample of 2,463 participants, of which about one quarter were between 20-and 35-yearsold and three quarters were between 60-and 80-years-old. We analyzed cross-sectional data from the first available assessment, which had started in 2009. We created a single factor representing episodic memory. The construct was indicated by four items: (a) the verbal learning and memory test assessing auditory verbal learning. The sum of items recalled over five trials constituted the item score; (b) the face profession task assessing associative binding on the basis of recognition of incidentally encoded faceprofession pairs. Corrected hit rates for rearranged face-profession pairs were used as indicator; (c) the object location memory task assessing object-location memory with 12 colored photographs arranged on a 6 ϫ 6 grid. The sum of correct placements was used as the observed score; and (d) the scene encoding task assessing the ability of incidental scene encoding. A delayed recognition hit-rate was used as manifest variable. A more detailed description of these tasks was given by <ref type="bibr" target="#b18">Düzel et al. (2016)</ref>. The overall model had a good fit (CFI ϭ 0.997; RMSEA ϭ 0.032; p RMSEA ϭ 0.797). From the available covariates, we ad hoc selected a subset representing diverse indicators spanning psychosocial, demographic, and health-related indicators in an ad hoc manner. Among these, we chose to include age group (young/old), sex, years of education, marital status, and number of children; self-reports on sleep quality (day and night), smoking, healthy eating, the frequency of 6 Medoids are similar to centroids (or geometric centers), only that they always are members of the data set whereas centroids may lie between members.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Principal Coordinates</head><p>Principal Axis 1</p><p>Principal Axis 2 This document is copyrighted by the American Psychological Association or one of its allied publishers.</p><formula xml:id="formula_5">• • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • Figure 2.</formula><p>This article is intended solely for the personal use of the individual user and is not to be disseminated broadly.</p><p>social contacts (each with partner, relatives, friends, acquaintances, and neighbors), frequency of communication via telephone (each with partner, relatives, friends, acquaintances, and neighbors), the number of close friends, assessment of the current financial situation, time pressure, life satisfaction, optimism, goal engagement, and control beliefs (internal/external/other). Healthrelated indicators included occurrence of diseases (diabetes, asthma, coronary diseases, cancer, stroke, migraine, hypertension, depression, dementia, joint diseases, backpain, and sleeping disorders), and reports of sports activity and physical limitations in day-to-day work; we included positive and negative affect from the PANAS <ref type="bibr" target="#b73">(Watson, Clark, &amp; Tellegen, 1988)</ref>, the big five personality traits <ref type="bibr" target="#b32">(Lang, John, Lüdtke, Schupp, &amp; Wagner, 2011)</ref>, and the TICS <ref type="bibr" target="#b56">(Schulz &amp; Schlotz, 1999)</ref> for stress. From the subjective health questionnaire of the German Socioeconomic Panel (see <ref type="bibr" target="#b3">Böckenhoff et al., 2013)</ref>, we added self-rated overall wellbeing and health satisfaction, and further items of self-rated satisfaction with sleep, work, health, household tasks, and household income. All items were rated on 11-point Likert scales. We added three questionnaire items of loneliness (missing company of others, feeling left out, and feeling isolated). Lastly, we added dichotomous indicators of life events including severe injury or disease of self, severe injury or disease of the partner, death of a family member, divorce, severe conflict, financial burden, responsibility for a person in need of care, and relocation. This led to a data set with 73 potential predictors to explain heterogeneity with respect to a latent factor of episodic working memory.</p><p>With the single-factor model as outcome, a SEM forest was grown. We set m ϭ &gt;log 2 (73)? ϭ 7 (the dataset contains 73 potential predictors, so we randomly sample seven split candidates at each node), a forest size of 2,000, and computed variable importance. Results are plotted in Figure <ref type="figure">4</ref>. Unsurprisingly, age was by far the most influential effect on the episodic memory factor. This is in line with the life span perspective on the profound and continuous changes in EM in the sense of a decrease starting in middle adulthood with accelerating decline in very old age (cf. <ref type="bibr" target="#b58">Shing et al., 2010;</ref><ref type="bibr" target="#b61">Singer, Verhaeghen, Ghisletta, Lindenberger, &amp; Baltes, 2003)</ref>. The second most influential variable-with an estimated effect of a tenth of the aging effect-were work satisfaction, which may be seen as proxy for both general wellbeing and stress, and stress was found to negatively impact memory performance <ref type="bibr" target="#b71">(VonDras, Powless, Olson, Wheeler, &amp; Snudden, 2005;</ref><ref type="bibr" target="#b74">Wolf, 2009)</ref>. The appearance of relocation on a similar rank is surprising at first; one may hypothesize that relocation also is a major stressor in life. Alternatively, relocation may indicate that individuals are no longer able to live independently due to increasing frailty. Following up, we find hypertension. Hypertension is associated with impairments in cognitive functions in older adults <ref type="bibr" target="#b0">(Bender, Daugherty, &amp; Raz, 2013;</ref><ref type="bibr" target="#b52">Raz, Rodrigue, &amp; Acker, 2003)</ref>. A potential pathway links hypertension as an important cause of cerebrovascular disease (CVD) associated with mild cognitive impairment, defined as episodic memory impairment beyond the degree as seen in healthy aging <ref type="bibr" target="#b49">(Nordahl et al., 2005)</ref>. Hypertension and diabetes, both among the top predictors found by the forest, are primary risk factors for CVD, which stresses the importance to further research this proposed pathway. Physical limitations in day-to-day work is likely predictive in its role as proxy for overall health of an individual. Further predictors were education, the occurrence of joint diseases, then a indicators describing the personal family situation (being in a partnership), and back pain as a further disease-related predictor (with only a fortieth of the importance of age group). For sake of brevity, we omit the exact ordering of the remaining predictors. In summary, we conclude the forests successfully retrieved predictors previously found to be associated with impaired memory functioning or verbal intelligence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion</head><p>SEM forests constitute a hybrid of two modeling cultures (see <ref type="bibr" target="#b13">Breiman, 2001b;</ref><ref type="bibr" target="#b59">Shmueli, 2010</ref>, for perspectives on the differences): (a) structural equation modeling, which is a theory-driven, explanatory modeling approach that yields interpretable models with known statistical properties; and (b) random forest, which is a data-driven, predictive modeling technique allowing the retrieval of important subsets from large numbers of predictors with potentially complex interactions. Despite the fact that predictive modeling techniques are largely ignored for scientific inquiries, both explanatory and predictive modeling are useful for generating and testing theories <ref type="bibr" target="#b59">(Shmueli, 2010)</ref>. Predictive modeling complements explanatory modeling on multiple dimensions. As demonstrated in the SEM forest approach, predictive modeling can suggest improvements to explanatory models. In this way, predictive modeling helps to generate new hypotheses, particularly in large and complex data sets in which it is difficult to hypothesize (or difficult to winnow in on which variables to focus in on). Furthermore, predictive modeling approaches can be employed as a baseline of predictive accuracy that may serve as a benchmark for purely explanatory models: If predictive modeling is more accurate than some explanatory model, the explanatory model is likely to have room for improvement. This document is copyrighted by the American Psychological Association or one of its allied publishers. This article is intended solely for the personal use of the individual user and is not to be disseminated broadly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Benefits and Limitations</head><p>A key feature of both SEMs and simple decision trees is their readability and interpretability. In trees, feature interactions and resulting partitions can be easily visualized. In SEMs, the hypothesized structure of causes and effects between variables can be represented graphically (e.g., see von <ref type="bibr" target="#b72">Oertzen et al., 2015)</ref>. In contrast, as <ref type="bibr">Breiman (2001a)</ref> states, forests are "impenetrable as far as simple interpretation of its mechanisms go" (p. 23). The straightforward interpretation of a single tree is lost in the forest. Particularly, forests do not provide a straightforward hierarchical clustering of the participants into homogeneous subgroups. However, the variable importance estimate yields a structured approach to aggregate information about previously unmodeled variables across the trees. In the endeavor to explain phenomena, the question often arises which variables should be controlled for, respectively, what variables can provide predictive information; and how strong is their influence on the model of interest. SEM forests provide a starting point for researchers to address these questions in a rigorous and robust manner.</p><p>Variable importance and proximity measures provide a way to efficiently search empirical data for sources of variability. Our empirical results are supported by the interactions of demographic characteristics as found by traditional analysis techniques for the WAIS-R verbal factor, but they also go beyond the recovery of known associations. Whereas in SEM trees variables compete with one another for model impact, SEM forests assess competing variables' importances by the controlled utilization of randomness and, thus, provide an unbiased estimate of marginal importance for model fitness.</p><p>Still, SEM forests impose high computational burdens, and a forest analysis may thus be time demanding. For large sets of predictor variables, the number of model optimization runs in a SEM tree depends linearly on the number of potential split points and linearly on the number of observations <ref type="bibr">(Brandmaier et al., 2013b)</ref>. In trees, all predictors are tested at each level, whereas, in forests, random sampling of the predictors will drastically decrease the number of tests and, thus, decrease the number of potential split points, leading to a reduced asymptotic time complexity. Under certain circumstances, typically when a large number of predictors are chosen, a forest analysis can even be faster than a single tree analysis. Still, forests are typically created with hundreds to thousands of members, which in our experience prohibits computation on desktop machines for most models. However, models featuring a small covariance matrix of size 4 ϫ 4 and an ensemble with 1,000 members may be generated in less than 1 hr on a standard desktop computer. The ideal size of a forest depends on the number of predictors, their interactions, the heterogeneity of the data, and the complexity of the model. Currently, good guidelines for forest size are still missing and, when computational resources are sparse, researchers are advised to iteratively increase forest size until forest results empirically stabilize. Note that forests need not be regrown but can be grown by simply adding more trees.</p><p>Choosing the hyperparameters (i.e., parameters of the sampling procedure) remains a crucial decision when estimating variable importance using SEM forests. Typically, this entails selecting the resampling procedure (bootstrapping or subsampling), selecting the number of variables that are sampled at each level, and the number of trees in the forest. Currently, we heuristically rely on evidence from the research on (conditional) random forests. Further simulation work is needed to outline optimal hyperparameters for creating SEM forests. Currently, we set the number of trees to 2,000 and empirically examine whether variable importance converges as the number of trees increases to decide whether a larger forest is needed. We heuristically set the number of candidate predictors to the logarithm of the total number of predictors and use subsampling as resampling scheme. As noted earlier, the choice of hyperparameters may influence the results of a forest. For example, when choosing the number of candidate variables, there is a trade-off between diversity and stability: A low number of candidate variables leads to more diverse trees at the cost of important predictors having a lower chance to enter any single tree. Other parameters that directly influence the likelihood ratio test, which ultimately underlies the variable selection mechanism, such as missingness or group imbalance in categorical variables, may bias variable selection and affect the power to detect important variables. Similarly, the measure of proximity is influenced not only by the choice of hyperparameters but also by the number of variables and their relative importance. Future work needs to address these questions from theoretical and empirical perspectives across many types of SEMs ranging from cross-sectional factor models to coupled latent differential equations.</p><p>As a tool for improving a theory-driven SEM, variable importance grants no free lunch. It is commonly accepted that theory construction and refinement benefit from measures that specify the relationship of relevant predictors to outcome variables (e.g., in the sense of regression), the relationship among relevant predictors (e.g., in the sense of moderation), or both. Variable importance falls short of either mark but offers two other benefits in turn. First, variable importance integrates the predictiveness of a given variable into a single score, and it does so in a way that includes all of its interactions with other predictors across all trees of the forest. It should be noted that alternative ways to derive such measures are available. For instance, <ref type="bibr" target="#b63">Strobl, Boulesteix, Kneib, Augustin, and Zeileis (2008)</ref> discussed conditional instead of marginal variable importance, and <ref type="bibr" target="#b22">Hapfelmeier, Hothorn, Ulm, and Strobl (2014)</ref> introduced an alternative variable importance scheme that replaces OOB scrambling by randomly distributing cases to each node while holding the ratio of cases over child nodes constant. In the presence of missing values of variables, this scheme was found to be superior over standard variable importance. The second benefit of variable importance is related to the first: Variable importance expresses the expected decrease in uncertainty for the entire outcome model instead of being restricted to a specific variable or relation in that model. Taken together, then, variable importance conveys useful summary information about predictor variables in relation to an outcome model. Nevertheless, it is difficult to indicate how exactly this information should be used to guide theory development. How can we best translate the results of a forest analysis, and the summary information conveyed by variable importance, into an improved parametric model, and ultimately build a better scientific theory? In our view, forests, and the measures derived from them, indicate whether there is any predictive potential in a set of variables not yet integrated into a hypothesized SEM. In case these variables have not yet been considered in the theory that led to the specification of the statistical model, their discovery may lead researchers to reconsider and augment This document is copyrighted by the American Psychological Association or one of its allied publishers.</p><p>This article is intended solely for the personal use of the individual user and is not to be disseminated broadly.</p><p>their theory. In this endeavor, variable importance provides researchers with hints at which variables to select. How to structurally integrate these variables into the original SEM remains an open question. We hope that this article along with the freely provided SEM tree and forest software spawns future research to address this question in greater detail and in large data sets that contain formerly hidden and theoretically relevant predictive information. SEM forests provide a structured approach to quantify variable importance in SEMs and support researchers in hypothesis generation and testing. As an extension of SEM trees, forests combine aspects of data-driven and theory-guided analysis in a single framework of theory-guided exploration (for an alternative, modelfree approach, see <ref type="bibr" target="#b43">Miller, Lubke, McArtor, &amp; Bergeman, 2016)</ref>. The cautionary note of <ref type="bibr">Brandmaier et al. (2013b)</ref> that was expressed in the context of SEM trees, is just as valid for forests: Exploratory methods do not provide a shortcut from data to theories, nor from data to knowledge. Researchers are still required to think about their observations, remind themselves of the assumptions of their models, of the intricacies of the data sampling process, and the focus of their field; then, backed by this knowledge, make reasonable and responsible decisions about their analyses. Last but not least, a data-driven model must be evaluated on an independent dataset before its tenability can be claimed. If confirmatory analyses are planned, researchers are advised to conduct them before proceeding to exploratory approaches <ref type="bibr" target="#b37">(McArdle, 2013;</ref><ref type="bibr" target="#b68">Tukey, 1962)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>When to Grow More Than One Tree</head><p>When should we use trees and forests at all? When analyzing large data sets, scientists often would like to know which subset of variables has an influence on the phenomenon of interest (be this a univariate outcome, a correlation, a hypothesized causal relation, or any other relationship between or properties of variables) and, thus, carries the potential to explain the phenomenon. This exploratory approach is legitimate if the original hypotheses guiding the study turn out to be untenable, if the data set includes variables beyond the scope of extant theories, or whenever prediction does not only serve to validate a theory but actually is a goal in itself (e.g., in clinical decision making). The utility of complementing theory testing with tree-type exploration increases further when the number of predictors is large relative to the number of cases, and when effects of predictors on outcomes are interactive and nonlinear. If these conditions are met, the question remains when we should be content with a single tree and when we should grow more than one tree. We believe that in most cases both should be done as complementary analyses. First, a single tree may be grown to show a partitional structure of the sample inducing groups with different observed data patterns. The decision nodes of the tree may inform researchers about variables that describe differences and that may be better accounted for in future models. The partitional structure itself may be informative about the kind of differences between subgroups, for example, different factor profiles in factor analysis, or different growth curves in longitudinal data. However, it must be stressed that the resulting partition may neither be true nor be the best possible. It merely is one way of partitioning the sample that is optimal according to the chosen tree induction algorithm. Thus, it seems reasonable to turn to SEM forests in a subsequent analysis step. The forest analysis induces random variation to the empirical sample to obtain better estimates of variables' importance for predicting differences in observed data patterns. This comes at the cost of losing a straightforward way to recover a concrete partition from a forest.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>In psychological research, the number of cases is often small compared with the number of variables measured but there are increasing number of studies that generate large data sets, for example, by using affordable and easily accessible online surveys, fused data sets from multisite studies, or data with a high density of measured variables. The latter is especially true for studies involving neuroimaging techniques or genetic associations. Likewise, purely behavioral data sets can benefit from exploratory analyses with trees and forests (e.g., if they comprise a large array of scales from various questionnaires). Tree and forest analyses may inform researchers about variables that provide additional information about the phenomenon or process they are hypothesizing about. Recent tree-based analyses of psychological data sets were conducted with longitudinally modeled child development <ref type="bibr">(Brandmaier et al., 2013b)</ref>, adult development <ref type="bibr">(Brandmaier et al., 2013a)</ref>, and late-life terminal decline <ref type="bibr" target="#b20">(Ghisletta, 2013)</ref> of cognitive functioning across age, or perceptions of stress <ref type="bibr" target="#b57">(Scott, Whitehead, Bergernan, &amp; Pitzer, 2013)</ref>. We share the hope of <ref type="bibr" target="#b65">Strobl, Malley, and Tutz (2009)</ref> that trees will become a standard tool of analysis in psychological research and other empirical fields. The flexibility of structural equation modeling to account for many research designs and the flexibility of trees and forests to account for the diversity of predictors encountered in many settings makes the method suitable as a generic tool of exploration after a first step of purely theory-driven modeling. SEM trees and forests will enable researchers to make more efficient use of their empirical data. Lastly, trees may be one more step toward bringing individual differences back into the focus of psychological research. data(factorData) factorModel &lt;-mxModel( "Factor Model", type="RAM", mxData(factorData,type="raw"), manifestVars = c("x1","x2","x3"), latentVars = "f", mxPath(from="f",to=c("x1","x2","x3"), free=c(FALSE,TRUE,TRUE), value=c(1,1,1), arrows=1, label=c("l1","l2","l3")), mxPath(from="one",to=c("f"), free=c(TRUE), value=c(1.0), arrows=1, label=c("mu_f")), mxPath(from="f",to=c("f"), free=c(TRUE), value=c(1.0), arrows=2, label=c("var_f")), mxPath(from="x1",to=c("x1"), free=c(TRUE), value=c(1.0), arrows=2, label=c("e")), mxPath(from="x2",to=c("x2"), free=c(TRUE), value=c(1.0), arrows=2, label=c("e")), mxPath(from="x3",to=c("x3"), free=c(TRUE), value=c(1.0), arrows=2, label=c("e")) ) # close model summary(factorFit &lt;-mxRun(factorModel))</p><p>Starting estimation with the full dataset using mxRun() will indicate errors in model syntax, or potential model estimation issues, prior to forest creation with the command semforest(). The following example uses a simulated dataset as presented in Appendix A. Three indicators are shown together with six covariates. Two covariates have high intercorrelations, two variables have moderate intercorrelation, and two variables are uncorrelated random noise. Each covariate is coded as two categories (0/1), with standard normal effects of cov1 and cov2 distributed as ᏺ(2.0, 0.3) and cov3 and cov4 distributed as ᏺ(1.0, 0.5) on the latent level factor score. The intention of this data structure is to show the utility of the SEM forest method when compared with a simple SEM tree analysis. When unmodeled covariates have overlapping information to different degrees, their impact within the model may be lost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Creating a Control Object</head><p>Users have control over parameters determining the growth process of a forest. The parameters may be changed depending on the model, the number of covariates, and the research hypotheses to be tested. The forest control options contain:</p><p>• num.trees -how many trees to create in the SEM Forest process.</p><p>• sampling -method for selecting cases in each SEM Tree.</p><p>• control -a SEM Tree control object as described in <ref type="bibr">Brandmaier, von Oertzen, McArdle, &amp; Lindenberger (2013b)</ref>.</p><p>• mtry -number of covariates to test at each node for splitting algorithm.</p><p>A control object with default settings is done with the following line: factorControls &lt;-semforest.control() # Change the Default settings in semforest.control() and semtree.control() factorControls$num.trees &lt;-1000 factorControls$semtree.control$method &lt;-"naive" Control objects will shape the forest (and individual trees) to fall within parameters established by the researcher. Please note the defaults are supplied not as suggested starting points, but in order to prevent computational overloads for novice users.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SEM Tree Interpretation</head><p>A SEM tree is fit to the factor model structure with the simulated dataset. Using the control object from the above step, we run the semtree() function: factorTree &lt;-semtree(model=factorModel, data=factorData, semforest.control= factorControls$semtree.control)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(Appendices continue)</head><p>This document is copyrighted by the American Psychological Association or one of its allied publishers. This article is intended solely for the personal use of the individual user and is not to be disseminated broadly.</p><p>The semforest() arguments are in the following order: SEM OpenMx model, dataset (with model variables and covariates: cov1-cov6), and researcher specified control object. This step may take some time to complete. On machines with more than one core available (and in cases where grid computing is available), users may want to use parallel processing to speed up computation of n trees. The R package "snowfall" is required and may be implemented in the following way: require(snowfall) sfInit(parallel=TRUE, cpus=2) sfClusterEval(require("OpenMx")) sfClusterEval(require("semtree"))</p><p>The above code will initialize a cluster of two CPUs in parallel and load the required packages remotely. The command semforest() will detect this environment automatically to distribute jobs over the available CPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Visualizing a Semforest Object</head><p>Once a semforest has been computed, a couple of tools are available to researchers to print and visualize results. Variable Importance is a graphical display of the impact of each potential splitting covariate for subsetting the overall model into a two group nested model. The impact of each covariate is graphed as bar plots or as average model improvement traced over iterations. . Variable importance plot for the simulated data fit to a factor model. Cov1 and cov2 have about equal importance for the forest analysis, cov3 and cov4 are also about equal in their influence, and cov5 and cov6 both have no influence on the forest model. This document is copyrighted by the American Psychological Association or one of its allied publishers.</p><p>This article is intended solely for the personal use of the individual user and is not to be disseminated broadly.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Left: A bar chart of variable importance for the WAIS-R factor model, quantified as average increase in model misfit due to randomization. Right: Convergence behavior of the absolute importance (y-axis) over the number of trees (x-axis). See the online article for the color version of this figure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Group-wise predictor means on a z-scale for two groups derived by k-medoids clustering of the proximity matrix. Categorical variables marital status and geographic region (direction) are represented in dummy coding. See the online article for the color version of this figure.</figDesc><graphic coords="8,181.67,152.45,98.54,50.18" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Figure 2. Proximity of participants with respect to the WAIS-R verbal factor model. The plot shows participants on the First two principal coordinates of the proximity matrix. The color gradient represents age from 16 (red) to 74 (blue). Levels of education are coded as follows: 0 -8 years (square), 9 -11 years (circle), 12 or more years (triangle). The dashed line separates people with respect to their employment status; cases on the left are unemployed, cases on the right are in the labor force. See the online article for the color version of this figure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Figure B2. Variable importance plot for the simulated data fit to a factor model. Cov1 and cov2 have about equal importance for the forest analysis, cov3 and cov4 are also about equal in their influence, and cov5 and cov6 both have no influence on the forest model. This document is copyrighted by the American Psychological Association or one of its allied publishers.This article is intended solely for the personal use of the individual user and is not to be disseminated broadly.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>were proposed, but others are possible. SEM trees extend conventional decision trees to model-based trees that feature a parametrical model of the outcomes in each leaf instead of a prediction about a univariate outcome. Regression trees, in which each leaf model contains a regression model, are a special case of SEM trees because regression is a special case of SEM. SEM trees allow a wider variety of linear models with both observed and latent variables in leafs of the tree. Whereas conventional decision trees provide a nonparametric model of the outcomes, SEM trees fuse the nonparametric nature of decision trees with parametric SEMs as outcomes. The logic behind SEM trees is simple. Given a hypothesized SEM, the sample is recursively split to find sub groups that maximally differ with respect to the parameters of the original SEM. Typically, the hypothesized SEM is the same for each leaf, and each leaf represents a sample best described with a set of parameter estimates for the hypothesized model (but seeBrandmaier, von Oertzen, McArdle, &amp; Lindenberger, 2013a, for hybrid SEM trees that even allow different models across leafs).</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Top 10 variable importance estimates for an episodic memory factor in the BASE-II study plotted on a log-scale. From most important to least important: Age group (young/old), work satisfaction (11-point scale), relocation (yes/no), hypertension (yes/no), physical limitations in day-today work, education (in years), diabetes (yes/no), arthrosis (yes/no), being in a relationship (yes/no), back pain (yes/no).</figDesc><table><row><cell>Age group</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Work Satisfaction</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Relocation</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Hypertension</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Physical Limitations</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Education</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Diabetes</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Arthrosis</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Relationship</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Back Pain</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>1</cell><cell>2</cell><cell>5</cell><cell>10</cell><cell>20</cell><cell>50 100 200</cell><cell>500</cell></row><row><cell>Figure 4.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>This document is copyrighted by the American Psychological Association or one of its allied publishers.This article is intended solely for the personal use of the individual user and is not to be disseminated broadly.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_1"><p>Unmodeled in the sense that this information is not part of the initial theory-driven SEM.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_2"><p>Details about the number of trees will be discussed later.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_3"><p>Explained variance is a measure of relative gains in predictive accuracy.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_4"><p>Hyperparameters are parameters that are not adjusted by the model fitting procedure but instead either set manually or adjusted by an external criterion. This document is copyrighted by the American Psychological Association or one of its allied publishers.This article is intended solely for the personal use of the individual user and is not to be disseminated broadly.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_5"><p>BRANDMAIER, PRINDLE, MCARDLE, AND LINDENBERGER</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_6"><p>SEM FORESTS</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_7"><p>Years of education were coded: 1 ¢ 0 -7 years; 2 ¢ 8 years; 3 ¢ 9 -11 years; 4 ¢ 12 years; 5 ¢ 13-15 years;</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_8"><p>¢ 16ϩ years.</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix B</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Example of a Factor Model Forest</head><p>To generate a SEM forest and visualize the results only a few steps are needed. In general, an OpenMx model, a dataset with variables observed in the model and covariates included, and a set of control parameters are the only requirements. We will provide steps for fitting an OpenMx model with data, creating a forest, and plotting variance importance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The OpenMx Model</head><p>SEM trees require model specification in OpenMx <ref type="bibr" target="#b4">(Boker et al., 2011)</ref>, a package for the statistical programming language R (R Core Team, 2013). OpenMx allows path and matrix specification of models. In the following, we present a path specification of a factor model with three observed variables (x1-x3) that are assumed to measure a single construct. The observed variables are indicated by a single latent factor, with all shared variance contained at the latent level, and unique variation residing in the residuals for each indicator. The following code will create a factor SEM, fit the dataset to the model, and provide model parameters and fit indices to the console.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(Appendices continue)</head><p>This document is copyrighted by the American Psychological Association or one of its allied publishers.</p><p>This article is intended solely for the personal use of the individual user and is not to be disseminated broadly.</p><p>The individual tree is the standard "naïve" splitting process, where all split points have the same potential influence independent of which covariate they indicate. Plotting a tree output is relatively straightforward, with multiple tree complications encountered in a random forest analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>plot(factorTree)</head><p>The output of this plot is shown in Figure <ref type="figure">B1</ref>. Of note is the structure of the splits found for subsetting the data at each node. Cov1 is selected initially (but was equally as influential as cov2 in the simulation design), while cov3 and cov2 were selected for secondary splits (with the impact of cov4 equal to cov3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Creating a Forest</head><p>The semforest command uses the elements described above to compile SEM trees following random forest logic. The most basic tree can be run by the following command line: factorForest &lt;-semforest(model=factorModel, data=factorData, Semforest. control=factorControls)</p><p>Alternatively, single trees can be extracted from a forest as follows: This article is intended solely for the personal use of the individual user and is not to be disseminated broadly.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Vascular risk moderates associations between hippocampal subfield volumes and memory</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Bender</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Daugherty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Raz</surname></persName>
		</author>
		<idno type="DOI">10.1162/jocn_a_00435</idno>
		<ptr target="http://dx.doi.org/10.1162/jocn_a_00435" />
	</analytic>
	<monogr>
		<title level="j">Journal of Cognitive Neuroscience</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1851" to="1862" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">An introduction to ensemble methods for data analysis</title>
		<author>
			<persName><forename type="first">R</forename><surname>Berk</surname></persName>
		</author>
		<idno type="DOI">10.1177/0049124105283119</idno>
		<ptr target="http://dx.doi.org/10.1177/0049124105283119" />
	</analytic>
	<monogr>
		<title level="j">Sociological Methods &amp; Research</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="263" to="295" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Cohort profile: The Berlin Aging Study II (BASE-II)</title>
		<author>
			<persName><forename type="first">L</forename><surname>Bertram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Böckenhoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Demuth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Düzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Eckardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">.</forename><forename type="middle">.</forename><surname>Steinhagen-Thiessen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename></persName>
		</author>
		<idno type="DOI">10.1093/ije/dyt018</idno>
		<ptr target="http://dx.doi.org/10.1093/ije/dyt018" />
	</analytic>
	<monogr>
		<title level="j">International Journal of Epidemiology</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="703" to="712" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The socio-economic module of the Berlin Aging Study II (SOEP-BASE): Description, structure, and questionnaire</title>
		<author>
			<persName><forename type="first">A</forename><surname>Böckenhoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sassenroth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kroh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Siedler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Eibich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">G</forename><surname>Wagner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SOEPpapers on Multidisciplinary Panel Data Research</title>
		<meeting><address><addrLine>Germany</addrLine></address></meeting>
		<imprint>
			<publisher>DIW</publisher>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note>Berlin</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">OpenMx: An open source extended structural equation modeling framework</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Boker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Neale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Maes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wilde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Spiegel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">R</forename><surname>Brick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">.</forename><forename type="middle">.</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename></persName>
		</author>
		<idno type="DOI">10.1007/s11336-010-9200-6</idno>
		<ptr target="http://dx.doi.org/10.1007/s11336-010-9200-6" />
	</analytic>
	<monogr>
		<title level="j">Psychometrika</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="page" from="306" to="317" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Recent developments on structural This document is copyrighted by the American Psychological Association or one of its allied publishers. This article is intended solely for the personal use of the individual user and is not to be disseminated broadly. equation models: Theory and applications</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Boker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Neale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rausch</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-1-4020-1958-6_9</idno>
		<ptr target="http://dx.doi.org/10.1007/978-1-4020-1958-6_9" />
		<editor>K. van Montfort, J. Oud, &amp; A. Satorra</editor>
		<imprint>
			<date type="published" when="2004">2004</date>
			<publisher>Kluwer Academic Publishers</publisher>
			<biblScope unit="page" from="151" to="174" />
			<pubPlace>Dordrecht, the Netherlands</pubPlace>
		</imprint>
	</monogr>
	<note>Latent differential equation modeling with multivariate multi-occasion indicators</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The two disciplines of scientific psychology, or: The disunity of psychology as a working hypothesis</title>
		<author>
			<persName><forename type="first">D</forename><surname>Borsboom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Kievit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cervone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">B</forename><surname>Hood</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-0-387-95922-1_4</idno>
		<ptr target="http://dx.doi.org/10.1007/978-0-387-95922-1_4" />
	</analytic>
	<monogr>
		<title level="m">Dynamic process methodology in the social and developmental sciences</title>
		<meeting><address><addrLine>New York, NY</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="67" to="97" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">semtree: Recursive partitioning of structural equation models in R</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Brandmaier</surname></persName>
		</author>
		<ptr target="http://www.brandmaier.de/semtree" />
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note>Computer software manual</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Brandmaier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Von Oertzen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Mcardle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Lindenberger</surname></persName>
		</author>
		<title level="m">Exploratory data mining with structural equation model trees</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Contemporary issues in exploratory data mining in the behavioral sciences</title>
		<editor>J. J. McArdle &amp; G. Ritschard</editor>
		<imprint>
			<publisher>Routledge</publisher>
			<biblScope unit="page" from="96" to="127" />
			<pubPlace>New York, NY</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Brandmaier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Von Oertzen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Mcardle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Lindenberger</surname></persName>
		</author>
		<idno type="DOI">10.1037/a0030001</idno>
		<ptr target="http://dx.doi.org/10.1037/a0030001" />
	</analytic>
	<monogr>
		<title level="m">Structural equation model trees</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="71" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Bagging predictors. Machine learning</title>
		<author>
			<persName><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
		<idno type="DOI">10.1023/A:1018054314350</idno>
		<ptr target="http://dx.doi.org/10.1023/A:1018054314350" />
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="123" to="140" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
		<idno type="DOI">10.1023/A:1010933404324</idno>
		<ptr target="http://dx.doi.org/10.1023/A:1010933404324" />
		<title level="m">Random forests. Machine Learning</title>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="5" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Statistical modeling: The two cultures (with comments and a rejoinder by the author)</title>
		<author>
			<persName><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
		<idno type="DOI">10.1214/ss/1009213726</idno>
		<ptr target="http://dx.doi.org/10.1214/ss/1009213726" />
	</analytic>
	<monogr>
		<title level="j">Statistical Science</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="199" to="231" />
			<date type="published" when="2001">2001b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Random forests</title>
		<author>
			<persName><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cutler</surname></persName>
		</author>
		<ptr target="http://www.stat.berkeley.edu/breiman/RandomForests/cc_home.htm" />
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note>Computer software manual</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Classification and regression trees</title>
		<author>
			<persName><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Olshen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Stone</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1984">1984</date>
			<publisher>Wadsworth International</publisher>
			<pubPlace>Belmont, CA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Bühlmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yu</surname></persName>
		</author>
		<idno type="DOI">10.1214/aos/1031689014</idno>
		<ptr target="http://dx.doi.org/10.1214/aos/1031689014" />
		<title level="m">Analyzing bagging. The Annals of Statistics</title>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="927" to="961" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Gene selection and classification of microarray data using random forest</title>
		<author>
			<persName><forename type="first">R</forename><surname>Díaz-Uriarte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>De Andres</surname></persName>
		</author>
		<idno type="DOI">10.1186/1471-2105-7-3</idno>
		<ptr target="http://dx.doi.org/10.1186/1471-2105-7-3" />
	</analytic>
	<monogr>
		<title level="j">BMC Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The subjective health horizon questionnaire (SHH-Q): assessing future time perspectives for facets of an active lifestyle</title>
		<author>
			<persName><forename type="first">S</forename><surname>Düzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Voelkle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Düzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gerstorf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Drewelies</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Steinhagen-Thiessen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">.</forename><forename type="middle">.</forename><surname>Lindenberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename></persName>
		</author>
		<idno type="DOI">10.1159/000441493</idno>
		<ptr target="http://dx.doi.org/10.1159/000441493" />
	</analytic>
	<monogr>
		<title level="j">Gerontology</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="page" from="345" to="353" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Do we need hundreds of classifiers to solve real world classification problems?</title>
		<author>
			<persName><forename type="first">M</forename><surname>Fernández-Delgado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Cernadas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Barro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Amorim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="3133" to="3181" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Recursive partitioning to study terminal decline in the berlin aging study</title>
		<author>
			<persName><forename type="first">P</forename><surname>Ghisletta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Contemporary issues in exploratory data mining in the behavioral sciences</title>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Mcardle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Ritschard</surname></persName>
		</editor>
		<meeting><address><addrLine>New York, NY</addrLine></address></meeting>
		<imprint>
			<publisher>Routledge</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="405" to="428" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Some distance properties of latent root and vector methods used in multivariate analysis</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Gower</surname></persName>
		</author>
		<idno type="DOI">10.2307/2333639</idno>
		<ptr target="http://dx.doi.org/10.2307/2333639" />
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="page" from="325" to="338" />
			<date type="published" when="1966">1966</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A new variable importance measure for random forests with missing data</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hapfelmeier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hothorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ulm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Strobl</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11222-012-9349-1</idno>
		<ptr target="http://dx.doi.org/10.1007/s11222-012-9349-1" />
	</analytic>
	<monogr>
		<title level="j">Statistics and Computing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="21" to="34" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A new variable selection approach using random forests</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hapfelmeier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ulm</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.csda.2012.09.020</idno>
		<ptr target="http://dx.doi.org/10.1016/j.csda.2012.09.020" />
	</analytic>
	<monogr>
		<title level="j">Computational Statistics &amp; Data Analysis</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="50" to="69" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">The elements of statistical learning</title>
		<author>
			<persName><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Friedman</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-0-387-21606-5</idno>
		<ptr target="http://dx.doi.org/10.1007/978-0-387-21606-5" />
		<imprint>
			<date type="published" when="2001">2001</date>
			<publisher>Springer</publisher>
			<pubPlace>New York, NY</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A practical and theoretical guide to measurement invariance in aging research</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Horn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Mcardle</surname></persName>
		</author>
		<idno type="DOI">10.1080/03610739208253916</idno>
		<ptr target="http://dx.doi.org/10.1080/03610739208253916" />
	</analytic>
	<monogr>
		<title level="j">Experimental Aging Research</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="117" to="144" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A general approach to confirmatory maximum likelihood factor analysis</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">G</forename><surname>Jöreskog</surname></persName>
		</author>
		<idno type="DOI">10.1007/BF02289343</idno>
		<ptr target="http://dx.doi.org/10.1007/BF02289343" />
	</analytic>
	<monogr>
		<title level="j">Psychometrika</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="183" to="202" />
			<date type="published" when="1969">1969</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Estimation and testing of simplex models</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">G</forename><surname>Jöreskog</surname></persName>
		</author>
		<idno type="DOI">10.1111/j.2044-8317.1970.tb00439.x</idno>
		<ptr target="http://dx.doi.org/10.1111/j.2044-8317.1970.tb00439.x" />
	</analytic>
	<monogr>
		<title level="j">British Journal of Mathematical and Statistical Psychology</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="121" to="145" />
			<date type="published" when="1970">1970</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Genetic and lifestyle predictors of 15-year longitudinal change in episodic memory</title>
		<author>
			<persName><forename type="first">M</forename><surname>Josefsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>De Luna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pudas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-G</forename><surname>Nilsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Nyberg</surname></persName>
		</author>
		<idno type="DOI">10.1111/jgs.12000</idno>
		<ptr target="http://dx.doi.org/10.1111/jgs.12000" />
	</analytic>
	<monogr>
		<title level="j">Journal of the American Geriatrics Society</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="2308" to="2312" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Age and WAIS-R intelligence in a national sample of adults in the 20-to 74-year age range: A cross-sectional analysis with educational level controlled</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Kaufman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Reynolds</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Mclean</surname></persName>
		</author>
		<idno type="DOI">10.1016/0160-2896%2889%2990020-2</idno>
		<ptr target="http://dx.doi.org/10.1016/0160-2896" />
	</analytic>
	<monogr>
		<title level="j">Intelligence</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="90020" to="90022" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Simpson&apos;s paradox in psychological science: A practical guide</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Kievit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">E</forename><surname>Frankenhuis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Waldorp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Borsboom</surname></persName>
		</author>
		<idno type="DOI">10.3389/fpsyg.2013.00513</idno>
		<ptr target="http://dx.doi.org/10.3389/fpsyg.2013.00513" />
	</analytic>
	<monogr>
		<title level="m">Frontiers in Psychology</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Toward an idiothetic psychology of personality</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Lamiell</surname></persName>
		</author>
		<idno type="DOI">10.1037/0003-066X.36.3.276</idno>
		<ptr target="http://dx.doi.org/10.1037/0003-066X.36.3.276" />
	</analytic>
	<monogr>
		<title level="j">American Psychologist</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="276" to="289" />
			<date type="published" when="1981">1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Short assessment of the big five: Robust across survey methods except telephone interviewing</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">R</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Lüdtke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schupp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">G</forename><surname>Wagner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behavior Research Methods</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="548" to="567" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">On selecting indicators for multivariate measurement and modeling with latent variables: When &quot;good&quot; indicators are bad and &quot;bad&quot; indicators are good</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">D</forename><surname>Little</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Lindenberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Nesselroade</surname></persName>
		</author>
		<idno type="DOI">10.1037/1082-989X.4.2.192</idno>
		<ptr target="http://dx.doi.org/10.1037/1082-989X.4.2.192" />
	</analytic>
	<monogr>
		<title level="j">Psychological Methods</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="192" to="211" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Split selection methods for classification trees</title>
		<author>
			<persName><forename type="first">W</forename><surname>Loh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistica Sinica</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="815" to="840" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Structural factor analysis experiments with incomplete data</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Mcardle</surname></persName>
		</author>
		<idno type="DOI">10.1207/s15327906mbr2904_5</idno>
		<ptr target="http://dx.doi.org/10.1207/s15327906mbr2904_5" />
	</analytic>
	<monogr>
		<title level="j">Multivariate Behavioral Research</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="409" to="454" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Latent variable modeling of differences and changes with longitudinal data</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Mcardle</surname></persName>
		</author>
		<idno type="DOI">10.1146/annurev.psych.60.110707.163612</idno>
		<ptr target="http://dx.doi.org/10.1146/annurev.psych.60.110707.163612" />
	</analytic>
	<monogr>
		<title level="j">Annual Review of Psychology</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="577" to="605" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Exploratory data mining using decision trees in the behavioral sciences</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Mcardle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Contemporary issues in exploratory data mining in the behavioral sciences</title>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Mcardle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Ritschard</surname></persName>
		</editor>
		<meeting><address><addrLine>New York, NY</addrLine></address></meeting>
		<imprint>
			<publisher>Routledge</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="3" to="47" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Latent growth curves within developmental structural equation models</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Mcardle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Epstein</surname></persName>
		</author>
		<idno type="DOI">10.2307/1130295</idno>
		<ptr target="http://dx.doi.org/10.2307/1130295" />
	</analytic>
	<monogr>
		<title level="j">Child Development</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page" from="110" to="133" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Modeling incomplete longitudinal and cross-sectional data using latent growth structural models</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Mcardle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hamagami</surname></persName>
		</author>
		<idno type="DOI">10.1080/03610739208253917</idno>
		<ptr target="http://dx.doi.org/10.1080/03610739208253917" />
	</analytic>
	<monogr>
		<title level="j">Experimental Aging Research</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="145" to="166" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Latent difference score structural models for linear dynamic analyses with incomplete longitudinal data: New methods for the analysis of change</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Mcardle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hamagami</surname></persName>
		</author>
		<idno type="DOI">10.1037/10409-005</idno>
		<ptr target="http://dx.doi.org/10.1037/10409-005" />
	</analytic>
	<monogr>
		<title level="m">New methods for the analysis of change</title>
		<editor>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Collins</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Sayer</surname></persName>
		</editor>
		<meeting><address><addrLine>Washington, DC</addrLine></address></meeting>
		<imprint>
			<publisher>American Psychological Association</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="139" to="175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Longitudinal data analysis using structural equation models</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Mcardle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Nesselroade</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>American Psychological Association</publisher>
			<pubPlace>Washington, DC</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Age-based construct validation using structural equation modeling</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Mcardle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Prescott</surname></persName>
		</author>
		<idno type="DOI">10.1080/03610739208253915</idno>
		<ptr target="http://dx.doi.org/10.1080/03610739208253915" />
	</analytic>
	<monogr>
		<title level="j">Experimental Aging Research</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="87" to="115" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Finding structure in data using multivariate tree boosting</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">H</forename><surname>Lubke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Mcartor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">S</forename><surname>Bergeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Methods</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="583" to="602" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">This document is copyrighted by the American Psychological Association or one of its allied publishers. This article is intended solely for the personal use of the individual user and is not to be disseminated broadly</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">A manifesto on psychology as idiographic science: Bringing the person back into scientific psychology, this time forever</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">C M</forename><surname>Molenaar</surname></persName>
		</author>
		<idno type="DOI">10.1207/s15366359mea0204_1</idno>
		<ptr target="http://dx.doi.org/10.1207/s15366359mea0204_1" />
	</analytic>
	<monogr>
		<title level="j">Measurement</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="201" to="218" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Latent variable analysis: Growth mixture modeling and related techniques for longitudinal data</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">O</forename><surname>Muthén</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The SAGE handbook of quantitative methodology for the social sciences</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Kaplan</surname></persName>
		</editor>
		<meeting><address><addrLine>Thousand Oaks, CA</addrLine></address></meeting>
		<imprint>
			<publisher>SAGE Publications</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="345" to="370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Focus article: Idiographic filters for psychological constructs</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Nesselroade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gerstorf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Hardy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ram</surname></persName>
		</author>
		<idno type="DOI">10.1080/15366360701741807</idno>
		<ptr target="http://dx.doi.org/10.1080/15366360701741807" />
	</analytic>
	<monogr>
		<title level="j">Measurement: Interdisciplinary Research &amp; Perspective</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="217" to="235" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Pooling lagged covariance structures based on short, multivariate time series for dynamic factor analysis</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Nesselroade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">C M</forename><surname>Molenaar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Statistical strategies for small sample research Nesselroade</title>
		<editor>
			<persName><forename type="first">R</forename><forename type="middle">H</forename><surname>Hoyle</surname></persName>
		</editor>
		<meeting><address><addrLine>Newbury Park, CA</addrLine></address></meeting>
		<imprint>
			<publisher>Sage</publisher>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="223" to="251" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Different mechanisms of episodic memory failure in mild cognitive impairment</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">W</forename><surname>Nordahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ranganath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Yonelinas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Decarli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">R</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Jagust</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neuropsychologia.2005.01.003</idno>
		<ptr target="http://dx.doi.org/10.1016/j.neuropsychologia.2005.01.003" />
	</analytic>
	<monogr>
		<title level="j">Neuropsychologia</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="1688" to="1697" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Induction of decision trees</title>
		<author>
			<persName><forename type="first">J</forename><surname>Quinlan</surname></persName>
		</author>
		<idno type="DOI">10.1007/BF00116251</idno>
		<ptr target="http://dx.doi.org/10.1007/BF00116251" />
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="1986">1986</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="81" to="106" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">C4. 5: Programs for machine learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Quinlan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993">1993</date>
			<publisher>Morgan Kaufmann</publisher>
			<pubPlace>San Francisco, CA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Hypertension and the brain: Vulnerability of the prefrontal regions and executive functions</title>
		<author>
			<persName><forename type="first">N</forename><surname>Raz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Rodrigue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Acker</surname></persName>
		</author>
		<idno type="DOI">10.1037/0735-7044.117.6.1169</idno>
		<ptr target="http://dx.doi.org/10.1037/0735-7044.117.6.1169" />
	</analytic>
	<monogr>
		<title level="j">Behavioral Neuroscience</title>
		<imprint>
			<biblScope unit="volume">117</biblScope>
			<biblScope unit="page" from="1169" to="1180" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">R: A language and environment for statistical computing</title>
		<author>
			<orgName type="collaboration">R Core Team.</orgName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note>Computer software manual</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Demographic characteristics and IQ among adults: Analysis of the WAIS-R standardization sample as a function of the stratification variables</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Reynolds</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Chastain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Kaufman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Mclean</surname></persName>
		</author>
		<idno type="DOI">10.1016/0022-4405%2887%2990035-5</idno>
		<ptr target="http://dx.doi.org/10.1016/0022-4405" />
	</analytic>
	<monogr>
		<title level="j">Journal of School Psychology</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="90035" to="90035" />
			<date type="published" when="1987">1987</date>
			<pubPlace>Vienna, Austria</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Silhouettes: A graphical aid to the interpretation and validation of cluster analysis</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Rousseeuw</surname></persName>
		</author>
		<idno type="DOI">10.1016/0377-0427%2887%2990125-7</idno>
		<ptr target="http://dx.doi.org/10.1016/0377-0427(87" />
	</analytic>
	<monogr>
		<title level="j">Journal of Computational and Applied Mathematics</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="90125" to="90127" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">The trier inventory for the assessment of chronic stress (tics): scale construction, statistical testing, and validation of the scale work overload</title>
		<author>
			<persName><forename type="first">P</forename><surname>Schulz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Schlotz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Diagnostica</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="8" to="19" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Understanding global perceptions of stress in adulthood through treebased exploratory data mining</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">B</forename><surname>Scott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">R</forename><surname>Whitehead</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">S</forename><surname>Bergernan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Pitzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">contemporary issues in exploratory data mining in the behavioral sciences</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Mcardle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Ritschard</surname></persName>
		</editor>
		<meeting><address><addrLine>New York, NY</addrLine></address></meeting>
		<imprint>
			<publisher>Routledge</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="371" to="404" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Episodic memory across the lifespan: The contributions of associative and strategic components</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">L</forename><surname>Shing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Werkle-Bergner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Brehmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Lindenberger</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neubiorev.2009.11.002</idno>
		<idno>11.002</idno>
		<ptr target="http://dx.doi.org/10.1016/j.neubiorev" />
	</analytic>
	<monogr>
		<title level="j">Neuroscience &amp; Biobehavioral Reviews</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="1080" to="1091" />
			<date type="published" when="2009">2010. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">To explain or to predict?</title>
		<author>
			<persName><forename type="first">G</forename><surname>Shmueli</surname></persName>
		</author>
		<idno type="DOI">10.1214/10-STS330</idno>
		<ptr target="http://dx.doi.org/10.1214/10-STS330" />
	</analytic>
	<monogr>
		<title level="j">Statistical Science</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="289" to="310" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">The interpretation of interaction in contingency tables</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">H</forename><surname>Simpson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society: Series B</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="238" to="241" />
			<date type="published" when="1951">1951</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">The fate of cognition in very old age: Six-year longitudinal findings in the berlin aging study (base)</title>
		<author>
			<persName><forename type="first">T</forename><surname>Singer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Verhaeghen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ghisletta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Lindenberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">B</forename><surname>Baltes</surname></persName>
		</author>
		<idno type="DOI">10.1037/0882-7974.18.2.318</idno>
		<ptr target="http://dx.doi.org/10.1037/0882-7974.18.2.318" />
	</analytic>
	<monogr>
		<title level="j">Psychology and Aging</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="318" to="331" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">The detection of interaction effects. A report on a computer program for the selection of optimal combinations of explanatory variables</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sonquist</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Morgan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1964">1964</date>
			<pubPlace>Ann Arbor, MI</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Survey Research Centre, The Institute for Social Research, University of Michigan</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Conditional variable importance for random forests</title>
		<author>
			<persName><forename type="first">C</forename><surname>Strobl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A.-L</forename><surname>Boulesteix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kneib</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Augustin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zeileis</surname></persName>
		</author>
		<idno type="DOI">10.1186/1471-2105-9-307</idno>
		<ptr target="http://dx.doi.org/10.1186/1471-2105-9-307" />
	</analytic>
	<monogr>
		<title level="j">BMC Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">307</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<author>
			<persName><forename type="first">C</forename><surname>Strobl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Boulesteix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zeileis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hothorn</surname></persName>
		</author>
		<idno type="DOI">10.1186/1471-2105-8-2</idno>
		<ptr target="http://dx.doi.org/10.1186/1471-2105-8-2" />
	</analytic>
	<monogr>
		<title level="m">Bias in random forest variable importance measures: Illustrations, sources and a solution</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">25</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">An introduction to recursive partitioning: Rationale, application and characteristics of classification and regression trees, bagging and random forests</title>
		<author>
			<persName><forename type="first">C</forename><surname>Strobl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Tutz</surname></persName>
		</author>
		<idno type="DOI">10.1037/a0016973</idno>
		<ptr target="http://dx.doi.org/10.1037/a0016973" />
	</analytic>
	<monogr>
		<title level="j">Psychological Methods</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="323" to="348" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Danger: High power! Exploring the statistical properties of a test for random forest variable importance</title>
		<author>
			<persName><forename type="first">C</forename><surname>Strobl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zeileis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
		<respStmt>
			<orgName>Department of Statistics, University of Munich</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. Rep. No. 017</note>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Learning theory and multivariate experiment: Illustration by determination of generalized learning curves</title>
		<author>
			<persName><forename type="first">L</forename><surname>Tucker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Handbook of multivariate experimental psychology</title>
		<editor>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Cattell</surname></persName>
		</editor>
		<meeting><address><addrLine>Chicago. IL</addrLine></address></meeting>
		<imprint>
			<publisher>Rand McNally &amp; Company</publisher>
			<date type="published" when="1966">1966</date>
			<biblScope unit="page" from="476" to="501" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">The future of data analysis</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Tukey</surname></persName>
		</author>
		<idno type="DOI">10.1214/aoms/1177704711</idno>
		<ptr target="http://dx.doi.org/10.1214/aoms/1177704711" />
	</analytic>
	<monogr>
		<title level="j">The Annals of Mathematical Statistics</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1" to="67" />
			<date type="published" when="1962">1962</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Mining data with random forests: A survey and results of new tests</title>
		<author>
			<persName><forename type="first">A</forename><surname>Verikas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gelzinis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bacauskiene</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.patcog.2010.08.011</idno>
		<ptr target="http://dx.doi.org/10.1016/j.patcog.2010.08.011" />
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="330" to="349" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Latent class cluster analysis</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Vermunt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Magidson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Applied latent class analysis</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Hagenaars</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Mccutcheon</surname></persName>
		</editor>
		<meeting><address><addrLine>Cambridge, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="89" to="106" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Differential effects of everyday stress on the episodic memory test performances of young, mid-life, and older adults</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Vondras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Powless</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Olson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wheeler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Snudden</surname></persName>
		</author>
		<idno type="DOI">10.1080/13607860412331323782</idno>
		<ptr target="http://dx.doi.org/10.1080/13607860412331323782" />
	</analytic>
	<monogr>
		<title level="j">Aging &amp; Mental Health</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="60" to="70" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Structural equation modeling with ⍀nyx</title>
		<author>
			<persName><forename type="first">T</forename><surname>Von Oertzen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Brandmaier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tsang</surname></persName>
		</author>
		<idno type="DOI">10.1080/10705511.2014.935842</idno>
		<ptr target="http://dx.doi.org/10.1080/10705511.2014.935842" />
	</analytic>
	<monogr>
		<title level="j">Structural Equation Modeling: A Multidisciplinary Journal</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="148" to="161" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Development and validation of brief measures of positive and negative affect: The PANAS scales</title>
		<author>
			<persName><forename type="first">D</forename><surname>Watson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tellegen</surname></persName>
		</author>
		<idno type="DOI">10.1037/0022-3514.54.6.1063</idno>
		<ptr target="http://dx.doi.org/10.1037/0022-3514.54.6.1063" />
	</analytic>
	<monogr>
		<title level="j">Journal of personality and social psychology</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page" from="1063" to="1070" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Stress and memory in humans: Twelve years of progress?</title>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">T</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Brain Research</title>
		<imprint>
			<biblScope unit="volume">1293</biblScope>
			<biblScope unit="page" from="142" to="154" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<title level="m" type="main">This document is copyrighted by the American Psychological Association or one of its allied publishers. This article is intended solely for the personal use of the individual user and is not to be disseminated broadly</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
